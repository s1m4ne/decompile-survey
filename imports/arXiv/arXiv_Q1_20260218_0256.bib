@article{arXiv:1905.08325,
  title = {Towards Neural Decompilation},
  author = {Omer Katz and Yuval Olshaker and Yoav Goldberg and Eran Yahav},
  abstract = {We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code. The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high. We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97\% and 88\% of our benchmarks respectively.},
  journal = {arXiv preprint arXiv:1905.08325},
  year = {2019},
  month = {05},
  eprint = {1905.08325},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.LG},
  updated = {2019-05-20T20:02:53Z},
  published = {2019-05-20T20:02:53Z},
  url = {https://arxiv.org/abs/1905.08325v1},
  pdf = {https://arxiv.org/pdf/1905.08325v1},
}

@article{arXiv:2005.11315,
  title = {Java Decompiler Diversity and its Application to Meta-decompilation},
  author = {Nicolas Harrand and César Soto-Valero and Martin Monperrus and Benoit Baudry},
  abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84\%, respectively 78\%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6\% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler.},
  journal = {arXiv preprint arXiv:2005.11315},
  year = {2020},
  month = {05},
  eprint = {2005.11315},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {arXiv admin note: substantial text overlap with arXiv:1908.06895},
  updated = {2020-05-21T19:20:23Z},
  published = {2020-05-21T19:20:23Z},
  url = {https://arxiv.org/abs/2005.11315v1},
  pdf = {https://arxiv.org/pdf/2005.11315v1},
  doi = {10.1016/j.jss.2020.110645},
  note = {Journal reference: Journal of Systems and Software, 2020},
}

@article{arXiv:1908.06895,
  title = {The Strengths and Behavioral Quirks of Java Bytecode Decompilers},
  author = {Nicolas Harrand and César Soto-Valero and Martin Monperrus and Benoit Baudry},
  abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84\% of classes of our dataset and semantically equivalent code output for 78\% of classes.},
  journal = {arXiv preprint arXiv:1908.06895},
  year = {2019},
  month = {08},
  eprint = {1908.06895},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {11 pages, 6 figures, 9 listings, 3 tables},
  updated = {2019-08-19T15:54:04Z},
  published = {2019-08-19T15:54:04Z},
  url = {https://arxiv.org/abs/1908.06895v1},
  pdf = {https://arxiv.org/pdf/1908.06895v1},
  doi = {10.1109/scam.2019.00019},
  note = {Journal reference: Proceedings of the 19th IEEE International Working Conference on Source Code Analysis and Manipulation (SCAM 2019)},
}

@article{arXiv:2107.07809,
  title = {A method for decompilation of AMD GCN kernels to OpenCL},
  author = {K. I. Mihajlenko and M. A. Lukin and A. S. Stankevich},
  abstract = {Introduction: Decompilers are useful tools for software analysis and support in the absence of source code. They are available for many hardware architectures and programming languages. However, none of the existing decompilers support modern AMD GPU architectures such as AMD GCN and RDNA. Purpose: We aim at developing the first assembly decompiler tool for a modern AMD GPU architecture that generates code in the OpenCL language, which is widely used for programming GPGPUs. Results: We developed the algorithms for the following operations: preprocessing assembly code, searching data accesses, extracting system values, decompiling arithmetic operations and recovering data types. We also developed templates for decompilation of branching operations. Practical relevance: We implemented the presented algorithms in Python as a tool called OpenCLDecompiler, which supports a large subset of AMD GCN instructions. This tool automatically converts disassembled GPGPU code into the equivalent OpenCL code, which reduces the effort required to analyze assembly code.},
  journal = {arXiv preprint arXiv:2107.07809},
  year = {2021},
  month = {07},
  eprint = {2107.07809},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.DC},
  comment = {10 pages, 5 figures},
  updated = {2021-07-16T10:32:54Z},
  published = {2021-07-16T10:32:54Z},
  url = {https://arxiv.org/abs/2107.07809v1},
  pdf = {https://arxiv.org/pdf/2107.07809v1},
  doi = {10.31799/1684-8853-2021-2-33-42},
  note = {Journal reference: Information and Control Systems, 2021, no. 2, pp. 33-42},
}

@article{arXiv:2108.06363,
  title = {Augmenting Decompiler Output with Learned Variable Names and Types},
  author = {Qibin Chen and Jeremy Lacomis and Edward J. Schwartz and Claire Le Goues and Graham Neubig and Bogdan Vasilescu},
  abstract = {A common tool used by security professionals for reverse-engineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4\% of the time and the original types 75.8\% of the time.},
  journal = {arXiv preprint arXiv:2108.06363},
  year = {2021},
  month = {08},
  eprint = {2108.06363},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.PL},
  comment = {17 pages to be published in USENIX Security '22},
  updated = {2021-08-13T18:58:12Z},
  published = {2021-08-13T18:58:12Z},
  url = {https://arxiv.org/abs/2108.06363v1},
  pdf = {https://arxiv.org/pdf/2108.06363v1},
}

@article{arXiv:2501.04183,
  title = {Decompiling for Constant-Time Analysis},
  author = {Santiago Arranz-Olmos and Gilles Barthe and Lionel Blatter and Youcef Bouzid and Sören van der Wall and Zhiyuan Zhang},
  abstract = {Cryptographic libraries are a main target of timing side-channel attacks. A practical means to protect against these attacks is to adhere to the constant-time (CT) policy. However, it is hard to write constant-time code, and even constant-time code can be turned vulnerable by mainstream compilers. So how can we verify that binary code is constant-time? The obvious answer is to use binary-level CT tools. To do so, a common approach is to use decompilers or lifters as a front-end for CT analysis tools operating on source code or IR. Unfortunately, this approach is problematic with current decompilers. To illustrate this fact, we use the recent Clangover vulnerability and other constructed examples to show that five popular decompilers eliminate CT violations, rendering them not applicable with the approach. In this paper, we develop foundations to asses whether a decompiler is fit for the Decompile-then-Analyze approach. We propose CT transparency, which states that a transformation neither eliminates nor introduces CT violations, and a general method for proving that a program transformation is CT transparent. Then, we build CT-RetDec, a CT analysis tool based on a modified version of the LLVM-based decompiler RetDec. We evaluate CT-RetDec on a benchmark of real-world vulnerabilities in binaries, and show that the modifications had significant impact on CT-RetDec's performance. As a contribution of independent interest, we found that popular tools for binary-level CT analysis rely on decompiler-like transformations before analysis. We show that two such tools employ transformations that are not CT transparent, and, consequently, that they incorrectly accept non-CT programs. While our examples are very specific and do not invalidate the general approach of these tools, we advocate that tool developers counter such potential issues by proving the transparency of such transformations.},
  journal = {arXiv preprint arXiv:2501.04183},
  year = {2025},
  month = {01},
  eprint = {2501.04183},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  updated = {2025-10-14T08:35:32Z},
  published = {2025-01-07T23:29:01Z},
  url = {https://arxiv.org/abs/2501.04183v3},
  pdf = {https://arxiv.org/pdf/2501.04183v3},
}

@article{arXiv:2502.12221,
  title = {ReF Decompile: Relabeling and Function Call Enhanced Decompile},
  author = {Yunlong Feng and Bohan Li and Xiaoming Shi and Qingfu Zhu and Wanxiang Che},
  abstract = {The goal of decompilation is to convert compiled low-level code (e.g., assembly code) back into high-level programming languages, enabling analysis in scenarios where source code is unavailable. This task supports various reverse engineering applications, such as vulnerability identification, malware analysis, and legacy software migration. The end-to-end decompile method based on large langauge models (LLMs) reduces reliance on additional tools and minimizes manual intervention due to its inherent properties. However, previous end-to-end methods often lose critical information necessary for reconstructing control flow structures and variables when processing binary files, making it challenging to accurately recover the program's logic. To address these issues, we propose the \\textbf\{ReF Decompile\} method, which incorporates the following innovations: (1) The Relabelling strategy replaces jump target addresses with labels, preserving control flow clarity. (2) The Function Call strategy infers variable types and retrieves missing variable information from binary files. Experimental results on the Humaneval-Decompile Benchmark demonstrate that ReF Decompile surpasses comparable baselines and achieves state-of-the-art (SOTA) performance of \$61.43\\\%\$.},
  journal = {arXiv preprint arXiv:2502.12221},
  year = {2025},
  month = {02},
  eprint = {2502.12221},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2025-02-17T12:38:57Z},
  published = {2025-02-17T12:38:57Z},
  url = {https://arxiv.org/abs/2502.12221v1},
  pdf = {https://arxiv.org/pdf/2502.12221v1},
}

@article{arXiv:2409.20343,
  title = {Demystifying and Assessing Code Understandability in Java Decompilation},
  author = {Ruixin Qin and Yifan Xiong and Yifei Lu and Minxue Pan},
  abstract = {Decompilation, the process of converting machine-level code into readable source code, plays a critical role in reverse engineering. Given that the main purpose of decompilation is to facilitate code comprehension in scenarios where the source code is unavailable, the understandability of decompiled code is of great importance. In this paper, we propose the first empirical study on the understandability of Java decompiled code and obtained the following findings: (1) Understandability of Java decompilation is considered as important as its correctness, and decompilation understandability issues are even more commonly encountered than decompilation failures. (2) A notable percentage of code snippets decompiled by Java decompilers exhibit significantly lower or higher levels of understandability in comparison to their original source code. (3) Unfortunately, Cognitive Complexity demonstrates relatively acceptable precision while low recall in recognizing these code snippets exhibiting diverse understandability during decompilation. (4) Even worse, perplexity demonstrates lower levels of precision and recall in recognizing such code snippets. Inspired by the four findings, we further proposed six code patterns and the first metric for the assessment of decompiled code understandability. This metric was extended from Cognitive Complexity, with six more rules harvested from an exhaustive manual analysis into 1287 pairs of source code snippets and corresponding decompiled code. This metric was also validated using the original and updated dataset, yielding an impressive macro F1-score of 0.88 on the original dataset, and 0.86 on the test set.},
  journal = {arXiv preprint arXiv:2409.20343},
  year = {2024},
  month = {09},
  eprint = {2409.20343},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {18 pages, 16 figures},
  updated = {2024-09-30T14:44:00Z},
  published = {2024-09-30T14:44:00Z},
  url = {https://arxiv.org/abs/2409.20343v1},
  pdf = {https://arxiv.org/pdf/2409.20343v1},
}

@article{arXiv:2310.06530,
  title = {Refining Decompiled C Code with Large Language Models},
  author = {Wai Kin Wong and Huaijin Wang and Zongjie Li and Zhibo Liu and Shuai Wang and Qiyi Tang and Sen Nie and Shi Wu},
  abstract = {A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly. This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable. We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler -- IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75\% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions.},
  journal = {arXiv preprint arXiv:2310.06530},
  year = {2023},
  month = {10},
  eprint = {2310.06530},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2023-11-28T19:09:54Z},
  published = {2023-10-10T11:22:30Z},
  url = {https://arxiv.org/abs/2310.06530v2},
  pdf = {https://arxiv.org/pdf/2310.06530v2},
}

@article{arXiv:2502.04536,
  title = {Idioms: Neural Decompilation With Joint Code and Type Definition Prediction},
  author = {Luke Dramko and Claire Le Goues and Edward J. Schwartz},
  abstract = {Decompilers are important tools for reverse engineers that help them analyze software at a higher level of abstraction than assembly code. Unfortunately, because compilation is lossy, deterministic decompilers produce code that is missing many of the details that make source code readable in the first place, like variable names and types. Neural decompilers, on the other hand, offer the ability to statistically fill in these details. Existing work in neural decompilation, however, suffers from substantial limitations that preclude its use on real code, such as the inability to define composite types, which is essential to fully specify function semantics. In this work, we introduce a new dataset, Realtype, that includes substantially more complicated and realistic types than existing neural decompilation benchmarks, and Idioms, a new neural decompilation approach to finetune any LLM into a neural decompiler capable of generating the appropriate user-defined type definitions alongside the decompiled code. We show that our approach yields state-of-the-art results in neural decompilation. On the most challenging existing benchmark, ExeBench, our model achieves 54.4\% accuracy vs. 46.3\% for LLM4Decompile and 37.5\% for Nova; on Realtype, our model performs at least 95\% better.},
  journal = {arXiv preprint arXiv:2502.04536},
  year = {2025},
  month = {02},
  eprint = {2502.04536},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CR},
  updated = {2025-06-17T16:39:31Z},
  published = {2025-02-06T22:13:40Z},
  url = {https://arxiv.org/abs/2502.04536v2},
  pdf = {https://arxiv.org/pdf/2502.04536v2},
}

@article{arXiv:2409.11157,
  title = {The Incredible Shrinking Context... in a Decompiler Near You},
  author = {Sifis Lagouvardos and Yannis Bollanos and Neville Grech and Yannis Smaragdakis},
  abstract = {Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrknr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrknr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly "forgetting" control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5\% of contracts (compared to \textasciitilde{}95\%), covers (i.e., reaches and manages to decompile) 67\% more code, and reduces key imprecision metrics by over 65\%.},
  journal = {arXiv preprint arXiv:2409.11157},
  year = {2024},
  month = {09},
  eprint = {2409.11157},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  comment = {Full version of ISSTA 2025 paper},
  updated = {2025-04-17T14:34:36Z},
  published = {2024-09-17T13:10:38Z},
  url = {https://arxiv.org/abs/2409.11157v2},
  pdf = {https://arxiv.org/pdf/2409.11157v2},
}

@article{arXiv:2301.00969,
  title = {Boosting Neural Networks to Decompile Optimized Binaries},
  author = {Ying Cao and Ruigang Liang and Kai Chen and Peiwei Hu},
  abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21\% higher accuracy than state-of-the-art neural decompilation frameworks.},
  journal = {arXiv preprint arXiv:2301.00969},
  year = {2023},
  month = {01},
  eprint = {2301.00969},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.CR},
  updated = {2023-01-03T06:45:54Z},
  published = {2023-01-03T06:45:54Z},
  url = {https://arxiv.org/abs/2301.00969v1},
  pdf = {https://arxiv.org/pdf/2301.00969v1},
  doi = {10.1145/3564625.3567998},
}

@article{arXiv:2411.02278,
  title = {Is This the Same Code? A Comprehensive Study of Decompilation Techniques for WebAssembly Binaries},
  author = {Wei-Cheng Wu and Yutian Yan and Hallgrimur David Egilsson and David Park and Steven Chan and Christophe Hauser and Weihang Wang},
  abstract = {WebAssembly is a low-level bytecode language designed for client-side execution in web browsers. The need for decompilation techniques that recover high-level source code from WASM binaries has grown as WASM continues to gain widespread adoption and its security concerns. However little research has been done to assess the quality of decompiled code from WASM. This paper aims to fill this gap by conducting a comprehensive comparative analysis between decompiled C code from WASM binaries and state-of-the-art native binary decompilers. We presented a novel framework for empirically evaluating C-based decompilers from various aspects including correctness/ readability/ and structural similarity. The proposed metrics are validated practicality in decompiler assessment and provided insightful observations regarding the characteristics and constraints of existing decompiled code. This in turn contributes to bolstering the security and reliability of software systems that rely on WASM and native binaries.},
  journal = {arXiv preprint arXiv:2411.02278},
  year = {2024},
  month = {11},
  eprint = {2411.02278},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {SecureComm'24: Proceedings of the 20th EAI International Conference on Security and Privacy in Communication Networks},
  updated = {2024-11-04T17:08:03Z},
  published = {2024-11-04T17:08:03Z},
  url = {https://arxiv.org/abs/2411.02278v1},
  pdf = {https://arxiv.org/pdf/2411.02278v1},
  author_affiliations = {Wei-Cheng Wu: Dartmouth College; Yutian Yan: University of Southern California; Hallgrimur David Egilsson: University of Southern California; David Park: University of Southern California; Steven Chan: University of Southern California; Christophe Hauser: Dartmouth College; Weihang Wang: University of Southern California},
}

@article{arXiv:2212.08950,
  title = {Beyond the C: Retargetable Decompilation using Neural Machine Translation},
  author = {Iman Hosseini and Brendan Dolan-Gavitt},
  abstract = {The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software. Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation. Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code. In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages. We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation.},
  journal = {arXiv preprint arXiv:2212.08950},
  year = {2022},
  month = {12},
  eprint = {2212.08950},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.CL cs.PL},
  updated = {2022-12-17T20:45:59Z},
  published = {2022-12-17T20:45:59Z},
  url = {https://arxiv.org/abs/2212.08950v1},
  pdf = {https://arxiv.org/pdf/2212.08950v1},
  doi = {10.14722/bar.2022.23009},
}

@article{arXiv:2403.05286,
  title = {LLM4Decompile: Decompiling Binary Code with Large Language Models},
  author = {Hanzhuo Tan and Qi Luo and Jing Li and Yuqun Zhang},
  abstract = {Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100\% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2\% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results. Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile},
  journal = {arXiv preprint arXiv:2403.05286},
  year = {2024},
  month = {03},
  eprint = {2403.05286},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.CL},
  updated = {2024-10-22T03:58:20Z},
  published = {2024-03-08T13:10:59Z},
  url = {https://arxiv.org/abs/2403.05286v3},
  pdf = {https://arxiv.org/pdf/2403.05286v3},
  doi = {10.18653/v1/2024.emnlp-main.203},
  note = {Journal reference: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
}

@article{arXiv:1906.12029,
  title = {A Neural-based Program Decompiler},
  author = {Cheng Fu and Huili Chen and Haolan Liu and Xinyun Chen and Yuandong Tian and Farinaz Koushanfar and Jishen Zhao},
  abstract = {Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82\\\% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0\\\% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70\\\% program accuracy.},
  journal = {arXiv preprint arXiv:1906.12029},
  year = {2019},
  month = {06},
  eprint = {1906.12029},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.LG},
  updated = {2019-06-28T03:29:38Z},
  published = {2019-06-28T03:29:38Z},
  url = {https://arxiv.org/abs/1906.12029v1},
  pdf = {https://arxiv.org/pdf/1906.12029v1},
}

@article{arXiv:2409.03119,
  title = {Register Aggregation for Hardware Decompilation},
  author = {Varun Rao and Zachary D. Sisco},
  abstract = {Hardware decompilation reverses logic synthesis, converting a gate-level digital electronic design, or netlist, back up to hardware description language (HDL) code. Existing techniques decompile data-oriented features in netlists, like loops and modules, but struggle with sequential logic. In particular, they cannot decompile memory elements, which pose difficulty due to their deconstruction into individual bits and the feedback loops they form in the netlist. Recovering multi-bit registers and memory blocks from netlists would expand the applications of hardware decompilation, notably towards retargeting technologies (e.g. FPGAs to ASICs) and decompiling processor memories. We devise a method for register aggregation, to identify relationships between the data flip-flops in a netlist and group them into registers and memory blocks, resulting in HDL code that instantiates these memory elements. We aggregate flip-flops by identifying common enable pins, and derive the bit-order of the resulting registers using functional dependencies. This scales similarly to memory blocks, where we repeat the algorithm in the second dimension with special attention to the read, write, and address ports of each memory block. We evaluate our technique over a dataset of 13 gate-level netlists, comprising circuits from binary multipliers to CPUs, and we compare the quantity and widths of recovered registers and memory blocks with the original source code. The technique successfully recovers memory elements in all of the tested circuits, even aggregating beyond the source code expectation. In 10 / 13 circuits, all source code memory elements are accounted for, and we are able to compact up to 2048 disjoint bits into a single memory block.},
  journal = {arXiv preprint arXiv:2409.03119},
  year = {2024},
  month = {09},
  eprint = {2409.03119},
  archivePrefix = {arXiv},
  primaryClass = {cs.AR},
  categories = {cs.AR cs.PL},
  comment = {6 pages, 6 figures},
  updated = {2024-09-04T23:06:13Z},
  published = {2024-09-04T23:06:13Z},
  url = {https://arxiv.org/abs/2409.03119v1},
  pdf = {https://arxiv.org/pdf/2409.03119v1},
}

@article{arXiv:2205.06719,
  title = {dewolf: Improving Decompilation by leveraging User Surveys},
  author = {Steffen Enders and Eva-Maria C. Behner and Niklas Bergmann and Mariia Rybalka and Elmar Padilla and Er Xue Hui and Henry Low and Nicholas Sim},
  abstract = {Analyzing third-party software such as malware or firmware is a crucial task for security analysts. Although various approaches for automatic analysis exist and are the subject of ongoing research, analysts often have to resort to manual static analysis to get a deep understanding of a given binary sample. Since the source code of encountered samples is rarely available, analysts regularly employ decompilers for easier and faster comprehension than analyzing a binary's disassembly. In this paper, we introduce our decompilation approach dewolf. We developed a variety of improvements over the previous academic state-of-the-art decompiler and some novel algorithms to enhance readability and comprehension, focusing on manual analysis. To evaluate our approach and to obtain a better insight into the analysts' needs, we conducted three user surveys. The results indicate that dewolf is suitable for malware comprehension and that its output quality noticeably exceeds Ghidra and Hex-Rays in certain aspects. Furthermore, our results imply that decompilers aiming at manual analysis should be highly configurable to respect individual user preferences. Additionally, future decompilers should not necessarily follow the unwritten rule to stick to the code-structure dictated by the assembly in order to produce readable output. In fact, the few cases where dewolf already cracks this rule lead to its results considerably exceeding other decompilers. We publish a prototype implementation of dewolf and all survey results on GitHub.},
  journal = {arXiv preprint arXiv:2205.06719},
  year = {2022},
  month = {05},
  eprint = {2205.06719},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2022-05-13T15:54:47Z},
  published = {2022-05-13T15:54:47Z},
  url = {https://arxiv.org/abs/2205.06719v1},
  pdf = {https://arxiv.org/pdf/2205.06719v1},
  doi = {10.14722/bar.2023.23001},
}

@article{arXiv:2101.08116,
  title = {Improving type information inferred by decompilers with supervised machine learning},
  author = {Javier Escalada and Ted Scully and Francisco Ortin},
  abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1\% F1-measure, whereas the best decompiler obtains a 30\% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers.},
  journal = {arXiv preprint arXiv:2101.08116},
  year = {2021},
  month = {01},
  eprint = {2101.08116},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.LG cs.PL},
  updated = {2021-02-24T11:01:27Z},
  published = {2021-01-19T11:45:46Z},
  url = {https://arxiv.org/abs/2101.08116v2},
  pdf = {https://arxiv.org/pdf/2101.08116v2},
  author_affiliations = {Javier Escalada: University of Oviedo; Ted Scully: Cork Institute of Technology; Francisco Ortin: University of Oviedo},
}

@article{arXiv:2112.15491,
  title = {Semantics-Recovering Decompilation through Neural Machine Translation},
  author = {Ruigang Liang and Ying Cao and Peiwei Hu and Jinwen He and Kai Chen},
  abstract = {Decompilation transforms low-level program languages (PL) (e.g., binary code) into high-level PLs (e.g., C/C++). It has been widely used when analysts perform security analysis on software (systems) whose source code is unavailable, such as vulnerability search and malware analysis. However, current decompilation tools usually need lots of experts' efforts, even for years, to generate the rules for decompilation, which also requires long-term maintenance as the syntax of high-level PL or low-level PL changes. Also, an ideal decompiler should concisely generate high-level PL with similar functionality to the source low-level PL and semantic information (e.g., meaningful variable names), just like human-written code. Unfortunately, existing manually-defined rule-based decompilation techniques only functionally restore the low-level PL to a similar high-level PL and are still powerless to recover semantic information. In this paper, we propose a novel neural decompilation approach to translate low-level PL into accurate and user-friendly high-level PL, effectively improving its readability and understandability. Furthermore, we implement the proposed approaches called SEAM. Evaluations on four real-world applications show that SEAM has an average accuracy of 94.41\%, which is much better than prior neural machine translation (NMT) models. Finally, we evaluate the effectiveness of semantic information recovery through a questionnaire survey, and the average accuracy is 92.64\%, which is comparable or superior to the state-of-the-art compilers.},
  journal = {arXiv preprint arXiv:2112.15491},
  year = {2021},
  month = {12},
  eprint = {2112.15491},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.PL cs.SE},
  updated = {2021-12-22T07:08:08Z},
  published = {2021-12-22T07:08:08Z},
  url = {https://arxiv.org/abs/2112.15491v1},
  pdf = {https://arxiv.org/pdf/2112.15491v1},
}

@article{arXiv:2501.04811,
  title = {Fast, Fine-Grained Equivalence Checking for Neural Decompilers},
  author = {Luke Dramko and Claire Le Goues and Edward J. Schwartz},
  abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models have substantial weaknesses, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign, a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides-which parts of the functions are equivalent and how well the variable names match-is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.},
  journal = {arXiv preprint arXiv:2501.04811},
  year = {2025},
  month = {01},
  eprint = {2501.04811},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.CR cs.SE},
  updated = {2025-01-08T19:59:48Z},
  published = {2025-01-08T19:59:48Z},
  url = {https://arxiv.org/abs/2501.04811v1},
  pdf = {https://arxiv.org/pdf/2501.04811v1},
}

@article{arXiv:1912.10092,
  title = {Sum-Product Network Decompilation},
  author = {Cory J. Butz and Jhonatan S. Oliveira and Robert Peharz},
  abstract = {There exists a dichotomy between classical probabilistic graphical models, such as Bayesian networks (BNs), and modern tractable models, such as sum-product networks (SPNs). The former generally have intractable inference, but provide a high level of interpretability, while the latter admits a wide range of tractable inference routines, but are typically harder to interpret. Due to this dichotomy, tools to convert between BNs and SPNs are desirable. While one direction -- compiling BNs into SPNs -- is well discussed in Darwiche's seminal work on arithmetic circuit compilation, the converse direction -- decompiling SPNs into BNs -- has received surprisingly little attention. In this paper, we fill this gap by proposing SPN2BN, an algorithm that decompiles an SPN into a BN. SPN2BN has several salient features when compared to the only other two works decompiling SPNs. Most significantly, the BNs returned by SPN2BN are minimal independence-maps that are more parsimonious with respect to the introduction of latent variables. Secondly, the output BN produced by SPN2BN can be precisely characterized with respect to a compiled BN. More specifically, a certain set of directed edges will be added to the input BN, giving what we will call the moral-closure. Lastly, it is established that our compilation-decompilation process is idempotent. This has practical significance as it limits the size of the decompiled SPN.},
  journal = {arXiv preprint arXiv:1912.10092},
  year = {2019},
  month = {12},
  eprint = {1912.10092},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  categories = {cs.AI cs.LG},
  updated = {2020-05-19T17:52:00Z},
  published = {2019-12-20T20:39:28Z},
  url = {https://arxiv.org/abs/1912.10092v2},
  pdf = {https://arxiv.org/pdf/1912.10092v2},
}

@article{arXiv:2511.01763,
  title = {Context-Guided Decompilation: A Step Towards Re-executability},
  author = {Xiaohan Wang and Yuxin Hu and Kevin Leach},
  abstract = {Binary decompilation plays an important role in software security analysis, reverse engineering, and malware understanding when source code is unavailable. However, existing decompilation techniques often fail to produce source code that can be successfully recompiled and re-executed, particularly for optimized binaries. Recent advances in large language models (LLMs) have enabled neural approaches to decompilation, but the generated code is typically only semantically plausible rather than truly executable, limiting their practical reliability. These shortcomings arise from compiler optimizations and the loss of semantic cues in compiled code, which LLMs struggle to recover without contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid decompilation framework that leverages in-context learning (ICL) to guide LLMs toward generating re-executable source code. We evaluate our method across multiple datasets, optimization levels, and compilers, demonstrating around 40\\\% improvement in re-executability over state-of-the-art decompilation methods while maintaining robustness.},
  journal = {arXiv preprint arXiv:2511.01763},
  year = {2025},
  month = {11},
  eprint = {2511.01763},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  updated = {2025-11-03T17:21:39Z},
  published = {2025-11-03T17:21:39Z},
  url = {https://arxiv.org/abs/2511.01763v1},
  pdf = {https://arxiv.org/pdf/2511.01763v1},
}

@article{arXiv:2507.18792,
  title = {Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges},
  author = {Zixu Zhou},
  abstract = {Decompiling Rust binaries is challenging due to the language's rich type system, aggressive compiler optimizations, and widespread use of high-level abstractions. In this work, we conduct a benchmark-driven evaluation of decompilation quality across core Rust features and compiler build modes. Our automated scoring framework shows that generic types, trait methods, and error handling constructs significantly reduce decompilation quality, especially in release builds. Through representative case studies, we analyze how specific language constructs affect control flow, variable naming, and type information recovery. Our findings provide actionable insights for tool developers and highlight the need for Rust-aware decompilation strategies.},
  journal = {arXiv preprint arXiv:2507.18792},
  year = {2025},
  month = {07},
  eprint = {2507.18792},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.SE},
  updated = {2025-07-24T20:26:42Z},
  published = {2025-07-24T20:26:42Z},
  url = {https://arxiv.org/abs/2507.18792v1},
  pdf = {https://arxiv.org/pdf/2507.18792v1},
}

@article{arXiv:2406.17233,
  title = {Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement},
  author = {Yunlong Feng and Dechuan Teng and Yang Xu and Honglin Mu and Xiao Xu and Libo Qin and Qingfu Zhu and Wanxiang Che},
  abstract = {Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc\$\textasciicircum{}2\$dec) method recompiles the LLM's decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90\% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41\%. The code, data, and models are available at https://github.com/AlongWY/sccdec.},
  journal = {arXiv preprint arXiv:2406.17233},
  year = {2024},
  month = {06},
  eprint = {2406.17233},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CL},
  comment = {EMNLP 2024 Findings},
  updated = {2024-10-03T08:43:25Z},
  published = {2024-06-25T02:37:53Z},
  url = {https://arxiv.org/abs/2406.17233v2},
  pdf = {https://arxiv.org/pdf/2406.17233v2},
}

@article{arXiv:2412.07538,
  title = {Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?},
  author = {D. Cotroneo and F. C. Grasso and R. Natella and V. Orbinato},
  abstract = {Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.},
  journal = {arXiv preprint arXiv:2412.07538},
  year = {2024},
  month = {12},
  eprint = {2412.07538},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI cs.LG},
  updated = {2025-03-29T14:19:09Z},
  published = {2024-12-10T14:17:14Z},
  url = {https://arxiv.org/abs/2412.07538v2},
  pdf = {https://arxiv.org/pdf/2412.07538v2},
}

@article{arXiv:1908.06748,
  title = {Adabot: Fault-Tolerant Java Decompiler},
  author = {Zhiming Li and Qing Wu and Kun Qian},
  abstract = {Reverse Engineering(RE) has been a fundamental task in software engineering. However, most of the traditional Java reverse engineering tools are strictly rule defined, thus are not fault-tolerant, which pose serious problem when noise and interference were introduced into the system. In this paper, we view reverse engineering as a statistical machine translation task instead of rule-based task, and propose a fault-tolerant Java decompiler based on machine translation models. Our model is based on attention-based Neural Machine Translation (NMT) and Transformer architectures. First, we measure the translation quality on both the redundant and purified datasets. Next, we evaluate the fault-tolerance(anti-noise ability) of our framework on test sets with different unit error probability (UEP). In addition, we compare the suitability of different word segmentation algorithms for decompilation task. Experimental results demonstrate that our model is more robust and fault-tolerant compared to traditional Abstract Syntax Tree (AST) based decompilers. Specifically, in terms of BLEU-4 and Word Error Rate (WER), our performance has reached 94.50\% and 2.65\% on the redundant test set; 92.30\% and 3.48\% on the purified test set.},
  journal = {arXiv preprint arXiv:1908.06748},
  year = {2019},
  month = {08},
  eprint = {1908.06748},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CL},
  comment = {8 pages},
  updated = {2019-10-15T15:23:09Z},
  published = {2019-08-14T05:17:04Z},
  url = {https://arxiv.org/abs/1908.06748v2},
  pdf = {https://arxiv.org/pdf/1908.06748v2},
}

@article{arXiv:2505.12668,
  title = {Decompile-Bench: Million-Scale Binary-Source Function Pairs for Real-World Binary Decompilation},
  author = {Hanzhuo Tan and Xiaolong Tian and Hanrui Qi and Jiaming Liu and Zuchen Gao and Siyi Wang and Qi Luo and Jing Li and Yuqun Zhang},
  abstract = {Recent advances in LLM-based decompilers have been shown effective to convert low-level binaries into human-readable source code. However, there still lacks a comprehensive benchmark that provides large-scale binary-source function pairs, which is critical for advancing the LLM decompilation technology. Creating accurate binary-source mappings incurs severe issues caused by complex compilation settings and widespread function inlining that obscure the correspondence between binaries and their original source code. Previous efforts have either relied on used contest-style benchmarks, synthetic binary-source mappings that diverge significantly from the mappings in real world, or partially matched binaries with only code lines or variable names, compromising the effectiveness of analyzing the binary functionality. To alleviate these issues, we introduce Decompile-Bench, the first open-source dataset comprising two million binary-source function pairs condensed from 100 million collected function pairs, i.e., 450GB of binaries compiled from permissively licensed GitHub projects. For the evaluation purposes, we also developed a benchmark Decompile-Bench-Eval including manually crafted binaries from the well-established HumanEval and MBPP, alongside the compiled GitHub repositories released after 2025 to mitigate data leakage issues. We further explore commonly-used evaluation metrics to provide a thorough assessment of the studied LLM decompilers and find that fine-tuning with Decompile-Bench causes a 20\% improvement over previous benchmarks in terms of the re-executability rate. Our code and data has been released in HuggingFace and Github. https://github.com/albertan017/LLM4Decompile},
  journal = {arXiv preprint arXiv:2505.12668},
  year = {2025},
  month = {05},
  eprint = {2505.12668},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2025-10-19T12:55:35Z},
  published = {2025-05-19T03:34:33Z},
  url = {https://arxiv.org/abs/2505.12668v2},
  pdf = {https://arxiv.org/pdf/2505.12668v2},
}

@article{arXiv:1909.09029,
  title = {DIRE: A Neural Approach to Decompiled Identifier Naming},
  author = {Jeremy Lacomis and Pengcheng Yin and Edward J. Schwartz and Miltiadis Allamanis and Claire Le Goues and Graham Neubig and Bogdan Vasilescu},
  abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
  journal = {arXiv preprint arXiv:1909.09029},
  year = {2019},
  month = {09},
  eprint = {1909.09029},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {2019 International Conference on Automated Software Engineering},
  updated = {2019-10-03T15:42:43Z},
  published = {2019-09-19T14:57:31Z},
  url = {https://arxiv.org/abs/1909.09029v2},
  pdf = {https://arxiv.org/pdf/1909.09029v2},
}

@article{arXiv:2406.11346,
  title = {WaDec: Decompiling WebAssembly Using Large Language Model},
  author = {Xinyu She and Yanjie Zhao and Haoyu Wang},
  abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art's 116.94\%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%.},
  journal = {arXiv preprint arXiv:2406.11346},
  year = {2024},
  month = {06},
  eprint = {2406.11346},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {This paper was accepted by ASE 2024},
  updated = {2024-09-11T10:05:37Z},
  published = {2024-06-17T09:08:30Z},
  url = {https://arxiv.org/abs/2406.11346v3},
  pdf = {https://arxiv.org/pdf/2406.11346v3},
}

@article{arXiv:2509.06402,
  title = {NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables},
  author = {Yilin Li and Guozhu Meng and Mingyang Sun and Yanzhong Wang and Kun Sun and Hailong Chang and Yuekang Li},
  abstract = {On-device deep learning models have extensive real world demands. Deep learning compilers efficiently compile models into executables for deployment on edge devices, but these executables may face the threat of reverse engineering. Previous studies have attempted to decompile DNN executables, but they face challenges in handling compilation optimizations and analyzing quantized compiled models. In this paper, we present NeuroDeX to unlock diverse support in decompiling DNN executables. NeuroDeX leverages the semantic understanding capabilities of LLMs along with dynamic analysis to accurately and efficiently perform operator type recognition, operator attribute recovery and model reconstruction. NeuroDeX can recover DNN executables into high-level models towards compilation optimizations, different architectures and quantized compiled models. We conduct experiments on 96 DNN executables across 12 common DNN models. Extensive experimental results demonstrate that NeuroDeX can decompile non-quantized executables into nearly identical high-level models. NeuroDeX can recover functionally similar high-level models for quantized executables, achieving an average top-1 accuracy of 72\%. NeuroDeX offers a more comprehensive and effective solution compared to previous DNN executables decompilers.},
  journal = {arXiv preprint arXiv:2509.06402},
  year = {2025},
  month = {09},
  eprint = {2509.06402},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.CR},
  updated = {2025-11-03T12:13:43Z},
  published = {2025-09-08T07:47:58Z},
  url = {https://arxiv.org/abs/2509.06402v2},
  pdf = {https://arxiv.org/pdf/2509.06402v2},
}

@article{arXiv:2407.02733,
  title = {STRIDE: Simple Type Recognition In Decompiled Executables},
  author = {Harrison Green and Edward J. Schwartz and Claire Le Goues and Bogdan Vasilescu},
  abstract = {Decompilers are widely used by security researchers and developers to reverse engineer executable code. While modern decompilers are adept at recovering instructions, control flow, and function boundaries, some useful information from the original source code, such as variable types and names, is lost during the compilation process. Our work aims to predict these variable types and names from the remaining information. We propose STRIDE, a lightweight technique that predicts variable names and types by matching sequences of decompiler tokens to those found in training data. We evaluate it on three benchmark datasets and find that STRIDE achieves comparable performance to state-of-the-art machine learning models for both variable retyping and renaming while being much simpler and faster. We perform a detailed comparison with two recent SOTA transformer-based models in order to understand the specific factors that make our technique effective. We implemented STRIDE in fewer than 1000 lines of Python and have open-sourced it under a permissive license at https://github.com/hgarrereyn/STRIDE.},
  journal = {arXiv preprint arXiv:2407.02733},
  year = {2024},
  month = {07},
  eprint = {2407.02733},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2024-07-03T01:09:41Z},
  published = {2024-07-03T01:09:41Z},
  url = {https://arxiv.org/abs/2407.02733v1},
  pdf = {https://arxiv.org/pdf/2407.02733v1},
}

@article{arXiv:2410.00061,
  title = {Neural Decompiling of Tracr Transformers},
  author = {Hannes Thurnherr and Kaspar Riesen},
  abstract = {Recently, the transformer architecture has enabled substantial progress in many areas of pattern recognition and machine learning. However, as with other neural network models, there is currently no general method available to explain their inner workings. The present paper represents a first step towards this direction. We utilize \\textit\{Transformer Compiler for RASP\} (Tracr) to generate a large dataset of pairs of transformer weights and corresponding RASP programs. Based on this dataset, we then build and train a model, with the aim of recovering the RASP code from the compiled model. We demonstrate that the simple form of Tracr compiled transformer weights is interpretable for such a decompiler model. In an empirical evaluation, our model achieves exact reproductions on more than 30\\\% of the test objects, while the remaining 70\\\% can generally be reproduced with only few errors. Additionally, more than 70\\\% of the programs, produced by our model, are functionally equivalent to the ground truth, and therefore a valid decompilation of the Tracr compiled transformer weights.},
  journal = {arXiv preprint arXiv:2410.00061},
  year = {2024},
  month = {09},
  eprint = {2410.00061},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.AI},
  updated = {2024-09-29T13:12:39Z},
  published = {2024-09-29T13:12:39Z},
  url = {https://arxiv.org/abs/2410.00061v1},
  pdf = {https://arxiv.org/pdf/2410.00061v1},
  doi = {10.1007/978-3-031-71602-7\_3},
  note = {Journal reference: Artificial Neural Networks in Pattern Recognition, Lecture Notes in Computer Science, vol. 14252, Springer, 2024, pp. 25-36},
}

@article{arXiv:2509.22114,
  title = {SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin},
  author = {Hanzhuo Tan and Weihao Li and Xiaolong Tian and Siyi Wang and Jiaming Liu and Jing Li and Yuqun Zhang},
  abstract = {Large Language Models (LLMs) have emerged as a promising approach for binary decompilation. However, the existing LLM-based decompilers still are somewhat limited in effectively presenting a program's source-level structure with its original identifiers. To mitigate this, we introduce SK2Decompile, a novel two-phase approach to decompile from the skeleton (semantic structure) to the skin (identifier) of programs. Specifically, we first apply a Structure Recovery model to translate a program's binary code to an Intermediate Representation (IR) as deriving the program's "skeleton", i.e., preserving control flow and data structures while obfuscating all identifiers with generic placeholders. We also apply reinforcement learning to reward the model for producing program structures that adhere to the syntactic and semantic rules expected by compilers. Second, we apply an Identifier Naming model to produce meaningful identifiers which reflect actual program semantics as deriving the program's "skin". We train the Identifier Naming model with a separate reinforcement learning objective that rewards the semantic similarity between its predictions and the reference code. Such a two-phase decompilation process facilitates advancing the correctness and readability of decompilation independently. Our evaluations indicate that SK2Decompile, significantly outperforms the SOTA baselines, achieving 21.6\% average re-executability rate gain over GPT-5-mini on the HumanEval dataset and 29.4\% average R2I improvement over Idioms on the GitHub2025 benchmark.},
  journal = {arXiv preprint arXiv:2509.22114},
  year = {2025},
  month = {09},
  eprint = {2509.22114},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2025-09-26T09:35:46Z},
  published = {2025-09-26T09:35:46Z},
  url = {https://arxiv.org/abs/2509.22114v1},
  pdf = {https://arxiv.org/pdf/2509.22114v1},
}

@article{arXiv:2509.14646,
  title = {SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation},
  author = {Yongpan Wang and Xin Xu and Xiaojie Zhu and Xiaodong Gu and Beijun Shen},
  abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose \\saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of \\saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed \\salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare \\saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that \\saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\\\% TCP rate on Decompile-Eval with a 10.6\\\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
  journal = {arXiv preprint arXiv:2509.14646},
  year = {2025},
  month = {09},
  eprint = {2509.14646},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.PL},
  comment = {13 pages, 7 figures},
  updated = {2025-09-18T05:57:15Z},
  published = {2025-09-18T05:57:15Z},
  url = {https://arxiv.org/abs/2509.14646v1},
  pdf = {https://arxiv.org/pdf/2509.14646v1},
}

@article{arXiv:2510.19615,
  title = {FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation},
  author = {Zhiping Zhou and Xiaohong Li and Ruitao Feng and Yao Zhang and Yuekang Li and Wenbu Feng and Yunqian Wang and Yuqing Li},
  abstract = {Decompilation converts machine code into human-readable form, enabling analysis and debugging without source code. However, fidelity issues often degrade the readability and semantic accuracy of decompiled output. Existing methods, such as variable renaming or structural simplification, provide partial improvements but lack robust detection and correction, particularly for complex closed-source binaries. We present FidelityGPT, a framework that enhances decompiled code accuracy and readability by systematically detecting and correcting semantic distortions. FidelityGPT introduces distortion-aware prompt templates tailored to closed-source settings and integrates Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity algorithm to locate distorted lines and retrieve semantically similar code from a database. A variable dependency algorithm further mitigates long-context limitations by analyzing redundant variables and integrating their dependencies into the prompt context. Evaluated on 620 function pairs from a binary similarity benchmark, FidelityGPT achieved an average detection accuracy of 89\% and a precision of 83\%. Compared to the state-of-the-art DeGPT (Fix Rate 83\%, Corrected Fix Rate 37\%), FidelityGPT attained 94\% FR and 64\% CFR, demonstrating significant gains in accuracy and readability. These results highlight its potential to advance LLM-based decompilation and reverse engineering.},
  journal = {arXiv preprint arXiv:2510.19615},
  year = {2025},
  month = {10},
  eprint = {2510.19615},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CR},
  updated = {2025-10-22T14:11:44Z},
  published = {2025-10-22T14:11:44Z},
  url = {https://arxiv.org/abs/2510.19615v1},
  pdf = {https://arxiv.org/pdf/2510.19615v1},
  doi = {10.14722/ndss.2026.230989},
}

@article{arXiv:2505.11340,
  title = {DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in Real-World Scenarios},
  author = {Zeyu Gao and Yuxin Cui and Hao Wang and Siliang Qin and Yuanda Wang and Bolun Zhang and Chao Zhang},
  abstract = {Decompilers are fundamental tools for critical security tasks, from vulnerability discovery to malware analysis, yet their evaluation remains fragmented. Existing approaches primarily focus on syntactic correctness through synthetic micro-benchmarks or subjective human ratings, failing to address real-world requirements for semantic fidelity and analyst usability. We present DecompileBench, the first comprehensive framework that enables effective evaluation of decompilers in reverse engineering workflows through three key components: \\textit\{real-world function extraction\} (comprising 23,400 functions from 130 real-world programs), \\textit\{runtime-aware validation\}, and \\textit\{automated human-centric assessment\} using LLM-as-Judge to quantify the effectiveness of decompilers in reverse engineering workflows. Through a systematic comparison between six industrial-strength decompilers and six recent LLM-powered approaches, we demonstrate that LLM-based methods surpass commercial tools in code understandability despite 52.2\% lower functionality correctness. These findings highlight the potential of LLM-based approaches to transform human-centric reverse engineering. We open source \\href\{https://github.com/Jennieett/DecompileBench\}\{DecompileBench\} to provide a framework to advance research on decompilers and assist security experts in making informed tool selections based on their specific requirements.},
  journal = {arXiv preprint arXiv:2505.11340},
  year = {2025},
  month = {05},
  eprint = {2505.11340},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  updated = {2025-05-16T15:07:43Z},
  published = {2025-05-16T15:07:43Z},
  url = {https://arxiv.org/abs/2505.11340v1},
  pdf = {https://arxiv.org/pdf/2505.11340v1},
}

@article{arXiv:2506.19624,
  title = {Decompiling Smart Contracts with a Large Language Model},
  author = {Isaac David and Liyi Zhou and Dawn Song and Arthur Gervais and Kaihua Qin},
  abstract = {The widespread lack of broad source code verification on blockchain explorers such as Etherscan, where despite 78,047,845 smart contracts deployed on Ethereum (as of May 26, 2025), a mere 767,520 (< 1\%) are open source, presents a severe impediment to blockchain security. This opacity necessitates the automated semantic analysis of on-chain smart contract bytecode, a fundamental research challenge with direct implications for identifying vulnerabilities and understanding malicious behavior. Prevailing decompilers struggle to reverse bytecode in a readable manner, often yielding convoluted code that critically hampers vulnerability analysis and thwarts efforts to dissect contract functionalities for security auditing. This paper addresses this challenge by introducing a pioneering decompilation pipeline that, for the first time, successfully leverages Large Language Models (LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable and semantically faithful Solidity code. Our novel methodology first employs rigorous static program analysis to convert bytecode into a structured three-address code (TAC) representation. This intermediate representation then guides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset of 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity. This approach uniquely recovers meaningful variable names, intricate control flow, and precise function signatures. Our extensive empirical evaluation demonstrates a significant leap beyond traditional decompilers, achieving an average semantic similarity of 0.82 with original source and markedly superior readability. The practical viability and effectiveness of our research are demonstrated through its implementation in a publicly accessible system, available at https://evmdecompiler.com.},
  journal = {arXiv preprint arXiv:2506.19624},
  year = {2025},
  month = {06},
  eprint = {2506.19624},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2025-06-24T13:42:59Z},
  published = {2025-06-24T13:42:59Z},
  url = {https://arxiv.org/abs/2506.19624v1},
  pdf = {https://arxiv.org/pdf/2506.19624v1},
}

@article{arXiv:2103.12801,
  title = {Variable Name Recovery in Decompiled Binary Code using Constrained Masked Language Modeling},
  author = {Pratyay Banerjee and Kuntal Kumar Pal and Fish Wang and Chitta Baral},
  abstract = {Decompilation is the procedure of transforming binary programs into a high-level representation, such as source code, for human analysts to examine. While modern decompilers can reconstruct and recover much information that is discarded during compilation, inferring variable names is still extremely difficult. Inspired by recent advances in natural language processing, we propose a novel solution to infer variable names in decompiled code based on Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as Transformers and BERT. Our solution takes \\textit\{raw\} decompiler output, the less semantically meaningful code, as input, and enriches it using our proposed \\textit\{finetuning\} technique, Constrained Masked Language Modeling. Using Constrained Masked Language Modeling introduces the challenge of predicting the number of masked tokens for the original variable name. We address this \\textit\{count of token prediction\} challenge with our post-processing algorithm. Compared to the state-of-the-art approaches, our trained VarBERT model is simpler and of much better performance. We evaluated our model on an existing large-scale data set with 164,632 binaries and showed that it can predict variable names identical to the ones present in the original source code up to 84.15\\\% of the time.},
  journal = {arXiv preprint arXiv:2103.12801},
  year = {2021},
  month = {03},
  eprint = {2103.12801},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.CL cs.CR},
  comment = {Work In Progress},
  updated = {2021-03-23T19:09:22Z},
  published = {2021-03-23T19:09:22Z},
  url = {https://arxiv.org/abs/2103.12801v1},
  pdf = {https://arxiv.org/pdf/2103.12801v1},
}

@article{arXiv:2601.14598,
  title = {HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation},
  author = {Yonatan Gizachew Achamyeleh and Harsh Thomare and Mohammad Abdullah Al Faruque},
  abstract = {Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \\textsc\{HELIOS\}, a framework that reframes LLM-based decompilation as a structured reasoning task. \\textsc\{HELIOS\} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build. On HumanEval-Decompile for \\texttt\{x86\\\_64\}, \\textsc\{HELIOS\} raises average object file compilability from 45.0\\\% to 85.2\\\% for Gemini\textasciitilde{}2.0 and from 71.4\\\% to 89.6\\\% for GPT-4.1\textasciitilde{}Mini. With compiler feedback, compilability exceeds 94\\\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \\textsc\{HELIOS\} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \\textsc\{HELIOS\} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.},
  journal = {arXiv preprint arXiv:2601.14598},
  year = {2026},
  month = {01},
  eprint = {2601.14598},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  updated = {2026-01-31T02:51:51Z},
  published = {2026-01-21T02:37:33Z},
  url = {https://arxiv.org/abs/2601.14598v2},
  pdf = {https://arxiv.org/pdf/2601.14598v2},
}

@article{arXiv:2507.22066,
  title = {CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation},
  author = {Dylan Manuel and Paul Rad},
  abstract = {The generation of large, high-quality datasets for code understanding and generation remains a significant challenge, particularly when aligning decompiled binaries with their original source code. To address this, we present CodableLLM, a Python framework designed to automate the creation and curation of datasets by mapping decompiled functions to their corresponding source functions. This process enhances the alignment between decompiled and source code representations, facilitating the development of large language models (LLMs) capable of understanding and generating code across multiple abstraction levels. CodableLLM supports multiple programming languages and integrates with existing decompilers and parsers to streamline dataset generation. This paper presents the design and implementation of CodableLLM, evaluates its performance in dataset creation, and compares it to existing tools in the field. The results demonstrate that CodableLLM offers a robust and efficient solution for generating datasets tailored for code-focused LLMS.},
  journal = {arXiv preprint arXiv:2507.22066},
  year = {2025},
  month = {07},
  eprint = {2507.22066},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CR},
  updated = {2025-07-02T15:15:12Z},
  published = {2025-07-02T15:15:12Z},
  url = {https://arxiv.org/abs/2507.22066v1},
  pdf = {https://arxiv.org/pdf/2507.22066v1},
}

@article{arXiv:2503.07215,
  title = {The CodeInverter Suite: Control-Flow and Data-Mapping Augmented Binary Decompilation with LLMs},
  author = {Peipei Liu and Jian Sun and Rongkang Sun and Li Chen and Zhaoteng Yan and Peizheng Zhang and Dapeng Sun and Dawei Wang and Xiaoling Zhang and Dan Li},
  abstract = {Binary decompilation plays a vital role in various cybersecurity and software engineering tasks. Recently, end-to-end decompilation methods powered by large language models (LLMs) have garnered significant attention due to their ability to generate highly readable source code with minimal human intervention. However, existing LLM-based approaches face several critical challenges, including limited capability in reconstructing code structure and logic, low accuracy in data recovery, concerns over data security and privacy, and high computational resource requirements. To address these issues, we develop the CodeInverter Suite, making three contributions: (1) the CodeInverter Workflow (CIW) is a novel prompt engineering workflow that incorporates control flow graphs (CFG) and explicit data mappings to improve LLM-based decompilation. (2) Using CIW on well-known source code datasets, we curate the CodeInverter Dataset (CID), a domain-specific dataset containing 8.69 million samples that contains CFGs and data mapping tables. (3) We train the CoderInverter Models (CIMs) on CID, generating two lightweight LLMs (with 1.3B and 6.7B parameters) intended for efficient inference in privacy-sensitive or resource-constrained environments. Extensive experiments on two benchmarks demonstrate that the CIW substantially enhances the performance of various LLMs across multiple metrics. Our CIM-6.7B can achieve state-of-the-art decompilation performance, outperforming existing LLMs even with over 100x more parameters in decompilation tasks, an average improvement of 11.03\% in re-executability, 6.27\% in edit similarity.},
  journal = {arXiv preprint arXiv:2503.07215},
  year = {2025},
  month = {03},
  eprint = {2503.07215},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.PL},
  updated = {2025-05-26T15:58:11Z},
  published = {2025-03-10T11:52:48Z},
  url = {https://arxiv.org/abs/2503.07215v2},
  pdf = {https://arxiv.org/pdf/2503.07215v2},
}

@article{arXiv:2506.10125,
  title = {D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning},
  author = {Muqi Zou and Hongyu Cai and Hongwei Wu and Zion Leonahenahe Basque and Arslan Khan and Berkay Celik and Dave and Tian and Antonio Bianchi and Ruoyu and Wang and Dongyan Xu},
  abstract = {As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3\% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2\% of all the functions produced by the native decompiler.},
  journal = {arXiv preprint arXiv:2506.10125},
  year = {2025},
  month = {06},
  eprint = {2506.10125},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  updated = {2025-08-15T18:26:50Z},
  published = {2025-06-11T19:09:08Z},
  url = {https://arxiv.org/abs/2506.10125v2},
  pdf = {https://arxiv.org/pdf/2506.10125v2},
  author_affiliations = {Muqi Zou: Jing; Hongyu Cai: Jing; Hongwei Wu: Jing; Zion Leonahenahe Basque: Jing; Arslan Khan: Jing; Berkay Celik: Jing; Dave: Jing; Tian: Fish; Antonio Bianchi: Fish; Ruoyu: Fish},
}

@article{arXiv:2501.08670,
  title = {Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery},
  author = {Zeqin Liao and Yuhong Nan and Zixu Gao and Henglong Liang and Sicheng Hao and Peifan Reng and Zibin Zheng},
  abstract = {Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \\system\{\} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39\% for method boundaries, 90.39\% for variable types, and 80.65\% for contract attributes.},
  journal = {arXiv preprint arXiv:2501.08670},
  year = {2025},
  month = {01},
  eprint = {2501.08670},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {This is the author version of the article accepted for publication in IEEE Transactions on Software Engineering},
  updated = {2025-10-16T13:40:44Z},
  published = {2025-01-15T09:04:30Z},
  url = {https://arxiv.org/abs/2501.08670v2},
  pdf = {https://arxiv.org/pdf/2501.08670v2},
}

@article{arXiv:2305.12520,
  title = {SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly},
  author = {Jordi Armengol-Estapé and Jackson Woodruff and Chris Cummins and Michael F. P. O'Boyle},
  abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from ExeBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
  journal = {arXiv preprint arXiv:2305.12520},
  year = {2023},
  month = {05},
  eprint = {2305.12520},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.AI},
  updated = {2024-02-15T15:42:02Z},
  published = {2023-05-21T17:31:39Z},
  url = {https://arxiv.org/abs/2305.12520v3},
  pdf = {https://arxiv.org/pdf/2305.12520v3},
}

@article{arXiv:2410.15275,
  title = {SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract},
  author = {Eason Chen and Xinyi Tang and Zimo Xiao and Chuangji Li and Shizhuo Li and Wu Tingguan and Siyun Wang and Kostas Kryptos Chalkias},
  abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33\% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization},
  journal = {arXiv preprint arXiv:2410.15275},
  year = {2024},
  month = {10},
  eprint = {2410.15275},
  archivePrefix = {arXiv},
  primaryClass = {cs.HC},
  categories = {cs.HC cs.SE},
  comment = {Paper accepted at ACM The Web Conference 2025},
  updated = {2025-01-31T16:31:56Z},
  published = {2024-10-20T04:19:32Z},
  url = {https://arxiv.org/abs/2410.15275v2},
  pdf = {https://arxiv.org/pdf/2410.15275v2},
  doi = {10.1145/3696410.3714790},
}

@article{arXiv:2301.01701,
  title = {Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries},
  author = {Ali Al-Kaswan and Toufique Ahmed and Maliheh Izadi and Anand Ashok Sawant and Premkumar Devanbu and Arie van Deursen},
  abstract = {Reverse engineering binaries is required to understand and analyse programs for which the source code is unavailable. Decompilers can transform the largely unreadable binaries into a more readable source code-like representation. However, reverse engineering is time-consuming, much of which is taken up by labelling the functions with semantic information. While the automated summarisation of decompiled code can help Reverse Engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise decompiled binary functions. Furthermore, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by generating synthetic datasets and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82, and 44.21 for summarising source, decompiled, and synthetically stripped decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
  journal = {arXiv preprint arXiv:2301.01701},
  year = {2023},
  month = {01},
  eprint = {2301.01701},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI cs.LG cs.SE},
  comment = {SANER 2023 Technical Track Camera Ready},
  updated = {2023-01-13T14:47:19Z},
  published = {2023-01-04T16:56:33Z},
  url = {https://arxiv.org/abs/2301.01701v2},
  pdf = {https://arxiv.org/pdf/2301.01701v2},
}

@article{arXiv:2202.12336,
  title = {Automatically Mitigating Vulnerabilities in Binary Programs via Partially Recompilable Decompilation},
  author = {Pemma Reiter and Hui Jun Tay and Westley Weimer and Adam Doupé and Ruoyu Wang and Stephanie Forrest},
  abstract = {Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD). PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically work on an entire binary, our approach succeeds because it is limited to a few functions, like those identified by our binary fault localization. We evaluate these assumptions and find that, without any grammar or compilation restrictions, 70-89\% of individual functions are successfully decompiled and recompiled with sufficient type recovery. In comparison, only 1.7\% of the full C-binaries succeed. When decompilation succeeds, PRD produces test-equivalent binaries 92.9\% of the time. In addition, we evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, performs as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with these same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, our evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries.},
  journal = {arXiv preprint arXiv:2202.12336},
  year = {2022},
  month = {02},
  eprint = {2202.12336},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  updated = {2023-06-12T16:28:54Z},
  published = {2022-02-24T19:48:45Z},
  url = {https://arxiv.org/abs/2202.12336v2},
  pdf = {https://arxiv.org/pdf/2202.12336v2},
}

@article{arXiv:2406.04568,
  title = {StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation},
  author = {Weike Fang and Zhejian Zhou and Junzhou He and Weihang Wang},
  abstract = {WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.},
  journal = {arXiv preprint arXiv:2406.04568},
  year = {2024},
  month = {06},
  eprint = {2406.04568},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI cs.LG},
  comment = {9 pages. In the Proceedings of the 41st International Conference on Machine Learning (ICML' 24)},
  updated = {2024-06-07T01:08:17Z},
  published = {2024-06-07T01:08:17Z},
  url = {https://arxiv.org/abs/2406.04568v1},
  pdf = {https://arxiv.org/pdf/2406.04568v1},
}

@article{arXiv:2411.04981,
  title = {Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries},
  author = {Dylan Manuel and Nafis Tanveer Islam and Joseph Khoury and Ana Nunez and Elias Bou-Harb and Peyman Najafirad},
  abstract = {Security experts reverse engineer (decompile) binary code to identify critical security vulnerabilities. The limited access to source code in vital systems - such as firmware, drivers, and proprietary software used in Critical Infrastructures (CI) - makes this analysis even more crucial on the binary level. Even with available source code, a semantic gap persists after compilation between the source and the binary code executed by the processor. This gap may hinder the detection of vulnerabilities in source code. That being said, current research on Large Language Models (LLMs) overlooks the significance of decompiled binaries in this area by focusing solely on source code. In this work, we are the first to empirically uncover the substantial semantic limitations of state-of-the-art LLMs when it comes to analyzing vulnerabilities in decompiled binaries, largely due to the absence of relevant datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary code vulnerability dataset. Our dataset is multi-architecture and multi-optimization, focusing on C/C++ due to their wide usage in CI and association with numerous vulnerabilities. Specifically, we curate 150,872 samples of vulnerable and non-vulnerable decompiled binary code for the task of (i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv) recovering function names in the domain of decompiled binaries. Subsequently, we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance increase of 19\%, 24\%, and 21\% in the capabilities of CodeLlama, Llama3, and CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally, using DeBinVul, we report a high performance of 80-90\% on the vulnerability classification task. Furthermore, we report improved performance in function name recovery and vulnerability description tasks.},
  journal = {arXiv preprint arXiv:2411.04981},
  year = {2024},
  month = {11},
  eprint = {2411.04981},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI},
  updated = {2024-11-07T18:54:31Z},
  published = {2024-11-07T18:54:31Z},
  url = {https://arxiv.org/abs/2411.04981v1},
  pdf = {https://arxiv.org/pdf/2411.04981v1},
}

@article{arXiv:0710.4700,
  title = {A Decompilation Approach to Partitioning Software for Microprocessor/FPGA Platforms},
  author = {Greg Stitt and Frank Vahid},
  abstract = {In this paper, we present a software compilation approach for microprocessor/FPGA platforms that partitions a software binary onto custom hardware implemented in the FPGA. Our approach imposes less restrictions on software tool flow than previous compiler approaches, allowing software designers to use any software language and compiler. Our approach uses a back-end partitioning tool that utilizes decompilation techniques to recover important high-level information, resulting in performance comparable to high-level compiler-based approaches.},
  journal = {arXiv preprint arXiv:0710.4700},
  year = {2007},
  month = {10},
  eprint = {0710.4700},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {Submitted on behalf of EDAA (http://www.edaa.com/)},
  updated = {2007-10-25T09:22:50Z},
  published = {2007-10-25T09:22:50Z},
  url = {https://arxiv.org/abs/0710.4700v1},
  pdf = {https://arxiv.org/pdf/0710.4700v1},
  note = {Journal reference: Dans Design, Automation and Test in Europe - DATE'05, Munich : Allemagne (2005)},
}

@article{arXiv:2504.08310,
  title = {DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search},
  author = {Shubing Xie and Aritra Sarkar and Sebastian Feld},
  abstract = {Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains. This work addresses the challenge of explainability in quantum architecture search (QAS) by introducing a novel genetic programming-based decompiler framework for reverse-engineering high-level quantum algorithms from low-level circuit representations. The proposed approach, implemented in the open-source tool DeQompile, employs program synthesis techniques, including symbolic regression and abstract syntax tree manipulation, to distill interpretable Qiskit algorithms from quantum assembly language. Validation of benchmark algorithms demonstrates the efficacy of our tool. By integrating the decompiler with online learning frameworks, this research potentiates explainable QAS by fostering the development of generalizable and provable quantum algorithms.},
  journal = {arXiv preprint arXiv:2504.08310},
  year = {2025},
  month = {04},
  eprint = {2504.08310},
  archivePrefix = {arXiv},
  primaryClass = {quant-ph},
  categories = {quant-ph cs.NE},
  updated = {2025-04-11T07:23:14Z},
  published = {2025-04-11T07:23:14Z},
  url = {https://arxiv.org/abs/2504.08310v1},
  pdf = {https://arxiv.org/pdf/2504.08310v1},
}

@article{arXiv:2306.07856,
  title = {Bayesian Program Learning by Decompiling Amortized Knowledge},
  author = {Alessandro B. Palmarini and Christopher G. Lucas and N. Siddharth},
  abstract = {DreamCoder is an inductive program synthesis system that, whilst solving problems, learns to simplify search in an iterative wake-sleep procedure. The cost of search is amortized by training a neural search policy, reducing search breadth and effectively "compiling" useful information to compose program solutions across tasks. Additionally, a library of program components is learnt to compress and express discovered solutions in fewer components, reducing search depth. We present a novel approach for library learning that directly leverages the neural search policy, effectively "decompiling" its amortized knowledge to extract relevant program components. This provides stronger amortized inference: the amortized knowledge learnt to reduce search breadth is now also used to reduce search depth. We integrate our approach with DreamCoder and demonstrate faster domain proficiency with improved generalization on a range of domains, particularly when fewer example solutions are available.},
  journal = {arXiv preprint arXiv:2306.07856},
  year = {2023},
  month = {06},
  eprint = {2306.07856},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  categories = {cs.AI cs.LG cs.SE},
  updated = {2024-05-31T15:14:58Z},
  published = {2023-06-13T15:35:01Z},
  url = {https://arxiv.org/abs/2306.07856v3},
  pdf = {https://arxiv.org/pdf/2306.07856v3},
}

@article{arXiv:2210.01075,
  title = {Decompiling x86 Deep Neural Network Executables},
  author = {Zhibo Liu and Yuanyuan Yuan and Shuai Wang and Xiaofei Xie and Lei Ma},
  abstract = {Due to their widespread use on heterogeneous hardware devices, deep learning (DL) models are compiled into executables by DL compilers to fully leverage low-level hardware primitives. This approach allows DL computations to be undertaken at low cost across a variety of computing platforms, including CPUs, GPUs, and various hardware accelerators. We present BTD (Bin to DNN), a decompiler for deep neural network (DNN) executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. BTD delivers a practical framework to process DNN executables compiled by different DL compilers and with full optimizations enabled on x86 platforms. It employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators. Our evaluation reveals that BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recovered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable. We show that BTD can boost two representative attacks, adversarial example generation and knowledge stealing, against DNN executables. We also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching.},
  journal = {arXiv preprint arXiv:2210.01075},
  year = {2022},
  month = {10},
  eprint = {2210.01075},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.LG},
  comment = {The extended version of a paper to appear in the Proceedings of the 32nd USENIX Security Symposium, 2023, (USENIX Security '23), 25 pages},
  updated = {2022-10-04T11:45:23Z},
  published = {2022-10-03T16:48:18Z},
  url = {https://arxiv.org/abs/2210.01075v2},
  pdf = {https://arxiv.org/pdf/2210.01075v2},
}

@article{arXiv:2601.09035,
  title = {A Decompilation-Driven Framework for Malware Detection with Large Language Models},
  author = {Aniesh Chawla and Udbhav Prasad},
  abstract = {The parallel evolution of Large Language Models (LLMs) with advanced code-understanding capabilities and the increasing sophistication of malware presents a new frontier for cybersecurity research. This paper evaluates the efficacy of state-of-the-art LLMs in classifying executable code as either benign or malicious. We introduce an automated pipeline that first decompiles Windows executable into a C code using Ghidra disassembler and then leverages LLMs to perform the classification. Our evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. We demonstrate that a fine-tuned model, trained on curated malware and benign datasets, significantly outperforms its vanilla counterpart. However, the performance of even this specialized model degrades notably when encountering newer malware. This finding demonstrates the critical need for continuous fine-tuning with emerging threats to maintain model effectiveness against the changing coding patterns and behaviors of malicious software.},
  journal = {arXiv preprint arXiv:2601.09035},
  year = {2026},
  month = {01},
  eprint = {2601.09035},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI},
  comment = {6 pages, published in 2025 IEMCON},
  updated = {2026-01-14T00:00:26Z},
  published = {2026-01-14T00:00:26Z},
  url = {https://arxiv.org/abs/2601.09035v1},
  pdf = {https://arxiv.org/pdf/2601.09035v1},
}

@article{arXiv:2408.07181,
  title = {VulCatch: Enhancing Binary Vulnerability Detection through CodeT5 Decompilation and KAN Advanced Feature Extraction},
  author = {Abdulrahman Hamman Adama Chukkol and Senlin Luo and Kashif Sharif and Yunusa Haruna and Muhammad Muhammad Abdullahi},
  abstract = {Binary program vulnerability detection is critical for software security, yet existing deep learning approaches often rely on source code analysis, limiting their ability to detect unknown vulnerabilities. To address this, we propose VulCatch, a binary-level vulnerability detection framework. VulCatch introduces a Synergy Decompilation Module (SDM) and Kolmogorov-Arnold Networks (KAN) to transform raw binary code into pseudocode using CodeT5, preserving high-level semantics for deep analysis with tools like Ghidra and IDA. KAN further enhances feature transformation, enabling the detection of complex vulnerabilities. VulCatch employs word2vec, Inception Blocks, BiLSTM Attention, and Residual connections to achieve high detection accuracy (98.88\%) and precision (97.92\%), while minimizing false positives (1.56\%) and false negatives (2.71\%) across seven CVE datasets.},
  journal = {arXiv preprint arXiv:2408.07181},
  year = {2024},
  month = {08},
  eprint = {2408.07181},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI cs.LG cs.SE},
  updated = {2024-08-13T19:46:50Z},
  published = {2024-08-13T19:46:50Z},
  url = {https://arxiv.org/abs/2408.07181v1},
  pdf = {https://arxiv.org/pdf/2408.07181v1},
}

@article{arXiv:2105.05159,
  title = {Proving LTL Properties of Bitvector Programs and Decompiled Binaries (Extended)},
  author = {Yuandong Cyrus Liu and Chengbin Pang and Daniel Dietsch and Eric Koskinen and Ton-Chanh Le and Georgios Portokalidis and Jun Xu},
  abstract = {There is increasing interest in applying verification tools to programs that have bitvector operations (eg., binaries). SMT solvers, which serve as a foundation for these tools, have thus increased support for bitvector reasoning through bit-blasting and linear arithmetic approximations. In this paper we show that similar linear arithmetic approximation of bitvector operations can be done at the source level through transformations. Specifically, we introduce new paths that over-approximate bitvector operations with linear conditions/constraints, increasing branching but allowing us to better exploit the well-developed integer reasoning and interpolation of verification tools. We show that, for reachability of bitvector programs, increased branching incurs negligible overhead yet, when combined with integer interpolation optimizations, enables more programs to be verified. We further show this exploitation of integer interpolation in the common case also enables competitive termination verification of bitvector programs and leads to the first effective technique for LTL verification of bitvector programs. Finally, we provide an in-depth case study of decompiled ("lifted") binary programs, which emulate X86 execution through frequent use of bitvector operations. We present a new tool DarkSea, the first tool capable of verifying reachability, termination, and LTL of lifted binaries.},
  journal = {arXiv preprint arXiv:2105.05159},
  year = {2021},
  month = {05},
  eprint = {2105.05159},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.FL cs.SE eess.SY},
  comment = {39 pages(including Appendix), 10 tables, 4 Postscript figures, accepted to APLAS 2021},
  updated = {2021-08-28T05:44:26Z},
  published = {2021-05-11T16:12:02Z},
  url = {https://arxiv.org/abs/2105.05159v2},
  pdf = {https://arxiv.org/pdf/2105.05159v2},
  author_affiliations = {Yuandong Cyrus Liu: Stevens Institute of Technology; Chengbin Pang: Stevens Institute of Technology; Daniel Dietsch: University of Freiburg; Eric Koskinen: Stevens Institute of Technology; Ton-Chanh Le: Stevens Institute of Technology; Georgios Portokalidis: Stevens Institute of Technology; Jun Xu: Stevens Institute of Technology},
}

@article{arXiv:0809.3503,
  title = {JDATATRANS for Array Obfuscation in Java Source Code to Defeat Reverse Engineering from Decompiled Codes},
  author = {Praveen Sivadasan and P Sojan Lal and Naveen Sivadasan},
  abstract = {Software obfuscation or obscuring a software is an approach to defeat the practice of reverse engineering a software for using its functionality illegally in the development of another software. Java applications are more amenable to reverse engineering and re-engineering attacks through methods such as decompilation because Java class files store the program in a semi complied form called 'byte' codes. The existing obfuscation systems obfuscate the Java class files. Obfuscated source code produce obfuscated byte codes and hence two level obfuscation (source code and byte code level) of the program makes it more resilient to reverse engineering attacks. But source code obfuscation is much more difficult due to richer set of programming constructs and the scope of the different variables used in the program and only very little progress has been made on this front. Hence programmers resort to adhoc manual ways of obscuring their program which makes it difficult for its maintenance and usability. To address this issue partially, we developed a user friendly tool JDATATRANS to obfuscate Java source code by obscuring the array usages. Using various array restructuring techniques such as 'array splitting', 'array folding' and 'array flattening', in addition to constant hiding, our system obfuscate the input Java source code and produce an obfuscated Java source code that is functionally equivalent to the input program. We also perform a number of experiments to measure the potency, resilience and cost incurred by our tool.},
  journal = {arXiv preprint arXiv:0809.3503},
  year = {2008},
  month = {09},
  eprint = {0809.3503},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {Manuscript submitted to ACM COMPUTE 2009 Conference,Bangalore},
  updated = {2008-09-20T13:18:26Z},
  published = {2008-09-20T13:18:26Z},
  url = {https://arxiv.org/abs/0809.3503v1},
  pdf = {https://arxiv.org/pdf/0809.3503v1},
}

@article{arXiv:1909.12252,
  title = {Synthesizing Structured CAD Models with Equality Saturation and Inverse Transformations},
  author = {Chandrakana Nandi and Max Willsey and Adam Anderson and James R. Wilcox and Eva Darulova and Dan Grossman and Zachary Tatlock},
  abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
  journal = {arXiv preprint arXiv:1909.12252},
  year = {2019},
  month = {09},
  eprint = {1909.12252},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.CG},
  comment = {14 pages},
  updated = {2020-04-12T18:15:51Z},
  published = {2019-09-26T16:58:08Z},
  url = {https://arxiv.org/abs/1909.12252v3},
  pdf = {https://arxiv.org/pdf/1909.12252v3},
  doi = {10.1145/3385412.3386012},
  note = {Journal reference: PLDI 2020},
}

@article{arXiv:2304.03854,
  title = {Revisiting Deep Learning for Variable Type Recovery},
  author = {Kevin Cao and Kevin Leach},
  abstract = {Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.},
  journal = {arXiv preprint arXiv:2304.03854},
  year = {2023},
  month = {04},
  eprint = {2304.03854},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG},
  comment = {In The 31st International Conference on Program Comprehension(ICPC 2023 RENE)},
  updated = {2023-04-07T22:28:28Z},
  published = {2023-04-07T22:28:28Z},
  url = {https://arxiv.org/abs/2304.03854v1},
  pdf = {https://arxiv.org/pdf/2304.03854v1},
}

@article{arXiv:2103.05221,
  title = {Learning to Find Usages of Library Functions in Optimized Binaries},
  author = {Toufique Ahmed and Premkumar Devanbu and Anand Ashok Sawant},
  abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, "natural" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to "decompile" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
  journal = {arXiv preprint arXiv:2103.05221},
  year = {2021},
  month = {03},
  eprint = {2103.05221},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2021-09-17T01:34:54Z},
  published = {2021-03-09T04:48:03Z},
  url = {https://arxiv.org/abs/2103.05221v2},
  pdf = {https://arxiv.org/pdf/2103.05221v2},
  doi = {10.1109/TSE.2021.3106572},
  note = {Journal reference: Transactions on Software Engineering (2021)},
}

@article{arXiv:2411.13220,
  title = {CF-GKAT: Efficient Validation of Control-Flow Transformations},
  author = {Cheng Zhang and Tobias Kappé and David E. Narváez and Nico Naus},
  abstract = {Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT's abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren's goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation.},
  journal = {arXiv preprint arXiv:2411.13220},
  year = {2024},
  month = {11},
  eprint = {2411.13220},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  updated = {2025-01-16T16:17:00Z},
  published = {2024-11-20T11:28:23Z},
  url = {https://arxiv.org/abs/2411.13220v2},
  pdf = {https://arxiv.org/pdf/2411.13220v2},
  doi = {10.1145/3704857},
}

@article{arXiv:1205.4813,
  title = {Securing SQLJ Source Codes from Business Logic Disclosure by Data Hiding Obfuscation},
  author = {Praveen Sivadasan and P. Sojan Lal},
  abstract = {Information security is protecting information from unauthorized access, use, disclosure, disruption, modification, perusal and destruction. CAIN model suggest maintaining the Confidentiality, Authenticity, Integrity and Non-repudiation (CAIN) of information. Oracle 8i, 9i and 11g Databases support SQLJ framework allowing embedding of SQL statements in Java Programs and providing programmer friendly means to access the Oracle database. As cloud computing technology is becoming popular, SQLJ is considered as a flexible and user friendly language for developing distributed applications in grid architectures. SQLJ source codes are translated to java byte codes and decompilation is generation of source codes from intermediate byte codes. The intermediate SQLJ application byte codes are open to decompilation, allowing a malicious reader to forcefully decompile it for understanding confidential business logic or data from the codes. To the best of our knowledge, strong and cost effective techniques exist for Oracle Database security, but still data security techniques are lacking for client side applications, giving possibility for revelation of confidential business data. Data obfuscation is hiding the data in codes and we suggest enhancing the data security in SQLJ source codes by data hiding, to mitigate disclosure of confidential business data, especially integers in distributed applications.},
  journal = {arXiv preprint arXiv:1205.4813},
  year = {2012},
  month = {05},
  eprint = {1205.4813},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.DB cs.DC},
  comment = {4 pages,3 Figures},
  updated = {2012-05-22T06:18:32Z},
  published = {2012-05-22T06:18:32Z},
  url = {https://arxiv.org/abs/1205.4813v1},
  pdf = {https://arxiv.org/pdf/1205.4813v1},
}

@article{arXiv:1810.04789,
  title = {Applications of Graph Integration to Function Comparison and Malware Classification},
  author = {Michael A. Slawinski and Andy Wortman},
  abstract = {We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled .NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms.},
  journal = {arXiv preprint arXiv:1810.04789},
  year = {2018},
  month = {10},
  eprint = {1810.04789},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  categories = {cs.AI},
  updated = {2019-11-13T22:38:05Z},
  published = {2018-10-11T00:14:46Z},
  url = {https://arxiv.org/abs/1810.04789v6},
  pdf = {https://arxiv.org/pdf/1810.04789v6},
}

@article{arXiv:2505.22010,
  title = {VulBinLLM: LLM-powered Vulnerability Detection for Stripped Binaries},
  author = {Nasir Hussain and Haohan Chen and Chanh Tran and Philip Huang and Zhuohao Li and Pravir Chugh and William Chen and Ashish Kundu and Yuan Tian},
  abstract = {Recognizing vulnerabilities in stripped binary files presents a significant challenge in software security. Although some progress has been made in generating human-readable information from decompiled binary files with Large Language Models (LLMs), effectively and scalably detecting vulnerabilities within these binary files is still an open problem. This paper explores the novel application of LLMs to detect vulnerabilities within these binary files. We demonstrate the feasibility of identifying vulnerable programs through a combined approach of decompilation optimization to make the vulnerabilities more prominent and long-term memory for a larger context window, achieving state-of-the-art performance in binary vulnerability analysis. Our findings highlight the potential for LLMs to overcome the limitations of traditional analysis methods and advance the field of binary vulnerability detection, paving the way for more secure software systems. In this paper, we present Vul-BinLLM , an LLM-based framework for binary vulnerability detection that mirrors traditional binary analysis workflows with fine-grained optimizations in decompilation and vulnerability reasoning with an extended context. In the decompilation phase, Vul-BinLLM adds vulnerability and weakness comments without altering the code structure or functionality, providing more contextual information for vulnerability reasoning later. Then for vulnerability reasoning, Vul-BinLLM combines in-context learning and chain-of-thought prompting along with a memory management agent to enhance accuracy. Our evaluations encompass the commonly used synthetic dataset Juliet to evaluate the potential feasibility for analysis and vulnerability detection in C/C++ binaries. Our evaluations show that Vul-BinLLM is highly effective in detecting vulnerabilities on the compiled Juliet dataset.},
  journal = {arXiv preprint arXiv:2505.22010},
  year = {2025},
  month = {05},
  eprint = {2505.22010},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2025-05-28T06:17:56Z},
  published = {2025-05-28T06:17:56Z},
  url = {https://arxiv.org/abs/2505.22010v1},
  pdf = {https://arxiv.org/pdf/2505.22010v1},
}

@article{arXiv:2306.02546,
  title = {Symbol Preference Aware Generative Models for Recovering Variable Names from Stripped Binary},
  author = {Xiangzhe Xu and Zhuo Zhang and Zian Su and Ziyang Huang and Shiwei Feng and Yapeng Ye and Nan Jiang and Danning Xie and Siyuan Cheng and Lin Tan and Xiangyu Zhang},
  abstract = {Decompilation aims to recover the source code form of a binary executable. It has many security applications, such as malware analysis, vulnerability detection, and code hardening. A prominent challenge in decompilation is to recover variable names. We propose a novel technique that leverages the strengths of generative models while mitigating model biases. We build a prototype, GenNm, from pre-trained generative models CodeGemma-2B, CodeLlama-7B, and CodeLlama-34B. We finetune GenNm on decompiled functions and teach models to leverage contextual information. GenNm includes names from callers and callees while querying a function, providing rich contextual information within the model's input token limitation. We mitigate model biases by aligning the output distribution of models with symbol preferences of developers. Our results show that GenNm improves the state-of-the-art name recovery precision by 5.6-11.4 percentage points on two commonly used datasets and improves the state-of-the-art by 32\% (from 17.3\% to 22.8\%) in the most challenging setup where ground-truth variable names are not seen in the training dataset.},
  journal = {arXiv preprint arXiv:2306.02546},
  year = {2023},
  month = {06},
  eprint = {2306.02546},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2024-12-09T06:45:17Z},
  published = {2023-06-05T02:39:48Z},
  url = {https://arxiv.org/abs/2306.02546v4},
  pdf = {https://arxiv.org/pdf/2306.02546v4},
}

@article{arXiv:2007.09696,
  title = {STAN: Towards Describing Bytecodes of Smart Contract},
  author = {Xiaoqi Li and Ting Chen and Xiapu Luo and Tao Zhang and Le Yu and Zhou Xu},
  abstract = {More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1\% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users.},
  journal = {arXiv preprint arXiv:2007.09696},
  year = {2020},
  month = {07},
  eprint = {2007.09696},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CR},
  comment = {In Proc. of the 20th IEEE International Conference on Software Quality, Reliability and Security (QRS), 2020},
  updated = {2020-07-19T15:48:35Z},
  published = {2020-07-19T15:48:35Z},
  url = {https://arxiv.org/abs/2007.09696v1},
  pdf = {https://arxiv.org/pdf/2007.09696v1},
  note = {Journal reference: In Proc. of the 20th IEEE International Conference on Software Quality, Reliability and Security (QRS), 2020},
}

@article{arXiv:2009.03846,
  title = {On Architecture to Architecture Mapping for Concurrency},
  author = {Soham Chakraborty},
  abstract = {Mapping programs from one architecture to another plays a key role in technologies such as binary translation, decompilation, emulation, virtualization, and application migration. Although multicore architectures are ubiquitous, the state-of-the-art translation tools do not handle concurrency primitives correctly. Doing so is rather challenging because of the subtle differences in the concurrency models between architectures. In response, we address various aspects of the challenge. First, we develop correct and efficient translations between the concurrency models of two mainstream architecture families: x86 and ARM (versions 7 and 8). We develop direct mappings between x86 and ARMv8 and ARMv7, and fence elimination algorithms to eliminate redundant fences after direct mapping. Although our mapping utilizes ARMv8 as an intermediate model for mapping between x86 and ARMv7, we argue that it should not be used as an intermediate model in a decompiler because it disallows common compiler transformations. Second, we propose and implement a technique for inserting memory fences for safely migrating programs between different architectures. Our technique checks robustness against x86 and ARM, and inserts fences upon robustness violations. Our experiments demonstrate that in most of the programs both our techniques introduce significantly fewer fences compared to naive schemes for porting applications across these architectures.},
  journal = {arXiv preprint arXiv:2009.03846},
  year = {2020},
  month = {09},
  eprint = {2009.03846},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.AR},
  updated = {2020-09-08T16:28:28Z},
  published = {2020-09-08T16:28:28Z},
  url = {https://arxiv.org/abs/2009.03846v1},
  pdf = {https://arxiv.org/pdf/2009.03846v1},
}

@article{arXiv:2511.00076,
  title = {Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves},
  author = {Zihao Wan and Pau Tong Lin Xu and Fuwen Luo and Ziyue Wang and Peng Li and Yang Liu},
  abstract = {While Vision-language Models (VLMs) have demonstrated strong semantic capabilities, their ability to interpret the underlying geometric structure of visual information is less explored. Pictographic characters, which combine visual form with symbolic structure, provide an ideal test case for this capability. We formulate this visual recognition challenge in the mathematical domain, where each character is represented by an executable program of geometric primitives. This is framed as a program synthesis task, training a VLM to decompile raster images into programs composed of Bézier curves. Our model, acting as a "visual decompiler", demonstrates performance superior to strong zero-shot baselines, including GPT-4o. The most significant finding is that when trained solely on modern Chinese characters, the model is able to reconstruct ancient Oracle Bone Script in a zero-shot context. This generalization provides strong evidence that the model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to a more structured form of visual understanding.},
  journal = {arXiv preprint arXiv:2511.00076},
  year = {2025},
  month = {10},
  eprint = {2511.00076},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG},
  updated = {2025-10-29T15:26:34Z},
  published = {2025-10-29T15:26:34Z},
  url = {https://arxiv.org/abs/2511.00076v1},
  pdf = {https://arxiv.org/pdf/2511.00076v1},
}

@article{arXiv:2503.03969,
  title = {Trim My View: An LLM-Based Code Query System for Module Retrieval in Robotic Firmware},
  author = {Sima Arasteh and Pegah Jandaghi and Nicolaas Weideman and Dennis Perepech and Mukund Raghothaman and Christophe Hauser and Luis Garcia},
  abstract = {The software compilation process has a tendency to obscure the original design of the system and makes it difficult both to identify individual components and discern their purpose simply by examining the resulting binary code. Although decompilation techniques attempt to recover higher-level source code from the machine code in question, they are not fully able to restore the semantics of the original functions. Furthermore, binaries are often stripped of metadata, and this makes it challenging to reverse engineer complex binary software. In this paper we show how a combination of binary decomposition techniques, decompilation passes, and LLM-powered function summarization can be used to build an economical engine to identify modules in stripped binaries and associate them with high-level natural language descriptions. We instantiated this technique with three underlying open-source LLMs -- CodeQwen, DeepSeek-Coder and CodeStral -- and measured its effectiveness in identifying modules in robotics firmware. This experimental evaluation involved 467 modules from four devices from the ArduPilot software suite, and showed that CodeStral, the best-performing backend LLM, achieves an average F1-score of 0.68 with an online running time of just a handful of seconds.},
  journal = {arXiv preprint arXiv:2503.03969},
  year = {2025},
  month = {03},
  eprint = {2503.03969},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  comment = {11 pages, 5 figures},
  updated = {2025-03-05T23:40:17Z},
  published = {2025-03-05T23:40:17Z},
  url = {https://arxiv.org/abs/2503.03969v1},
  pdf = {https://arxiv.org/pdf/2503.03969v1},
}

@article{arXiv:2602.08857,
  title = {Discovering Interpretable Algorithms by Decompiling Transformers to RASP},
  author = {Xinting Huang and Aleksandra Bakalova and Satwik Bhattamishra and William Merrill and Michael Hahn},
  abstract = {Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.},
  journal = {arXiv preprint arXiv:2602.08857},
  year = {2026},
  month = {02},
  eprint = {2602.08857},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.AI cs.CL},
  comment = {101 pages, 92 figures},
  updated = {2026-02-09T16:22:29Z},
  published = {2026-02-09T16:22:29Z},
  url = {https://arxiv.org/abs/2602.08857v1},
  pdf = {https://arxiv.org/pdf/2602.08857v1},
}

@article{arXiv:1102.2339,
  title = {A decompilation of the pi-calculus and its application to termination},
  author = {Roberto Amadio},
  abstract = {We study the correspondence between a concurrent lambda-calculus in administrative, continuation passing style and a pi-calculus and we derive a termination result for the latter.},
  journal = {arXiv preprint arXiv:1102.2339},
  year = {2011},
  month = {02},
  eprint = {1102.2339},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  updated = {2011-02-11T12:53:14Z},
  published = {2011-02-11T12:53:14Z},
  url = {https://arxiv.org/abs/1102.2339v1},
  pdf = {https://arxiv.org/pdf/1102.2339v1},
  author_affiliations = {Roberto Amadio: PPS},
}

@article{arXiv:2312.05275,
  title = {Exploring the Limits of ChatGPT in Software Security Applications},
  author = {Fangzhou Wu and Qingzhao Zhang and Ati Priya Bajaj and Tiffany Bao and Ning Zhang and Ruoyu "Fish" Wang and Chaowei Xiao},
  abstract = {Large language models (LLMs) have undergone rapid evolution and achieved remarkable results in recent times. OpenAI's ChatGPT, backed by GPT-3.5 or GPT-4, has gained instant popularity due to its strong capability across a wide range of tasks, including natural language tasks, coding, mathematics, and engaging conversations. However, the impacts and limits of such LLMs in system security domain are less explored. In this paper, we delve into the limits of LLMs (i.e., ChatGPT) in seven software security applications including vulnerability detection/repair, debugging, debloating, decompilation, patching, root cause analysis, symbolic execution, and fuzzing. Our exploration reveals that ChatGPT not only excels at generating code, which is the conventional application of language models, but also demonstrates strong capability in understanding user-provided commands in natural languages, reasoning about control and data flows within programs, generating complex data structures, and even decompiling assembly code. Notably, GPT-4 showcases significant improvements over GPT-3.5 in most security tasks. Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.},
  journal = {arXiv preprint arXiv:2312.05275},
  year = {2023},
  month = {12},
  eprint = {2312.05275},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI},
  updated = {2023-12-08T03:02:37Z},
  published = {2023-12-08T03:02:37Z},
  url = {https://arxiv.org/abs/2312.05275v1},
  pdf = {https://arxiv.org/pdf/2312.05275v1},
}

@article{arXiv:0904.3458,
  title = {JConstHide: A Framework for Java Source Code Constant Hiding},
  author = {Praveen Sivadasan and P Sojan Lal},
  abstract = {Software obfuscation or obscuring a software is an approach to defeat the practice of reverse engineering a software for using its functionality illegally in the development of another software. Java applications are more amenable to reverse engineering and re-engineering attacks through methods such as decompilation because Java class files store the program in a semi complied form called byte codes. The existing obfuscation systems obfuscate the Java class files. Obfuscated source code produce obfuscated byte codes and hence two level obfuscation (source code and byte code level) of the program makes it more resilient to reverse engineering attacks . But source code obfuscation is much more difficult due to richer set of programming constructs and the scope of the different variables used in the program and only very little progress has been made on this front. We in this paper are proposing a framework named JConstHide for hiding constants, especially integers in the java source codes, to defeat reverse engineering through decompilation. To the best of our knowledge, no data hiding software are available for java source code constant hiding.},
  journal = {arXiv preprint arXiv:0904.3458},
  year = {2009},
  month = {04},
  eprint = {0904.3458},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2009-04-22T13:15:31Z},
  published = {2009-04-22T13:15:31Z},
  url = {https://arxiv.org/abs/0904.3458v1},
  pdf = {https://arxiv.org/pdf/0904.3458v1},
}

@article{arXiv:1509.06083,
  title = {Reasoning About LLVM Code Using Codewalker},
  author = {David S. Hardin},
  abstract = {This paper reports on initial experiments using J Moore's Codewalker to reason about programs compiled to the Low-Level Virtual Machine (LLVM) intermediate form. Previously, we reported on a translator from LLVM to the applicative subset of Common Lisp accepted by the ACL2 theorem prover, producing executable ACL2 formal models, and allowing us to both prove theorems about the translated models as well as validate those models by testing. That translator provided many of the benefits of a pure decompilation into logic approach, but had the disadvantage of not being verified. The availability of Codewalker as of ACL2 7.0 has provided an opportunity to revisit this idea, and employ a more trustworthy decompilation into logic tool. Thus, we have employed the Codewalker method to create an interpreter for a subset of the LLVM instruction set, and have used Codewalker to analyze some simple array-based C programs compiled to LLVM form. We discuss advantages and limitations of the Codewalker-based method compared to the previous method, and provide some challenge problems for future Codewalker development.},
  journal = {arXiv preprint arXiv:1509.06083},
  year = {2015},
  month = {09},
  eprint = {1509.06083},
  archivePrefix = {arXiv},
  primaryClass = {cs.LO},
  categories = {cs.LO cs.PL},
  comment = {In Proceedings ACL2 2015, arXiv:1509.05526},
  updated = {2015-09-21T00:35:28Z},
  published = {2015-09-21T00:35:28Z},
  url = {https://arxiv.org/abs/1509.06083v1},
  pdf = {https://arxiv.org/pdf/1509.06083v1},
  author_affiliations = {David S. Hardin: Rockwell Collins},
  doi = {10.4204/EPTCS.192.7},
  note = {Journal reference: EPTCS 192, 2015, pp. 79-92},
}

@article{arXiv:1901.10073,
  title = {DeClassifier: Class-Inheritance Inference Engine for Optimized C++ Binaries},
  author = {Rukayat Ayomide Erinfolami and Aravind Prakash},
  abstract = {Recovering class inheritance from C++ binaries has several security benefits including problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining--or worse exclusion--due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5\% and 71.4\% true positive directed edges in the class hierarchy tree under O0 and O2 optimizations respectively, (2) a combination of ctor+dtor analysis provides much better inference than ctor only analysis.},
  journal = {arXiv preprint arXiv:1901.10073},
  year = {2019},
  month = {01},
  eprint = {1901.10073},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {13 pages of main paper including references, 1 page of appendix, 2 figures and 10 tables},
  updated = {2019-02-15T22:17:12Z},
  published = {2019-01-29T02:37:58Z},
  url = {https://arxiv.org/abs/1901.10073v2},
  pdf = {https://arxiv.org/pdf/1901.10073v2},
}

@article{arXiv:2005.11839,
  title = {Tezla, an Intermediate Representation for Static Analysis of Michelson Smart Contracts},
  author = {João Santos Reis and Paul Crocker and Simão Melo de Sousa},
  abstract = {This paper introduces Tezla, an intermediate representation of Michelson smart contracts that eases the design of static smart contract analysers. This intermediate representation uses a store and preserves the semantics, ow and resource usage of the original smart contract. This enables properties like gas consumption to be statically verified. We provide an automated decompiler of Michelson smart contracts to Tezla. In order to support our claim about the adequacy of Tezla, we develop a static analyser that takes advantage of the Tezla representation of Michelson smart contracts to prove simple but non-trivial properties.},
  journal = {arXiv preprint arXiv:2005.11839},
  year = {2020},
  month = {05},
  eprint = {2005.11839},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  updated = {2020-05-24T20:49:13Z},
  published = {2020-05-24T20:49:13Z},
  url = {https://arxiv.org/abs/2005.11839v1},
  pdf = {https://arxiv.org/pdf/2005.11839v1},
}

@article{arXiv:2108.07639,
  title = {Learning C to x86 Translation: An Experiment in Neural Compilation},
  author = {Jordi Armengol-Estapé and Michael F. P. O'Boyle},
  abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
  journal = {arXiv preprint arXiv:2108.07639},
  year = {2021},
  month = {08},
  eprint = {2108.07639},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  categories = {cs.AI cs.PL},
  comment = {Published in AIPLANS 2021},
  updated = {2022-12-16T11:21:46Z},
  published = {2021-08-17T14:11:15Z},
  url = {https://arxiv.org/abs/2108.07639v2},
  pdf = {https://arxiv.org/pdf/2108.07639v2},
  note = {Journal reference: Armengol-Estapé, J. and O'Boyle, M. Learning C to x86 translation: An experiment in neural compilation. In Advances in Programming Languages and Neurosymbolic Systems Workshop, 2021. URL \\url\{https://openreview.net/forum?id=444ug\_EYXet\}},
}

@article{arXiv:2002.04694,
  title = {Adversarial Robustness for Code},
  author = {Pavol Bielik and Martin Vechev},
  abstract = {Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code such as finding and fixing bugs, code completion, decompilation, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) combining existing and novel techniques to improve robustness while preserving high accuracy.},
  journal = {arXiv preprint arXiv:2002.04694},
  year = {2020},
  month = {02},
  eprint = {2002.04694},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.PL cs.SE stat.ML},
  comment = {Proceedings of the 37th International Conference on Machine Learning, Online, PMLR 119, 2020},
  updated = {2020-08-15T12:35:28Z},
  published = {2020-02-11T21:32:14Z},
  url = {https://arxiv.org/abs/2002.04694v2},
  pdf = {https://arxiv.org/pdf/2002.04694v2},
}

@article{arXiv:2504.00694,
  title = {On Benchmarking Code LLMs for Android Malware Analysis},
  author = {Yiling He and Hongyu She and Xingzhi Qian and Xinran Zheng and Zhuo Chen and Zhan Qin and Lorenzo Cavallaro},
  abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.},
  journal = {arXiv preprint arXiv:2504.00694},
  year = {2025},
  month = {04},
  eprint = {2504.00694},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.LG},
  comment = {This paper has been accepted to the 34th ACM SIGSOFT ISSTA Companion (LLMSC Workshop 2025)},
  updated = {2025-04-23T16:07:20Z},
  published = {2025-04-01T12:05:49Z},
  url = {https://arxiv.org/abs/2504.00694v2},
  pdf = {https://arxiv.org/pdf/2504.00694v2},
  doi = {10.1145/3713081.3731745},
}

@article{arXiv:1004.3250,
  title = {Watermarking Java Programs using Dummy Methods with Dynamically Opaque Predicates},
  author = {Zaenal Akbar},
  abstract = {Software piracy, the illegal using, copying, and resale of applications is a major concern for anyone develops software. Software developers also worry about their applications being reverse engineered by extracting data structures and algorithms from an application and incorporated into competitor's code. A defense against software piracy is watermarking, a process that embeds a secret message in a cover software. Watermarking is a method that does not aim to stop piracy copying, but to prove ownership of the software and possibly even the data structures and algorithms used in the software. The language Java was designed to be compiled into a platform independent bytecode format. Much of the information contained in the source code remains in the bytecode, which means that decompilation is easier than with traditional native codes. In this thesis, we present a technique for watermarking Java programs by using a never-executed dummy method (Monden et.al., 2000) combined with opaque predicates (Collberg et.al., 1998; Arboit, 2002) and improved with dynamically opaque predicates (Palsberg et.al., 2000). This work presents a method to construct a dynamic opaque predicates by grouping two or more opaque predicates according to predefined rules. Any software watermarking technique will exhibit a trade-off between resilience, data rate, cost, and stealth. To evaluate the quality of a watermarking scheme we must also know how well it stands up to different types of attacks. Ideally, we would like our watermarks to survive translation (compilation, decompilation, and binary translation), optimization, and obfuscation. Add a single watermark will increasing source code approximate 3.854 bytes with dummy method that cover up to 15 characters, two dynamic data structures, two threads and two opaque predicates. Application loading-time increase approximate 6108 milliseconds.},
  journal = {arXiv preprint arXiv:1004.3250},
  year = {2010},
  month = {04},
  eprint = {1004.3250},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  comment = {95 pages, In Indonesia, Master Thesis, Sepuluh November Institute of Technology, Indonesia, February 2004},
  updated = {2010-04-19T16:55:46Z},
  published = {2010-04-19T16:55:46Z},
  url = {https://arxiv.org/abs/1004.3250v1},
  pdf = {https://arxiv.org/pdf/1004.3250v1},
}

@article{arXiv:2601.09986,
  title = {Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT},
  author = {Cheng Zhang and Qiancheng Fu and Hang Ji and Ines Santacruz Del Valle and Alexandra Silva and Marco Gaboardi},
  abstract = {This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems.},
  journal = {arXiv preprint arXiv:2601.09986},
  year = {2026},
  month = {01},
  eprint = {2601.09986},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.LO},
  comment = {Conditionally Accepted at ESOP 2026},
  updated = {2026-01-22T19:50:23Z},
  published = {2026-01-15T01:59:57Z},
  url = {https://arxiv.org/abs/2601.09986v2},
  pdf = {https://arxiv.org/pdf/2601.09986v2},
}

@article{arXiv:2509.03037,
  title = {TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum},
  author = {Shuzheng Wang and Yue Huang and Zhuoer Xu and Yuming Huang and Jing Tang},
  abstract = {Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet comprehensive security analysis remains difficult due to unverified code, proxy-based architectures, and the reliance on manual inspection of complex execution traces. Existing approaches fall into two main categories: anomaly transaction detection, which flags suspicious transactions but offers limited insight into specific attack strategies hidden in execution traces inside transactions, and code vulnerability detection, which cannot analyze unverified contracts and struggles to show how identified flaws are exploited in real incidents. As a result, analysts must still manually align transaction traces with contract code to reconstruct attack scenarios and conduct forensics. To address this gap, TraceLLM is proposed as a framework that leverages LLMs to integrate execution trace-level detection with decompiled contract code. We introduce a new anomaly execution path identification algorithm and an LLM-refined decompile tool to identify vulnerable functions and provide explicit attack paths to LLM. TraceLLM establishes the first benchmark for joint trace and contract code-driven security analysis. For comparison, proxy baselines are created by jointly transmitting the results of three representative code analysis along with raw traces to LLM. TraceLLM identifies attacker and victim addresses with 85.19\\\% precision and produces automated reports with 70.37\\\% factual precision across 27 cases with ground truth expert reports, achieving 25.93\\\% higher accuracy than the best baseline. Moreover, across 148 real-world Ethereum incidents, TraceLLM automatically generates reports with 66.22\\\% expert-verified accuracy, demonstrating strong generalizability.},
  journal = {arXiv preprint arXiv:2509.03037},
  year = {2025},
  month = {09},
  eprint = {2509.03037},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.ET cs.SE},
  updated = {2025-09-03T05:53:56Z},
  published = {2025-09-03T05:53:56Z},
  url = {https://arxiv.org/abs/2509.03037v1},
  pdf = {https://arxiv.org/pdf/2509.03037v1},
}

@article{arXiv:1805.02751,
  title = {Security and Privacy Analyses of Internet of Things Children's Toys},
  author = {Gordon Chu and Noah Apthorpe and Nick Feamster},
  abstract = {This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially-available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule (COPPA) as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many IoT toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks.},
  journal = {arXiv preprint arXiv:1805.02751},
  year = {2018},
  month = {05},
  eprint = {1805.02751},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.CY},
  comment = {8 pages, 8 figures; publication version},
  updated = {2018-08-29T00:35:58Z},
  published = {2018-05-07T21:23:47Z},
  url = {https://arxiv.org/abs/1805.02751v2},
  pdf = {https://arxiv.org/pdf/1805.02751v2},
  doi = {10.1109/JIOT.2018.2866423},
  note = {Journal reference: IEEE Internet of Things Journal (IoT-J), 2018},
}

@article{arXiv:0801.3690,
  title = {Ensuring Spreadsheet Integrity with Model Master},
  author = {Jocelyn Paine},
  abstract = {We have developed the Model Master (MM) language for describing spreadsheets, and tools for converting MM programs to and from spreadsheets. The MM decompiler translates a spreadsheet into an MM program which gives a concise summary of its calculations, layout, and styling. This is valuable when trying to understand spreadsheets one has not seen before, and when checking for errors. The MM compiler goes the other way, translating an MM program into a spreadsheet. This makes possible a new style of development, in which spreadsheets are generated from textual specifications. This can reduce error rates compared to working directly with the raw spreadsheet, and gives important facilities for code reuse. MM programs also offer advantages over Excel files for the interchange of spreadsheets.},
  journal = {arXiv preprint arXiv:0801.3690},
  year = {2008},
  month = {01},
  eprint = {0801.3690},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.HC},
  comment = {15 pages; substantive references; code examples},
  updated = {2008-01-24T00:32:29Z},
  published = {2008-01-24T00:32:29Z},
  url = {https://arxiv.org/abs/0801.3690v1},
  pdf = {https://arxiv.org/pdf/0801.3690v1},
  note = {Journal reference: Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2001 17-38 ISBN:1 86166 179 7},
}

@article{arXiv:2404.00786,
  title = {There and Back Again: A Netlist's Tale with Much Egraphin'},
  author = {Gus Henry Smith and Zachary D. Sisco and Thanawat Techaumnuaiwit and Jingtao Xia and Vishal Canumalla and Andrew Cheung and Zachary Tatlock and Chandrakana Nandi and Jonathan Balkind},
  abstract = {EDA toolchains are notoriously unpredictable, incomplete, and error-prone; the generally-accepted remedy has been to re-imagine EDA tasks as compilation problems. However, any compiler framework we apply must be prepared to handle the wide range of EDA tasks, including not only compilation tasks like technology mapping and optimization (the "there"\} in our title), but also decompilation tasks like loop rerolling (the "back again"). In this paper, we advocate for equality saturation -- a term rewriting framework -- as the framework of choice when building hardware toolchains. Through a series of case studies, we show how the needs of EDA tasks line up conspicuously well with the features equality saturation provides.},
  journal = {arXiv preprint arXiv:2404.00786},
  year = {2024},
  month = {03},
  eprint = {2404.00786},
  archivePrefix = {arXiv},
  primaryClass = {cs.AR},
  categories = {cs.AR cs.PL},
  updated = {2024-03-31T20:20:48Z},
  published = {2024-03-31T20:20:48Z},
  url = {https://arxiv.org/abs/2404.00786v1},
  pdf = {https://arxiv.org/pdf/2404.00786v1},
}

@article{arXiv:2304.02260,
  title = {Feature Engineering Using File Layout for Malware Detection},
  author = {Jeongwoo Kim and Eun-Sun Cho and Joon-Young Paik},
  abstract = {Malware detection on binary executables provides a high availability to even binaries which are not disassembled or decompiled. However, a binary-level approach could cause ambiguity problems. In this paper, we propose a new feature engineering technique that use minimal knowledge about the internal layout on a binary. The proposed feature avoids the ambiguity problems by integrating the information about the layout with structural entropy. The experimental results show that our feature improves accuracy and F1-score by 3.3\% and 0.07, respectively, on a CNN based malware detector with realistic benign and malicious samples.},
  journal = {arXiv preprint arXiv:2304.02260},
  year = {2023},
  month = {04},
  eprint = {2304.02260},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {2pages, no figures, This manuscript was presented in the poster session of The Annual Computer Security Applications Conference (ACSAC) 2020},
  updated = {2023-04-05T07:24:12Z},
  published = {2023-04-05T07:24:12Z},
  url = {https://arxiv.org/abs/2304.02260v1},
  pdf = {https://arxiv.org/pdf/2304.02260v1},
}

@article{arXiv:2202.01142,
  title = {Pop Quiz! Can a Large Language Model Help With Reverse Engineering?},
  author = {Hammond Pearce and Benjamin Tan and Prashanth Krishnamurthy and Farshad Khorrami and Ramesh Karri and Brendan Dolan-Gavitt},
  abstract = {Large language models (such as OpenAI's Codex) have demonstrated impressive zero-shot multi-task capabilities in the software domain, including code explanation. In this work, we examine if this ability can be used to help with reverse engineering. Specifically, we investigate prompting Codex to identify the purpose, capabilities, and important variable names or values from code, even when the code is produced through decompilation. Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model. We present an extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks: of the 136,260 questions we posed, it answered 72,754 correctly. A key takeaway is that while promising, LLMs are not yet ready for zero-shot reverse engineering.},
  journal = {arXiv preprint arXiv:2202.01142},
  year = {2022},
  month = {02},
  eprint = {2202.01142},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.CR cs.LG},
  comment = {18 pages, 19 figures. Linked dataset: https://doi.org/10.5281/zenodo.5949075},
  updated = {2022-02-02T17:09:15Z},
  published = {2022-02-02T17:09:15Z},
  url = {https://arxiv.org/abs/2202.01142v1},
  pdf = {https://arxiv.org/pdf/2202.01142v1},
}

@article{arXiv:2403.10571,
  title = {JaxDecompiler: Redefining Gradient-Informed Software Design},
  author = {Pierrick Pochelu},
  abstract = {Among numerical libraries capable of computing gradient descent optimization, JAX stands out by offering more features, accelerated by an intermediate representation known as Jaxpr language. However, editing the Jaxpr code is not directly possible. This article introduces JaxDecompiler, a tool that transforms any JAX function into an editable Python code, especially useful for editing the JAX function generated by the gradient function. JaxDecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by JAX. We highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.},
  journal = {arXiv preprint arXiv:2403.10571},
  year = {2024},
  month = {03},
  eprint = {2403.10571},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.LG cs.SE},
  updated = {2024-03-14T20:32:31Z},
  published = {2024-03-14T20:32:31Z},
  url = {https://arxiv.org/abs/2403.10571v1},
  pdf = {https://arxiv.org/pdf/2403.10571v1},
}

@article{arXiv:2010.00774,
  title = {Proof Repair across Type Equivalences},
  author = {Talia Ringer and RanDair Porter and Nathaniel Yazdani and John Leo and Dan Grossman},
  abstract = {We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in PUMPKIN Pi, an extension to the PUMPKIN PATCH Coq plugin suite for proof repair. We demonstrate PUMPKIN Pi's flexibility on eight case studies, including supporting a benchmark from a user study, easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.},
  journal = {arXiv preprint arXiv:2010.00774},
  year = {2020},
  month = {10},
  eprint = {2010.00774},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  comment = {Tool repository with code guide: https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/GUIDE.md},
  updated = {2021-05-12T03:31:08Z},
  published = {2020-10-02T04:49:32Z},
  url = {https://arxiv.org/abs/2010.00774v4},
  pdf = {https://arxiv.org/pdf/2010.00774v4},
  doi = {10.1145/3453483.3454033},
}

@article{arXiv:2308.00250,
  title = {CONSTRUCT: A Program Synthesis Approach for Reconstructing Control Algorithms from Embedded System Binaries in Cyber-Physical Systems},
  author = {Ali Shokri and Alexandre Perez and Souma Chowdhury and Chen Zeng and Gerald Kaloor and Ion Matei and Peter-Patel Schneider and Akshith Gunasekaran and Shantanu Rane},
  abstract = {We introduce a novel approach to automatically synthesize a mathematical representation of the control algorithms implemented in industrial cyber-physical systems (CPS), given the embedded system binary. The output model can be used by subject matter experts to assess the system's compliance with the expected behavior and for a variety of forensic applications. Our approach first performs static analysis on decompiled binary files of the controller to create a sketch of the mathematical representation. Then, we perform an evolutionary-based search to find the correct semantic for the created representation, i.e., the control law. We demonstrate the effectiveness of the introduced approach in practice via three case studies conducted on two real-life industrial CPS.},
  journal = {arXiv preprint arXiv:2308.00250},
  year = {2023},
  month = {08},
  eprint = {2308.00250},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2023-08-01T03:10:55Z},
  published = {2023-08-01T03:10:55Z},
  url = {https://arxiv.org/abs/2308.00250v1},
  pdf = {https://arxiv.org/pdf/2308.00250v1},
}

@article{arXiv:1608.08970,
  title = {J-Viz: Sibling-First Recursive Graph Drawing for Visualizing Java Bytecode},
  author = {Md. Jawaherul Alam and Michael T. Goodrich and Timothy Johnson},
  abstract = {We describe a graph visualization tool for visualizing Java bytecode. Our tool, which we call J-Viz, visualizes connected directed graphs according to a canonical node ordering, which we call the sibling-first recursive (SFR) numbering. The particular graphs we consider are derived from applying Shiver's k-CFA framework to Java bytecode, and our visualizer includes helpful links between the nodes of an input graph and the Java bytecode that produced it, as well as a decompiled version of that Java bytecode. We show through several case studies that the canonical drawing paradigm used in J-Viz is effective for identifying potential security vulnerabilities and repeated use of the same code in Java applications.},
  journal = {arXiv preprint arXiv:1608.08970},
  year = {2016},
  month = {08},
  eprint = {1608.08970},
  archivePrefix = {arXiv},
  primaryClass = {cs.DS},
  categories = {cs.DS},
  updated = {2016-08-31T18:03:18Z},
  published = {2016-08-31T18:03:18Z},
  url = {https://arxiv.org/abs/1608.08970v1},
  pdf = {https://arxiv.org/pdf/1608.08970v1},
}

@article{arXiv:2311.13721,
  title = {Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning},
  author = {Nan Jiang and Chengxiao Wang and Kevin Liu and Xiangzhe Xu and Lin Tan and Xiangyu Zhang and Petr Babkin},
  abstract = {Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58\% (absolute percentage point improvement) higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17\% Recall@1, showing promising abilities on both assembly generation and understanding tasks.},
  journal = {arXiv preprint arXiv:2311.13721},
  year = {2023},
  month = {11},
  eprint = {2311.13721},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  comment = {Published as a conference paper at ICLR 2025},
  updated = {2025-11-11T01:45:48Z},
  published = {2023-11-22T22:27:54Z},
  url = {https://arxiv.org/abs/2311.13721v7},
  pdf = {https://arxiv.org/pdf/2311.13721v7},
}

@article{arXiv:2406.06637,
  title = {Exploring the Efficacy of Large Language Models (GPT-4) in Binary Reverse Engineering},
  author = {Saman Pordanesh and Benjamin Tan},
  abstract = {This study investigates the capabilities of Large Language Models (LLMs), specifically GPT-4, in the context of Binary Reverse Engineering (RE). Employing a structured experimental approach, we analyzed the LLM's performance in interpreting and explaining human-written and decompiled codes. The research encompassed two phases: the first on basic code interpretation and the second on more complex malware analysis. Key findings indicate LLMs' proficiency in general code understanding, with varying effectiveness in detailed technical and security analyses. The study underscores the potential and current limitations of LLMs in reverse engineering, revealing crucial insights for future applications and improvements. Also, we examined our experimental methodologies, such as methods of evaluation and data constraints, which provided us with a technical vision for any future research activity in this field.},
  journal = {arXiv preprint arXiv:2406.06637},
  year = {2024},
  month = {06},
  eprint = {2406.06637},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  updated = {2024-06-09T09:23:58Z},
  published = {2024-06-09T09:23:58Z},
  url = {https://arxiv.org/abs/2406.06637v1},
  pdf = {https://arxiv.org/pdf/2406.06637v1},
}

@article{arXiv:2501.17766,
  title = {Formally Verified Binary-level Pointer Analysis},
  author = {Freek Verbeek and Ali Shokri and Daniel Engel and Binoy Ravindran},
  abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows "meaningful" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
  journal = {arXiv preprint arXiv:2501.17766},
  year = {2025},
  month = {01},
  eprint = {2501.17766},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2025-01-29T16:57:15Z},
  published = {2025-01-29T16:57:15Z},
  url = {https://arxiv.org/abs/2501.17766v1},
  pdf = {https://arxiv.org/pdf/2501.17766v1},
}

@article{arXiv:1705.05637,
  title = {Text-based Adventures of the Golovin AI Agent},
  author = {Bartosz Kostka and Jaroslaw Kwiecien and Jakub Kowalski and Pawel Rychlikowski},
  abstract = {The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.},
  journal = {arXiv preprint arXiv:1705.05637},
  year = {2017},
  month = {05},
  eprint = {1705.05637},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  categories = {cs.AI},
  updated = {2017-05-16T10:55:08Z},
  published = {2017-05-16T10:55:08Z},
  url = {https://arxiv.org/abs/1705.05637v1},
  pdf = {https://arxiv.org/pdf/1705.05637v1},
  doi = {10.1109/CIG.2017.8080433},
}

@article{arXiv:2303.08746,
  title = {J-Parallelio -- automatic parallelization framework for Java virtual machine code},
  author = {Krzysztof Stuglik and Piotr Listkiewicz and Mateusz Kulczyk and Marcin Pietron},
  abstract = {Manual translation of the algorithms from sequential version to its parallel counterpart is time consuming and can be done only with the specific knowledge of hardware accelerator architecture, parallel programming or programming environment. The automation of this process makes porting the code much easier and faster. The key aspect in this case is how efficient the generated parallel code will be. The paper describes J-Parallelio, the framework for automatic analysis of the bytecode source codes and its parallelisation on multicore processors. The process consists of a few steps. First step is a process of decompilation of JVM and its translation to internal abstract syntax tree, the dependency extraction and memory analysis is performed. Finally, the mapping process is performed which consists of a set of rules responsible for translating the input virtual machine source code to its parallel version. The main novelty is that it can deal with pure Java virtual machine and can generate parallel code for multicore processors. This makes the system portable and it can work with different languages based on JVM after some small modifications. The efficiency of automatically translated source codes were compared with their manually written counterparts on chosen benchmarks.},
  journal = {arXiv preprint arXiv:2303.08746},
  year = {2023},
  month = {02},
  eprint = {2303.08746},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  categories = {cs.DC},
  updated = {2023-02-07T19:10:36Z},
  published = {2023-02-07T19:10:36Z},
  url = {https://arxiv.org/abs/2303.08746v1},
  pdf = {https://arxiv.org/pdf/2303.08746v1},
}

@article{arXiv:1007.3250,
  title = {Verification of Java Bytecode using Analysis and Transformation of Logic Programs},
  author = {Elvira Albert and Miguel Gómez-Zamalloa and Laurent Hubert and German Puebla},
  abstract = {State of the art analyzers in the Logic Programming (LP) paradigm are nowadays mature and sophisticated. They allow inferring a wide variety of global properties including termination, bounds on resource consumption, etc. The aim of this work is to automatically transfer the power of such analysis tools for LP to the analysis and verification of Java bytecode (JVML). In order to achieve our goal, we rely on well-known techniques for meta-programming and program specialization. More precisely, we propose to partially evaluate a JVML interpreter implemented in LP together with (an LP representation of) a JVML program and then analyze the residual program. Interestingly, at least for the examples we have studied, our approach produces very simple LP representations of the original JVML programs. This can be seen as a decompilation from JVML to high-level LP source. By reasoning about such residual programs, we can automatically prove in the CiaoPP system some non-trivial properties of JVML programs such as termination, run-time error freeness and infer bounds on its resource consumption. We are not aware of any other system which is able to verify such advanced properties of Java bytecode.},
  journal = {arXiv preprint arXiv:1007.3250},
  year = {2010},
  month = {07},
  eprint = {1007.3250},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  updated = {2010-07-19T19:46:43Z},
  published = {2010-07-19T19:46:43Z},
  url = {https://arxiv.org/abs/1007.3250v1},
  pdf = {https://arxiv.org/pdf/1007.3250v1},
  doi = {10.1007/978-3-540-69611-7\_8},
  note = {Journal reference: The International Symposium on Practical Aspects of Declarative Languages 4354 (2007) 124-139},
}

@article{arXiv:2502.06854,
  title = {Can Large Language Models Understand Intermediate Representations in Compilers?},
  author = {Hailong Jiang and Jianfeng Zhu and Yao Wan and Bo Fang and Hongyu Zhang and Ruoming Jin and Qiang Guan},
  abstract = {Intermediate Representations (IRs) play a critical role in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. In this paper, we present an explorative empirical study evaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3, DeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically, we assess model performance across four core tasks: control flow graph reconstruction, decompilation, code summarization, and execution reasoning. While LLMs exhibit competence in parsing IR syntax and identifying high-level structures, they consistently struggle with instruction-level reasoning, especially in control flow reasoning, loop handling, and dynamic execution. Common failure modes include misinterpreting branching instructions, omitting critical operations, and relying on heuristic reasoning rather than precise instruction-level logic. Our findings highlight the need for IR-specific enhancements in LLM design. We recommend fine-tuning on structured IR datasets and integrating control-flow-sensitive architectures to improve model effectiveness. All experimental data and source code are publicly available at},
  journal = {arXiv preprint arXiv:2502.06854},
  year = {2025},
  month = {02},
  eprint = {2502.06854},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.AI cs.CL},
  updated = {2025-06-05T15:48:54Z},
  published = {2025-02-07T17:23:48Z},
  url = {https://arxiv.org/abs/2502.06854v2},
  pdf = {https://arxiv.org/pdf/2502.06854v2},
}

@article{arXiv:1608.04303,
  title = {SandBlaster: Reversing the Apple Sandbox},
  author = {Răzvan Deaconescu and Luke Deshotels and Mihai Bucicoiu and William Enck and Lucas Davi and Ahmad-Reza Sadeghi},
  abstract = {In order to limit the damage of malware on Mac OS X and iOS, Apple uses sandboxing, a kernel-level security layer that provides tight constraints for system calls. Particularly used for Apple iOS, sandboxing prevents apps from executing potentially dangerous actions, by defining rules in a sandbox profile. Investigating Apple's built-in sandbox profiles is difficult as they are compiled and stored in binary format. We present SandBlaster, a software bundle that is able to reverse/decompile Apple binary sandbox profiles to their original human readable SBPL (SandBox Profile Language) format. We use SandBlaster to reverse all built-in Apple iOS binary sandbox profiles for iOS 7, 8 and 9. Our tool is, to the best of our knowledge, the first to provide a full reversing of the Apple sandbox, shedding light into the inner workings of Apple sandbox profiles and providing essential support for security researchers and professionals interested in Apple security mechanisms.},
  journal = {arXiv preprint arXiv:1608.04303},
  year = {2016},
  month = {08},
  eprint = {1608.04303},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.OS},
  comment = {25 pages, 9 figures, 14 listings This report is an auxiliary document to the paper "SandScout: Automatic Detection of Flaws in iOS Sandbox Profiles", to be presented at the ACM Conference on Computer and Communications Security (CCS) 2016},
  updated = {2016-08-15T15:26:22Z},
  published = {2016-08-15T15:26:22Z},
  url = {https://arxiv.org/abs/1608.04303v1},
  pdf = {https://arxiv.org/pdf/1608.04303v1},
}

@article{arXiv:2305.13504,
  title = {Neural Machine Translation for Code Generation},
  author = {Dharma KC and Clayton T. Morrison},
  abstract = {Neural machine translation (NMT) methods developed for natural language processing have been shown to be highly successful in automating translation from one natural language to another. Recently, these NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. In the literature, a variety of different input scenarios have been explored, including generating code based on natural language description, lower-level representations such as binary or assembly (neural decompilation), partial representations of source code (code completion and repair), and source code in another language (code translation). In this paper we survey the NMT for code generation literature, cataloging the variety of methods that have been explored according to input and output representations, model architectures, optimization techniques used, data sets, and evaluation methods. We discuss the limitations of existing methods and future research directions},
  journal = {arXiv preprint arXiv:2305.13504},
  year = {2023},
  month = {05},
  eprint = {2305.13504},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  categories = {cs.CL cs.LG},
  comment = {33 pages, 1 figure},
  updated = {2023-05-22T21:43:12Z},
  published = {2023-05-22T21:43:12Z},
  url = {https://arxiv.org/abs/2305.13504v1},
  pdf = {https://arxiv.org/pdf/2305.13504v1},
}

@article{arXiv:2204.14205,
  title = {Symbolic Synthesis of Clifford Circuits and Beyond},
  author = {Matthew Amy and Owen Bennett-Gibbs and Neil J. Ross},
  abstract = {Path sums are a convenient symbolic formalism for quantum operations with applications to the simulation, optimization, and verification of quantum protocols. Unlike quantum circuits, path sums are not limited to unitary operations, but can express arbitrary linear ones. Two problems, therefore, naturally arise in the study of path sums: the unitarity problem and the extraction problem. The former is the problem of deciding whether a given path sum represents a unitary operator. The latter is the problem of constructing a quantum circuit, given a path sum promised to represent a unitary operator. In this paper, we show that the unitarity problem is co-NP-hard in general, but that it is in P when restricted to Clifford path sums. We then provide an algorithm to synthesize a Clifford circuit from a unitary Clifford path sum. The circuits produced by our extraction algorithm are of the form C1-H-C2, where C1 and C2 are Hadamard-free circuits and H is a layer of Hadamard gates. We also provide a heuristic generalization of our extraction algorithm to arbitrary path sums. While this algorithm is not guaranteed to succeed, it often succeeds and typically produces natural looking circuits. Alongside applications to the optimization and decompilation of quantum circuits, we demonstrate the capability of our algorithm by synthesizing the standard quantum Fourier transform directly from a path sum.},
  journal = {arXiv preprint arXiv:2204.14205},
  year = {2022},
  month = {04},
  eprint = {2204.14205},
  archivePrefix = {arXiv},
  primaryClass = {quant-ph},
  categories = {quant-ph},
  comment = {In Proceedings QPL 2022, arXiv:2311.08375},
  updated = {2023-11-15T11:43:19Z},
  published = {2022-04-29T16:33:42Z},
  url = {https://arxiv.org/abs/2204.14205v2},
  pdf = {https://arxiv.org/pdf/2204.14205v2},
  author_affiliations = {Matthew Amy: Simon Fraser University; Owen Bennett-Gibbs: McGill University; Neil J. Ross: Dalhousie University},
  doi = {10.4204/EPTCS.394.17},
  note = {Journal reference: EPTCS 394, 2023, pp. 343-362},
}

@article{arXiv:1009.4000,
  title = {Malicious cryptography techniques for unreversable (malicious or not) binaries},
  author = {Eric Filiol},
  abstract = {Fighting against computer malware require a mandatory step of reverse engineering. As soon as the code has been disassemblied/decompiled (including a dynamic analysis step), there is a hope to understand what the malware actually does and to implement a detection mean. This also applies to protection of software whenever one wishes to analyze them. In this paper, we show how to amour code in such a way that reserse engineering techniques (static and dymanic) are absolutely impossible by combining malicious cryptography techniques developped in our laboratory and new types of programming (k-ary codes). Suitable encryption algorithms combined with new cryptanalytic approaches to ease the protection of (malicious or not) binaries, enable to provide both total code armouring and large scale polymorphic features at the same time. A simple 400 Kb of executable code enables to produce a binary code and around \$2\textasciicircum{}\{140\}\$ mutated forms natively while going far beyond the old concept of decryptor.},
  journal = {arXiv preprint arXiv:1009.4000},
  year = {2010},
  month = {09},
  eprint = {1009.4000},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {17 pages, 2 figures, accepted for presentation at H2HC'10},
  updated = {2010-09-21T05:49:33Z},
  published = {2010-09-21T05:49:33Z},
  url = {https://arxiv.org/abs/1009.4000v1},
  pdf = {https://arxiv.org/pdf/1009.4000v1},
}

@article{arXiv:2403.13839,
  title = {depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers},
  author = {Kaichao You and Runsheng Bai and Meng Cao and Jianmin Wang and Ion Stoica and Mingsheng Long},
  abstract = {PyTorch \\texttt\{2.x\} introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, adapting to the PyTorch compiler to full potential can be challenging. The compiler operates at the Python bytecode level, making it appear as an opaque box. To address this, we introduce \\texttt\{depyf\}, a tool designed to demystify the inner workings of the PyTorch compiler. \\texttt\{depyf\} decompiles bytecode generated by PyTorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, \\texttt\{depyf\} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is \\href\{https://github.com/thuml/depyf\}\{ openly available\} and is recognized as a \\href\{https://pytorch.org/ecosystem/\}\{PyTorch ecosystem project\}.},
  journal = {arXiv preprint arXiv:2403.13839},
  year = {2024},
  month = {03},
  eprint = {2403.13839},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.AI cs.PL},
  comment = {16 pages, 2 figures},
  updated = {2024-03-14T16:17:14Z},
  published = {2024-03-14T16:17:14Z},
  url = {https://arxiv.org/abs/2403.13839v1},
  pdf = {https://arxiv.org/pdf/2403.13839v1},
}

@article{arXiv:2305.06902,
  title = {REMaQE: Reverse Engineering Math Equations from Executables},
  author = {Meet Udeshi and Prashanth Krishnamurthy and Hammond Pearce and Ramesh Karri and Farshad Khorrami},
  abstract = {Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This paper proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool "tmon". To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow.},
  journal = {arXiv preprint arXiv:2305.06902},
  year = {2023},
  month = {05},
  eprint = {2305.06902},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2024-04-11T04:55:33Z},
  published = {2023-05-11T15:45:45Z},
  url = {https://arxiv.org/abs/2305.06902v2},
  pdf = {https://arxiv.org/pdf/2305.06902v2},
  doi = {10.1145/3699674},
  note = {Journal reference: Transactions on Cyber-Physical Systems, Volume 8, Issue 4 (2024)},
}

@article{arXiv:2510.15567,
  title = {MalCVE: Malware Detection and CVE Association Using Large Language Models},
  author = {Eduard Andrei Cristea and Petter Molnes and Jingyue Li},
  abstract = {Malicious software attacks are having an increasingly significant economic impact. Commercial malware detection software can be costly, and tools that attribute malware to the specific software vulnerabilities it exploits are largely lacking. Understanding the connection between malware and the vulnerabilities it targets is crucial for analyzing past threats and proactively defending against current ones. In this study, we propose an approach that leverages large language models (LLMs) to detect binary malware, specifically within JAR files, and uses LLM capabilities combined with retrieval-augmented generation (RAG) to identify Common Vulnerabilities and Exposures (CVEs) that malware may exploit. We developed a proof-of-concept tool, MalCVE, that integrates binary code decompilation, deobfuscation, LLM-based code summarization, semantic similarity search, and LLM-based CVE classification. We evaluated MalCVE using a benchmark dataset of 3,839 JAR executables. MalCVE achieved a mean malware-detection accuracy of 97\%, at a fraction of the cost of commercial solutions. In particular, the results demonstrate that LLM-based code summarization enables highly accurate and explainable malware identification. MalCVE is also the first tool to associate CVEs with binary malware, achieving a recall@10 of 65\%, which is comparable to studies that perform similar analyses on source code.},
  journal = {arXiv preprint arXiv:2510.15567},
  year = {2025},
  month = {10},
  eprint = {2510.15567},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  updated = {2026-02-02T17:25:17Z},
  published = {2025-10-17T11:55:46Z},
  url = {https://arxiv.org/abs/2510.15567v2},
  pdf = {https://arxiv.org/pdf/2510.15567v2},
  doi = {10.1145/3793655.3793735},
}

@article{arXiv:2207.03578,
  title = {Code Translation with Compiler Representations},
  author = {Marc Szafraniec and Baptiste Roziere and Hugh Leather and Francois Charton and Patrick Labatut and Gabriel Synnaeve},
  abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java -> Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.},
  journal = {arXiv preprint arXiv:2207.03578},
  year = {2022},
  month = {06},
  eprint = {2207.03578},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.CL cs.LG},
  comment = {9 pages},
  updated = {2023-04-24T10:12:18Z},
  published = {2022-06-30T14:21:57Z},
  url = {https://arxiv.org/abs/2207.03578v5},
  pdf = {https://arxiv.org/pdf/2207.03578v5},
}

@article{arXiv:2505.07360,
  title = {BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models},
  author = {Xiuwei Shang and Guoqiang Chen and Shaoyin Cheng and Benlong Wu and Li Hu and Gangyang Li and Weiming Zhang and Nenghai Yu},
  abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
  journal = {arXiv preprint arXiv:2505.07360},
  year = {2025},
  month = {05},
  eprint = {2505.07360},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {23 pages, 5 figures, to be published in IJCAI 2025},
  updated = {2025-05-12T08:54:07Z},
  published = {2025-05-12T08:54:07Z},
  url = {https://arxiv.org/abs/2505.07360v1},
  pdf = {https://arxiv.org/pdf/2505.07360v1},
}

@article{arXiv:2410.09401,
  title = {A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion},
  author = {Lixia Zhang and Tianxu Liu and Kaihui Shen and Cheng Chen},
  abstract = {With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness. This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware. In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability. The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.},
  journal = {arXiv preprint arXiv:2410.09401},
  year = {2024},
  month = {10},
  eprint = {2410.09401},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.AI},
  updated = {2024-10-12T07:10:44Z},
  published = {2024-10-12T07:10:44Z},
  url = {https://arxiv.org/abs/2410.09401v1},
  pdf = {https://arxiv.org/pdf/2410.09401v1},
}

@article{arXiv:1512.08546,
  title = {When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries},
  author = {Aylin Caliskan and Fabian Yamaguchi and Edwin Dauber and Richard Harang and Konrad Rieck and Rachel Greenstadt and Arvind Narayanan},
  abstract = {The ability to identify authors of computer programs based on their coding style is a direct threat to the privacy and anonymity of programmers. While recent work found that source code can be attributed to authors with high accuracy, attribution of executable binaries appears to be much more difficult. Many distinguishing features present in source code, e.g. variable names, are removed in the compilation process, and compiler optimization may alter the structure of a program, further obscuring features that are known to be useful in determining authorship. We examine programmer de-anonymization from the standpoint of machine learning, using a novel set of features that include ones obtained by decompiling the executable binary to source code. We adapt a powerful set of techniques from the domain of source code authorship attribution along with stylistic representations embedded in assembly, resulting in successful de-anonymization of a large set of programmers. We evaluate our approach on data from the Google Code Jam, obtaining attribution accuracy of up to 96\% with 100 and 83\% with 600 candidate programmers. We present an executable binary authorship attribution approach, for the first time, that is robust to basic obfuscations, a range of compiler optimization settings, and binaries that have been stripped of their symbol tables. We perform programmer de-anonymization using both obfuscated binaries, and real-world code found "in the wild" in single-author GitHub repositories and the recently leaked Nulled.IO hacker forum. We show that programmers who would like to remain anonymous need to take extreme countermeasures to protect their privacy.},
  journal = {arXiv preprint arXiv:1512.08546},
  year = {2015},
  month = {12},
  eprint = {1512.08546},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {15 pages},
  updated = {2017-12-18T00:18:42Z},
  published = {2015-12-28T22:28:51Z},
  url = {https://arxiv.org/abs/1512.08546v3},
  pdf = {https://arxiv.org/pdf/1512.08546v3},
  doi = {10.14722/ndss.2018.23304},
}

@article{arXiv:1809.11037,
  title = {A Systematic Study on Static Control Flow Obfuscation Techniques in Java},
  author = {Renuka Kumar and Anjana Mariam Kurian},
  abstract = {Control flow obfuscation (CFO) alters the control flow path of a program without altering its semantics. Existing literature has proposed several techniques; however, a quick survey reveals a lack of clarity in the types of techniques proposed, and how many are unique. What is also unclear is whether there is a disparity in the theory and practice of CFO. In this paper, we systematically study CFO techniques proposed for Java programs, both from papers and commercially available tools. We evaluate 13 obfuscators using a dataset of 16 programs with varying software characteristics, and different obfuscator parameters. Each program is carefully reverse engineered to study the effect of obfuscation. Our study reveals that there are 36 unique techniques proposed in the literature and 7 from tools. Three of the most popular commercial obfuscators implement only 13 of the 36 techniques in the literature. Thus there appears to be a gap between the theory and practice of CFO. We propose a novel classification of the obfuscation techniques based on the underlying component of a program that is transformed. We identify the techniques that are potent against reverse engineering attacks, both from the perspective of a human analyst and an automated program decompiler. Our analysis reveals that majority of the tools do not implement these techniques, thus defeating the protection obfuscation offers. We furnish examples of select techniques and discuss our findings. To the best of our knowledge, we are the first to assemble such a research. This study will be useful to software designers to decide upon the best techniques to use based upon their needs, for researchers to understand the state-of-the-art and for commercial obfuscator developers to develop new techniques.},
  journal = {arXiv preprint arXiv:1809.11037},
  year = {2018},
  month = {09},
  eprint = {1809.11037},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {20 pages, 3 tables},
  updated = {2018-09-28T14:05:05Z},
  published = {2018-09-28T14:05:05Z},
  url = {https://arxiv.org/abs/1809.11037v1},
  pdf = {https://arxiv.org/pdf/1809.11037v1},
}

@article{arXiv:2410.22677,
  title = {Is Function Similarity Over-Engineered? Building a Benchmark},
  author = {Rebecca Saul and Chang Liu and Noah Fleischmann and Richard Zak and Kristopher Micinski and Edward Raff and James Holt},
  abstract = {Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSE-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple basline, one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing, is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.},
  journal = {arXiv preprint arXiv:2410.22677},
  year = {2024},
  month = {10},
  eprint = {2410.22677},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.CR},
  comment = {To appear in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks},
  updated = {2024-10-30T03:59:46Z},
  published = {2024-10-30T03:59:46Z},
  url = {https://arxiv.org/abs/2410.22677v1},
  pdf = {https://arxiv.org/pdf/2410.22677v1},
}

@article{arXiv:1603.05495,
  title = {Polymorphic Type Inference for Machine Code},
  author = {Matthew Noonan and Alexey Loginov and David Cok},
  abstract = {For many compiled languages, source-level types are erased very early in the compilation process. As a result, further compiler passes may convert type-safe source into type-unsafe machine code. Type-unsafe idioms in the original source and type-unsafe optimizations mean that type information in a stripped binary is essentially nonexistent. The problem of recovering high-level types by performing type inference over stripped machine code is called type reconstruction, and offers a useful capability in support of reverse engineering and decompilation. In this paper, we motivate and develop a novel type system and algorithm for machine-code type inference. The features of this type system were developed by surveying a wide collection of common source- and machine-code idioms, building a catalog of challenging cases for type reconstruction. We found that these idioms place a sophisticated set of requirements on the type system, inducing features such as recursively-constrained polymorphic types. Many of the features we identify are often seen only in expressive and powerful type systems used by high-level functional languages. Using these type-system features as a guideline, we have developed Retypd: a novel static type-inference algorithm for machine code that supports recursive types, polymorphism, and subtyping. Retypd yields more accurate inferred types than existing algorithms, while also enabling new capabilities such as reconstruction of pointer const annotations with 98\% recall. Retypd can operate on weaker program representations than the current state of the art, removing the need for high-quality points-to information that may be impractical to compute.},
  journal = {arXiv preprint arXiv:1603.05495},
  year = {2016},
  month = {03},
  eprint = {1603.05495},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.LO},
  comment = {Full version with appendices, for PLDI 2016},
  updated = {2016-03-18T20:09:56Z},
  published = {2016-03-17T14:09:14Z},
  url = {https://arxiv.org/abs/1603.05495v2},
  pdf = {https://arxiv.org/pdf/1603.05495v2},
}

@article{arXiv:1404.2697,
  title = {To Share or Not to Share in Client-Side Encrypted Clouds},
  author = {Duane Wilson and Giuseppe Ateniese},
  abstract = {With the advent of cloud computing, a number of cloud providers have arisen to provide Storage-as-a-Service (SaaS) offerings to both regular consumers and business organizations. SaaS (different than Software-as-a-Service in this context) refers to an architectural model in which a cloud provider provides digital storage on their own infrastructure. Three models exist amongst SaaS providers for protecting the confidentiality data stored in the cloud: 1) no encryption (data is stored in plain text), 2) server-side encryption (data is encrypted once uploaded), and 3) client-side encryption (data is encrypted prior to upload). This paper seeks to identify weaknesses in the third model, as it claims to offer 100\% user data confidentiality throughout all data transactions (e.g., upload, download, sharing) through a combination of Network Traffic Analysis, Source Code Decompilation, and Source Code Disassembly. The weaknesses we uncovered primarily center around the fact that the cloud providers we evaluated were each operating in a Certificate Authority capacity to facilitate data sharing. In this capacity, they assume the role of both certificate issuer and certificate authorizer as denoted in a Public-Key Infrastructure (PKI) scheme - which gives them the ability to view user data contradicting their claims of 100\% data confidentiality. We have collated our analysis and findings in this paper and explore some potential solutions to address these weaknesses in these sharing methods. The solutions proposed are a combination of best practices associated with the use of PKI and other cryptographic primitives generally accepted for protecting the confidentiality of shared information.},
  journal = {arXiv preprint arXiv:1404.2697},
  year = {2014},
  month = {04},
  eprint = {1404.2697},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.DC},
  updated = {2014-11-20T01:21:15Z},
  published = {2014-04-10T05:22:32Z},
  url = {https://arxiv.org/abs/1404.2697v2},
  pdf = {https://arxiv.org/pdf/1404.2697v2},
  note = {Journal reference: Information Security, Lecture Notes in Computer Science Volume 8783, 2014, pp 401-412},
}

@article{arXiv:2505.16366,
  title = {ReCopilot: Reverse Engineering Copilot in Binary Analysis},
  author = {Guoqiang Chen and Huiqi Sun and Daguang Liu and Zhiqi Wang and Qiang Wang and Bin Yin and Lu Liu and Lingyun Ying},
  abstract = {Binary analysis plays a pivotal role in security domains such as malware detection and vulnerability discovery, yet it remains labor-intensive and heavily reliant on expert knowledge. General-purpose large language models (LLMs) perform well in programming analysis on source code, while binaryspecific LLMs are underexplored. In this work, we present ReCopilot, an expert LLM designed for binary analysis tasks. ReCopilot integrates binary code knowledge through a meticulously constructed dataset, encompassing continue pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO) stages. It leverages variable data flow and call graph to enhance context awareness and employs test-time scaling to improve reasoning capabilities. Evaluations on a comprehensive binary analysis benchmark demonstrate that ReCopilot achieves state-of-the-art performance in tasks such as function name recovery and variable type inference on the decompiled pseudo code, outperforming both existing tools and LLMs by 13\%. Our findings highlight the effectiveness of domain-specific training and context enhancement, while also revealing challenges in building super long chain-of-thought. ReCopilot represents a significant step toward automating binary analysis with interpretable and scalable AI assistance in this domain.},
  journal = {arXiv preprint arXiv:2505.16366},
  year = {2025},
  month = {05},
  eprint = {2505.16366},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2025-05-22T08:21:39Z},
  published = {2025-05-22T08:21:39Z},
  url = {https://arxiv.org/abs/2505.16366v1},
  pdf = {https://arxiv.org/pdf/2505.16366v1},
}

@article{arXiv:2303.04477,
  title = {Graph Neural Networks Enhanced Smart Contract Vulnerability Detection of Educational Blockchain},
  author = {Zhifeng Wang and Wanxuan Wu and Chunyan Zeng and Jialong Yao and Yang Yang and Hongmin Xu},
  abstract = {With the development of blockchain technology, more and more attention has been paid to the intersection of blockchain and education, and various educational evaluation systems and E-learning systems are developed based on blockchain technology. Among them, Ethereum smart contract is favored by developers for its ``event-triggered" mechanism for building education intelligent trading systems and intelligent learning platforms. However, due to the immutability of blockchain, published smart contracts cannot be modified, so problematic contracts cannot be fixed by modifying the code in the educational blockchain. In recent years, security incidents due to smart contract vulnerabilities have caused huge property losses, so the detection of smart contract vulnerabilities in educational blockchain has become a great challenge. To solve this problem, this paper proposes a graph neural network (GNN) based vulnerability detection for smart contracts in educational blockchains. Firstly, the bytecodes are decompiled to get the opcode. Secondly, the basic blocks are divided, and the edges between the basic blocks according to the opcode execution logic are added. Then, the control flow graphs (CFG) are built. Finally, we designed a GNN-based model for vulnerability detection. The experimental results show that the proposed method is effective for the vulnerability detection of smart contracts. Compared with the traditional approaches, it can get good results with fewer layers of the GCN model, which shows that the contract bytecode and GCN model are efficient in vulnerability detection.},
  journal = {arXiv preprint arXiv:2303.04477},
  year = {2023},
  month = {03},
  eprint = {2303.04477},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.LG},
  comment = {8 pages, 8 figures},
  updated = {2023-03-08T09:58:58Z},
  published = {2023-03-08T09:58:58Z},
  url = {https://arxiv.org/abs/2303.04477v1},
  pdf = {https://arxiv.org/pdf/2303.04477v1},
}

@article{arXiv:2509.06052,
  title = {Empirical Study of Code Large Language Models for Binary Security Patch Detection},
  author = {Qingyuan Li and Binchang Li and Cuiyun Gao and Shuzheng Gao and Zongjie Li},
  abstract = {Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \\textbf\{19,448\} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \\textbf\{19\} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation.},
  journal = {arXiv preprint arXiv:2509.06052},
  year = {2025},
  month = {09},
  eprint = {2509.06052},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI cs.CR},
  updated = {2025-09-07T13:31:43Z},
  published = {2025-09-07T13:31:43Z},
  url = {https://arxiv.org/abs/2509.06052v1},
  pdf = {https://arxiv.org/pdf/2509.06052v1},
}

@article{arXiv:2511.11439,
  title = {Retrofit: Continual Learning with Bounded Forgetting for Security Applications},
  author = {Yiling He and Junchi Lei and Hongyu She and Shuo Shao and Xinran Zheng and Yiping Liu and Zhan Qin and Lorenzo Cavallaro},
  abstract = {Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference. We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2\% to 38.6\% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.},
  journal = {arXiv preprint arXiv:2511.11439},
  year = {2025},
  month = {11},
  eprint = {2511.11439},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  categories = {cs.LG cs.AI},
  updated = {2025-11-14T16:07:03Z},
  published = {2025-11-14T16:07:03Z},
  url = {https://arxiv.org/abs/2511.11439v1},
  pdf = {https://arxiv.org/pdf/2511.11439v1},
}

@article{arXiv:2504.05002,
  title = {SmartBugBert: BERT-Enhanced Vulnerability Detection for Smart Contract Bytecode},
  author = {Jiuyang Bu and Wenkai Li and Zongwei Li and Zeng Zhang and Xiaoqi Li},
  abstract = {Smart contracts deployed on blockchain platforms are vulnerable to various security vulnerabilities. However, only a small number of Ethereum contracts have released their source code, so vulnerability detection at the bytecode level is crucial. This paper introduces SmartBugBert, a novel approach that combines BERT-based deep learning with control flow graph (CFG) analysis to detect vulnerabilities directly from bytecode. Our method first decompiles smart contract bytecode into optimized opcode sequences, extracts semantic features using TF-IDF, constructs control flow graphs to capture execution logic, and isolates vulnerable CFG fragments for targeted analysis. By integrating both semantic and structural information through a fine-tuned BERT model and LightGBM classifier, our approach effectively identifies four critical vulnerability types: transaction-ordering, access control, self-destruct, and timestamp dependency vulnerabilities. Experimental evaluation on 6,157 Ethereum smart contracts demonstrates that SmartBugBert achieves 90.62\% precision, 91.76\% recall, and 91.19\% F1-score, significantly outperforming existing detection methods. Ablation studies confirm that the combination of semantic features with CFG information substantially enhances detection performance. Furthermore, our approach maintains efficient detection speed (0.14 seconds per contract), making it practical for large-scale vulnerability assessment.},
  journal = {arXiv preprint arXiv:2504.05002},
  year = {2025},
  month = {04},
  eprint = {2504.05002},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2026-01-31T04:35:47Z},
  published = {2025-04-07T12:30:12Z},
  url = {https://arxiv.org/abs/2504.05002v2},
  pdf = {https://arxiv.org/pdf/2504.05002v2},
}

@article{arXiv:2601.16681,
  title = {From Transactions to Exploits: Automated PoC Synthesis for Real-World DeFi Attacks},
  author = {Xing Su and Hao Wu and Hanzhong Liang and Yunlin Jiang and Yuxi Cheng and Yating Liu and Fengyuan Xu},
  abstract = {Blockchain systems are increasingly targeted by on-chain attacks that exploit contract vulnerabilities to extract value rapidly and stealthily, making systematic analysis and reproduction highly challenging. In practice, reproducing such attacks requires manually crafting proofs-of-concept (PoCs), a labor-intensive process that demands substantial expertise and scales poorly. In this work, we present the first automated framework for synthesizing verifiable PoCs directly from on-chain attack executions. Our key insight is that attacker logic can be recovered from low-level transaction traces via trace-driven reverse engineering, and then translated into executable exploits by leveraging the code-generation capabilities of large language models (LLMs). To this end, we propose TracExp, which localizes attack-relevant execution contexts from noisy, multi-contract traces and introduces a novel dual-decompiler to transform concrete executions into semantically enriched exploit pseudocode. Guided by this representation, TracExp synthesizes PoCs and refines them to preserve exploitability-relevant semantics. We evaluate TracExp on 321 real-world attacks over the past 20 months. TracExp successfully synthesizes PoCs for 93\% of incidents, with 58.78\% being directly verifiable, at an average cost of only \\\$0.07 per case. Moreover, TracExp enabled the release of a large number of previously unavailable PoCs to the community, earning a \$900 bounty and demonstrating strong practical impact.},
  journal = {arXiv preprint arXiv:2601.16681},
  year = {2026},
  month = {01},
  eprint = {2601.16681},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  comment = {14 pages, 10 figures},
  updated = {2026-01-23T11:52:50Z},
  published = {2026-01-23T11:52:50Z},
  url = {https://arxiv.org/abs/2601.16681v1},
  pdf = {https://arxiv.org/pdf/2601.16681v1},
}

@article{arXiv:2003.05039,
  title = {Devil is Virtual: Reversing Virtual Inheritance in C++ Binaries},
  author = {Rukayat Ayomide Erinfolami and Aravind Prakash},
  abstract = {Complexities that arise from implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious to virtual inheritance. Conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10\\\% on Linux and 12.5\\\% on Windows) of real-world C++ programs including Mysql and libstdc++. (2) we show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These false positves and negatives either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95\\\% and 95.5\\\% (GCC -O0) and a minimum of 77.5\\\% and 73.8\\\% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.},
  journal = {arXiv preprint arXiv:2003.05039},
  year = {2020},
  month = {03},
  eprint = {2003.05039},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {Accepted at CCS20. This is a technical report version},
  updated = {2020-06-03T18:11:28Z},
  published = {2020-03-10T23:51:39Z},
  url = {https://arxiv.org/abs/2003.05039v2},
  pdf = {https://arxiv.org/pdf/2003.05039v2},
}

@article{arXiv:0903.2199,
  title = {On the Generation of Test Data for Prolog by Partial Evaluation},
  author = {Miguel Gomez-Zamalloa and Elvira Albert and German Puebla},
  abstract = {In recent work, we have proposed an approach to Test Data Generation (TDG) of imperative bytecode by partial evaluation (PE) of CLP which consists in two phases: (1) the bytecode program is first transformed into an equivalent CLP program by means of interpretive compilation by PE, (2) a second PE is performed in order to supervise the generation of test-cases by execution of the CLP decompiled program. The main advantages of TDG by PE include flexibility to handle new coverage criteria, the possibility to obtain test-case generators and its simplicity to be implemented. The approach in principle can be directly applied for TDG of any imperative language. However, when one tries to apply it to a declarative language like Prolog, we have found as a main difficulty the generation of test-cases which cover the more complex control flow of Prolog. Essentially, the problem is that an intrinsic feature of PE is that it only computes non-failing derivations while in TDG for Prolog it is essential to generate test-cases associated to failing computations. Basically, we propose to transform the original Prolog program into an equivalent Prolog program with explicit failure by partially evaluating a Prolog interpreter which captures failing derivations w.r.t. the input program. Another issue that we discuss in the paper is that, while in the case of bytecode the underlying constraint domain only manipulates integers, in Prolog it should properly handle the symbolic data manipulated by the program. The resulting scheme is of interest for bringing the advantages which are inherent in TDG by PE to the field of logic programming.},
  journal = {arXiv preprint arXiv:0903.2199},
  year = {2009},
  month = {03},
  eprint = {0903.2199},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL cs.SE},
  comment = {Paper presented at the 18th Workshop on Logic-based Methods in Programming Environments (WLPE2008) (Report-No: WLPE/2008). Paper submitted by a co-editor of the Workshop proceedings},
  updated = {2009-03-12T15:51:25Z},
  published = {2009-03-12T15:51:25Z},
  url = {https://arxiv.org/abs/0903.2199v1},
  pdf = {https://arxiv.org/pdf/0903.2199v1},
}

@article{arXiv:2602.06325,
  title = {Identifying Adversary Tactics and Techniques in Malware Binaries with an LLM Agent},
  author = {Zhou Xuan and Xiangzhe Xu and Mingwei Zheng and Louis Zheng-Hua Tan and Jinyao Guo and Tiantai Zhang and Le Yu and Chengpeng Wang and Xiangyu Zhang},
  abstract = {Understanding TTPs (Tactics, Techniques, and Procedures) in malware binaries is essential for security analysis and threat intelligence, yet remains challenging in practice. Real-world malware binaries are typically stripped of symbols, contain large numbers of functions, and distribute malicious behavior across multiple code regions, making TTP attribution difficult. Recent large language models (LLMs) offer strong code understanding capabilities, but applying them directly to this task faces challenges in identifying analysis entry points, reasoning under partial observability, and misalignment with TTP-specific decision logic. We present TTPDetect, the first LLM agent for recognizing TTPs in stripped malware binaries. TTPDetect combines dense retrieval with LLM-based neural retrieval to narrow the space of analysis entry points. TTPDetect further employs a function-level analyzing agent consisting of a Context Explorer that performs on-demand, incremental context retrieval and a TTP-Specific Reasoning Guideline that achieves inference-time alignment. We build a new dataset that labels decompiled functions with TTPs across diverse malware families and platforms. TTPDetect achieves 93.25\% precision and 93.81\% recall on function-level TTP recognition, outperforming baselines by 10.38\% and 18.78\%, respectively. When evaluated on real world malware samples, TTPDetect recognizes TTPs with a precision of 87.37\%. For malware with expert-written reports, TTPDetect recovers 85.7\% of the documented TTPs and further discovers, on average, 10.5 previously unreported TTPs per malware.},
  journal = {arXiv preprint arXiv:2602.06325},
  year = {2026},
  month = {02},
  eprint = {2602.06325},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  updated = {2026-02-06T02:42:48Z},
  published = {2026-02-06T02:42:48Z},
  url = {https://arxiv.org/abs/2602.06325v1},
  pdf = {https://arxiv.org/pdf/2602.06325v1},
}

@article{arXiv:2601.21253,
  title = {CovAgent: Overcoming the 30\% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation},
  author = {Wei Minn and Biniam Fisseha Demissie and Yan Naing Tun and Jiakun Liu and Mariano Ceccato and Lwin Khin Shar and David Lo},
  abstract = {Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30\% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1\%, 116.3\% and 179.7\% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.},
  journal = {arXiv preprint arXiv:2601.21253},
  year = {2026},
  month = {01},
  eprint = {2601.21253},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {Under Review; We open-source CovAgent at https://osf.io/vxgwm/?view\_only=3edf2abe5a12419fa6413933da851847},
  updated = {2026-01-29T04:21:11Z},
  published = {2026-01-29T04:21:11Z},
  url = {https://arxiv.org/abs/2601.21253v1},
  pdf = {https://arxiv.org/pdf/2601.21253v1},
}

@article{arXiv:2306.14168,
  title = {FastBCSD: Fast and Efficient Neural Network for Binary Code Similarity Detection},
  author = {Chensen Huang and Guibo Zhu and Guojing Ge and Taihao Li and Jinqiao Wang},
  abstract = {Binary code similarity detection (BCSD) has various applications, including but not limited to vulnerability detection, plagiarism detection, and malware detection. Previous research efforts mainly focus on transforming binary code to assembly code strings using reverse compilation and then using pre-trained deep learning models with large parameters to obtain feature representation vector of binary code. While these models have proven to be effective in representing binary code, their large parameter size leads to considerable computational expenses during both training and inference. In this paper, we present a lightweight neural network, called FastBCSD, that employs a dynamic instruction vector encoding method and takes only assembly code as input feature to achieve comparable accuracy to the pre-training models while reducing the computational resources and time cost. On the BinaryCorp dataset, our method achieves a similar average MRR score to the state-of-the-art pre-training-based method (jTrans), while on the BinaryCorp 3M dataset, our method even outperforms the latest technology by 0.01. Notably, FastBCSD has a much smaller parameter size (13.4M) compared to jTrans (87.88M), and its latency time is 1/5 of jTrans on NVIDIA GTX 1080Ti.},
  journal = {arXiv preprint arXiv:2306.14168},
  year = {2023},
  month = {06},
  eprint = {2306.14168},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2023-06-25T08:22:10Z},
  published = {2023-06-25T08:22:10Z},
  url = {https://arxiv.org/abs/2306.14168v1},
  pdf = {https://arxiv.org/pdf/2306.14168v1},
}

