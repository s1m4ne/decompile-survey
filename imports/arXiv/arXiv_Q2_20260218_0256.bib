@article{arXiv:2304.04658,
  title = {GraphBinMatch: Graph-based Similarity Learning for Cross-Language Binary and Source Code Matching},
  author = {Ali TehraniJamsaz and Hanze Chen and Ali Jannesari},
  abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate our approach performance on single-language binary-to-source code matching. Experimental results show that GraphBinMatch outperforms state-of-the-art significantly, with improvements as high as 15\% over the F1 score.},
  journal = {arXiv preprint arXiv:2304.04658},
  year = {2023},
  month = {04},
  eprint = {2304.04658},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2023-04-10T15:36:31Z},
  published = {2023-04-10T15:36:31Z},
  url = {https://arxiv.org/abs/2304.04658v1},
  pdf = {https://arxiv.org/pdf/2304.04658v1},
}

@article{arXiv:2407.06375,
  title = {Macaw: A Machine Code Toolbox for the Busy Binary Analyst},
  author = {Ryan G. Scott and Brett Boston and Benjamin Davis and Iavor Diatchki and Mike Dodds and Joe Hendrix and Daniel Matichuk and Kevin Quick and Tristan Ravitch and Valentin Robert and Benjamin Selfridge and Andrei Stefănescu and Daniel Wagner and Simon Winwood},
  abstract = {When attempting to understand the behavior of an executable, a binary analyst can make use of many different techniques. These include program slicing, dynamic instrumentation, binary-level rewriting, symbolic execution, and formal verification, all of which can uncover insights into how a piece of machine code behaves. As a result, there is no one-size-fits-all binary analysis tool, so a binary analysis researcher will often combine several different tools. Sometimes, a researcher will even need to design new tools to study problems that existing frameworks are not well equipped to handle. Designing such tools from complete scratch is rarely time- or cost-effective, however, given the scale and complexity of modern ISAs. We present Macaw, a modular framework that makes it possible to rapidly build reliable binary analysis tools across a range of use cases. Statically typed functional programming techniques are used pervasively throughout Macaw -- these range from using functional optimization passes to encoding tricky architectural invariants at the type level to statically check correctness properties. The level of assurance that functional programming ideas afford us allow us to iterate rapidly on Macaw while still having confidence that the underlying semantics are correct. Over a decade of development, we have used Macaw to support an industrial research team in building tools for machine code-related tasks. As such, the name 'Macaw' refers not just to the framework, but also a suite of tools that are built on top of it. We describe Macaw in depth and describe the different static and dynamic analyses that it performs, many powered by an SMT-based symbolic execution engine. We put a particular focus on interoperability between machine code and higher-level languages, including binary lifting from x86 to LLVM, as well verifying the correctness of mixed C and assembly code.},
  journal = {arXiv preprint arXiv:2407.06375},
  year = {2024},
  month = {07},
  eprint = {2407.06375},
  archivePrefix = {arXiv},
  primaryClass = {cs.PL},
  categories = {cs.PL},
  updated = {2025-02-19T01:35:57Z},
  published = {2024-07-08T20:36:34Z},
  url = {https://arxiv.org/abs/2407.06375v3},
  pdf = {https://arxiv.org/pdf/2407.06375v3},
}

@article{arXiv:2009.00956,
  title = {CcNav: Understanding Compiler Optimizations in Binary Code},
  author = {Sabin Devkota and Pascal Aschwanden and Adam Kunen and Matthew Legendre and Katherine E. Isaacs},
  abstract = {Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.},
  journal = {arXiv preprint arXiv:2009.00956},
  year = {2020},
  month = {09},
  eprint = {2009.00956},
  archivePrefix = {arXiv},
  primaryClass = {cs.HC},
  categories = {cs.HC},
  comment = {IEEE VIS VAST 2020},
  updated = {2020-11-07T08:18:45Z},
  published = {2020-09-02T11:24:19Z},
  url = {https://arxiv.org/abs/2009.00956v2},
  pdf = {https://arxiv.org/pdf/2009.00956v2},
  doi = {10.1109/TVCG.2020.3030357},
}

@article{arXiv:2405.19581,
  title = {Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases},
  author = {Zian Su and Xiangzhe Xu and Ziyang Huang and Kaiyuan Zhang and Xiangyu Zhang},
  abstract = {Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3\% relative gain in CHRF and a 16.7\% relative gain in a GPT4-based metric for summarization, as well as a 6.7\% and 7.4\% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.},
  journal = {arXiv preprint arXiv:2405.19581},
  year = {2024},
  month = {05},
  eprint = {2405.19581},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI cs.CL},
  updated = {2024-10-30T16:12:36Z},
  published = {2024-05-30T00:17:44Z},
  url = {https://arxiv.org/abs/2405.19581v2},
  pdf = {https://arxiv.org/pdf/2405.19581v2},
}

@article{arXiv:2505.07360,
  title = {BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models},
  author = {Xiuwei Shang and Guoqiang Chen and Shaoyin Cheng and Benlong Wu and Li Hu and Gangyang Li and Weiming Zhang and Nenghai Yu},
  abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
  journal = {arXiv preprint arXiv:2505.07360},
  year = {2025},
  month = {05},
  eprint = {2505.07360},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  comment = {23 pages, 5 figures, to be published in IJCAI 2025},
  updated = {2025-05-12T08:54:07Z},
  published = {2025-05-12T08:54:07Z},
  url = {https://arxiv.org/abs/2505.07360v1},
  pdf = {https://arxiv.org/pdf/2505.07360v1},
}

@article{arXiv:2509.14646,
  title = {SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation},
  author = {Yongpan Wang and Xin Xu and Xiaojie Zhu and Xiaodong Gu and Beijun Shen},
  abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose \\saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of \\saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed \\salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare \\saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that \\saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\\\% TCP rate on Decompile-Eval with a 10.6\\\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
  journal = {arXiv preprint arXiv:2509.14646},
  year = {2025},
  month = {09},
  eprint = {2509.14646},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.PL},
  comment = {13 pages, 7 figures},
  updated = {2025-09-18T05:57:15Z},
  published = {2025-09-18T05:57:15Z},
  url = {https://arxiv.org/abs/2509.14646v1},
  pdf = {https://arxiv.org/pdf/2509.14646v1},
}

@article{arXiv:2201.07420,
  title = {Cross-Language Binary-Source Code Matching with Intermediate Representations},
  author = {Yi Gui and Yao Wan and Hongyu Zhang and Huifang Huang and Yulei Sui and Guandong Xu and Zhiyuan Shao and Hai Jin},
  abstract = {Binary-source code matching plays an important role in many security and software engineering related tasks such as malware detection, reverse engineering and vulnerability assessment. Currently, several approaches have been proposed for binary-source code matching by jointly learning the embeddings of binary code and source code in a common vector space. Despite much effort, existing approaches target on matching the binary code and source code written in a single programming language. However, in practice, software applications are often written in different programming languages to cater for different requirements and computing platforms. Matching binary and source code across programming languages introduces additional challenges when maintaining multi-language and multi-platform applications. To this end, this paper formulates the problem of cross-language binary-source code matching, and develops a new dataset for this new problem. We present a novel approach XLIR, which is a Transformer-based neural network by learning the intermediate representations for both binary and source code. To validate the effectiveness of XLIR, comprehensive experiments are conducted on two tasks of cross-language binary-source code matching, and cross-language source-source code matching, on top of our curated dataset. Experimental results and analysis show that our proposed XLIR with intermediate representations significantly outperforms other state-of-the-art models in both of the two tasks.},
  journal = {arXiv preprint arXiv:2201.07420},
  year = {2022},
  month = {01},
  eprint = {2201.07420},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  comment = {SANER2022},
  updated = {2022-01-19T05:17:02Z},
  published = {2022-01-19T05:17:02Z},
  url = {https://arxiv.org/abs/2201.07420v1},
  pdf = {https://arxiv.org/pdf/2201.07420v1},
}

@article{arXiv:1512.08546,
  title = {When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries},
  author = {Aylin Caliskan and Fabian Yamaguchi and Edwin Dauber and Richard Harang and Konrad Rieck and Rachel Greenstadt and Arvind Narayanan},
  abstract = {The ability to identify authors of computer programs based on their coding style is a direct threat to the privacy and anonymity of programmers. While recent work found that source code can be attributed to authors with high accuracy, attribution of executable binaries appears to be much more difficult. Many distinguishing features present in source code, e.g. variable names, are removed in the compilation process, and compiler optimization may alter the structure of a program, further obscuring features that are known to be useful in determining authorship. We examine programmer de-anonymization from the standpoint of machine learning, using a novel set of features that include ones obtained by decompiling the executable binary to source code. We adapt a powerful set of techniques from the domain of source code authorship attribution along with stylistic representations embedded in assembly, resulting in successful de-anonymization of a large set of programmers. We evaluate our approach on data from the Google Code Jam, obtaining attribution accuracy of up to 96\% with 100 and 83\% with 600 candidate programmers. We present an executable binary authorship attribution approach, for the first time, that is robust to basic obfuscations, a range of compiler optimization settings, and binaries that have been stripped of their symbol tables. We perform programmer de-anonymization using both obfuscated binaries, and real-world code found "in the wild" in single-author GitHub repositories and the recently leaked Nulled.IO hacker forum. We show that programmers who would like to remain anonymous need to take extreme countermeasures to protect their privacy.},
  journal = {arXiv preprint arXiv:1512.08546},
  year = {2015},
  month = {12},
  eprint = {1512.08546},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {15 pages},
  updated = {2017-12-18T00:18:42Z},
  published = {2015-12-28T22:28:51Z},
  url = {https://arxiv.org/abs/1512.08546v3},
  pdf = {https://arxiv.org/pdf/1512.08546v3},
  doi = {10.14722/ndss.2018.23304},
}

@article{arXiv:2003.12901,
  title = {liOS: Lifting iOS apps for fun and profit},
  author = {Julian Schütte and Dennis Titze},
  abstract = {Although iOS is the second most popular mobile operating system and is often considered the more secure one, approaches to automatically analyze iOS applications are scarce and generic app analysis frameworks do not exist. This is on the one hand due to the closed ecosystem putting obstacles in the way of reverse engineers and on the other hand due to the complexity of reverse engineering and analyzing app binaries. Reliably lifting accurate call graphs, control flows, and data dependence graphs from binary code, as well as reconstructing object-oriented high-level concepts is a non-trivial task and the choice of the lifted target representation determines the analysis capabilities. None of the various existing intermediate representations is a perfect fit for all types of analysis, while the detection of vulnerabilities requires techniques ranging from simple pattern matching to complex inter-procedural data flow analyses. We address this gap by introducing liOS, a binary lifting and analysis framework for iOS applications that extracts lifted information from several frontends and unifies them in a "supergraph" representation that tolerates missing parts and is further extended and interlinked by liOS "passes". A static analysis of the binary is then realized in the form of graph traversal queries, which can be considered as an advancement of classic program query languages. We illustrate this approach by means of a typical JavaScript/Objective-C bridge, which can lead to remote code execution in iOS applications.},
  journal = {arXiv preprint arXiv:2003.12901},
  year = {2020},
  month = {03},
  eprint = {2003.12901},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2020-03-28T22:30:12Z},
  published = {2020-03-28T22:30:12Z},
  url = {https://arxiv.org/abs/2003.12901v1},
  pdf = {https://arxiv.org/pdf/2003.12901v1},
}

@article{arXiv:2301.13346,
  title = {Icicle: A Re-Designed Emulator for Grey-Box Firmware Fuzzing},
  author = {Michael Chesser and Surya Nepal and Damith C. Ranasinghe},
  abstract = {Emulation-based fuzzers enable testing binaries without source code, and facilitate testing embedded applications where automated execution on the target hardware architecture is difficult and slow. The instrumentation techniques added to extract feedback and guide input mutations towards generating effective test cases is at the core of modern fuzzers. But, modern emulation-based fuzzers have evolved by re-purposing general-purpose emulators; consequently, developing and integrating fuzzing techniques, such as instrumentation methods, are difficult and often added in an ad-hoc manner, specific to an instruction set architecture (ISA). This limits state-of-the-art fuzzing techniques to few ISAs such as x86/x86-64 or ARM/AArch64; a significant problem for firmware fuzzing of diverse ISAs. This study presents our efforts to re-think emulation for fuzzing. We design and implement a fuzzing-specific, multi-architecture emulation framework -- Icicle. We demonstrate the capability to add instrumentation once, in an architecture agnostic manner, with low execution overhead. We employ Icicle as the emulator for a state-of-the-art ARM firmware fuzzer -- Fuzzware -- and replicate results. Significantly, we demonstrate the availability of new instrumentation in Icicle enabled the discovery of new bugs. We demonstrate the fidelity of Icicle and efficacy of architecture agnostic instrumentation by discovering LAVA-M benchmark bugs, requiring a known and specific operational capability of instrumentation techniques, across a diverse set of instruction set architectures (x86-64, ARM/AArch64, RISC-V, MIPS). Further, to demonstrate the effectiveness of Icicle to discover bugs in a currently unsupported architecture in emulation-based fuzzers, we perform a fuzzing campaign with real-world MSP430 firmware binaries and discovered 7 new bugs.},
  journal = {arXiv preprint arXiv:2301.13346},
  year = {2023},
  month = {01},
  eprint = {2301.13346},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  comment = {Accepted ISSTA 2023. Code: https://github.com/icicle-emu/icicle},
  updated = {2023-06-21T06:03:14Z},
  published = {2023-01-31T00:32:29Z},
  url = {https://arxiv.org/abs/2301.13346v2},
  pdf = {https://arxiv.org/pdf/2301.13346v2},
}

@article{arXiv:2209.03441,
  title = {Same Coverage, Less Bloat: Accelerating Binary-only Fuzzing with Coverage-preserving Coverage-guided Tracing},
  author = {Stefan Nagy and Anh Nguyen-Tuong and Jason D. Hiser and Jack W. Davidson and Matthew Hicks},
  abstract = {Coverage-guided fuzzing's aggressive, high-volume testing has helped reveal tens of thousands of software security flaws. While executing billions of test cases mandates fast code coverage tracing, the nature of binary-only targets leads to reduced tracing performance. A recent advancement in binary fuzzing performance is Coverage-guided Tracing (CGT), which brings orders-of-magnitude gains in throughput by restricting the expense of coverage tracing to only when new coverage is guaranteed. Unfortunately, CGT suits only a basic block coverage granularity -- yet most fuzzers require finer-grain coverage metrics: edge coverage and hit counts. It is this limitation which prohibits nearly all of today's state-of-the-art fuzzers from attaining the performance benefits of CGT. This paper tackles the challenges of adapting CGT to fuzzing's most ubiquitous coverage metrics. We introduce and implement a suite of enhancements that expand CGT's introspection to fuzzing's most common code coverage metrics, while maintaining its orders-of-magnitude speedup over conventional always-on coverage tracing. We evaluate their trade-offs with respect to fuzzing performance and effectiveness across 12 diverse real-world binaries (8 open- and 4 closed-source). On average, our coverage-preserving CGT attains near-identical speed to the present block-coverage-only CGT, UnTracer; and outperforms leading binary- and source-level coverage tracers QEMU, Dyninst, RetroWrite, and AFL-Clang by 2-24x, finding more bugs in less time.},
  journal = {arXiv preprint arXiv:2209.03441},
  year = {2022},
  month = {09},
  eprint = {2209.03441},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR cs.SE},
  comment = {CCS '21: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  updated = {2022-09-07T19:58:32Z},
  published = {2022-09-07T19:58:32Z},
  url = {https://arxiv.org/abs/2209.03441v1},
  pdf = {https://arxiv.org/pdf/2209.03441v1},
  doi = {10.1145/3460120.3484787},
}

