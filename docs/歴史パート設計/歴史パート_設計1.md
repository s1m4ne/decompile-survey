以下は「デコンパイル（decompilation）の歴史」をテーマに、**再現性が高く・査読で突っ込まれにくい**サーベイ論文にするための、**推奨構成**と**論文選定方法（プロトコル）**です。最初に、こちらで短時間の調査として「歴史の大枠（重要マイルストーン）」を把握した上で、既存の“歴史を扱うサーベイ／SoK／SLR”の型を取り込み、最終形の構成と選定法を設計します（下記）。

---

## 1. まず「デコンパイル史」の大枠（短時間で把握した内容）

歴史サーベイで揉めやすいのは「どこまでをデコンパイルと呼ぶか（機械語 vs バイトコード、擬似コード vs 再コンパイル可能コード）」なので、まず流れを **(A) 技術基盤（コンパイラ理論）→ (B) 逆コンパイル研究 → (C) 実用デコンパイラ普及 → (D) 評価・ベンチマーク → (E) LLM/データ駆動** のように分解して押さえるのが安全です。

### 技術的マイルストーン（“デコンパイラの中核アルゴリズム”の源流）

* **1970**: CFG（制御フローグラフ）と区間（interval）など、制御フロー解析の古典的枠組みが整理される（後の構造化・領域分解にも影響）。 ([ACM Digital Library][1])
* **1980**: CFGの**構造解析（structural analysis）**が提案され、構造化（while/if などへの復元）に繋がる流れの基礎になる。 ([ScienceDirect][2])

### “逆コンパイル/デコンパイル研究”としての代表的な流れ

* **1994**: Cifuentes の博士論文が「逆コンパイル（reverse compilation）技術」を体系化した代表的資料として頻繁に参照される。 ([yurichev.com][3])
* **1997**: Java バイトコードのデコンパイル（Javaソース復元）についての研究が早期に議論され、バイトコード領域は「型情報が残る」など機械語とは異なる性質で発展。 ([USENIX][4])
* **2001**: 低水準コードの**証明指向（proof-directed）**のデコンパイルのように、PL（プログラミング言語）側の理論に寄った系統の研究も出てくる。 ([スプリンガー][5])
* **2004〜2007**: 実デコンパイラ（例：Boomerang 系）でSSAなどの表現をデコンパイルに使う議論がまとまり、研究と実装が近づく。例として Van Emmerik の “SSA for Decompilation” など。 ([yurichev.com][6])

### 実用（商用・OSS）デコンパイラ普及の節目

* **2007**: Hex-Rays Decompiler がβ→1.0公開の節目（実務での“Cっぽい擬似コード”文化を強く加速）。 ([Hex-Rays][7])
* **2013**: セキュリティ文脈での “native x86 decompilation” がUSENIX Securityに出て、構造化や意味保存を強く意識した研究が目立つ（研究コミュニティ側の節目として扱いやすい）。 ([USENIX][8])
* **2015**: 「gotoを減らす」など**構造化（structuring）**を正面から扱う研究が目立つ（例：NDSS “No More Gotos”）。 ([NDSS Symposium][9])
* **2017（末）**: RetDec がオープンソース化されたことが、研究再現性や比較実験の面で重要な節目として扱える。 ([botconf.eu][10])

### 2020年代：評価枠組み／人間中心／LLM駆動

* **2022**: “人間がどうデコンパイルするか”をデータ化する研究（人間中心・評価の方向）。 ([USENIX][11])
* **2023**: デコンパイラ出力に対するユーザ調査で改善点を抽出する研究（実務ニーズを測る）。 ([NDSS Symposium][12])
* **2024**: デコンパイラの忠実性（fidelity）問題の分類や、構造化アルゴリズム評価が進む（“評価可能性”が論文化される）。 ([USENIX][13])
* **2025**: 大規模ベンチマークや評価フレームワーク提案が増え、比較可能性が上がる（例：DecompileBench/Decompile-Bench のような流れ）。 ([arXiv][14])

> 注意：1960年代〜80年代の“デコンパイラという道具”の歴史は一次資料が散っており、ウェブ上の年表（コミュニティwiki等）は便利ですが、査読では「一次ソースは？」と突っ込まれやすいです。なので後述の方法で、**wiki年表は“手がかり”扱い（探索のseed）**に落とすのが安全です。 ([decompilation.wiki][15])

---

## 2. 既存の“歴史・体系化”サーベイから借りるべき型

あなたの目的（歴史サーベイ＋突っ込まれ耐性）だと、**「歴史叙述」×「Systematization（SoK）」×「Systematic Literature Review（SLR）」** のハイブリッドが強いです。

* **歴史サーベイの型（定義→歴史→現状→将来）**の例：Data Reverse Engineering の歴史サーベイ（WCRE 2000）。この手の構成は「歴史を扱う論文」として受け入れられやすい。 ([gnu.org][16])
* **SoK（体系化）**の型：disassembly など近接領域で「何が分かっていて何が弱いか」を整理するスタイルが確立。あなたの論文でも“デコンパイルの各段（type recovery / structuring / SSA / etc）”を整理するのに有効。 ([arXiv][17])
* **SLR（再現性の型）**：検索式、DB、除外基準、PRISMAフロー、snowballing手順を明示する。ソフトウェア工学ではKitchenham系のガイドラインが定番で、snowballingはWohlinがよく引用される。 ([ScienceDirect][18])
* **PRISMA 2020**：本来は医療系起源ですが、“何件ヒット→何件除外→理由”の透明性はレビュー耐性を上げやすい（CSでも採用例は増えています）。 ([BMJ][19])
* 参考：最近の大規模SLRの書きぶり（例：LLM for SE のSLR）。「手順の書き方」見本として便利。 ([arXiv][20])

---

## 3. 推奨する論文の全体構成（査読で突っ込まれにくい形）

ここからが本題で、**歴史を主題にしつつ“技術サーベイとしても成立”**させる構成案です。
（歴史だけの叙述にすると「ただの年表」で弱くなりがちなので、技術段の体系化を入れます。）

### タイトル案

* **SoK: The History and Evolution of Decompilation**
* **A Systematic and Historical Survey of Decompilation: From Reverse Compilation to LLM-Augmented Decompilers**

### 0. Abstract

* 何を「デコンパイル」と定義したか（scope）
* 何を体系化したか（timeline + taxonomy）
* どうやって文献を集めたか（SLR要約：DB/期間/件数/除外）

### 1. Introduction

* 研究質問（RQs）を明示（例）

  * RQ1: デコンパイル技術はどの課題（構造化・型復元・意味復元等）を軸にどう発展したか？
  * RQ2: 実用デコンパイラの節目（商用・OSS）は研究潮流にどう影響したか？
  * RQ3: 評価（ベンチマーク/ユーザ調査/再コンパイル可能性）はいつ・どう整備されたか？
* 貢献：

  1. 年代別の歴史叙述
  2. 技術段のtaxonomy
  3. 文献データセット（再現性の核）

### 2. Scope & Definitions

* 対象を明確化（ここが一番レビューで効きます）

  * native machine-code decompilation（x86/ARM等）
  * bytecode decompilation（JVM/.NET等）
  * “pseudocode” と “recompilable decompilation” を区別
* 関連領域との境界：disassembly / binary lifting / binary translation との差

### 3. Methodology (SLR + Historical Evidence)

ここを**査読耐性の柱**にします（後述の選定方法をそのまま記載）。

* データソース（ACM/IEEE/USENIX/NDSS/Springer/arXiv…）
* 検索式（完全公開）
* スクリーニング（含む/除外基準）
* snowballing（後方/前方）手順
* 重複排除方法
* 品質評価（Quality assessment）
* データ抽出スキーマ（どの項目を抜くか）
* 再現性パッケージ（Zenodo/GitHubで公開するもの）

### 4. A Chronological History of Decompilation

**年代で語る章**（でも“技術課題”と必ず紐付ける）

* 4.1 Prehistory (〜1990s): コンパイラ理論・構造化の源流（CFG解析、interval/structural analysis）
* 4.2 Foundational era (1990s): reverse compilationとしての体系化、初期の研究デコンパイラ
* 4.3 Practical expansion (2000s): 実用ツールの成熟（商用・OSS）、IR/SSA利用
* 4.4 Security-driven era (2010s): セキュリティ会議での発展（構造化・意味保存・スケール）
* 4.5 Data/LLM era (2020s): ベンチマーク・人間中心・LLM補助

### 5. Technical Taxonomy (Pipeline view)

歴史章と独立に、**デコンパイラの工程を分解**して整理（SoKっぽい章）

* CFG recovery / disassembly dependency
* Lifting / IR設計
* Data-flow, SSA, expression recovery
* Control-flow structuring（goto/irreducible対応）
* Type recovery（構造体/配列/ポインタ）
* Symbol/identifier recovery, readability, deobfuscation
* “Correctness”の定義（semantic equivalence / recompilable / human usefulness）

### 6. Tools & Ecosystem (Evidence-based)

* 学術実装：RetDec など（論文・リポジトリ・アーティファクト）
* 商用ツール：Hex-Rays 等（リリース史は一次資料で裏取り）
* プラットフォーム化：Ghidra等（可能なら一次資料で節目を確定）

### 7. Evaluation & Benchmarks (How we measure progress)

* 何が“良いデコンパイル”か（fidelity taxonomy、ユーザ調査、再コンパイル可能性）
* ベンチマークの歴史（いつから比較可能になったか）
* 人間中心評価（RE作業の観測）

### 8. Open Problems & Future Directions

* 依然として難しい問題（irreducible CFG、最適化、型/変数名、ABI、例外処理…）
* LLMはどこに効き、どこが危ないか（評価・再現性を絡める）

### 9. Threats to Validity

* 選定バイアス、灰色文献の扱い、商用ツールの再現性問題
* 1960〜80年代の一次資料欠落リスク、など

### 10. Conclusion

### Appendix（強く推奨）

* A. 検索式（DBごとに完全一致で載せる）
* B. PRISMAフローダイアグラム
* C. 収集した文献一覧（BibTeX/CSV）
* D. 抽出項目テンプレ（data extraction form）
* E. 年表（timeline table、証拠レベル付き）

---

## 4. 論文選定方法（再現性を最大化するプロトコル案）

ここは「あなたがそのまま論文のMethod章に貼れる」粒度で書きます。

### 4.1 事前に固定するもの（“後出し”を防ぐ）

* **カットオフ日**：例）2026-02-11（今日）までに公開されているもの
* **対象言語**：原則英語（日本語資料は「ツール史の一次資料」に限り例外、など）
* **対象の定義**：

  * “decompilation” を **低水準→高水準への復元**と定義し、最低限「制御/データの復元」を扱うもの
  * disassemblyのみ、liftingのみは原則除外（ただし歴史上の基盤として例外枠を作る）

### 4.2 データソース（DB）を複線化

最低限このセットだと突っ込まれにくいです（分野の分散が効く）：

* ACM Digital Library
* IEEE Xplore
* USENIX（Security等）
* NDSS（The Internet Society）
* SpringerLink（ESOP等PL系が入る）
* arXiv（近年のLLM/ベンチマークが多い）
* 追加（可能なら）：Scopus / Web of Science（網羅性の根拠に使える）

※「どのDBを使ったか」は再現性の中心なので、論文内と付録で固定します。

### 4.3 検索式（例：再現可能なBooleanを先に決める）

**キーワードは“用語揺れ”が最大の落とし穴**です。最低限これを含めるのが安全：

* コア語：decompil* OR "reverse compil*" OR "source recovery" OR "pseudo-code"
* 機械語系：binary OR "machine code" OR x86 OR ARM OR "native code"
* 工程語：structur* OR "control flow" OR "type reconstruction" OR "type inference" OR SSA OR "intermediate representation" OR lifting

例（一般形）：

> (decompil* OR "reverse compil*" OR "source recovery") AND (binary OR "machine code" OR bytecode)

加えて、**バイトコード系の別クエリ**を分ける（混ぜるとノイズが増える）：

> (decompil* AND (bytecode OR JVM OR ".NET" OR "C#"))

### 4.4 スクリーニング手順（PRISMAで可視化）

PRISMA 2020の流れをCS向けに流用し、**件数と除外理由を必ず残す**。 ([BMJ][19])

**段階**

1. **Identification**：DB検索で候補収集（検索日とクエリをログ化）
2. **Deduplication**：DOI/タイトル/著者/年で重複排除
3. **Title/Abstract screening**：含む/除外の一次判定
4. **Full-text screening**：本文で最終判定
5. **Included**：最終コーパス確定

### 4.5 Snowballing（“歴史サーベイ”では必須）

歴史はDB検索だけだと取りこぼしが出やすいので、**snowballingを最初から必須工程にする**のが査読耐性に効きます。Wohlinのガイドラインがよく引用されます。 ([wohlin.eu][21])

* **Backward snowballing**：採択論文の参考文献を辿る
* **Forward snowballing**：採択論文を引用している文献を辿る

実務上は、まず **seed set（種論文）** を作るのがコツです。seedには、分野の節目になっているもの（例：Cifuentes 1994、Brumley 2013、Yakdan 2015、RetDec など）を入れると効率が良いです。 ([yurichev.com][3])

### 4.6 含む基準 / 除外基準（例）

**Include**

* 低水準（機械語/バイトコード）→高水準（C/Java/C#等）復元を主要課題として扱う
* 具体的な手法提案・評価・実装・ベンチマーク・ユーザ研究
* ピアレビュー論文、博士論文、主要会議のプロシーディング
* （歴史章用の例外）主要ツールの一次資料（公式リリースノート等）

**Exclude**

* 逆アセンブルのみ／CFG recoveryのみで“decompilation”に踏み込まない
* バイナリ解析一般（シンボリック実行等）でデコンパイルが主題でない
* 教科書的解説だけで新規知見のない記事（ただし歴史一次資料なら別枠）

### 4.7 品質評価（Quality Assessment）

SLRでよくやるチェックを、デコンパイル向けに最小限カスタムします：

* Q1: 問題設定とスコープが明確か
* Q2: 手法が再現可能な粒度で記述されているか
* Q3: 評価があるか（比較・ベンチ・定性的でも）
* Q4: データ/コード/ツールが公開されているか（アーティファクト）
* Q5: 制約（対象ISA、最適化、言語、ABI）が明記されているか

→ 各論文に0/1/2点で付け、付録に出すと突っ込まれにくいです。

### 4.8 “歴史”特有の工夫：証拠レベル（Evidence tiers）を導入

歴史サーベイは、商用ツールやwiki年表など**灰色文献**をどう扱うかが地雷です。そこで最初から：

* Tier 1: ピアレビュー論文、学位論文（一次・最強）
* Tier 2: 公式ドキュメント／公式ブログ／リリースノート（ツール史の一次資料）
* Tier 3: コミュニティwiki、個人ブログ（手がかり。単独では事実認定しない）

のように宣言し、**Tier 3は“探索のseed”としてのみ利用**すると明記します。
（例：コミュニティ年表は便利だが、最終的には論文/公式資料で裏取りする方針。） ([decompilation.wiki][15])

---

## 5. 最初に作ると強い「再現性パッケージ」（査読対策の即効薬）

以下をGitHub/Zenodo等で公開できる形にしておくと、査読での突っ込みが激減しやすいです。

* `search_log/`：DB別の検索式、検索日、ヒット件数（CSV）
* `dedup/`：重複排除ルール（DOI優先など）
* `screening/`：除外理由コード表（例：E1=not decompilation, E2=disassembly only…）
* `prisma/`：PRISMAフロー図（PRISMA 2020 テンプレでも可） ([PRISMA statement][22])
* `extraction_form.xlsx`：抽出項目テンプレ
* `corpus.bib`：最終採択文献のBibTeX
* `timeline.csv`：年表（出来事、根拠、Tier、文献ID）

---

## 6. すぐ使える「seed set」候補（最初の雪だるま起点）

「歴史」を語る上で節目になりやすく、snowballingの起点として強いものを挙げます（あなたの論文の“背景理解”にも直結）：

* Cifuentes（1994, PhD）reverse compilationの体系化 ([yurichev.com][3])
* Proebsting（1997, USENIX）Java bytecode decompilation ([USENIX][4])
* Katsumata & Ohori（2001, ESOP）proof-directed decompilation ([スプリンガー][5])
* Brumley et al.（2013, USENIX Security）native x86 decompilation ([USENIX][8])
* Yakdan et al.（2015, NDSS）No More Gotos（構造化） ([NDSS Symposium][9])
* RetDec（2017頃〜）OSS化（比較実験・再現性の節目） ([botconf.eu][10])
* Hex-Rays decompiler（2007公開の節目を一次資料で確定可能） ([Hex-Rays][7])
* dewolf（2023）ユーザ調査（実務評価の軸） ([NDSS Symposium][12])
* Dramko（2024）fidelity issue taxonomy（評価観点の整理） ([USENIX][13])

---

## 7. まとめ：あなた向けの“最適解”はこれ

あなたの要件（歴史＋査読耐性＋再現性）に対しては、

* **本文の骨格**：
  **「歴史（年代別）」＋「技術工程のtaxonomy（SoK）」＋「評価/ベンチマーク史」**
* **方法の骨格**：
  **SLR（Kitchenham系）＋ snowballing（Wohlin）＋ PRISMA可視化 ＋ 証拠レベル（Tier）**

のハイブリッドが最も安全で強いです。 ([ScienceDirect][18])

---

もし次に進めるなら、あなたの代わりに（このチャット内で）

1. **研究質問（RQs）の確定版（3〜5個）**
2. **検索式（ACM/IEEE/USENIX/Springer/arXiv用に最適化した具体形）**
3. **抽出項目テンプレ（CSV/Excel項目案）**
4. **年表の“章立て”（Era分割案＋各Eraで必ず入れる論点）**
   まで、今の設計をさらに「そのまま論文に貼れる」形に落とし込めます。

[1]: https://dl.acm.org/doi/pdf/10.1145/390013.808479?utm_source=chatgpt.com "Control flow analysis"
[2]: https://www.sciencedirect.com/science/article/pii/0096055180900077/pdf?_valck=1&md5=cf0090fceea2f849220c73ed5391992a&pid=1-s2.0-0096055180900077-main.pdf&utm_source=chatgpt.com "STRUCTURAL ANALYSIS: A NEW APPROACH TO FLOW ..."
[3]: https://yurichev.com/mirrors/DCC_decompilation_thesis.pdf?utm_source=chatgpt.com "Reverse Compilation Techniques by Cristina Cifuentes"
[4]: https://www.usenix.org/publications/library/proceedings/coots97/full_papers/proebsting2/proebsting2.pdf?utm_source=chatgpt.com "Decompilation in Java (Does Bytecode Reveal Source?)"
[5]: https://link.springer.com/chapter/10.1007/3-540-45309-1_23?utm_source=chatgpt.com "Proof-Directed De-compilation of Low-Level Code | SpringerLink"
[6]: https://yurichev.com/mirrors/vanEmmerik_ssa.pdf?utm_source=chatgpt.com "Static Single Assignment for Decompilation"
[7]: https://hex-rays.com/blog/ida-celebrating-30-years-of-binary-analysis-innovation?utm_source=chatgpt.com "IDA: celebrating 30 years of binary analysis innovation"
[8]: https://www.usenix.org/system/files/conference/usenixsecurity13/sec13-paper_schwartz.pdf?utm_source=chatgpt.com "Preserving Structural Analysis and Iterative Control-Flow ..."
[9]: https://www.ndss-symposium.org/wp-content/uploads/2017/09/11_4_2.pdf?utm_source=chatgpt.com "No More Gotos: Decompilation Using Pattern-Independent ..."
[10]: https://www.botconf.eu/wp-content/uploads/formidable/2/2017-KroustekMatulaZemek-retdec-slides-botconf-2017.pdf?utm_source=chatgpt.com "RetDec: An Open-Source Machine-Code Decompiler"
[11]: https://www.usenix.org/system/files/sec22-burk.pdf?utm_source=chatgpt.com "How Humans Decompile and What We Can Learn From It"
[12]: https://www.ndss-symposium.org/wp-content/uploads/2023/03/bar2023-23001-paper.pdf?utm_source=chatgpt.com "dewolf: Improving Decompilation by leveraging User Surveys"
[13]: https://www.usenix.org/system/files/usenixsecurity24-dramko.pdf?utm_source=chatgpt.com "A Taxonomy of C Decompiler Fidelity Issues"
[14]: https://arxiv.org/html/2505.11340v1?utm_source=chatgpt.com "A Comprehensive Benchmark for Evaluating Decompilers ..."
[15]: https://decompilation.wiki/decompilers/dec-history-1/?utm_source=chatgpt.com "History of Decompilation (1960-2007)"
[16]: https://www.gnu.org/software/ferret/project/docs/dre_historical.pdf?utm_source=chatgpt.com "Data Reverse Engineering: A Historical Survey"
[17]: https://arxiv.org/abs/2007.14266?utm_source=chatgpt.com "SoK: All You Ever Wanted to Know About x86/x64 Binary Disassembly But Were Afraid to Ask"
[18]: https://www.sciencedirect.com/science/article/abs/pii/S0950584908001390?utm_source=chatgpt.com "Systematic literature reviews in software engineering"
[19]: https://www.bmj.com/content/372/bmj.n71?utm_source=chatgpt.com "The PRISMA 2020 statement: an updated guideline for ..."
[20]: https://arxiv.org/html/2308.10620v6?utm_source=chatgpt.com "Large Language Models for Software Engineering"
[21]: https://www.wohlin.eu/ease14.pdf?utm_source=chatgpt.com "Guidelines for Snowballing in Systematic Literature Studies ..."
[22]: https://www.prisma-statement.org/prisma-2020-flow-diagram?utm_source=chatgpt.com "PRISMA 2020 flow diagram"
