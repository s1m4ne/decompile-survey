
@inproceedings{wiedemeier_walking_2025,
	address = {New York, NY, USA},
	series = {{CCS} '25},
	title = {Walking {The} {Last} {Mile}: {Studying} {Decompiler} {Output} {Correction} in {Practice}},
	isbn = {979-8-4007-1525-9},
	url = {https://doi.org/10.1145/3719027.3765040},
	doi = {10.1145/3719027.3765040},
	abstract = {The increasing prevalence of Python has spurred interest in decompiling Python PYC bytecode. This work presents the first large-scale study on human-assisted Python decompilation in the wild, leveraging extensive data from pylingual.io, spanning 181,646 PYC binaries, 9,003 user-submitted patches, and 393 accuracy-verified patches. We investigate how reverse engineers respond to inaccurate decompilation and identify factors influencing their efforts to achieve accurate decompilation. We complement this unprecedented observational data with a controlled user study that isolates the technical difficulty of patching imperfect Python decompilations.By contrasting real-world patching behavior with that of the controlled setting, we discover that reversers' decision to repair a decompilation result is more strongly driven by the semantic content of the program (e.g., malware binaries or malicious tools) than by the technical difficulty of the patch. That is, a reverser's motivation is more important than their expertise.Our study reveals common patterns observed in the patching process, including how users approached the patching task, the types of errors they encountered, and the strategies they employed to resolve them. We also examine the strengths and limitations of assistive tools in the pursuit of perfect decompilation. Our findings offer unique insights into the practical dynamics of human-decompiler interaction, providing actionable recommendations for integrating human intelligence into the decompilation workflow and demonstrating the research potential of reliable decompilation accuracy verification.},
	booktitle = {Proceedings of the 2025 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Wiedemeier, Joshua and Klancher, Simon and Flores, Joel and Zheng, Max and Park, Jaehyun and Cha, Sang Kil and Jee, Kangkook},
	year = {2025},
	keywords = {decompilation, reverse engineering, malware, user study},
	pages = {2489--2503},
}

@inproceedings{giesen_poster_2025,
	address = {New York, NY, USA},
	series = {{CCS} '25},
	title = {Poster: {Code} {HarvETHter}: {Corpus}-{Driven} {Decompilation} of {Ethereum} {Smart} {Contracts}},
	isbn = {979-8-4007-1525-9},
	url = {https://doi.org/10.1145/3719027.3760714},
	doi = {10.1145/3719027.3760714},
	abstract = {This poster introduces HarvETHter, a smart contract decompiler for EVM-based platforms such as Ethereum, Binance, and Polygon. We present the corpus completeness hypothesis, which we investigate through HarvETHter. Relying on our hypothesis, HarvETHter sources knowledge of the Ethereum blockchain and leverages it to decompile smart contracts to Solidity source code.},
	booktitle = {Proceedings of the 2025 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Giesen, Jens-Rene and Scholz, Christian and Davi, Lucas},
	year = {2025},
	keywords = {decompilation, security, provenance \&amp, smart contracts},
	pages = {4752--4754},
}

@article{dramko_fast_2025,
	address = {New York, NY, USA},
	title = {Fast, {Fine}-{Grained} {Equivalence} {Checking} for {Neural} {Decompilers}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3772368},
	doi = {10.1145/3772368},
	abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models are generally inadequate, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign,1 a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides—which parts of the functions are equivalent and how well the variable names match—is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Dramko, Luke and Le Goues, Claire and Schwartz, Edward J.},
	month = oct,
	year = {2025},
	keywords = {Program Analysis, Alignment, Program Equivalence},
	annote = {Just Accepted},
}

@inproceedings{soni_benchmarking_2025,
	address = {New York, NY, USA},
	series = {{SURE} '25},
	title = {Benchmarking {Binary} {Type} {Inference} {Techniques} in {Decompilers}},
	isbn = {979-8-4007-1910-3},
	url = {https://doi.org/10.1145/3733822.3764675},
	doi = {10.1145/3733822.3764675},
	abstract = {Decompilation is the process of translating low-level, machine-executable code back into a high-level representation. Decompilers–tools that perform this translation–are essential for reverse engineers and security professionals, supporting critical tasks within their workflows. However, due to the inherent loss of information during compilation as a result of optimizations, inlining, and other compiler-specific transformations, decompiled output is often incomplete or inaccurate.A central challenge in decompilation is accurate type inference: the reconstruction of high-level type information for variables based on low-level code patterns and memory access behaviors. Despite ongoing advancements in decompilation research, there is a notable lack of comprehensive comparative studies evaluating the type inference capabilities of existing decompilers.This paper presents a benchmark study of five decompilers, focusing on their ability to infer types at both the function and variable levels. We conduct the evaluation on a dataset of binaries compiled from the Nixpkgs collection at both -O0 and -O2 optimization levels, allowing us to assess decompiler performance across unoptimized and optimized executables. The results highlight the relative strengths and weaknesses of each decompiler and identify recurring scenarios in which incorrect type information is produced.},
	booktitle = {Proceedings of the 2025 {Workshop} on {Software} {Understanding} and {Reverse} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Soni, Vedant and Dutcher, Audrey and Bao, Tiffany and Wang, Ruoyu},
	year = {2025},
	keywords = {Decompilation, Benchmark, Type Inference},
	pages = {48--60},
}

@inproceedings{lerner_ideco_2025,
	address = {New York, NY, USA},
	series = {{SURE} '25},
	title = {ideco: {A} {Framework} for {Improving} {Non}-{C} {Decompilation}},
	isbn = {979-8-4007-1910-3},
	url = {https://doi.org/10.1145/3733822.3764668},
	doi = {10.1145/3733822.3764668},
	abstract = {We introduce the ideco framework for improving the decompilation of non-C programming languages. ideco provides users with the ability to create rules which rewrite parts of the decompilation.We show that by using a small set of rules, the number of lines of decompiled code for binaries written in C++, Swift, Go, and Rust can be decreased by 5\% to 10\%. In addition, by using GPT-4o and GPT-4.1-mini as test subjects, we show that a reverse engineering task is easier to solve when its decompilation is processed by ideco.},
	booktitle = {Proceedings of the 2025 {Workshop} on {Software} {Understanding} and {Reverse} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Lerner, Sam},
	year = {2025},
	keywords = {Decompilation, Reverse Engineering, Software Understanding},
	pages = {17--40},
}

@inproceedings{sirlanci_empirical_2025,
	address = {New York, NY, USA},
	series = {{ASIA} {CCS} '25},
	title = {An {Empirical} {Study} of {C} {Decompilers}: {Performance} {Metrics} and {Error} {Taxonomy}},
	isbn = {979-8-4007-1410-8},
	url = {https://doi.org/10.1145/3708821.3733877},
	doi = {10.1145/3708821.3733877},
	abstract = {Decompilation aims to simplify reverse engineering by transforming binary code into a high-level representation, such as C-like code. To determine the current progress towards perfect decompilation, and to identify and quantify open problems for future work, we perform the first comprehensive empirical study on state-of-the-art C decompilers using our framework, DiscScope, which employs symbolic execution and differential analysis to spot discrepancies in decompilation at intermediate program states, pinpointing the exact location where the decompiler makes errors. Using DiscScope and a dataset we built containing benign and malicious real-world programs, we measure the current performance of decompilers in a realistic setting. Our dataset contains programs compiled with different compilers (GCC and Clang) and with 7 different optimization levels. Manual validation of DiscScope shows that it is 96.3\% accurate in identifying diverged and equivalent decompilation, which we then use to analyze 1,081,413 total decompiler outputs to build and quantify a taxonomy of open problems.},
	booktitle = {Proceedings of the 20th {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Sirlanci, Melih and Yagemann, Carter and Lin, Zhiqiang},
	year = {2025},
	keywords = {Decompilation, Binary Analysis, Symbolic Execution},
	pages = {1707--1723},
}

@article{udeshi_remend_2025,
	address = {New York, NY, USA},
	title = {{REMEND}: {Neural} {Decompilation} for {Reverse} {Engineering} {Math} {Equations} from {Binary} {Executables}},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3749988},
	doi = {10.1145/3749988},
	abstract = {Analysis of binary executables implementing mathematical equations can benefit from the reverse engineering of semantic information about the implementation. Traditional algorithmic reverse engineering tools either do not recover semantic information or rely on dynamic analysis and symbolic execution with high reverse engineering time. Algorithmic tools also require significant re-engineering effort to target new platforms and languages. Recently, neural methods for decompilation have been developed to recover human-like source code, but they do not extract semantic information explicitly. We develop REMEND, a neural decompilation framework to reverse engineer math equations from binaries to explicitly recover program semantics like data flow and order of operations. REMEND combines a transformer encoder-decoder model for neural decompilation with algorithmic processing for enhanced symbolic reasoning necessary for math equations. REMEND is the first work to demonstrate that transformers for neural decompilation go beyond source code and reason about program semantics in the form of math equations. We train on a synthetically generated dataset containing multiple implementations and compilations of math equations to produce a robust neural decompilation model and demonstrate retargettability. REMEND obtains an accuracy of 89.8\% to 92.4\% across three Instruction Set Architectures (ISAs), three optimization levels, and two programming languages with a single trained model, extending the capability of state-of-the-art neural decompilers. We achieve high accuracy with a small model of upto 12 million parameters and an average execution time of 0.132 seconds per function. On a real-world dataset collected from open-source programs, REMEND generalizes better than state-of-the-art neural decompilers despite being trained with synthetic data, achieving 8\% higher accuracy. The synthetic and real-world datasets are provided at .},
	journal = {ACM Trans. Intell. Syst. Technol.},
	publisher = {Association for Computing Machinery},
	author = {Udeshi, Meet and Krishnamurthy, Prashanth and Karri, Ramesh and Khorrami, Farshad},
	month = jul,
	year = {2025},
	keywords = {reverse engineering, math equations, neural decompilation},
	annote = {Just Accepted},
}

@article{wong_decllm_2025,
	address = {New York, NY, USA},
	title = {{DecLLM}: {LLM}-{Augmented} {Recompilable} {Decompilation} for {Enabling} {Programmatic} {Use} of {Decompiled} {Code}},
	volume = {2},
	url = {https://doi.org/10.1145/3728958},
	doi = {10.1145/3728958},
	abstract = {Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path. In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70\% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30\% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.},
	number = {ISSTA},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Wong, Wai Kin and Wu, Daoyuan and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
	month = jun,
	year = {2025},
	keywords = {Reverse Engineering, Large Language Model, Recompilable Decompilation},
}

@article{lagouvardos_incredible_2025,
	address = {New York, NY, USA},
	title = {The {Incredible} {Shrinking} {Context}... in a {Decompiler} {Near} {You}},
	volume = {2},
	url = {https://doi.org/10.1145/3728935},
	doi = {10.1145/3728935},
	abstract = {Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrnkr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrnkr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly “forgetting” control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5\% of contracts (compared to ∼95\% for Elipmoc), covers (i.e., reaches and manages to decompile) 67\% more code than Heimdall-rs, and reduces key imprecision metrics by over 65\%, compared again to Elipmoc.},
	number = {ISSTA},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Lagouvardos, Sifis and Bollanos, Yannis and Grech, Neville and Smaragdakis, Yannis},
	month = jun,
	year = {2025},
	keywords = {Decompilation, Program Analysis, Datalog, Ethereum, Smart Contracts},
	annote = {Full version of ISSTA 2025 paper},
}

@article{su_disco_2025,
	address = {New York, NY, USA},
	title = {{DiSCo}: {Towards} {Decompiling} {EVM} {Bytecode} to {Source} {Code} using {Large} {Language} {Models}},
	volume = {2},
	url = {https://doi.org/10.1145/3729373},
	doi = {10.1145/3729373},
	abstract = {Understanding the Ethereum smart contract bytecode is essential for ensuring cryptoeconomics security. However, existing decompilers primarily convert bytecode into pseudocode, which is not easily comprehensible for general users, potentially leading to misunderstanding of contract behavior and increased vulnerability to scams or exploits. In this paper, we propose DiSCo, the first LLMs-based EVM decompilation pipeline, which aims to enable LLMs to understand the opaque bytecode and lift it into smart contract code. DiSCo introduces three core technologies. First, a logic-invariant intermediate representation is proposed to reproject the low-level bytecode into high-level abstracted units. The second technique involves semantic enhancement based on a novel type-aware graph model to infer stripped variables during compilation, enhancing the lifting effect. The third technology is a flexible method incorporating code specifications to construct LLM-comprehensible prompts for source code generation. Extensive experiments illustrate that our generated code guarantees a high compilability rate at 75\%, with differential fuzzing pass rate averaging at 50\%. Manual validation results further indicate that the generated solidity contracts significantly outperforms baseline methods in tasks such as code comprehension and attack reproduction.},
	number = {FSE},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Su, Xing and Liang, Hanzhong and Wu, Hao and Niu, Ben and Xu, Fengyuan and Zhong, Sheng},
	month = jun,
	year = {2025},
	keywords = {Decompilation, EVM bytecode, Large Language Models, Smart Contract, Source Code Generation},
}

@inproceedings{chen_suigpt_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{SuiGPT} {MAD}: {Move} {AI} {Decompiler} to {Improve} {Transparency} and {Auditability} on {Non}-{Open}-{Source} {Blockchain} {Smart} {Contract}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714790},
	doi = {10.1145/3696410.3714790},
	abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33\% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization. Moreover, MAD's methodology could potentially extend to other smart contract languages, like Solidity, further enhancing Web3 transparency.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Wu, Tingguan and Wang, Siyun and Chalkias, Kostas Kryptos},
	year = {2025},
	keywords = {smart contract, auditing tools, large language models, move, prompt engineering, sui, transparency, web applications, web3},
	pages = {1567--1576},
}

@inproceedings{cotroneo_can_2025,
	address = {New York, NY, USA},
	series = {{EuroSec}'25},
	title = {Can {Neural} {Decompilation} {Assist} {Vulnerability} {Prediction} on {Binary} {Code}?},
	isbn = {979-8-4007-1563-1},
	url = {https://doi.org/10.1145/3722041.3723097},
	doi = {10.1145/3722041.3723097},
	abstract = {Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.},
	booktitle = {Proceedings of the 18th {European} {Workshop} on {Systems} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Cotroneo, Domenico and Grasso, Francesco C. and Natella, Roberto and Orbinato, Vittorio},
	year = {2025},
	keywords = {Security, Deep Learning, Binary Analysis, Neural Decompilation, Vulnerability Prediction},
	pages = {26--32},
}

@inproceedings{she_wadec_2024,
	address = {New York, NY, USA},
	series = {{ASE} '24},
	title = {{WaDec}: {Decompiling} {WebAssembly} {Using} {Large} {Language} {Model}},
	isbn = {979-8-4007-1248-7},
	url = {https://doi.org/10.1145/3691620.3695020},
	doi = {10.1145/3691620.3695020},
	abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art's 116.94\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
	year = {2024},
	pages = {481--492},
}

@inproceedings{cao_evaluating_2024,
	address = {New York, NY, USA},
	series = {{ISSTA} 2024},
	title = {Evaluating the {Effectiveness} of {Decompilers}},
	isbn = {979-8-4007-0612-7},
	url = {https://doi.org/10.1145/3650212.3652144},
	doi = {10.1145/3650212.3652144},
	abstract = {In software security tasks like malware analysis and vulnerability mining, reverse engineering is pivotal, with C decompilers playing a crucial role in understanding program semantics. However, reverse engineers still predominantly rely on assembly code rather than decompiled code when analyzing complex binaries. This practice underlines the limitations of current decompiled code, which hinders its effectiveness in reverse engineering. Identifying and analyzing the problems of existing decompilers and making targeted improvements can effectively enhance the efficiency of software analysis. In this study, we systematically evaluate current mainstream decompilers’ semantic consistency and readability. Semantic evaluation results show that the state-of-the-art decompiler Hex-Rays has about 55\% accuracy at almost all optimization, which contradicts the common belief among many reverse engineers that decompilers are usually accurate. Readability evaluation indicates that despite years of efforts to improve the readability of the decompiled code, decompilers’ template-based approach still predominantly yields code akin to binary structures rather than human coding patterns. Additionally, our human study indicates that to enhance decompilers’ accuracy and readability, introducing human or compiler-aware strategies like a speculate-verify-correct approach to obtain recompilable decompiled code and iteratively refine it to more closely resemble the original binary, potentially offers a more effective optimization method than relying on static analysis and rule expansion.},
	booktitle = {Proceedings of the 33rd {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Ying and Zhang, Runze and Liang, Ruigang and Chen, Kai},
	year = {2024},
	keywords = {Reverse Engineering, Decompiler, Software Testing},
	pages = {491--502},
}

@article{eom_r2i_2024,
	address = {New York, NY, USA},
	title = {{R2I}: {A} {Relative} {Readability} {Metric} for {Decompiled} {Code}},
	volume = {1},
	url = {https://doi.org/10.1145/3643744},
	doi = {10.1145/3643744},
	abstract = {Decompilation is a process of converting a low-level machine code snippet back into a high-level programming language such as C. It serves as a basis to aid reverse engineers in comprehending the contextual semantics of the code. In this respect, commercial decompilers like Hex-Rays have made significant strides in improving the readability of decompiled code over time. While previous work has proposed the metrics for assessing the readability of source code, including identifiers, variable names, function names, and comments, those metrics are unsuitable for measuring the readability of decompiled code primarily due to i) the lack of rich semantic information in the source and ii) the presence of erroneous syntax or inappropriate expressions. In response, to the best of our knowledge, this work first introduces R2I, the Relative Readability Index, a specialized metric tailored to evaluate decompiled code in a relative context quantitatively. In essence, R2I can be computed by i) taking code snippets across different decompilers as input and ii) extracting pre-defined features from an abstract syntax tree. For the robustness of R2I, we thoroughly investigate the enhancement efforts made by (non-)commercial decompilers and academic research to promote code readability, identifying 31 features to yield a reliable index collectively. Besides, we conducted a user survey to capture subjective factors such as one’s coding styles and preferences. Our empirical experiments demonstrate that R2I is a versatile metric capable of representing the relative quality of decompiled code (e.g., obfuscation, decompiler updates) and being well aligned with human perception in our survey.},
	number = {FSE},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Eom, Haeun and Kim, Dohee and Lim, Sori and Koo, Hyungjoon and Hwang, Sungjae},
	month = jul,
	year = {2024},
	keywords = {Decompiler, Code Metric, Code Readability, Decompiled Code},
}

@article{lu_understanding_2024,
	address = {New York, NY, USA},
	title = {Understanding and {Finding} {Java} {Decompiler} {Bugs}},
	volume = {8},
	url = {https://doi.org/10.1145/3649860},
	doi = {10.1145/3649860},
	abstract = {Java decompilers are programs that perform the reverse process of Java compilers, i.e., they translate Java bytecode to Java source code. They are essential for reverse engineering purposes and have become more sophisticated and reliable over the years. However, it remains challenging for modern Java decompilers to reliably perform correct decompilation on real-world programs. To shed light on the key challenges of Java decompilation, this paper provides the first systematic study on the characteristics and causes of bugs in mature, widely-used Java decompilers. We conduct the study by investigating 333 unique bugs from three popular Java decompilers. Our key findings and observations include: (1) Although most of the reported bugs were found when decompiling large, real-world code, 40.2\% of them have small test cases for bug reproduction; (2) Over 80\% of the bugs manifest as exceptions, syntactic errors, or semantic errors, and bugs with source code artifacts are very likely semantic errors; (3) 57.7\%,39.0\%, and 41.1\% of the bugs respectively are attributed to three stages of decompilers—loading structure entities from bytecode, optimizing these entities, and generating source code from these entities; (4) Bugs in decompilers’ type inference are the most complex to fix; and (5) Region restoration for structures like loop, sugaring for special structures like switch, and type inference of variables of generic types or indistinguishable types are the three most significant challenges in Java decompilation, which to some extent explains our findings in (3) and (4).Based on these findings, we present JD-Tester, a differential testing framework for Java decompilers, and our experience of using it in testing the three popular Java decompilers. JD-Tester utilizes different Java program generators to construct executable Java tests and finds exceptions, syntactic, and semantic inconsistencies (i.e. bugs) between a generated test and its compiled-decompiled version (through compilation and execution). In total, we have found 62 bugs in the three decompilers, demonstrating both the effectiveness of JD-Tester, and the importance of testing and validating Java decompilers.},
	number = {OOPSLA1},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Lu, Yifei and Hou, Weidong and Pan, Minxue and Li, Xuandong and Su, Zhendong},
	month = apr,
	year = {2024},
	keywords = {Reverse Engineering, Decompiler, Differential Testing},
}

@inproceedings{armengol-estape_slade_2024,
	series = {{CGO} '24},
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	isbn = {979-8-3503-9509-9},
	url = {https://doi.org/10.1109/CGO57630.2024.10444788},
	doi = {10.1109/CGO57630.2024.10444788},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect.This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code.We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	booktitle = {Proceedings of the 2024 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {IEEE Press},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F. P.},
	year = {2024},
	keywords = {decompilation, neural decompilation, language models, transformer, type inference},
	pages = {67--80},
}

@article{sisco_loop_2023,
	address = {New York, NY, USA},
	title = {Loop {Rerolling} for {Hardware} {Decompilation}},
	volume = {7},
	url = {https://doi.org/10.1145/3591237},
	doi = {10.1145/3591237},
	abstract = {We introduce the new problem of hardware decompilation. Analogous to software decompilation, hardware decompilation is about analyzing a low-level artifact—in this case a netlist, i.e., a graph of wires and logical gates representing a digital circuit—in order to recover higher-level programming abstractions, and using those abstractions to generate code written in a hardware description language (HDL). The overall problem of hardware decompilation requires a number of pieces. In this paper we focus on one specific piece of the puzzle: a technique we call hardware loop rerolling. Hardware loop rerolling leverages clone detection and program synthesis techniques to identify repeated logic in netlists (such as would be synthesized from loops in the original HDL code) and reroll them into syntactic loops in the recovered HDL code. We evaluate hardware loop rerolling for hardware decompilation over a set of hardware design benchmarks written in the PyRTL HDL and industry standard SystemVerilog. Our implementation identifies and rerolls loops in 52 out of 53 of the netlists in our benchmark suite, and we show three examples of how hardware decompilation can provide concrete benefits: transpilation between HDLs, faster simulation times over netlists (with mean speedup of 6x), and artifact compaction (39\% smaller on average).},
	number = {PLDI},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Sisco, Zachary D. and Balkind, Jonathan and Sherwood, Timothy and Hardekopf, Ben},
	month = jun,
	year = {2023},
	keywords = {hardware decompilation, loop rerolling, program synthesis},
}

@article{dramko_dire_2023,
	address = {New York, NY, USA},
	title = {{DIRE} and its {Data}: {Neural} {Decompiled} {Variable} {Renamings} with {Respect} to {Software} {Class}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3546946},
	doi = {10.1145/3546946},
	abstract = {The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE’s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23\% more frequently than the original DIRE model.},
	number = {2},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Dramko, Luke and Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Ed and Allamanis, Miltiadis and Neubig, Graham and Vasilescu, Bogdan and Le Goues, Claire},
	month = mar,
	year = {2023},
	keywords = {decompilation, Machine learning, data provenance},
}

@inproceedings{tan_splendid_2023,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2023},
	title = {{SPLENDID}: {Supporting} {Parallel} {LLVM}-{IR} {Enhanced} {Natural} {Decompilation} for {Interactive} {Development}},
	isbn = {978-1-4503-9918-0},
	url = {https://doi.org/10.1145/3582016.3582058},
	doi = {10.1145/3582016.3582058},
	abstract = {Manually writing parallel programs is difficult and error-prone. Automatic parallelization could address this issue, but profitability can be limited by not having facts known only to the programmer. A parallelizing compiler that collaborates with the programmer can increase the coverage and performance of parallelization while reducing the errors and overhead associated with manual parallelization. Unlike collaboration involving analysis tools that report program properties or make parallelization suggestions to the programmer, decompiler-based collaboration could leverage the strength of existing parallelizing compilers to provide programmers with a natural compiler-parallelized starting point for further parallelization or refinement. Despite this potential, existing decompilers fail to do this because they do not generate portable parallel source code compatible with any compiler of the source language. This paper presents SPLENDID, an LLVM-IR to C/OpenMP decompiler that enables collaborative parallelization by producing standard parallel OpenMP code. Using published manual parallelization of the PolyBench benchmark suite as a reference, SPLENDID's collaborative approach produces programs twice as fast as either Polly-based automatic parallelization or manual parallelization alone. SPLENDID's portable parallel code is also more natural than that from existing decompilers, obtaining a 39x higher average BLEU score.},
	booktitle = {Proceedings of the 28th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 3},
	publisher = {Association for Computing Machinery},
	author = {Tan, Zujun and Chon, Yebin and Kruse, Michael and Doerfert, Johannes and Xu, Ziyang and Homerding, Brian and Campanoni, Simone and August, David I.},
	year = {2023},
	keywords = {decompilation, collaborative parallelization},
	pages = {679--693},
}

@inproceedings{cao_boosting_2022,
	address = {New York, NY, USA},
	series = {{ACSAC} '22},
	title = {Boosting {Neural} {Networks} to {Decompile} {Optimized} {Binaries}},
	isbn = {978-1-4503-9759-9},
	url = {https://doi.org/10.1145/3564625.3567998},
	doi = {10.1145/3564625.3567998},
	abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21\% higher accuracy than state-of-the-art neural decompilation frameworks.},
	booktitle = {Proceedings of the 38th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei},
	year = {2022},
	pages = {508--518},
}

@inproceedings{gregorio_e-apk_2022,
	address = {New York, NY, USA},
	series = {{SBLP} '22},
	title = {E-{APK}: {Energy} {Pattern} {Detection} in {Decompiled} {Android} {Applications}},
	isbn = {978-1-4503-9744-5},
	url = {https://doi.org/10.1145/3561320.3561328},
	doi = {10.1145/3561320.3561328},
	abstract = {Energy efficiency is a non-functional requirement that developers must consider. This requirement is particularly relevant when building software for battery-operated devices like mobile ones: a long-lasting battery is an essential requirement for an enjoyable user experience. It has been shown that many mobile applications include inefficiencies that cause battery to be drained faster than necessary. Some of these inefficiencies result from software patterns that have been catalogued in the literature. The catalogues often provide more energy-efficient alternatives. While the related literature is vast, most approaches so far assume as a fundamental requirement that one has access to the source code of an application in order to be able to analyse it. This requirement makes independent energy analysis challenging, or even impossible, e.g. for a mobile user or, most significantly, an App Store trying to provide information on how efficient an application being submitted for publication is. Our work studies the viability of looking for known energy patterns in applications by decompiling them and analysing the resulting code. For this, we decompiled and analysed 236 open-source applications. We extended an existing tool to aid in this process, making it capable of seamlessly decompiling and analysing android applications. With the collected data, we performed a comparative analysis of the presence of energy patterns between the source code and the decompiled code. While further research is required to more assertively say if this type of static analysis is viable, our results point in a promising direction with 163 applications, approximately 69\%, containing the same number of detected patterns in both source code and the release APK.},
	booktitle = {Proceedings of the {XXVI} {Brazilian} {Symposium} on {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Gregório, Nelson and Fernandes, João Paulo and Bispo, João and Medeiros, Sérgio},
	year = {2022},
	keywords = {static analysis, android, code patterns, compilers, decompiler, energy efficiency, metaprogramming, mobile},
	pages = {50--58},
}

@article{grech_elipmoc_2022,
	address = {New York, NY, USA},
	title = {Elipmoc: advanced decompilation of {Ethereum} smart contracts},
	volume = {6},
	url = {https://doi.org/10.1145/3527321},
	doi = {10.1145/3527321},
	abstract = {Smart contracts on the Ethereum blockchain greatly benefit from cutting-edge analysis techniques and pose significant challenges. A primary challenge is the extremely low-level representation of deployed contracts. We present Elipmoc, a decompiler for the next generation of smart contract analyses. Elipmoc is an evolution of Gigahorse, the top research decompiler, dramatically improving over it and over other state-of-the-art tools, by employing several high-precision techniques and making them scalable. Among these techniques are a new kind of context sensitivity (termed “transactional sensitivity”) that provides a more effective static abstraction of distinct dynamic executions; a path-sensitive (yet scalable, through path merging) algorithm for inference of function arguments and returns; and a fully context sensitive private function reconstruction process. As a result, smart contract security analyses and reverse-engineering tools built on top of Elipmoc achieve high scalability, precision and completeness. Elipmoc improves over all notable past decompilers, including its predecessor, Gigahorse, and the state-of-the-art industrial tool, Panoramix, integrated into the primary Ethereum blockchain explorer, Etherscan. Elipmoc produces decompiled contracts with fully resolved operands at a rate of 99.5\% (compared to 62.8\% for Gigahorse), and achieves much higher completeness in code decompilation than Panoramix—e.g., up to 67\% more coverage of external call statements—while being over 5x faster. Elipmoc has been the enabler for recent (independent) discoveries of several exploitable vulnerabilities on popular protocols, over funds in the many millions of dollars.},
	number = {OOPSLA1},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Grech, Neville and Lagouvardos, Sifis and Tsatiris, Ilias and Smaragdakis, Yannis},
	month = apr,
	year = {2022},
	keywords = {Security, Decompilation, Program Analysis, Datalog, Ethereum, Smart Contracts, Blockchain},
}

@inproceedings{gussoni_comb_2020,
	address = {New York, NY, USA},
	series = {{ASIA} {CCS} '20},
	title = {A {Comb} for {Decompiled} {C} {Code}},
	isbn = {978-1-4503-6750-9},
	url = {https://doi.org/10.1145/3320269.3384766},
	doi = {10.1145/3320269.3384766},
	abstract = {Decompilers are fundamental tools to perform security assessments of third-party software. The quality of decompiled code can be a game changer in order to reduce the time and effort required for analysis. This paper proposes a novel approach to restructure the control flow graph recovered from binary programs in a semantics-preserving fashion. The algorithm is designed from the ground up with the goal of producing C code that is both goto-free and drastically reducing the mental load required for an analyst to understand it. As a result, the code generated with this technique is well-structured, idiomatic, readable, easy to understand and fully exploits the expressiveness of C language. The algorithm has been implemented on top of the revng static binary analysis framework. The resulting decompiler, revngc, is compared on real-world binaries with state-of-the-art commercial and open source tools. The results show that our decompilation process introduces between 40\% and 50\% less extra cyclomatic complexity.},
	booktitle = {Proceedings of the 15th {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Gussoni, Andrea and Di Federico, Alessandro and Fezzardi, Pietro and Agosta, Giovanni},
	year = {2020},
	keywords = {decompilation, reverse engineering, control flow restructuring, goto},
	pages = {637--651},
}

@inproceedings{liu_how_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {How far we have come: testing decompilation correctness of {C} decompilers},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3397370},
	doi = {10.1145/3395363.3397370},
	abstract = {A C decompiler converts an executable (the output from a C compiler) into source code. The recovered C source code, once recompiled, will produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications, including legacy software migration, security retrofitting, software comprehension, and to act as the first step in launching adversarial software exploitations. As the paramount component and the trust base in numerous cybersecurity tasks, C decompilers have enabled the analysis of malware, ransomware, and promoted cybersecurity professionals’ understanding of vulnerabilities in real-world systems. In contrast to this flourishing market, our observation is that in academia, outputs of C decompilers (i.e., recovered C source code) are still not extensively used. Instead, the intermediate representations are often more desired for usage when developing applications such as binary security retrofitting. We acknowledge that such conservative approaches in academia are a result of widespread and pessimistic views on the decompilation correctness. However, in conventional software engineering and security research, how much of a problem is, for instance, reusing a piece of simple legacy code by taking the output of modern C decompilers? In this work, we test decompilation correctness to present an up-to-date understanding regarding modern C decompilers. We detected a total of 1,423 inputs that can trigger decompilation errors from four popular decompilers, and with extensive manual effort, we identified 13 bugs in two open-source decompilers. Our findings show that the overly pessimistic view of decompilation correctness leads researchers to underestimate the potential of modern decompilers; the state-of-the-art decompilers certainly care about the functional correctness, and they are making promising progress. However, some tasks that have been studied for years in academia, such as type inference and optimization, still impede C decompilers from generating quality outputs more than is reflected in the literature. These issues rarely receive enough attention and can lead to great confusion that misleads users.},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhibo and Wang, Shuai},
	year = {2020},
	keywords = {Reverse Engineering, Decompiler, Software Testing},
	pages = {475--487},
}

@inproceedings{jaffe_meaningful_2018,
	address = {New York, NY, USA},
	series = {{ICPC} '18},
	title = {Meaningful variable names for decompiled code: a machine translation approach},
	isbn = {978-1-4503-5714-2},
	url = {https://doi.org/10.1145/3196321.3196330},
	doi = {10.1145/3196321.3196330},
	abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2\% of the time, which represents a lower bound for performance.},
	booktitle = {Proceedings of the 26th {Conference} on {Program} {Comprehension}},
	publisher = {Association for Computing Machinery},
	author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan},
	year = {2018},
	keywords = {Codes, Source coding, Decompilation, Channel models, Distortion, Lower bound, Machine translation, Noise measurement, Renaming Identifiers, Software development management, Standards, Statistical Machine Translation, Translation, Understandability},
	pages = {20--30},
}

@article{nandi_functional_2018,
	address = {New York, NY, USA},
	title = {Functional programming for compiling and decompiling computer-aided design},
	volume = {2},
	url = {https://doi.org/10.1145/3236794},
	doi = {10.1145/3236794},
	abstract = {Desktop-manufacturing techniques like 3D printing are increasingly popular because they reduce the cost and complexity of producing customized objects on demand. Unfortunately, the vibrant communities of early adopters, often referred to as "makers," are not well-served by currently available software pipelines. Users today must compose idiosyncratic sequences of tools which are typically repurposed variants of proprietary software originally designed for expert specialists. This paper proposes fundamental programming-languages techniques to bring improved rigor, reduced complexity, and new functionality to the computer-aided design (CAD) software pipeline for applications like 3D-printing. Compositionality, denotational semantics, compiler correctness, and program synthesis all play key roles in our approach, starting from the perspective that solid geometry is a programming language. Specifically, we define a purely functional language for CAD called LambdaCAD and a polygon surface-mesh intermediate representation. We then define denotational semantics of both languages to 3D solids and a compiler from CAD to mesh accompanied by a proof of semantics preservation. We illustrate the utility of this foundation by developing a novel synthesis algorithm based on evaluation contexts to "reverse compile" difficult-to-edit meshes downloaded from online maker communities back to more-editable CAD programs. All our prototypes have been implemented in OCaml to enable further exploration of functional programming for desktop manufacturing.},
	number = {ICFP},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Nandi, Chandrakana and Wilcox, James R. and Panchekha, Pavel and Blau, Taylor and Grossman, Dan and Tatlock, Zachary},
	month = jul,
	year = {2018},
	keywords = {program synthesis, 3D printing, denotational semantics, language design},
}

@inproceedings{botacin_revenge_2020,
	address = {New York, NY, USA},
	series = {{ROOTS}'19},
	title = {{RevEngE} is a dish served cold: {Debug}-{Oriented} {Malware} {Decompilation} and {Reassembly}},
	isbn = {978-1-4503-7775-1},
	url = {https://doi.org/10.1145/3375894.3375895},
	doi = {10.1145/3375894.3375895},
	abstract = {Malware analysis is key for cybersecurity overall improvement. Analysis tools have been evolving from complete static analyzers to decompilers. Malware decompilation allows for code inspection at higher abstraction levels, easing incident response. However, the decompilation procedure has many challenges, such as opaque constructions, irreversible mappings, semantic gap bridging, among others. In this paper, we propose a new approach that leverages the human analyst expertise to overcome decompilation challenges. We name this approach "DoD—debug-oriented decompilation", in which the analyst is able to reverse engineer the malware sample on his own and to instruct the decompiler to translate selected code portions (e.g., decision branches, fingerprinting functions, payloads etc.) into high level code. With DoD, the analyst might group all decompiled pieces into new code to be analyzed by other tool, or to develop a novel malware sample from previous pieces of code and thus exercise a Proof-of-Concept (PoC). To validate our approach, we propose RevEngE, the Reverse Engineering Engine for malware decompilation and reassembly, a set of GDB extensions that intercept and introspect into executed functions to build an Intermediate Representation (IR) in real-time, enabling any-time decompilation. We evaluate RevEngE with x86 ELF binaries collected from VirusShare, and show that a new malware sample created from the decompilation of independent functions of five known malware samples is considered "clean" by all VirusTotal's AVs.},
	booktitle = {Proceedings of the 3rd {Reversing} and {Offensive}-{Oriented} {Trends} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Botacin, Marcus and Galante, Lucas and de Geus, Paulo and Grégio, André},
	year = {2020},
}

@inproceedings{lacomis_dire_2020,
	series = {{ASE} '19},
	title = {{DIRE}: a neural approach to decompiled identifier naming},
	isbn = {978-1-7281-2508-4},
	url = {https://doi.org/10.1109/ASE.2019.00064},
	doi = {10.1109/ASE.2019.00064},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	booktitle = {Proceedings of the 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	year = {2020},
	pages = {628--639},
}

@inproceedings{jang_kerberoid_2019,
	address = {New York, NY, USA},
	series = {{CCS} '19},
	title = {Kerberoid: {A} {Practical} {Android} {App} {Decompilation} {System} with {Multiple} {Decompilers}},
	isbn = {978-1-4503-6747-9},
	url = {https://doi.org/10.1145/3319535.3363255},
	doi = {10.1145/3319535.3363255},
	abstract = {Decompilation is frequently used to analyze binary programs. In Android, however, decompilers all perform differently with varying apps due to their own characteristics. Obviously, there is no universal solution in all conditions. Based on this observation, we present a practical Android app decompilation system (called Kerberoid) that automatically stitches the results from multiple decompilers together to maximize the coverage and the accuracy of decompiled codes. We evaluate the performance of Kerberoid with 151 Android apps in which their corresponding source codes are publicly available. Kerberoid fully recovered all functions for 17\% of the apps tested and gained a similarity score over 50\% for 40\% of the apps tested, increased by 7\% and 9\%, respectively, compared with the best existing decompiler.},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Jang, Heejun and Jin, Beomjin and Hyun, Sangwon and Kim, Hyoungshick},
	year = {2019},
	keywords = {decompilation, reverse engineering, android apps, mobile security},
	pages = {2557--2559},
}

@inproceedings{grech_gigahorse_2019,
	series = {{ICSE} '19},
	title = {Gigahorse: thorough, declarative decompilation of smart contracts},
	url = {https://doi.org/10.1109/ICSE.2019.00120},
	doi = {10.1109/ICSE.2019.00120},
	abstract = {The rise of smart contracts—autonomous applications running on blockchains—has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract.We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a high-level 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts.Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts—e.g., Gigahorse can decompile over 99.98\% of deployed contracts, compared to 88\% for the recently-published Vandal decompiler and under 50\% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a "batteries included" approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Grech, Neville and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	year = {2019},
	keywords = {Security, Task analysis, Decompilation, decompilation, program analysis, Java, Program Analysis, Smart contracts, Ethereum, Blockchain, blockchain, ethereum, Virtual machining},
	pages = {1176--1186},
}

@inproceedings{zhong_hardening_2025,
	address = {New York, NY, USA},
	series = {{CCS} '25},
	title = {Hardening {Deep} {Neural} {Network} {Binaries} against {Reverse} {Engineering} {Attacks}},
	isbn = {979-8-4007-1525-9},
	url = {https://doi.org/10.1145/3719027.3765144},
	doi = {10.1145/3719027.3765144},
	abstract = {Deep Neural Networks (DNNs) are proprietary assets due to the expertise, confidential data, and high development costs involved in model training. Well-trained DNN models are compiled into DNN binaries to be efficiently executed on various platforms, such as edge devices and cloud infrastructures. Recent research on DNN binary decompilation shows the potential of stealing DNN models via binary reverse engineering techniques. While obfuscation is a well-studied technique to hamper binary reverse engineering, general obfuscation schemes are not designed for this new type of binary and have limitations in concealing information within DNN binaries due to the unique characteristics of DNN binaries. In this paper, we show that existing reverse engineering attacks on DNN binaries can recover 98.5\% of DNN operators from DNN binaries that have been obfuscated using general obfuscators. We then propose new obfuscation schemes tailored for DNN binaries, namely, (1) Flexible Operator Fusion; (2) Fake Operator Insertion; and (3) Operator Computation Reordering. We implement our dedicated obfuscation schemes as an end-to-end obfuscation toolchain called NeuroShield. Experiments show that NeuroShield is resilient to existing model reverse engineering attacks while introducing a reasonable overhead. Specifically, NeuroShield reduces the operator recovery rate to 3.03\% for CV models and 47.18\% for NLP models. Moreover, it has comparable binary size overhead and significantly lower execution time overhead (7.8\% - 36.1\%) compared to OLLVM, one of the commonly used general obfuscators.},
	booktitle = {Proceedings of the 2025 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Zhong, Zheng and Wu, Ruoyu and Wan, Junpeng and Zou, Muqi and Tian, Dave (Jing)},
	year = {2025},
	keywords = {binary analysis, reverse engineering, deep neural network},
	pages = {201--215},
}

@inproceedings{huang_recover_2025,
	address = {New York, NY, USA},
	series = {{CCS} '25},
	title = {Recover {Function} {Signature} from {Combined} {Constraints}},
	isbn = {979-8-4007-1525-9},
	url = {https://doi.org/10.1145/3719027.3765089},
	doi = {10.1145/3719027.3765089},
	abstract = {Recovering function signatures is a cornerstone of binary program analysis, yet it remains a challenging task. Existing methods either rely on disassembly-based constraints, which struggle with cross-architecture compatibility and scalability, or adopt learning-based approaches that are resource-intensive and often inaccurate. In this paper, we present CDA, a novel decompilation-based method for recovering function signatures that combines the strengths of multiple decompilers while mitigating their limitations. The core idea behind CDA is leveraging probabilistic constraints to estimate the likelihood of each function signature recovery result produced by decompilers, guided by inference rules specifically designed to address the limitations of decompilers. Based on these probabilities, CDA selects the recovery results with the highest likelihood as the final outcomes. We extensively evaluate CDA across five tasks — variadic function/position detection, parameter identification, return value detection, and parameter type recovery — comparing it against state-of-the-art tools, including IDA, Ghidra, Binary Ninja, and TYGR. Experimental results show that CDA outperforms baseline tools across multiple architectures (x64, x86, AArch64, Arm, and Mips) and optimization levels (O0-O3), highlighting its robustness and reliability in diverse compilation environments.},
	booktitle = {Proceedings of the 2025 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Haohui and Liu, Yue and Cheng, Yuxi and Wei, Haiyang and Liu, Jiamu and Wang, Yu and Wang, Linzhang},
	year = {2025},
	keywords = {reverse engineering, function signature recovery, probabilistic constraint},
	pages = {3386--3400},
}

@inproceedings{pilgun_demo_2025,
	address = {New York, NY, USA},
	series = {{CCS} '25},
	title = {Demo: {Reverse} {Engineering} {Android} {Apps} with {Code} {Coverage}},
	isbn = {979-8-4007-1525-9},
	url = {https://doi.org/10.1145/3719027.3762169},
	doi = {10.1145/3719027.3762169},
	abstract = {Reverse engineering Android apps remains a critical and labor-intensive task, particularly for analyzing novel malware. Analysts typically begin with decompiled Java code using tools like JaDX and often must correlate it with runtime information gathered from dynamic analysis. In this work, we present JaDX-ACVTool, a plugin that bridges this gap by integrating code coverage information from ACVTool directly into JaDX-GUI. Our approach highlights Java methods executed during analysis, enabling security analysts to quickly identify and navigate runtime-relevant code paths. Plugin repository: https://github.com/pilgun/jadx-acvtool},
	booktitle = {Proceedings of the 2025 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Pilgun, Aleksandr},
	year = {2025},
	keywords = {reverse engineering, android, code coverage, malware analysis},
	pages = {4725--4727},
}

@inproceedings{wang_orcas_2025,
	address = {New York, NY, USA},
	series = {{CIKM} '25},
	title = {{ORCAS}: {Obfuscation}-{Resilient} {Binary} {Code} {Similarity} {Analysis} using {Dominance} {Enhanced} {Semantic} {Graph}},
	isbn = {979-8-4007-2040-6},
	url = {https://doi.org/10.1145/3746252.3761266},
	doi = {10.1145/3746252.3761266},
	abstract = {Binary code similarity analysis (BCSA) serves as a foundational technique for binary analysis tasks such as vulnerability detection and malware identification. Existing graph based BCSA approaches capture more binary code semantics and demonstrate remarkable performance. However, when code obfuscation is applied, the unstable control flow structure degrades their performance. To address this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based on Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary code representation, capturing more binaries' implicit semantics without control flow structure, including inter-instruction relations (e.g., def-use), inter-basic block relations (i.e., dominance and post-dominance), and instruction-basic block relations. ORCAS takes binary functions from different obfuscation options, optimization levels, and instruction set architectures as input and scores their semantic similarity more robustly. Extensive experiments have been conducted on ORCAS against eight baseline approaches over the BinKit dataset. For example, ORCAS achieves an average 12.1\% PR-AUC improvement when using combined three obfuscation options compared to the state-of-the-art approaches. In addition, an original obfuscated real-world vulnerability dataset has been constructed and released to facilitate a more comprehensive research on obfuscated binary code analysis. ORCAS outperforms the state-of-the-art approaches over this newly released real-world vulnerability dataset by up to a recall improvement of 43\%.},
	booktitle = {Proceedings of the 34th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yufeng and Feng, Yuhong and Cao, Yixuan and Li, Haoran and Feng, Haiyue and Wang, Yifeng},
	year = {2025},
	keywords = {binary code similarity analysis, dominator tree, obfuscation-resilient},
	pages = {3198--3208},
}

@inproceedings{altamura_assessing_2025,
	address = {New York, NY, USA},
	series = {{CheckMATE} '25},
	title = {Assessing the {Effectiveness} of the {Tigress} {Obfuscator} {Against} {MOPSA} and {BinaryNinja}},
	isbn = {979-8-4007-1906-6},
	url = {https://doi.org/10.1145/3733817.3762702},
	doi = {10.1145/3733817.3762702},
	abstract = {We present an empirical evaluation of the Tigress obfuscator, focusing on its ability to degrade the precision of static analyses performed by two state-of-the-art tools: mopsa, a source-level static analyzer, and BinaryNinja\&nbsp; a binary-level decompiler and analysis platform. By applying a variety of lightweight yet diverse obfuscation strategies—such as control flow flattening, opaque predicates, and data encoding—we systematically assess how these transformations affect the analyzability of C programs. Our findings highlight the scenarios in which obfuscation successfully confuses analysis tools and those where it fails to do so.},
	booktitle = {Proceedings of the 2025 {Workshop} on {Research} on {Offensive} and {Defensive} {Techniques} in the {Context} of {Man} {At} {The} {End} ({MATE}) {Attacks}},
	publisher = {Association for Computing Machinery},
	author = {Altamura, Nicolò and Bragastini, Enrico and Campion, Marco and Dalla Preda, Mila},
	year = {2025},
	keywords = {Static Analysis, Code Obfuscation, Software Protection},
	pages = {29--37},
}

@inproceedings{sakamoto_toward_2025,
	address = {New York, NY, USA},
	series = {{SURE} '25},
	title = {Toward {Inferring} {Structural} {Semantics} from {Binary} {Code} {Using} {Graph} {Neural} {Networks}},
	isbn = {979-8-4007-1910-3},
	url = {https://doi.org/10.1145/3733822.3764673},
	doi = {10.1145/3733822.3764673},
	abstract = {Recovering semantic information from binary code is a fundamental challenge in reverse engineering, especially when source-level information is unavailable. We aim to analyze the types and roles of structural elements from the binary observed in the compiled program, focusing on their contextual usage patterns and associations to other members.We refer to such semantic aspects as structural semantics , meaning that cooccurring patterns of jointly updated structure members reveal the functional roles that can be inferred from their coupling, throughout this paper. Recent approaches have applied graph neural networks (GNNs) to data-flow graphs (DFGs) for variable type inference, but most rely on a single model architecture, such as the relational graph convolutional network (R-GCN). While effective, such models may overlook alternative patterns of structure member behavior. In this paper, we investigate the effectiveness of three alternative GNN architectures gated graph neural networks (GGNN), graph attention networks (GAT), and standard graph convolutional networks (GCN) in capturing structural semantics from binary-level data-flow graphs. We evaluate these models on real-world binaries compiled at multiple optimization levels, measuring their ability to infer semantic properties of structure members. Our results show that these architectures capture complementary aspects of structural semantics. GGNN is effective at modeling long-range dependencies, GAT suppresses irrelevant connections, and GCN offers computational simplicity. Different model architectures emphasize distinct aspects of structural semantics, capturing complementary patterns of how structure members are accessed together in memory. This demonstrates that architectural diversity provides richer perspectives for semantic inference in binary analysis.},
	booktitle = {Proceedings of the 2025 {Workshop} on {Software} {Understanding} and {Reverse} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Sakamoto, Noriki and Takeuchi, Kazuhiro},
	year = {2025},
	keywords = {Reverse Engineering, Decompiler},
	pages = {102--113},
}

@inproceedings{magin_towards_2025,
	address = {New York, NY, USA},
	series = {{SURE} '25},
	title = {Towards {Scalable} {Evaluation} of {Software} {Understanding}: {A} {Methodology} {Proposal}},
	isbn = {979-8-4007-1910-3},
	url = {https://doi.org/10.1145/3733822.3764672},
	doi = {10.1145/3733822.3764672},
	abstract = {In reverse engineering our goal is to build systems that help people to understand software. However, the field has not converged on a way to measure software understanding. In this paper, we make the case that understanding should be measured via performance on understanding-questions. We propose a method for constructing understanding-questions and evaluating answers at scale. We conduct a case study in which we apply our method and compare Ghidra’s default auto analysis with an analysis that supports binary constructs that are specific to Objective-C.},
	booktitle = {Proceedings of the 2025 {Workshop} on {Software} {Understanding} and {Reverse} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Magin, Florian and Wache, Magdalena and Scherf, Fabian and Fischer, Cléo and Zabel, Jonas},
	year = {2025},
	keywords = {Decompilation, Large Language Models, Evaluation, Understanding},
	pages = {61--67},
}

@article{dallaglio_highliner_2025,
	address = {New York, NY, USA},
	title = {Highliner: {Enhancing} {Binary} {Analysis} through {NLP}-{Based} {Instruction}-{Level} {Detection} of {C}++ {Inline} {Functions}},
	volume = {28},
	issn = {2471-2566},
	url = {https://doi.org/10.1145/3765521},
	doi = {10.1145/3765521},
	abstract = {The complexities introduced by compiler optimization have long stood as a significant obstacle in binary analysis and reverse engineering. Function inlining, in particular, complicates function recognition by replacing function calls with the entire body of the callee, mixing code from multiple functions. State-of-the-art approaches can identify inlined functions at basic block granularity, but cannot determine which instructions belong to each function and precisely deduce inlined boundaries. Without this information, further analyses such as decompilation cannot be performed effectively. This article presents Highliner, a novel approach that improves state-of-the-art approaches by identifying inline instances at instruction-level granularity. Highliner operates downstream of block-level detectors: given basic blocks reported by state-of-the-art approaches as belonging to a specific inlined function, it labels each instruction as Inlined or Not inlined and recovers the inlined-function boundaries. We treat the problem as a sequence tagging task typical of NLP and implement a learning-based technique involving instruction embedding and recurrent neural networks. We compile a dataset of open-source projects with different optimizations and use the DWARF debug information standard to construct labeled sequences of inline instructions. We use this dataset to train, validate, and test a sequence labeling architecture in which instructions are encoded via the pre-trained assembly language transformer PalmTree and then processed by an RNN-based classifier to produce binary predictions. When evaluated as a binary classifier, Highliner achieves an F1-score of 0.94 overall. In addition, when specifically tested on recognizing function boundaries, Highliner achieves an Accuracy of 0.82 on initial boundaries and 0.83 on final boundaries.},
	number = {4},
	journal = {ACM Trans. Priv. Secur.},
	publisher = {Association for Computing Machinery},
	author = {Dall'Aglio, Lorenzo and Binosi, Lorenzo and Carminati, Michele and Zanero, Stefano and Polino, Mario},
	month = oct,
	year = {2025},
	keywords = {reverse engineering, Binary analysis, function inlining, inline function recognition, natural language processing (NLP)},
}

@inproceedings{chen_clearagent_2025,
	address = {New York, NY, USA},
	series = {{LMPL} '25},
	title = {{ClearAgent}: {Agentic} {Binary} {Analysis} for {Effective} {Vulnerability} {Detection}},
	isbn = {979-8-4007-2148-9},
	url = {https://doi.org/10.1145/3759425.3763397},
	doi = {10.1145/3759425.3763397},
	abstract = {Statically detecting vulnerabilities at the binary level is crucial for the security of Commercial-Off-The-Shelf (COTS) software when source code is not available. However, traditional methods suffer from the inherent limitations of binary translation and static analysis, which hinder their scalability for complex real-world binaries. Recent efforts that leverage Large Language Models (LLMs) for vulnerability detection are still limited by possible hallucination, inaccurate code property retrieval, and insufficient guidance. In this paper, we propose a new agentic binary analysis framework ClearAgent, which features a novel binary interface that provides both LLM-friendly and analyzer-friendly tools to facilitate effective understanding of binary code semantics with rich context. ClearAgent works by automatically interacting with the interface and iteratively exploring for buggy binary code. For candidate bug reports, ClearAgent further tries to verify the existence of the vulnerability by constructing concrete inputs that can trigger the buggy locations.},
	booktitle = {Proceedings of the 1st {ACM} {SIGPLAN} {International} {Workshop} on {Language} {Models} and {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Xiang and Zhou, Anshunkang and Ye, Chengfeng and Zhang, Charles},
	year = {2025},
	keywords = {Binary Analysis, Agent, Vulnerability Detection},
	pages = {130--137},
}

@inproceedings{liu_function_2025,
	address = {New York, NY, USA},
	series = {{LMPL} '25},
	title = {Function {Renaming} in {Reverse} {Engineering} of {Embedded} {Device} {Firmware} with {ChatGPT}},
	isbn = {979-8-4007-2148-9},
	url = {https://doi.org/10.1145/3759425.3763387},
	doi = {10.1145/3759425.3763387},
	abstract = {Firmware reverse engineering is crucial for exposing internal mechanisms and identifying security vulnerabilities in embedded systems. While reconstructing the structural components of code is generally feasible, the absence of function names greatly complicates efforts to analyze and comprehend firmware logic. Motivated by the demonstrated code generation capabilities of large language models (LLMs), this paper investigates their potential to automate function renaming. We introduce FirmNamer, a prototype system designed to streamline the labor-intensive process of ana- lyzing decompiled code and assigning meaningful function names. FirmNamer accomplishes this by dynamically constructing LLM prompts based on extracted function code and contextual informa- tion. Extensive evaluation shows that FirmNamer achieves superior performance in function renaming, obtaining a functional precision of 86.6\% and a semantic precision of 49\%, thereby surpassing existing state-of-the-art approaches such as DeGPT, DEBIN, NFRE, NERO, and SYMLM.},
	booktitle = {Proceedings of the 1st {ACM} {SIGPLAN} {International} {Workshop} on {Language} {Models} and {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Puzhuo and Di, Peng and Jiang, Yu},
	year = {2025},
	keywords = {Large Language Model, Firmware, Function Summary},
	pages = {57--65},
}

@article{blazquez_practical_2025,
	address = {New York, NY, USA},
	title = {Practical {Android} {Software} {Protection} in the {Wild}},
	volume = {58},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3757735},
	doi = {10.1145/3757735},
	abstract = {Software protection refers to a range of methods used to protect applications against reverse engineering. Although this term is commonly used, distinctions arise in the specific tools and techniques utilized, such as packers, protectors, and obfuscators, as each category employs different strategies to defend applications against analysis. Given the growing importance of protecting intellectual property and sensitive user information stored in mobile applications, these protective measures have become indispensable. This article presents a taxonomy categorizing and describing the main techniques used to secure Android applications. Additionally, we analyze the available software tools designed to aid developers in protecting their applications, as well as their prevalence in the wild using a longitudinal dataset comprising nearly 2.5 million apps, including malicious software, pre-installed applications, and regular market application. Our key findings show that, although the use of software protection techniques has been steadily increasing over the last decade, they are still used only by a small fraction of applications in the Android ecosystem. Games and financial applications are by far the ones that most commonly use some form of protection, and we also observe noticeable differences between marketplaces.},
	number = {2},
	journal = {ACM Comput. Surv.},
	publisher = {Association for Computing Machinery},
	author = {Blazquez, Eduardo and Tapiador, Juan},
	month = sep,
	year = {2025},
	keywords = {Android software protection, anti-analysis techniques, Man-At-The-End (MATE) attacks, program obfuscation},
}

@incollection{nguyen_automating_2025,
	address = {New York, NY, USA},
	title = {Automating the conformity assessment of {Cyber}-{Physical} {Systems} software},
	isbn = {979-8-4007-1276-0},
	url = {https://doi.org/10.1145/3696630.3731468},
	abstract = {Cyber-physical systems (CPS) are tools used by humans to enhance the way they perform tasks. CPSs make tasks more efficient, more precise, and safer. Those systems are omnipresent in human lives, e.g., in cars with Advanced Driver Assistance Systems (ADAS), in Unmanned Aerial Vehicles (UAV) for self-balancing or even in medical devices. CPSs can read information from the real world, process it, and affect the real world back, considering constraints such as real-time processing. Furthermore, the safety and security of the software controlling the CPS are directly linked with the safety and security of human bystanders. The European Union (EU) has a process to assess the conformity of specific products exchanged within the EU to ensure the safety of its citizens. Recently, regulations and directives such as the Cyber Resilience Act (CRA) pressed European actors to provide compliant software products. Requirements on software started with the Medical Device Regulation (MDR) in 2017. However, technical requirements are challenging to understand from legal texts, and certification processes rely solely on manufacturer documentation. On the one hand, the EU has difficulty monitoring and opening the European market to products deemed compliant. On the other hand, manufacturers have difficulty understanding what is technically required of them when introducing products. This thesis aims to reconcile both parties.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Guillaume},
	year = {2025},
	pages = {1281--1284},
}

@incollection{alecci_toward_2025,
	address = {New York, NY, USA},
	title = {Toward {LLM}-{Driven} {GDPR} {Compliance} {Checking} for {Android} {Apps}},
	isbn = {979-8-4007-1276-0},
	url = {https://doi.org/10.1145/3696630.3728508},
	abstract = {Android apps extensively collect sensitive personal data from our devices daily. Despite stringent regulations like the European Union's General Data Protection Regulation (GDPR), many applications (apps) fail to comply with these legal requirements. While previous studies have focused on the compliance of privacy policies, checking how these policies are implemented in the actual code has not yet been extensively investigated. Moreover, previous efforts have often been limited in scope.This paper explores the potential of Large Language Models (LLMs) to address the challenge of verifying privacy regulation compliance in Android apps. Specifically, we address scenarios where source code is unavailable by investigating whether LLM can work with Smali code—a human-readable representation of Android byte-code extracted from APK files. Through this exploratory investigation, we aim to uncover if LLMs can bridge the gap between legal privacy requirements and their technical implementation in mobile apps. Through initial experiments, we assess the feasibility and effectiveness of a straightforward LLM-driven method for identifying compliance issues and provide directions for our future research efforts to improve our approach and perform large-scale experiments.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Alecci, Marco and Sannier, Nicolas and Ceci, Marcello and Abualhaija, Sallam and Samhi, Jordan and Bianculli, Domenico and Bissyandé, Tegawendé and Klein, Jacques},
	year = {2025},
	pages = {606--610},
}

@article{liu_keenhash_2025,
	address = {New York, NY, USA},
	title = {{KEENHash}: {Hashing} {Programs} into {Function}-{Aware} {Embeddings} for {Large}-{Scale} {Binary} {Code} {Similarity} {Analysis}},
	volume = {2},
	url = {https://doi.org/10.1145/3728911},
	doi = {10.1145/3728911},
	abstract = {Binary code similarity analysis (BCSA) is a crucial research area in many fields such as cybersecurity. Specifically, function-level diffing tools are the most widely used in BCSA: they perform function matching one by one for evaluating the similarity between binary programs. However, such methods need a high time complexity, making them unscalable in large-scale scenarios (e.g., 1/n-to-n search). Towards effective and efficient program-level BCSA, we propose KEENHash, a novel hashing approach that hashes binaries into program-level representations through large language model (LLM)-generated function embeddings. KEENHash condenses a binary into one compact and fixed-length program embedding using K-Means and Feature Hashing, allowing us to do effective and efficient large-scale program-level BCSA, surpassing the previous state-of-the-art methods. The experimental results show that KEENHash is at least 215 times faster than the state-of-the-art function matching tools while maintaining effectiveness. Furthermore, in a large-scale scenario with 5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while these tools will cost at least 56 days. We also evaluate KEENHash on the program clone search of large-scale BCSA across extensive datasets in 202,305 binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of them by at least 23.16\%, and displays remarkable superiority over them in the large-scale BCSA security scenario of malware detection.},
	number = {ISSTA},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhijie and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Liang Feng and Tang, Yutian},
	month = jun,
	year = {2025},
	keywords = {LLM, BCSA, Clone Search, Program},
}

@inproceedings{sun_fire_2025,
	address = {New York, NY, USA},
	series = {Internetware '25},
	title = {{FIRE}: {Smart} {Contract} {Bytecode} {Function} {Identification} via {Graph}-{Refined} {Hybrid} {Feature} {Encoding}},
	isbn = {979-8-4007-1926-4},
	url = {https://doi.org/10.1145/3755881.3755883},
	doi = {10.1145/3755881.3755883},
	abstract = {The growing popularity of smart contracts has spurred an increasing demand for efficient analysis of their bytecode. Reverse engineering plays a critical role in understanding and auditing smart contracts, with function identification being a key aspect. However, existing function identification techniques often struggle with scalability, accuracy, and adaptability across different contract versions. This paper presents FIRE (Smart Contract Bytecode Function Identification via Graph-Refined Hybrid Encoding), a novel approach to function identification in Ethereum smart contract bytecode. By leveraging hybrid encoding of basic blocks and incorporating a graph neural network (GNN) based on control flow graph (CFG), our method improves the effectiveness of function identification. The approach demonstrates strong generalization across contract versions and significantly reduces runtime. We evaluate FIRE on multiple datasets and show its superior performance compared to existing techniques, highlighting its potential for efficient smart contract bytecode analysis.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Internetware}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Yu and Bao, Lingfeng and Yang, Xiaohu},
	year = {2025},
	keywords = {Reverse Engineering, Machine Learning, Smart Contract, Function Identification, Graph Neural Network},
	pages = {378--388},
}

@article{zhu_misum_2025,
	address = {New York, NY, USA},
	title = {{MiSum}: {Multi}-modality {Heterogeneous} {Code} {Graph} {Learning} for {Multi}-intent {Binary} {Code} {Summarization}},
	volume = {2},
	url = {https://doi.org/10.1145/3715780},
	doi = {10.1145/3715780},
	abstract = {The current landscape of binary code summarization predominantly focuses on generating a single summary, which limits its utility and understanding for reverse engineers. Existing approaches often fail to meet the diverse needs of users, such as providing detailed insights into usage patterns, implementation nuances, and design rationale, as observed in the field of source code summarization. This highlights the need for multi-intent binary code summarization to enhance the effectiveness of reverse engineering processes. To address this gap, we propose MiSum, a novel method that leverages multi-modality heterogeneous code graph alignment and learning to integrate both assembly code and pseudo-code. MiSum introduces a unified multi-modality heterogeneous code graph (MM-HCG) that aligns assembly code graphs with pseudo-code graphs, capturing both low-level execution details and high-level structural information. We further propose multi-modality heterogeneous graph learning with heterogeneous mutual attention and message passing, which highlights important code blocks and discovers inter-dependencies across different code forms. Additionally, an intent-aware summary generator with an intent-aware attention mechanism is introduced to produce customized summaries tailored to multiple intents. Extensive experiments, including evaluations across various architectures and optimization levels, demonstrate that MiSum outperforms state-of-the-art baselines in BLEU, METEOR, and ROUGE-L metrics. Human evaluations validate its capability to effectively support reverse engineers in understanding diverse binary code intents, marking a significant advancement in binary code analysis.},
	number = {FSE},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Kangchen and Tian, Zhiliang and Wang, Shangwen and Chen, Weiguo and Dong, Zixuan and Leng, Mingyue and Mao, Xiaoguang},
	month = jun,
	year = {2025},
	keywords = {Reverse engineering, Large language models, Binary code understanding, Multi-intent code summarization, Multi-modality fusion},
}

@inproceedings{jin_ccci_2025,
	address = {New York, NY, USA},
	series = {{EASE} '25},
	title = {{CCCI}: {Code} {Completion} with {Contextual} {Information} for {Complex} {Data} {Transfer} {Tasks} {Using} {Large} {Language} {Models}},
	isbn = {979-8-4007-1385-9},
	url = {https://doi.org/10.1145/3756681.3756954},
	doi = {10.1145/3756681.3756954},
	abstract = {Unlike code generation, which involves creating code from scratch, code completion focuses on integrating new lines or blocks of code into an existing codebase. This process requires a deep understanding of the surrounding context, such as variable scope, object models, API calls, and database relations, to produce accurate results. These complex contextual dependencies make code completion a particularly challenging problem. Current models and approaches often fail to effectively incorporate such context, leading to inaccurate completions with low acceptance rates (around 30\%). For tasks like data transfer, which rely heavily on specific relationships and data structures, acceptance rates drop even further. This study introduces CCCI, a novel method for generating context-aware code completions specifically designed to address data transfer tasks. By integrating contextual information, such as database table relationships, object models, and library details into Large Language Models (LLMs), CCCI improves the accuracy of code completions. We evaluate CCCI using 289 Java snippets, extracted from over 819 operational scripts in an industrial setting. The results demonstrate that CCCI achieved a 49.1\% Build Pass rate and a 41.0\% CodeBLEU score, comparable to state-of-the-art methods that often struggle with complex task completion.},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Evaluation} and {Assessment} in {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jin, Hangzhan and Hamdaqa, Mohammad},
	year = {2025},
	keywords = {Code Completion, Contextual Information, Data Transfer, Large Language Models (LLMs)},
	pages = {126--135},
}

@inproceedings{santos_detecting_2025,
	address = {New York, NY, USA},
	series = {{SLE} '25},
	title = {Detecting {Resource} {Leaks} on {Android} with {Alpakka}},
	isbn = {979-8-4007-1884-7},
	url = {https://doi.org/10.1145/3732771.3742724},
	doi = {10.1145/3732771.3742724},
	abstract = {Mobile devices have become integral to our everyday lives, yet their utility hinges on their battery life. In Android apps, resource leaks caused by inefficient resource management are a significant contributor to battery drain and poor user experience. Our work introduces Alpakka, a source-to-source compiler for Android's Smali syntax. To showcase Alpakka's capabilities, we developed an Alpakka library capable of detecting and automatically correcting resource leaks in Android APK files. We demonstrate Alpakka's effectiveness through empirical testing on 124 APK files from 31 real-world Android apps in the DroidLeaks [12] dataset. In our analysis, Alpakka identified 93 unique resource leaks, of which we estimate 15\% are false positives. From these, we successfully applied automatic corrections to 45 of the detected resource leaks.},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Santos, Gustavo and Bispo, João and Mendes, Alexandra},
	year = {2025},
	keywords = {Decompiler, Android, Energy Efficiency, Green Software, Mobile Applications, Resource Leaks, Smali},
	pages = {199--211},
}

@inproceedings{ferreira_transpilejs_2025,
	address = {New York, NY, USA},
	series = {{SLE} '25},
	title = {{TranspileJS}, an {Intelligent} {Framework} for {Transpiling} {JavaScript} to {WebAssembly}},
	isbn = {979-8-4007-1884-7},
	url = {https://doi.org/10.1145/3732771.3742714},
	doi = {10.1145/3732771.3742714},
	abstract = {WebAssembly (Wasm) has emerged as a powerful binary format, enabling the seamless integration of languages like C and Rust into web applications. JavaScript (JS), the dominant language for client-side web development, has its code susceptible to tampering and intellectual property theft due to its transparency in browser environments. We introduce TranspileJS, a novel tool designed to enhance code security by automatically selecting and translating JS snippets into Wasm. TranspileJS leverages a multi-stage architecture that converts JS to TypeScript, which is compiled into Wasm using the AssemblyScript compiler. TranspileJS addresses the challenges posed by the fundamental differences between JS and Wasm, including dynamic typing, runtime behaviour mismatches, and standard library discrepancies, ensuring that the original behaviour of the code is preserved while maximising the amount of code transpiled. Our experiments show that TranspileJS successfully transpiles approximately one-third of the code in our dataset, with a performance impact of up to a 12.3\% increase in execution time. The transpilation process inherently obfuscates code, creating effects similar to standard obfuscation techniques, and generates a stealthy and resilient output. Furthermore, combining transpilation with WebAssembly-specific obfuscation techniques opens new possibilities for code protection and resistance against reverse engineering.},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {International} {Conference} on {Software} {Language} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Ferreira, José Pedro and Bispo, João and Lima, Susana},
	year = {2025},
	keywords = {Obfuscation, AssemblyScript, Compilation, Cybersecurity, JavaScript, Web Privacy, WebAssembly},
	pages = {71--83},
}

@inproceedings{he_benchmarking_2025,
	address = {New York, NY, USA},
	series = {{ISSTA} {Companion} '25},
	title = {On {Benchmarking} {Code} {LLMs} for {Android} {Malware} {Analysis}},
	isbn = {979-8-4007-1474-0},
	url = {https://doi.org/10.1145/3713081.3731745},
	doi = {10.1145/3713081.3731745},
	abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names.This paper presents Cama, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. Cama specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics—consistency, fidelity, and semantic relevance—enabling rigorous stability and effectiveness assessment and cross-model comparison.We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use Cama to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.},
	booktitle = {Proceedings of the 34th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {He, Yiling and She, Hongyu and Qian, Xingzhi and Zheng, Xinran and Chen, Zhuo and Qin, Zhan and Cavallaro, Lorenzo},
	year = {2025},
	keywords = {malware analysis, code LLM},
	pages = {153--160},
}

@inproceedings{zhou_regraph_2025,
	address = {New York, NY, USA},
	series = {{ISSTA} {Companion} '25},
	title = {{ReGraph}: {A} {Tool} for {Binary} {Similarity} {Identification}},
	isbn = {979-8-4007-1474-0},
	url = {https://doi.org/10.1145/3713081.3731728},
	doi = {10.1145/3713081.3731728},
	abstract = {Binary Code Similarity Detection (BCSD) is not only essential for security tasks such as vulnerability identification but also for code copying detection, yet it remains challenging due to binary stripping and diverse compilation environments. Existing methods tend to adopt increasingly complex neural networks for better accuracy performance. The computation time increases with the complexity. Even with powerful GPUs, the treatment of large-scale software becomes time-consuming. To address these issues, we present a framework called ReGraph to efficiently compare binary code functions across architectures and optimization levels. Our evaluation with public datasets highlights that ReGraph exhibits a significant speed advantage, performing 700 times faster than Natural Language Processing (NLP)-based methods while maintaining comparable accuracy results with respect to the state-of-the-art models.},
	booktitle = {Proceedings of the 34th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Li and Dacier, Marc and Konstantinou, Charalambos},
	year = {2025},
	keywords = {binary code re-optimization, binary code similarity detection, code lifting, code property graph, graph neural network},
	pages = {6--10},
}

@article{chen_ponzihunter_2025,
	address = {New York, NY, USA},
	title = {{PonziHunter}: {Hunting} {Ethereum} {Ponzi} {Contract} via {Static} {Analysis} and {Contrastive} {Learning} on the {Bytecode} {Level}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3735971},
	doi = {10.1145/3735971},
	abstract = {In recent years, blockchain technology has developed rapidly and received widespread attention. However, its pseudonymous and decentralized nature has also attracted many criminal activities. Ponzi schemes, a kind of classic financial scam, also hide their true face in smart contracts, causing massive financial losses to blockchain users. Although several methods have been proposed to detect Ponzi contracts, there are still limitations in broad applicability, semantics understanding, and adversarial robustness. In this paper, we propose PonziHunter, an intelligent framework for hunting Ponzi contracts on Ethereum. To tackle the problem of broad applicability, we train a detection model that does not require expert experience based on publicly available on-chain bytecode and off-chain contract labels. To tackle the problem of semantics understanding, we employ cross-function control flows and state variable dependencies to understand the logic of Ponzi contracts. Specifically, we decompile bytecodes into higher-order representations to analyze control flows and state variable dependencies and model the information as graph data. By combining the idea of code slicing, we identify the basic blocks related to Ponzi contract recognition. To tackle the problem of adversarial robustness, we model Ponzi contract recognition as a graph classification problem based on contrastive pre-training. We propose a data augmentation method for control flow graphs (CFGs), which preserves the basic blocks related to Ponzi contract recognition as much as possible during data perturbation. Experimental results show that PonziHunter outperforms state-of-the-art tools with average improvements of at least 4.77\% on real-world ground-truth data, and can newly discover 85 Ponzi contracts in the wild. More importantly, PonziHunter is robust against adversarial examples and can locate the critical basic blocks for smart Ponzi detection.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Chen, Jinze and Liu, Jieli and Wu, Jianlin and Lin, Dan and Wu, Jiajing and Zheng, Zibin},
	month = may,
	year = {2025},
	keywords = {static analysis, Smart contract, blockchain safety, contrastive learning, Ponzi scheme},
	annote = {Just Accepted},
}

@inproceedings{mudraje_reverse_2025,
	address = {New York, NY, USA},
	series = {{ENSsys} '25},
	title = {Reverse {Engineering} the {ESP32}-{C3} {Wi}-{Fi} {Drivers} for {Static} {Worst}-{Case} {Analysis} of {Intermittently}-{Powered} {Systems}},
	isbn = {979-8-4007-1606-5},
	url = {https://doi.org/10.1145/3722572.3727926},
	doi = {10.1145/3722572.3727926},
	abstract = {The Internet of Batteryless Things revolutionizes sustainable communication as it operates on harvested energy. This harvested energy is dependent on unpredictable environmental conditions; therefore, device operations, including those of its networking stack, must be resilient to power failures. Reactive intermittent computing provides an approach for solving this by notifications of impending power failures, which is implemented by monitoring the harvested energy buffered in a capacitor. However, to use this power-failure notification and guarantee forward progress, systems must break down tasks into atomic transactions that can be predictably finished before the energy runs out. Thus, static program-code analysis must determine the worst-case energy consumption (WCEC) of all transactions. In Wi-Fi–capable devices, drivers are often closed-source, which avoids the determination of WCEC bounds for transactions since static analysis requires all code along with its semantics.In this work, we integrate an energy-aware networking stack with reverse-engineered Wi-Fi drivers to enable full-stack WCEC analysis for physical transmission and reception of packets. Further, we extended a static worst-case analysis tool with a resource-consumption model of our Wi-Fi driver. Our evaluation with the RISC-V–based ESP32-C3 platform gives worst-case bounds with our static analysis approach for the transactions of the full communication stack, therefore showing that Wi-Fi–based reactive intermittent computing is feasible.},
	booktitle = {Proceedings of the 13th {International} {Workshop} on {Energy} {Harvesting} and {Energy}-{Neutral} {Sensing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mudraje, Ishwar and Vogelgesang, Kai and Devreker, Jasper and Gerhorst, Luis and Raffeck, Phillip and Wägemann, Peter and Herfet, Thorsten},
	year = {2025},
	keywords = {reverse engineering, static analysis, batteryless systems, intermittently-powered devices, IoT, Wi-Fi, worst-case energy consumption},
	pages = {1--7},
}

@article{liu_demystifying_2025,
	address = {New York, NY, USA},
	title = {Demystifying {React} {Native} {Android} {Apps} for {Static} {Analysis}},
	volume = {34},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3702977},
	doi = {10.1145/3702977},
	abstract = {React Native, an open source framework, simplifies cross-platform app development by allowing JavaScript-side code to interact with native-side code. Previous studies disregarded React Native, resulting in insufficient static analysis of React Native app code. This study initiates the investigation of challenges when statically analyzing React Native apps. We propose ReuNify to improve Soot-based static analysis coverage for JavaScript-side and native-side code. ReuNify converts Hermes bytecode to Soot’s intermediate representation. Hermes bytecode, compiled from JavaScript code and integrated into React Native apps, possesses a unique syntax that eludes current JavaScript analyzers. Additionally, we investigate opcode distribution and conduct in-depth analyses of the usage of opcode between popular apps and malware. We also propose a benchmark consisting of 97 control flow-related cases to validate the control flow recovery of the generated intermediate representation. Furthermore, we model the cross-language communication mechanisms of React Native to expand the static analysis coverage for native-side code. Our evaluation demonstrates that ReuNify enables an average increase of 84\% in reached nodes within the callgraph and further identifies an average of two additional privacy leaks in taint analysis. In summary, this article demonstrates that ReuNify significantly improves the static analysis for the React Native Android apps.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yonghui and Chen, Xiao and Liu, Pei and Samhi, Jordan and Grundy, John and Chen, Chunyang and Li, Li},
	month = apr,
	year = {2025},
	keywords = {Android, Static Analysis, Mobile App, React Native},
}

@inproceedings{verbeek_formally_2025,
	series = {{ICSE} '25},
	title = {Formally {Verified} {Binary}-{Level} {Pointer} {Analysis}},
	isbn = {979-8-3315-0569-1},
	url = {https://doi.org/10.1109/ICSE55347.2025.00231},
	doi = {10.1109/ICSE55347.2025.00231},
	abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the tradeoff between scalability and precision, such customization allows "meaningful" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
	booktitle = {Proceedings of the {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
	year = {2025},
	keywords = {binary analysis, formal methods, pointer analysis},
	pages = {42--53},
}

@inproceedings{berge_anti-games_2025,
	address = {New York, NY, USA},
	series = {{FDG} '25},
	title = {Anti-{Games}, {Fantasy} {Consoles}, and the {Rise} of {Speculative} {Game} {Development} on {Itch}.io},
	isbn = {979-8-4007-1856-4},
	url = {https://doi.org/10.1145/3723498.3723739},
	doi = {10.1145/3723498.3723739},
	abstract = {Amid the ongoing escalation of anti-worker and anti-player practices across the game industry, the creative practices of developers are shifting: jammers, students, hobbyists, fangamers, and other digital game “zinesters” [3] seek to reclaim game design as a personal art form. This paper examines the emergence and rise of speculative game design communities on itch.io—sites of ongoing creative resistance to corporate encroachment and exclusionary game culture. Specifically, the study documents the activities of “anti-game” jammers and fantasy console development communities on itch.io—two groups that create speculative games (that is, projects that are conceptual, arbitrary, and materially or procedurally unrealizable) to make space for personal play that is divested from mainstream engines, markets, and pretenses of game commodity: 1) Firstly, “anti-game” designers use game jams, zines, and Discord servers to develop and circulate not-games, games that refuse play, and games that cannot be played. These makers create provocative thought experiments, poems, manifestos, and other “unplayable” works that resist hyper-capitalist investment in bigger, longer, more graphically precise, live-service, metaversal, forever-games. 2) Simultaneously, developers on itch build projects for fantasy consoles—such as PICO-8 and Bitsy—and imagine hardware that is mutually-constituted, rooted in community, and decommodified. Ultimately, this paper argues that speculative design practices divest from concerns of “playability,” center unencumbered participation, and allow developers to envision and participate in a post-capitalist imaginary of hardware, software, and play.},
	booktitle = {Proceedings of the 20th {International} {Conference} on the {Foundations} of {Digital} {Games}},
	publisher = {Association for Computing Machinery},
	author = {Berge, PB},
	year = {2025},
	keywords = {anti-games, experimental game design, fantasy consoles, itch.io, Speculative game design, unplayable games},
}

@incollection{brossard_automatic_2025,
	address = {New York, NY, USA},
	title = {Automatic {Functions} {Annotations} through {Concrete} {Procedural} {Debugging} and {ELF} {Libification}},
	isbn = {979-8-4007-0629-5},
	url = {https://doi.org/10.1145/3672608.3707995},
	abstract = {In this article, we present a novel approach to program analysis through selective concrete execution. While static analysis of ELF binaries is necessarily limited by the theoretical undecidability of control-flow and data-flow analysis algorithms, we detail a new approach to reverse engineering through selective concrete execution of arbitrary functions within a x86\_64 GNU/Linux binary by transforming ELF applications into shared libraries. This approach, named "procedural debugging", allows us to empirically recover information about function parameters and return values without resorting to any disassembly or decompilation, which are undecidable in general. In turn, this dynamic approach may be used as a feedback loop into existing program analyzers, being them static, fuzzing, symbolic, or concolic, to enrich their understanding of application interfaces. We publish an open-source framework, named the Witchcraft Compiler Collection, under a permissive MIT/BSD license, implementing binary libification, procedural debugging, and automatic function prototype annotations with the hope of benefiting the security community.},
	booktitle = {Proceedings of the 40th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Brossard, Jonathan},
	year = {2025},
	pages = {1894--1898},
}

@inproceedings{yan_uefuzzer_2025,
	address = {New York, NY, USA},
	series = {{CNSSE} '25},
	title = {{UEFUZZER}: {Enabling} {Struct}-{Aware} {Fuzzing} on {UEFI} with {Static} {Analysis}},
	isbn = {979-8-4007-1361-3},
	url = {https://doi.org/10.1145/3732365.3732428},
	doi = {10.1145/3732365.3732428},
	abstract = {Since its extensive implementation in 2006, the Unified Extensible Firmware Interface (UEFI) has supplanted traditional BIOS as the industry standard, serving as an essential link between computer hardware and operating systems. UEFI's advantageous role in system design provides it with comprehensive access to system resources, beyond those of the operating system kernel. Consequently, detecting and thoroughly characterizing memory corruption vulnerabilities in UEFI firmware is essential for preserving the integrity and security of computer systems. The techniques for finding UEFI firmware vulnerabilities nowadays mostly face two main difficulties: First of all, the special character of UEFI firmware makes direct dynamic examination inside the operating system especially difficult; Second, conventional fuzzing techniques lead to ineffective testing with their shallow knowledge of UEFI input structures. We propose a novel approach combining static analysis with fuzzing techniques to help to reduce these limits. Our approach starts with static reverse engineering to fully understand the structural characteristics of inputs throughout several UEFI interfaces, followed by cross-validation against open-source firmware implementations. We then use this structured knowledge to guide the seed file mutation technique, hence improving the accuracy and efficiency of fuzzing activities.},
	booktitle = {Proceedings of the 2025 5th {International} {Conference} on {Computer} {Network} {Security} and {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Huaishuo and Cui, Baojiang},
	year = {2025},
	keywords = {Fuzzing, Intelligent Mutation, Security Analysis, UEFI Firmware},
	pages = {350--355},
}

@incollection{fang_wasserstein_2025,
	address = {New York, NY, USA},
	title = {Wasserstein {GAN}-{Based} {Android} {Certificate} {Validation} {Vulnerability} {Detection} {Framework}},
	isbn = {979-8-4007-1345-3},
	url = {https://doi.org/10.1145/3728725.3728761},
	abstract = {This paper presents an Android certificate validation vulnerability detection framework that integrates static analysis, adversarial sample training, and optimization via large language models. Centered around the Wasserstein Generative Adversarial Network, the framework aims to enhance the robustness and generalization ability of static detection models against complex and obfuscated attack patterns. Static analysis is performed on 2,000 popular Android applications from the androzoo dataset, with code and configuration features extracted, focusing on certificate-related API usage, permission declarations, and network security settings. Among them, 110 APKs are identified as containing high-risk certificate misconfigurations. Based on these features, a Transformer-based model is constructed for feature vectorization and vulnerability identification. To further improve model robustness, WGAN is employed to generate adversarial samples that mimic real-world evasive vulnerabilities, which are used for adversarial training. Additionally, large language models are used to perform In-Context Learning, enabling semantic filtering, sample refinement, and iterative generator feedback. This results in a closed-loop, self-improving detection pipeline that combines detection, optimization, and feedback. Experimental results across standard, adversarial, and cross-domain scenarios demonstrate that the proposed framework achieves an F1 score of 0.91 and reduces the false positive rate to 7.2\%, significantly outperforming traditional static detection tools in both accuracy and resilience.},
	booktitle = {Proceedings of the 2025 2nd {International} {Conference} on {Generative} {Artificial} {Intelligence} and {Information} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Fang, Yong},
	year = {2025},
	pages = {231--236},
}

@inproceedings{m_s_detection_2025,
	address = {New York, NY, USA},
	series = {{APIT} '25},
	title = {Detection of {Mobile} {Malware} ({ANDROID}) using {ML} and {Hybrid} {Analysis}},
	isbn = {979-8-4007-0728-5},
	url = {https://doi.org/10.1145/3726101.3726103},
	doi = {10.1145/3726101.3726103},
	abstract = {In recent times, malware has increasingly become a significant concern in information and technology security, as evidenced by the substantial rise in attacks on various devices, including computers, the internet, and mobile devices. Detecting zero-day malware has become a primary focus for security researchers. Given that Google’s Android is one of the most widely used mobile operating systems, attackers have shifted their attention to creating malware specifically targeting Android. Numerous security researchers have employed various machine learning algorithms to detect these new Android and other types of malware. In this paper, we propose a system which employs a hybrid approach of analysis to better detect malware.The proposed model relies on a two-way approach in order to detect malwares in android application. This two-way approach first involves running the target APK through a static analytical model, If the result from the first stage comes out to be malicious, then the APK does not go to the second stage for further analysis. But, if the result from the first stage comes out to be benign, it is sent for further analysis by involving hybrid analysis (static + dynamic approach). The results given by the second stage will be ultimately determining whether the APK given is malicious or benign. The source code is available at: https://github.com/Hurry-sh/CCNCS-Project},
	booktitle = {Proceedings of the 2025 7th {Asia} {Pacific} {Information} {Technology} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {M S, Abhijith and Druva, K R and K, Harish and D, Geetha and Rapate, Gauri Sameer},
	year = {2025},
	keywords = {Machine Learning, Malware Detection, Static Analysis, Network Security, Dynamic Analysis},
	pages = {10--15},
}

@article{zhang_cf-gkat_2025,
	address = {New York, NY, USA},
	title = {{CF}-{GKAT}: {Efficient} {Validation} of {Control}-{Flow} {Transformations}},
	volume = {9},
	url = {https://doi.org/10.1145/3704857},
	doi = {10.1145/3704857},
	abstract = {Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT’s abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren’s goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Cheng and Kappé, Tobias and Narváez, David E. and Naus, Nico},
	month = jan,
	year = {2025},
	keywords = {control flow recovery, Kleene algebra, Program equivalence},
}

@inproceedings{naus_poster_2024,
	address = {New York, NY, USA},
	series = {{CCS} '24},
	title = {Poster: {Formally} {Verified} {Binary} {Lifting} to {P}-{Code}},
	isbn = {979-8-4007-0636-3},
	url = {https://doi.org/10.1145/3658644.3691386},
	doi = {10.1145/3658644.3691386},
	abstract = {Analysis of binary software plays a critical role in software security. Reverse engineers analyze binaries to discover vulnerabilities, patch legacy software, and detect malware. Most of the reverse engineering tools have been developed from a practical point of view, and do not provide any guarantees with their results. Recently, formally verified reverse engineering and decompilation have gained traction. These formal tools are for the most part proof-of-concept systems not yet suitable for real-world reverse-engineering tasks. In this poster, we explore the idea of formalizing part of an existing decompilation tool instead. We focus on the lifting from assembly to the IR P-Code in one of the most popular decompilers, Ghidra. This step occurs immediately after disassembly. We are developing a proof system inside the Isabelle theorem prover, to automatically prove semantical equivalence between the assembly and P-Code instructions. We leverage machine-learned x86-64 semantics, to stay as close as possible to actual CPU behavior. This approach has uncovered several shortcomings in Ghidra's P-Code and the lifting it performs. By using a theorem prover, we obtain guarantees that our system of formal semantics and lifting is internally consistent. This work brings the powerful guarantees that formal methods provide in reverse engineering research to the real world.},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Naus, Nico and Verbeek, Freek and Atla, Sagar and Ravindran, Binoy},
	year = {2024},
	keywords = {decompilation, reverse engineering, binary lifting, formal verification, proof automation, theorem provers},
	pages = {4973--4975},
}

@inproceedings{verbeek_verifiably_2024,
	address = {New York, NY, USA},
	series = {{CCS} '24},
	title = {Verifiably {Correct} {Lifting} of {Position}-{Independent} x86-64 {Binaries} to {Symbolized} {Assembly}},
	isbn = {979-8-4007-0636-3},
	url = {https://doi.org/10.1145/3658644.3690244},
	doi = {10.1145/3658644.3690244},
	abstract = {We present an approach to lift position-independent x86-64 binaries to symbolized NASM. Symbolization is a decompilation step that enables binary patching: functions can be modified, and instructions can be interspersed. Moreover, it is the first abstraction step in a larger decompilation chain. The produced NASM is recompilable, and we extensively test the recompiled binaries to see if they exhibit the same behavior as the original ones. In addition to testing, the produced NASM is accompanied with a certificate, constructed in such a way that if all theorems in the certificate hold, symbolization has occurred correctly. The original and recompiled binary are lifted again with a third-party decompiler (Ghidra). These representations, as well as the certificate, are loaded into the Isabelle/HOL theorem prover, where proof scripts ensure that correctness can be proven automatically. We have applied symbolization to various stripped binaries from various sources, from various compilers, and ranging over various optimization levels. We show how symbolization enables binary-level patching, by tackling challenges originating from industry.},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Verbeek, Freek and Naus, Nico and Ravindran, Binoy},
	year = {2024},
	keywords = {binary analysis, formal methods, disassembly},
	pages = {2786--2798},
}

@inproceedings{xie_resym_2024,
	address = {New York, NY, USA},
	series = {{CCS} '24},
	title = {{ReSym}: {Harnessing} {LLMs} to {Recover} {Variable} and {Data} {Structure} {Symbols} from {Stripped} {Binaries}},
	isbn = {979-8-4007-0636-3},
	url = {https://doi.org/10.1145/3658644.3670340},
	doi = {10.1145/3658644.3670340},
	abstract = {Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, e.g., recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the token limitations inherent in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu},
	year = {2024},
	keywords = {program analysis, reverse engineering, large language models},
	pages = {4554--4568},
}

@article{udeshi_remaqe_2024,
	address = {New York, NY, USA},
	title = {{REMaQE}: {Reverse} {Engineering} {Math} {Equations} from {Executables}},
	volume = {8},
	issn = {2378-962X},
	url = {https://doi.org/10.1145/3699674},
	doi = {10.1145/3699674},
	abstract = {Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This article proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer equations from object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool “tmon.” To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow.},
	number = {4},
	journal = {ACM Trans. Cyber-Phys. Syst.},
	publisher = {Association for Computing Machinery},
	author = {Udeshi, Meet and Krishnamurthy, Prashanth and Pearce, Hammond and Karri, Ramesh and Khorrami, Farshad},
	month = jan,
	year = {2024},
	keywords = {symbolic execution, binary reverse engineering, embedded systems, mathematical equations},
}

@inproceedings{yong_wong_understanding_2024,
	address = {New York, NY, USA},
	series = {{ICMI} '24 {Companion}},
	title = {Understanding {LLMs} {Ability} to {Aid} {Malware} {Analysts} in {Bypassing} {Evasion} {Techniques}},
	isbn = {979-8-4007-0463-5},
	url = {https://doi.org/10.1145/3686215.3690147},
	doi = {10.1145/3686215.3690147},
	abstract = {Over the past few years, the threat of malware has become increasingly evident, posing a significant risk to cybersecurity worldwide and driving extensive research efforts to prevent and mitigate these attacks. Despite numerous efforts to automate malware analysis, these systems are constantly thwarted by evasive techniques developed by malware authors. As a result, the analysis of sophisticated evasive malware falls into the hands of human malware analysts, who must undertake the time-consuming process of overcoming each evasive technique to uncover malware’s malicious behaviors. This highlights the need for approaches that aid malware analysts in this process. Although active measures, such as forced execution and symbolic analysis, can automatically circumvent some evasive checks, they suffer from limitations like path explosion and fail to provide useful insights that analysts can use in their workflow. To fill this gap, we investigate how large language models (LLMs) can address shortcomings of symbolic analysis through the first comparative analysis between the two in bypassing evasion techniques. Our study leads to three key findings: (i) we find that LLMs outperform symbolic analysis in bypassing evasive code, especially in the presence of common code patterns, such as loops, which have historically posed a challenge for symbolic analysis, (ii) we show that LLMs correctly identify methods of bypassing evasive techniques in real-world malware, and (iii) we highlight how even in LLMs failure modes, human malware analysts can benefit from the step-by-step reasoning provided by the model.},
	booktitle = {Companion {Proceedings} of the 26th {International} {Conference} on {Multimodal} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Yong Wong, Miuyin and Valakuzhy, Kevin and Ahamad, Mustaque and Blough, Doug and Monrose, Fabian},
	year = {2024},
	keywords = {Large Language Model, Malware Analysis, Symbolic Analysis},
	pages = {36--40},
}

@inproceedings{yang_characterizing_2024,
	address = {New York, NY, USA},
	series = {{IMC} '24},
	title = {Characterizing the {Security} {Facets} of {IoT} {Device} {Setup}},
	isbn = {979-8-4007-0592-2},
	url = {https://doi.org/10.1145/3646547.3688433},
	doi = {10.1145/3646547.3688433},
	abstract = {In this work, we characterize the potential information leakage from IoT platforms during their setup phase. Setup involves an IoT device, its ”app”, and a cloud-based service. We assume that the on-device firmware is inaccessible, e.g., read-protected. We focus on the combination of information that can be extracted from analyzing the app and the local communication between the app and the IoT device. An attacker can trivially obtain the app, analyze its operation, and potentially eavesdrop on the wireless communication occurring during the setup phase. We develop a semi-automated general methodology involving off-the-shelf tools to examine information disclosure during the setup phase. We tested our methodology on twenty commodity-grade IoT devices. The outcome reveals a wide range of device-dependent choices for encryption at various layers and the potential for exposure of, among other things, device-identifying information and local networking (WiFi) credentials. Our methodology contributes towards a means to assess and ”certify” IoT devices.},
	booktitle = {Proceedings of the 2024 {ACM} on {Internet} {Measurement} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Han and Kuzniar, Carson and Jiang, Chengyan and Nikolaidis, Ioanis and Haque, Israat},
	year = {2024},
	keywords = {information leakage, iot, setup security, smart home},
	pages = {612--621},
}

@inproceedings{song_typefsl_2024,
	address = {New York, NY, USA},
	series = {{ASE} '24},
	title = {{TypeFSL}: {Type} {Prediction} from {Binaries} via {Inter}-procedural {Data}-flow {Analysis} and {Few}-shot {Learning}},
	isbn = {979-8-4007-1248-7},
	url = {https://doi.org/10.1145/3691620.3695502},
	doi = {10.1145/3691620.3695502},
	abstract = {Type recovery in stripped binaries is a critical and challenging task in reverse engineering, as it is the basis for many security applications (e.g., vulnerability detection). Traditional analysis methods are limited by software complexity and emerging types in real-world projects. To address these limitations, machine learning methods have been explored. However, the existing supervised learning approaches struggle with analyzing complicated and uncommon types due to the limited availability of samples. Additionally, none of the existing works can capture fine-grained and inter-procedural features in the binaries. In this paper, we present TypeFSL, a framework that addresses the challenge of imbalanced type distributions by incorporating few-shot learning and captures inter-procedural semantics through program slicing. Moreover, based on a dataset with 3,003,117 functions, TypeFSL achieves an average of 77.9\% and 84.6\% accuracy across all architecture and optimizations in 20-way 5-shot and 10-shot classification tasks. Our prototype outperforms existing techniques in prediction accuracy and obfuscation resistance. Finally, the case studies demonstrate how TypeFSL predicts uncommon and complicated types in practical analysis.},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Song, Zirui and Zhou, YuTong and Dong, Shuaike and Zhang, Ke and Zhang, Kehuan},
	year = {2024},
	keywords = {reverse engineering, few-shot learning, type recovery},
	pages = {1269--1281},
}

@inproceedings{zhao_models_2024,
	address = {New York, NY, USA},
	series = {{ASE} '24},
	title = {Models {Are} {Codes}: {Towards} {Measuring} {Malicious} {Code} {Poisoning} {Attacks} on {Pre}-trained {Model} {Hubs}},
	isbn = {979-8-4007-1248-7},
	url = {https://doi.org/10.1145/3691620.3695271},
	doi = {10.1145/3691620.3695271},
	abstract = {The proliferation of pre-trained models (PTMs) and datasets has led to the emergence of centralized model hubs like Hugging Face, which facilitate collaborative development and reuse. However, recent security reports have uncovered vulnerabilities and instances of malicious attacks within these platforms, highlighting growing security concerns. This paper presents the first systematic study of malicious code poisoning attacks on pre-trained model hubs, focusing on the Hugging Face platform. We conduct a comprehensive threat analysis, develop a taxonomy of model formats, and perform root cause analysis of vulnerable formats. While existing tools like Fickling and ModelScan offer some protection, they face limitations in semantic-level analysis and comprehensive threat detection. To address these challenges, we propose MalHug, an end-to-end pipeline tailored for Hugging Face that combines dataset loading script extraction, model deserialization, in-depth taint analysis, and heuristic pattern matching to detect and classify malicious code poisoning attacks in datasets and models. In collaboration with Ant Group, a leading financial technology company, we have implemented and deployed MalHug on a mirrored Hugging Face instance within their infrastructure, where it has been operational for over three months. During this period, MalHug has monitored more than 705K models and 176K datasets, uncovering 91 malicious models and 9 malicious dataset loading scripts. These findings reveal a range of security threats, including reverse shell, browser credential theft, and system reconnaissance. This work not only bridges a critical gap in understanding the security of the PTM supply chain but also provides a practical, industry-tested solution for enhancing the security of pre-trained model hubs.},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Jian and Wang, Shenao and Zhao, Yanjie and Hou, Xinyi and Wang, Kailong and Gao, Peiming and Zhang, Yuanchao and Wei, Chen and Wang, Haoyu},
	year = {2024},
	keywords = {code poisoning attacks, LLM supply chain, pre-trained model hub},
	pages = {2087--2098},
}

@article{li_varlifter_2024,
	address = {New York, NY, USA},
	title = {{VarLifter}: {Recovering} {Variables} and {Types} from {Bytecode} of {Solidity} {Smart} {Contracts}},
	volume = {8},
	url = {https://doi.org/10.1145/3689711},
	doi = {10.1145/3689711},
	abstract = {Since funds or tokens in smart contracts are maintained through specific state variables, contract audit, an effective means for security assurance, particularly focuses on these variables and their related operations. However, the absence of publicly accessible source code for numerous contracts, with only bytecode exposed, hinders audit efforts. Recovering variables and their types from Solidity bytecode is thus a critical task in smart contract analysis and audit, yet this is a challenging task because the bytecode loses variable and type information, only with low-level data operated by stack manipulations and untyped memory/storage accesses. The state-of-the-art smart contract decompilers miss identifying many variables and incorrectly infer the types for many identified variables. To this end, we propose VarLifter, a lifter dedicated to the precise and efficient recovery of typed variables. VarLifter interprets every read or written field of a data region as at least one potential variable, and after discarding falsely identified variables, it progressively refines the variable types based on the variable behaviors in the form of operation sequences. We evaluate VarLifter on 34,832 real-world Solidity smart contracts. VarLifter attains a precision of 97.48\% and a recall of 91.84\% for typed variable recovery. Moreover, VarLifter finishes analyzing 77\% of smart contracts in around 10 seconds per contract. If VarLifter is used to replace the variable recovery modules of the two state-of-the-art Solidity bytecode decompilers, 52.4\%, and 74.6\% more typed variables will be correctly recovered, respectively. The applications of VarLifter to contract decompilation, contract audit, and contract bytecode fuzzing illustrate that the recovered variable information improves many contract analysis tasks.},
	number = {OOPSLA2},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Li, Yichuan and Song, Wei and Huang, Jeff},
	month = oct,
	year = {2024},
	keywords = {smart contract, Blockchain, EVM, Solidity bytecode, variable recovery},
}

@inproceedings{botacin_what_2024,
	address = {New York, NY, USA},
	series = {{RAID} '24},
	title = {What do malware analysts want from academia? {A} survey on the state-of-the-practice to guide research developments},
	isbn = {979-8-4007-0959-3},
	url = {https://doi.org/10.1145/3678890.3678892},
	doi = {10.1145/3678890.3678892},
	abstract = {Malware analysis tasks are as fundamental for modern cybersecurity as they are challenging to perform. More than depending on any tool capability, malware analysis tasks depend on human analysts’ abilities, experiences, and practices when using the tools. Academic research has traditionally been focused on producing solutions to overcome malware analysis technical challenges, but are these solutions adopted in practice by malware analysts? Are these solutions useful? If not, how can the academic community improve its practices to foster adoption and cause a greater impact? To answer these questions, we surveyed 21 professional malware analysts working in different companies, from CSIRTs to AV companies, to hear their opinions about existing tools, practices, and the challenges they face in their daily tasks. In 31 questions, we cover a broad range of aspects, from the number of observed malware variants to the use of public sandboxes and the tools the analysts would like to exist to make their lives easier. We aim to bridge the gap between academic developments and malware practices. To do so, on the one hand, we suggest to the analysts the solutions proposed in the literature that could be integrated into their practices. On the other hand, we also point out to the academic community possible future directions to bridge existing development gaps that significantly affect malware analysis practices.},
	booktitle = {Proceedings of the 27th {International} {Symposium} on {Research} in {Attacks}, {Intrusions} and {Defenses}},
	publisher = {Association for Computing Machinery},
	author = {Botacin, Marcus},
	year = {2024},
	keywords = {Reverse Engineering, Malware, Malware Analysis, Analysis Tools},
	pages = {77--96},
}

@inproceedings{fu_android_2024,
	address = {New York, NY, USA},
	series = {{WSSE} '24},
	title = {An {Android} {Malware} {Detection} {Method} {Based} on {Native} {Code} and {LSTM}},
	isbn = {979-8-4007-1708-6},
	url = {https://doi.org/10.1145/3698062.3698068},
	doi = {10.1145/3698062.3698068},
	abstract = {The ubiquity of mobile Internet has made smartphones an essential component of modern life, yet they remain prime targets for information security threats. Despite a multitude of malware detection systems, skilled developers of malicious software adeptly conceal harmful code within applications, thus eluding standard detection techniques. A significant challenge in Android application reverse engineering is the extraction of critical code from ".so" files, known as native code, which is often viewed as a secure and clandestine strategy in Android development. Addressing this challenge, our research introduces a novel Android malware detection approach that leverages Long Short-Term Memory (LSTM) networks to scrutinize native code opcodes. By decompressing Android Application Packages (APKs) and extracting opcodes from shared libraries, we utilize the tf-idf algorithm for feature selection, facilitating the detection of obscured malicious code in a static analysis context. This methodology, bolstered by LSTM training, constructs a formidable framework for Android malware detection. Our empirical assessments confirm the method's effectiveness and superiority in detecting Android malware.},
	booktitle = {Proceedings of the 2024 {The} 6th {World} {Symposium} on {Software} {Engineering} ({WSSE})},
	publisher = {Association for Computing Machinery},
	author = {Fu, Hao and Wu, Shaofei},
	year = {2024},
	keywords = {native code, malware detection, Android applications, long short-term memory (LSTM), term frequency-inverse document frequency (TF-IDF)},
	pages = {38--44},
}

@inproceedings{nguyen_scalable_2024,
	address = {New York, NY, USA},
	series = {{ISSTA} 2024},
	title = {Scalable, {Sound}, and {Accurate} {Jump} {Table} {Analysis}},
	isbn = {979-8-4007-0612-7},
	url = {https://doi.org/10.1145/3650212.3680301},
	doi = {10.1145/3650212.3680301},
	abstract = {Jump tables are a common source of indirect jumps in binary code. Resolving these indirect jumps is critical for constructing a complete control-flow graph, which is an essential first step for most applications involving binaries, including binary hardening and instrumentation, binary analysis and fuzzing for vulnerability discovery, malware analysis and reverse engineering. Existing techniques for jump table analysis generally prioritize performance over soundness. While lack of soundness may be acceptable for applications such as decompilation, it can cause unpredictable runtime failures in binary instrumentation applications. We therefore present SJA, a new jump table analysis technique in this paper that is sound and scalable. Our analysis uses a novel abstract domain to systematically track the "structure" of computed code pointers without relying on syntactic pattern-matching that is common in previous works. In addition, we present a bounds analysis that efficiently and losslessly reasons about equality and inequality relations that arise in the context of jump tables. As a result, our system reduces miss rate by 35× over the next best technique. When evaluated on error rate based on F1-score, our technique outperforms the best previous techniques by 3×.},
	booktitle = {Proceedings of the 33rd {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Huan and Priyadarshan, Soumyakant and Sekar, R.},
	year = {2024},
	keywords = {reverse engineering, static analysis},
	pages = {541--552},
}

@inproceedings{xiong_atlas_2024,
	address = {New York, NY, USA},
	series = {{ISSTA} 2024},
	title = {Atlas: {Automating} {Cross}-{Language} {Fuzzing} on {Android} {Closed}-{Source} {Libraries}},
	isbn = {979-8-4007-0612-7},
	url = {https://doi.org/10.1145/3650212.3652133},
	doi = {10.1145/3650212.3652133},
	abstract = {Fuzzing is an effective method for detecting security bugs in software, and there have been quite a few effective works on fuzzing Android. Researchers have developed methods for fuzzing open-source native APIs and Java interfaces on actual Android devices. However, the realm of automatically fuzzing Android closed-source native libraries, particularly on emulators, remains insufficiently explored. There are two key challenges: firstly, the multi-language programming model inherent to Android; and secondly, the absence of a Java runtime environment within the emulator. To address these challenges, we propose Atlas, a practical automated fuzz framework for Android closed-source native libraries. Atlas consists of an automatic harness generator and a fuzzer containing the necessary runtime environment. The generator uses static analysis techniques to deduce the correct calling sequences and parameters of the native API according to the information from the "native world" and the "Java world". To maximize the practicality of the generated harness, Atlas heuristically optimizes the generated harness. The Fuzzer provides the essential Java runtime environment in the emulator, making it possible to fuzz the Android closed-source native libraries on a multi-core server. We have tested Atlas on 17 pre-installed apps from four Android vendors. Atlas generates 820 harnesses containing 767 native APIs, of which 78\% is practical. Meanwhile, Atlas has discovered 74 new security bugs with 16 CVEs assigned. The experiments show that Atlas can efficiently generate high-quality harnesses and find security bugs.},
	booktitle = {Proceedings of the 33rd {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Xiong, Hao and Dai, Qinming and Chang, Rui and Qiu, Mingran and Wang, Renxiang and Shen, Wenbo and Zhou, Yajin},
	year = {2024},
	keywords = {Android, static analysis, fuzzing, vulnerability},
	pages = {350--362},
}

@inproceedings{fissore_higher-order_2024,
	address = {New York, NY, USA},
	series = {{PPDP} '24},
	title = {Higher-{Order} unification for free!: {Reusing} the meta-language unification for the object language},
	isbn = {979-8-4007-0969-2},
	url = {https://doi.org/10.1145/3678232.3678233},
	doi = {10.1145/3678232.3678233},
	abstract = {Specifying and implementing a proof system from scratch requires significant effort. Logical Frameworks and Higher Order Logic Programming Languages provide dedicated, high-level meta languages to facilitate this task in two ways: 1) variable binding and substitution are for free when meta language binders represent object logic ones; 2) proof construction, and proof search, are greatly simplified by leveraging the unification procedure provided by the meta language. Notable examples of meta languages are Elf\&nbsp;[21], Twelf\&nbsp;[23], λ Prolog\&nbsp;[16], Beluga\&nbsp;[24], Abella\&nbsp;[8] and Isabelle\&nbsp;[31] which have been used to implement or specify many formal systems such as First Order Logic\&nbsp;[5], Set Theory\&nbsp;[20], Higher Order Logic\&nbsp;[19], and the Calculus of Constructions\&nbsp;[4]. The object logic we are interested in is Coq’s type theory\&nbsp;[28]. We aim to develop a higher-order unification-based proof search procedure using the meta language Elpi\&nbsp;[3], a dialect of λ Prolog. Elpi’s equational theory includes βη -equivalence and features a higher-order unification procedure ≃ m for the pattern fragment\&nbsp;[15]. Elpi offers an encoding of Coq terms that is suitable for meta programming\&nbsp;[6, 9, 26, 27] but that restricts ≃ m to first-order unification problems only. We refer to this basic encoding as \&lt;Formula format="inline"\&gt;\&lt;TexMath\&gt;\&lt;?TeX mathcal O?\&gt;\&lt;/TexMath\&gt;\&lt;AltText\&gt;Math 1\&lt;/AltText\&gt;\&lt;File name="ppdp2024-1-inline1" type="svg"/\&gt;\&lt;/Formula\&gt;. In this paper we translate unification problems in \&lt;Formula format="inline"\&gt;\&lt;TexMath\&gt;\&lt;?TeX mathcal O?\&gt;\&lt;/TexMath\&gt;\&lt;AltText\&gt;Math 2\&lt;/AltText\&gt;\&lt;File name="ppdp2024-1-inline2" type="svg"/\&gt;\&lt;/Formula\&gt; to an alternative encoding called \&lt;Formula format="inline"\&gt;\&lt;TexMath\&gt;\&lt;?TeX mathcal M?\&gt;\&lt;/TexMath\&gt;\&lt;AltText\&gt;Math 3\&lt;/AltText\&gt;\&lt;File name="ppdp2024-1-inline3" type="svg"/\&gt;\&lt;/Formula\&gt;, from which we derive ≃ o, the higher-order unification procedure of \&lt;Formula format="inline"\&gt;\&lt;TexMath\&gt;\&lt;?TeX mathcal O?\&gt;\&lt;/TexMath\&gt;\&lt;AltText\&gt;Math 4\&lt;/AltText\&gt;\&lt;File name="ppdp2024-1-inline4" type="svg"/\&gt;\&lt;/Formula\&gt;. ≃ o honours βη -equivalence for terms within the pattern fragment, and allows for the use of heuristics when the terms fall outside the pattern fragment. Moreover, as ≃ o delegates most of the work to ≃ m, it can be used to efficiently simulate a logic program in \&lt;Formula format="inline"\&gt;\&lt;TexMath\&gt;\&lt;?TeX mathcal O?\&gt;\&lt;/TexMath\&gt;\&lt;AltText\&gt;Math 5\&lt;/AltText\&gt;\&lt;File name="ppdp2024-1-inline5" type="svg"/\&gt;\&lt;/Formula\&gt; by taking advantage of unification-related optimizations of the meta language, such as clause indexing.},
	booktitle = {Proceedings of the 26th {International} {Symposium} on {Principles} and {Practice} of {Declarative} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Fissore, Davide and Tassi, Enrico},
	year = {2024},
	keywords = {Higher-Order Unification, Logic Programming, Meta-Programming},
}

@inproceedings{krook_welcome_2024,
	address = {New York, NY, USA},
	series = {Haskell 2024},
	title = {Welcome to the {Parti}(tioning) ({Functional} {Pearl}): {Using} {Rewrite} {Rules} and {Specialisation} to {Partition} {Haskell} {Programs}},
	isbn = {979-8-4007-1102-2},
	url = {https://doi.org/10.1145/3677999.3678276},
	doi = {10.1145/3677999.3678276},
	abstract = {Writing distributed applications is hard, as the programmer needs to describe the communication protocol between the different endpoints. If this is not done correctly, we can introduce bugs such as deadlocks and data races. Tierless and choreographic programming models aim to make this easier by describing the interactions of every endpoint in a single compilation unit. When such a program is compiled, ideally, a single endpoint is projected and the code for the other endpoints is removed. This leads to smaller binaries with fewer dependencies, and is called program partitioning. In this pearl, we show how we can use rewrite rules and specialisation to get GHC to partition our Haskell programs (almost) for free, if they are written using the Haste App or HasChor framework. As an example of why partitioning is useful, we show how an example application can be more easily built and deployed after being partitioned.},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Haskell} {Symposium}},
	publisher = {Association for Computing Machinery},
	author = {Krook, Robert and Hammersberg, Samuel},
	year = {2024},
	keywords = {Choreographic Programming, Haskell, Program Partitioning, Rewrite Rules, Specialisation, Tierless Programming},
	pages = {27--40},
}

@inproceedings{liao_smart_2024,
	address = {New York, NY, USA},
	series = {{DEBAI} '24},
	title = {Smart contract vulnerability detection based on dynamic and static combination},
	isbn = {979-8-4007-1026-1},
	url = {https://doi.org/10.1145/3700058.3700123},
	doi = {10.1145/3700058.3700123},
	abstract = {In the field of blockchain technology, smart contracts play a core role, but programming oversights may cause serious security risks. This study provides a comprehensive review of the types of smart contract security vulnerabilities and the development of detection techniques. This article conducts a comprehensive review of the current various detection methods, including static and dynamic analysis, and proposes a combined dynamic and static detection method for integer overflow vulnerabilities at the solidity code level and timestamp vulnerabilities at the blockchain system layer. It also analyzes a The performance of a series of mainstream detection tools in terms of detection accuracy and efficiency is compared in depth, and their advantages and limitations are analyzed.},
	booktitle = {Proceedings of the {International} {Conference} on {Digital} {Economy}, {Blockchain} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Xue},
	year = {2024},
	pages = {412--416},
}

@inproceedings{bove_large-scale_2024,
	address = {New York, NY, USA},
	series = {{ARES} '24},
	title = {A {Large}-{Scale} {Study} on the {Prevalence} and {Usage} of {TEE}-based {Features} on {Android}},
	isbn = {979-8-4007-1718-5},
	url = {https://doi.org/10.1145/3664476.3664486},
	doi = {10.1145/3664476.3664486},
	abstract = {In the realm of mobile security, where OS-based protections have proven insufficient against robust attackers, Trusted Execution Environments (TEEs) have emerged as a hardware-based security technology. Despite the industry’s persistence in advancing TEE technology, the impact on end users and developers remains largely unexplored. This study addresses this gap by conducting a large-scale analysis of TEE utilization in Android applications, focusing on the key areas of cryptography, digital rights management, biometric authentication, and secure dialogs. To facilitate our extensive analysis, we introduce Mobsec Analytika, a framework tailored for large-scale app examinations, which we make available to the research community. Through 333,475 popular Android apps, our analysis illuminates the implementation of TEE-related features and their contextual usage. Our findings reveal that TEE features are predominantly utilized indirectly through third-party libraries, with only 6.2\% of apps directly invoking the APIs. Moreover, the study reveals the underutilization of the recent TEE-based UI feature Protected Confirmation.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Bove, Davide},
	year = {2024},
	keywords = {android, mobile security, api, tee, usage},
}

@inproceedings{dong_pilot_2024,
	address = {New York, NY, USA},
	series = {{SEA4DQ} 2024},
	title = {A {Pilot} {Study} in {Surveying} {Data} {Challenges} of {Automatic} {Software} {Engineering} {Tasks}},
	isbn = {979-8-4007-0672-1},
	url = {https://doi.org/10.1145/3663530.3665020},
	doi = {10.1145/3663530.3665020},
	abstract = {The surge in automatic SE research aims to boost development efficiency and quality while reducing costs. However, challenges such as limited real-world project data and inadequate data conditions constrain the effectiveness of these methods. To systematically understand these challenges, our pilot study reviews prevalent data challenges across various SE tasks. Despite these challenges, thanks to the advances of large language model offers promising performance on SE tasks. Overall, this pilot survey focused on provide a quick retrospective review on SE data challenges and introduce practical LLM solutions from the SE community to mitigate these challenges.},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Software} {Engineering} and {AI} for {Data} {Quality} in {Cyber}-{Physical} {Systems}/{Internet} of {Things}},
	publisher = {Association for Computing Machinery},
	author = {Dong, Liming and Lu, Qinghua and Zhu, Liming},
	year = {2024},
	keywords = {LLM, Automatic Software Engineering, Data Challenge, Pilot Survey},
	pages = {6--11},
}

@inproceedings{brain_misconceptions_2024,
	address = {New York, NY, USA},
	series = {{SOAP} 2024},
	title = {Misconceptions about {Loops} in {C}},
	isbn = {979-8-4007-0621-9},
	url = {https://doi.org/10.1145/3652588.3663324},
	doi = {10.1145/3652588.3663324},
	abstract = {Loop analysis is a key component of static analysis tools. Unfortunately, there are several rare edge cases. As a tool moves from academic prototype to production-ready, obscure cases can and do occur. This results in loop analysis being a key source of late-discovered but significant algorithmic bugs. To avoid these, this paper presents a collection of examples and "folklore" challenges in loop analysis.},
	booktitle = {Proceedings of the 13th {ACM} {SIGPLAN} {International} {Workshop} on the {State} {Of} the {Art} in {Program} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Brain, Martin and Malkawi, Mahdi},
	year = {2024},
	keywords = {Static Analysis, Loop Analysis, Software Verification},
	pages = {60--66},
}

@inproceedings{braconaro_dataset_2025,
	address = {New York, NY, USA},
	series = {{CODASPY} '25},
	title = {A {Dataset} for {Evaluating} {LLMs} {Vulnerability} {Repair} {Performance} in {Android} {Applications}: {Data}/{Toolset} paper},
	isbn = {979-8-4007-1476-4},
	url = {https://doi.org/10.1145/3714393.3726486},
	doi = {10.1145/3714393.3726486},
	abstract = {Automated Program Repair (APR) is a well-established research area that enhances software reliability and security by automatically fixing bugs, reducing manual effort, and accelerating debugging. Despite progress in publishing benchmarks to evaluate APR tools, datasets specifically targeting Android are lacking.To address this gap, we introduce a dataset of 272 real-world violations of Google's Android Security Best Practices, identified by statically analyzing 113 real-world Android apps. In addition to the faulty code, we manually crafted repairs based on Google's guidelines, covering 176 Java-based and 96 XML-based violations from Android Java classes and Manifest files, respectively. Additionally, we leveraged our novel dataset to evaluate Large Language Models (LLMs) as they are the latest promising APR tools. In particular, we evaluated GPT-4o, Gemini 1.5 Flash and Gemini in Android Studio and we found that GPT-4o outperforms Google's models, demonstrating higher accuracy and robustness across a range of violations types. Hence, with this dataset, we aim to provide valuable insights for advancing APR research and improving tools for Android security.},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {Conference} on {Data} and {Application} {Security} and {Privacy}},
	publisher = {Association for Computing Machinery},
	author = {Braconaro, Elisa and Losiouk, Eleonora},
	year = {2025},
	keywords = {large language models, android vulnerabilities, automated program repair},
	pages = {353--358},
}

@article{pizzolotto_mitigating_2024,
	address = {New York, NY, USA},
	title = {Mitigating {Debugger}-based {Attacks} to {Java} {Applications} with {Self}-debugging},
	volume = {33},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3631971},
	doi = {10.1145/3631971},
	abstract = {Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode.In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them.We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Pizzolotto, Davide and Berlato, Stefano and Ceccato, Mariano},
	month = apr,
	year = {2024},
	keywords = {Anti-debugging, maliciuos reverse engineering, man at the end attacks, tampering attacks},
}

@inproceedings{yang_uncover_2024,
	address = {New York, NY, USA},
	series = {{ICSE} '24},
	title = {Uncover the {Premeditated} {Attacks}: {Detecting} {Exploitable} {Reentrancy} {Vulnerabilities} by {Identifying} {Attacker} {Contracts}},
	isbn = {979-8-4007-0217-4},
	url = {https://doi.org/10.1145/3597503.3639153},
	doi = {10.1145/3597503.3639153},
	abstract = {Reentrancy, a notorious vulnerability in smart contracts, has led to millions of dollars in financial loss. However, current smart contract vulnerability detection tools suffer from a high false positive rate in identifying contracts with reentrancy vulnerabilities. Moreover, only a small portion of the detected reentrant contracts can actually be exploited by hackers, making these tools less effective in securing the Ethereum ecosystem in practice.In this paper, we propose BlockWatchdog, a tool that focuses on detecting reentrancy vulnerabilities by identifying attacker contracts. These attacker contracts are deployed by hackers to exploit vulnerable contracts automatically. By focusing on attacker contracts, BlockWatchdog effectively detects truly exploitable reentrancy vulnerabilities by identifying reentrant call flow. Additionally, BlockWatchdog is capable of detecting new types of reentrancy vulnerabilities caused by poor designs when using ERC tokens or user-defined interfaces, which cannot be detected by current rule-based tools. We implement BlockWatchdog using cross-contract static dataflow techniques based on attack logic obtained from an empirical study that analyzes attacker contracts from 281 attack incidents. BlockWatchdog is evaluated on 421,889 Ethereum contract bytecodes and identifies 113 attacker contracts that target 159 victim contracts, leading to the theft of Ether and tokens valued at approximately 908.6 million USD. Notably, only 18 of the identified 159 victim contracts can be reported by current reentrancy detection tools.},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Shuo and Chen, Jiachi and Huang, Mingyuan and Zheng, Zibin and Huang, Yuan},
	year = {2024},
	keywords = {smart contract, ethereum, attacker identification, dataflow analysis, reentrancy},
}

@inproceedings{zhong_prettysmart_2024,
	address = {New York, NY, USA},
	series = {{ICSE} '24},
	title = {{PrettySmart}: {Detecting} {Permission} {Re}-delegation {Vulnerability} for {Token} {Behaviors} in {Smart} {Contracts}},
	isbn = {979-8-4007-0217-4},
	url = {https://doi.org/10.1145/3597503.3639140},
	doi = {10.1145/3597503.3639140},
	abstract = {As an essential component in Ethereum and other blockchains, token assets have been interacted with by diverse smart contracts. Effective permission policies of smart contracts must prevent token assets from being manipulated by unauthorized adversaries. Recent efforts have studied the accessibility of privileged functions or state variables to unauthorized users. However, little attention is paid to how publicly accessible functions of smart contracts can be manipulated by adversaries to steal users' digital assets. This attack is mainly caused by the permission re-delegation (PRD) vulnerability. In this work, we propose PrettySmart, a bytecode-level Permission re-delegation vulnerability detector for Smart contracts. Our study begins with an empirical study on 0.43 million open-source smart contracts, revealing that five types of widely-used permission constraints dominate 98\% of the studied contracts. Accordingly, we propose a mechanism to infer these permission constraints, as well as an algorithm to identify constraints that can be bypassed by unauthorized adversaries. Based on the identification of permission constraints, we propose to detect whether adversaries could manipulate the privileged token management functionalities of smart contracts. The experimental results on real-world datasets demonstrate the effectiveness of the proposed PrettySmart, which achieves the highest precision score and detects 118 new PRD vulnerabilities.},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhong, Zhijie and Zheng, Zibin and Dai, Hong-Ning and Xue, Qing and Chen, Junjia and Nan, Yuhong},
	year = {2024},
	keywords = {smart contract, vulnerability detection, permission control},
}

@inproceedings{liu_minimon_2024,
	address = {New York, NY, USA},
	series = {{ICSE} '24},
	title = {{MiniMon}: {Minimizing} {Android} {Applications} with {Intelligent} {Monitoring}-{Based} {Debloating}},
	isbn = {979-8-4007-0217-4},
	url = {https://doi.org/10.1145/3597503.3639113},
	doi = {10.1145/3597503.3639113},
	abstract = {The size of Android applications is getting larger to fulfill the requirements of various users. However, not all the features of the applications are needed and desired by a specific user. The unnecessary and non-desired features can increase the attack surface and consume system resources such as storage and memory. To address this issue, we propose a framework, MiniMon, to debloat unnecessary features from an Android app based on the logs of specific users' interactions with the app.However, rarely used features may not be recorded during the data collection, and users' preferences may change slightly over time. To address these challenges, we embed several solutions in our framework that can uncover user-desired features by learning and generalizing from the logs of how users interact with an application. MiniMon first collects the application methods that are executed when users interact with it. Then, given the collected executed methods and the call graph of the application, MiniMon applies 10 techniques to generalize from logs. These include three program analysis-based techniques, two graph clustering-based techniques, and five graph embedding-based techniques to identify the additional methods in an app that are similar to the logged executed methods. Finally, MiniMon generates a debloated application by removing methods that are not similar to the executed methods. To evaluate the performance of variants of MiniMon that use different generalization techniques, we create a benchmark for a controlled experiment. The results show that the graph embedding-based generalization technique that considers the information of all nodes in the call graph is the best, and can correctly uncover 75.5\% of the unobserved but desired behaviors and still debloat more than half of the app. We also conducted a user study that uncovers that the use of the intelligent (generalization) method of MiniMon boosts the overall user satisfaction rate by 37.6\%.},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Jiakun and Zhang, Zicheng and Hu, Xing and Thung, Ferdian and Maoz, Shahar and Gao, Debin and Toch, Eran and Zhao, Zhipeng and Lo, David},
	year = {2024},
	keywords = {android, log analysis, software debloating},
}

@inproceedings{xiao_research_2024,
	address = {New York, NY, USA},
	series = {{BDEIM} '23},
	title = {Research on {Improved} {OLLVM} {Based} on {Code} {Rearrangement} {Architecture}},
	isbn = {979-8-4007-1666-9},
	url = {https://doi.org/10.1145/3659211.3659272},
	doi = {10.1145/3659211.3659272},
	abstract = {Code obfuscation increases the difficulty of reverse engineering software and devices, improves the security of software and devices, and also prevents governments, enterprises, and socially important groups from the loss of information leakage. In recent years, with the continuous rise of LLVM architecture, OLLVM obfuscation solves cross-platform code obfuscation while increasing the difficulty of reverse engineering. for OLLVM obfuscation, we propose an improvement scheme for OLLVM obfuscation, which improves the degree of obfuscation of OLLVM and increases the difficulty of reverse engineering of software and device source code. Firstly, we propose an obfuscation scheme for code rearrangement; secondly, for control flow obfuscation, we propose an improvement scheme for NOLLVM control flow obfuscation and add an enhancement obfuscation module to increase the obfuscation capability of OLLVM, and lastly, the obfuscation effect of the code rearrangement scheme and the enhancement effect of the control flow obfuscation improvement module are verified through experiments and verified to improve the software and device source codes' Security.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Big} {Data} {Economy} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Yuxuan and Fei, Jinlong},
	year = {2024},
	pages = {350--358},
}

@inproceedings{spiess_stracebert_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {{STraceBERT}: {Source} {Code} {Retrieval} using {Semantic} {Application} {Traces}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3617852},
	doi = {10.1145/3611643.3617852},
	abstract = {Software reverse engineering is an essential task in software engineering and security, but it can be a challenging process, especially for adversarial artifacts. To address this challenge, we present STraceBERT, a novel approach that utilizes a Java dynamic analysis tool to record calls to core Java libraries, and pretrain a BERT-style model on the recorded application traces for effective method source code retrieval from a candidate set. Our experiments demonstrate the effectiveness of STraceBERT in retrieving the source code compared to existing approaches. Our proposed approach offers a promising solution to the problem of code retrieval in software reverse engineering and opens up new avenues for further research in this area.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Spiess, Claudio},
	year = {2023},
	keywords = {reverse engineering, neural information retrieval, tracing},
	pages = {2207--2209},
}

@inproceedings{zhao_deepinfer_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {{DeepInfer}: {Deep} {Type} {Inference} from {Smart} {Contract} {Bytecode}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3616343},
	doi = {10.1145/3611643.3616343},
	abstract = {Smart contracts play an increasingly important role in Ethereum platform. It provides various functions implementing numerous services, whose bytecode runs on Ethereum Virtual Machine. To use services by invoking corresponding functions, the callers need to know the function signatures. Moreover, such signatures provide crucial information for many downstream applications, e.g., identifying smart contracts, fuzzing, detecting vulnerabilities, etc. However, it is challenging to infer function signatures from the bytecode due to a lack of type information. Existing work solving this problem depended heavily on limited databases or hard-coded heuristic patterns. However, these approaches are hard to be adapted to semantic differences in distinct languages and various compiler versions when developing smart contracts. In this paper, we propose a novel framework DeepInfer that first leverages deep learning techniques to automatically infer function signatures and returns. The novelties of DeepInfer are: 1) DeepInfer lifts the bytecode into the Intermediate Representation (IR) to preserve code semantics; 2) DeepInfer extracts the type-related knowledge (e.g., critical data flows, constant values, and control flow graphs) from the IR to recover function signatures and returns. We conduct experiments on Solidity and Vyper smart contracts and the results show that DeepInfer performs faster and more accurate than existing tools, while being immune to changes in different languages and various compiler versions.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Kunsong and Li, Zihao and Li, Jianfeng and Ye, He and Luo, Xiapu and Chen, Ting},
	year = {2023},
	keywords = {Deep Learning, Type Inference, Smart Contract},
	pages = {745--757},
}

@inproceedings{han_systematic_2023,
	address = {New York, NY, USA},
	series = {{SaTS} '23},
	title = {Systematic {Analysis} of {Security} and {Vulnerabilities} in {Miniapps}},
	isbn = {979-8-4007-0258-7},
	url = {https://doi.org/10.1145/3605762.3624432},
	doi = {10.1145/3605762.3624432},
	abstract = {The past few years have witnessed a boom of miniapps, as lightweight applications, miniapps are of great importance in the mobile internet sector. Consequently, the security of miniapps can directly impact compromising the integrity of sensitive data, posing a potential threat to user privacy. However, after a thorough review of the various research efforts in miniapp security, we found that their actions in researching the safety of miniapp web interfaces are limited. This paper proposes a triad threat model focusing on users, servers and attackers to mitigate the security risk of miniapps. By following the principle of least privilege and the direction of permission consistency, we design a novel analysis framework for the security risk assessment of miniapps by this model. Then, we analyzed the correlation between the security risk assessment and the threat model associated with the miniapp. This analysis led to identifying potential scopes and categorisations with security risks. In the case study, we identify nine major categories of vulnerability issues, such as SQL injection, logical vulnerabilities and cross-site scripting. We also assessed a total of 50,628 security risk hazards and provide specific examples.},
	booktitle = {Proceedings of the 2023 {ACM} {Workshop} on {Secure} and {Trustworthy} {Superapps}},
	publisher = {Association for Computing Machinery},
	author = {Han, Yuyang and Ji, Xu and Wang, Zhiqiang and Zhang, Jianyi},
	year = {2023},
	keywords = {least privilege, miniapps, security risk, vulnerabilities},
	pages = {1--9},
}

@inproceedings{tao_jslibd_2023,
	address = {New York, NY, USA},
	series = {{SaTS} '23},
	title = {{JSLibD}: {Reliable} and {Heuristic} {Detection} of {Third}-party {Libraries} in {Miniapps}},
	isbn = {979-8-4007-0258-7},
	url = {https://doi.org/10.1145/3605762.3624428},
	doi = {10.1145/3605762.3624428},
	abstract = {Miniapps have become an indispensable part of people's lives. Meanwhile, the utilization of third-party libraries greatly streamlines, expedites, and enhances the development of miniapps. However, ensuring the security of these third-party libraries presents a challenge, as they may harbor security vulnerabilities, such as plaintext transmission. In this paper, we propose JSLibD, an automated extraction method for third-party libraries in miniapps. Unlike conventional extraction methods that heavily rely on prior knowledge, JSLibD introduces a heuristic prediction approach, comprising two integral components: a whitelist matching method to match the known libraries and a heuristic prediction method to extract the unknown libraries using function call relationships. The results demonstrate that JSLibD can efficiently match known libraries, and accurately predict unknown libraries, achieving an impressive precision rate of 85.9\% and a high recall rate of 97.2\%.},
	booktitle = {Proceedings of the 2023 {ACM} {Workshop} on {Secure} and {Trustworthy} {Superapps}},
	publisher = {Association for Computing Machinery},
	author = {Tao, Junjie and Shi, Jifei and Fan, Ming and Wang, Yin and Liu, Junfeng and Liu, Ting},
	year = {2023},
	keywords = {mobile security, miniapp, third-party library},
	pages = {11--16},
}

@inproceedings{song_multi-modality_2024,
	address = {New York, NY, USA},
	series = {{AAIA} '23},
	title = {A {Multi}-modality {Feature} {Fusion} {Method} for {Android} {Malware} {Detection}},
	isbn = {979-8-4007-0826-8},
	url = {https://doi.org/10.1145/3603273.3635055},
	doi = {10.1145/3603273.3635055},
	abstract = {The high market share and open-source nature of the Android system led to a significant increase in the number of malicious Android applications. It poses a lot of threats for users, such as financial costs, privacy breaches, and remote control. It is more efficient to construct accurate models to detect Android malware. We propose a novel Android malware detection framework MGIDroid. It considers two modality feature representations at the same: the function call graph (FCG) and Dex bytecode image features of Android applications. First, we construct an FCG that describes the relations between function calls for an Android application. We use GraphSAGE with the SAGPool model to extract FCG features. Next, we convert Dalvik Executable files of Android applications to Dex bytecode image, Resnet model with Convolutional Block Attention Module (CBAM) is adopted to extract image features that represent the data section of an Android application. Then, we use soft attention to fuse two modalities features to finish classification. Lastly, extensive experiments were conducted to evaluate the effectiveness of our approach. The results show that our proposed method outperforms other methods and achieves a high f1-score of 98.60\%.},
	booktitle = {Proceedings of the 2023 {International} {Conference} on {Advances} in {Artificial} {Intelligence} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Song, Jiahao and Li, Runzhi and Zhang, Zijiao},
	year = {2024},
	keywords = {deep learning, malware detection, Android malware, Multimodal Learning},
	pages = {380--384},
}

@inproceedings{yan_repackaged_2024,
	address = {New York, NY, USA},
	series = {{ICAICE} '23},
	title = {Repackaged {Android} {Apps} {Classification} {Based} on {Embedding} {Encoding} and {Decomposable} {Attention} {Neural} {Network}},
	isbn = {979-8-4007-0883-1},
	url = {https://doi.org/10.1145/3652628.3652786},
	doi = {10.1145/3652628.3652786},
	abstract = {Repackaging is one of the potential threats to the Android ecosystem as it deprives app developers of their benefits. When it is obfuscated to avoid detection, making it harder to repack and classify. State-of-the-art Android repackaged app classification approaches heavily rely on manually extracting dependency features as fingerprints. In this paper, we present a novel Android app repackaging classification method that utilizes the proposed EDA model to learn the semantic and sequence context information from smali embedding. It is extracted automatically from decompiled android .smali files. We leverage the high-frequency attention of embedding encoding to quickly identify public library subsequences. And our model uses intra-sequence attention for encoding between opcode embeddings within each .smali file to capture richer embedding encoding vectors. We make experiments on 8,000 android app pairs downloaded by Androzoo dataset. The results demonstrate that our method achieves 87.53\% accuracy for repackaged apps classification. It can effectively capture context between sequences, and exhibits better training and detection efficiency than time series-based deep neural network learning model.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Jinpei and Yang, Lu},
	year = {2024},
	pages = {951--956},
}

@inproceedings{xiong_hext5_2024,
	series = {{ASE} '23},
	title = {{HexT5}: {Unified} {Pre}-{Training} for {Stripped} {Binary} {Code} {Information} {Inference}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00099},
	doi = {10.1109/ASE56229.2023.00099},
	abstract = {Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pretraining objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
	year = {2024},
	keywords = {Binary codes, Task analysis, Deep Learning, Reverse Engineering, Semantics, deep learning, Natural languages, reverse engineering, Data mining, Computer languages, binary diffing, information inference, programming language model, Object recognition, Binary Diffing, Information Inference, Programming Language Model},
	pages = {774--786},
}

@inproceedings{xia_binary_2024,
	address = {New York, NY, USA},
	series = {{ICMLCA} '23},
	title = {A {Binary} {Function} {Name} {Prediction} {Method} {Based} on {Variable} {Alignment} and {Translation} {Model}},
	isbn = {979-8-4007-0944-9},
	url = {https://doi.org/10.1145/3650215.3650347},
	doi = {10.1145/3650215.3650347},
	abstract = {Binary function naming is a code analysis task that generates functional descriptions of functions, and its results can be applied in the fields of malicious code analysis, vulnerability causation analysis, and algorithm governance. Aiming at the shortcomings of the pseudocode abstract syntax tree being difficult to extract and the binary function naming scheme having low accuracy rate, a binary function naming prediction model A2N based on variable alignment and sequence translation model is proposed. First, A2N extracts the function variable features of binary files from debugging information and performs variable alignment with the pseudocode obtained from decompiling; then, it obtains the hierarchical structure of the binary functions and designs the node extraction rules to generate an abstract syntax tree AST for each function; then, extract the paths between the leaf nodes of the AST and serialize the tree structure to represent it; finally, with the help of the neural network translation model, establish a mapping between the AST and the binary function names to realize the prediction function. The experimental results show that compared with Dire, Nero and XFL models, the F1 value of A2N is improved by 84\%, 44\% and 14\% on file-level isolation experiments respectively, and the F1 value reaches 80.94\% on function-level isolation experiments.},
	booktitle = {Proceedings of the 2023 4th {International} {Conference} on {Machine} {Learning} and {Computer} {Application}},
	publisher = {Association for Computing Machinery},
	author = {Xia, Bing and Yin, Jiabin and Ge, Yunxiang and Yang, Ruinan},
	year = {2024},
	pages = {757--761},
}

@article{jin_compilation_2023,
	address = {New York, NY, USA},
	title = {A {Compilation} {Tool} for {Computation} {Offloading} in {ReRAM}-based {CIM} {Architectures}},
	volume = {20},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/3617686},
	doi = {10.1145/3617686},
	abstract = {Computing-in-Memory (CIM) architectures using Non-volatile Memories (NVMs) have emerged as a promising way to address the “memory wall” problem in traditional Von Neumann architectures. CIM accelerators can perform arithmetic or Boolean logic operations in NVMs by fully exploiting their high parallelism for bit-wise operations. These accelerators are often used in cooperation with general-purpose processors to speed up a wide variety of artificial neural network applications. In such a heterogeneous computing architecture, the legacy software should be redesigned and re-engineered to utilize new CIM accelerators. In this article, we propose a compilation tool to automatically migrate legacy programs to such heterogeneous architectures based on the low-level virtual machine (LLVM) compiler infrastructure. To accelerate some computations such as vector-matrix multiplication in CIM accelerators, we identify several typical computing patterns from LLVM intermediate representations, which are oblivious to high-level programming paradigms. Our compilation tool can modify accelerable LLVM IRs to offload them to CIM accelerators automatically, without re-engineering legacy software. Experimental results show that our compilation tool can translate many legacy programs to CIM-supported binary executables effectively, and improve application performance and energy efficiency by up to 51× and 309×, respectively, compared with general-purpose x86 processors.},
	number = {4},
	journal = {ACM Trans. Archit. Code Optim.},
	publisher = {Association for Computing Machinery},
	author = {Jin, Hai and Lei, Bo and Liu, Haikun and Liao, Xiaofei and Duan, Zhuohui and Ye, Chencheng and Zhang, Yu},
	month = oct,
	year = {2023},
	keywords = {accelerator, compilation, LLVM-IR, ReRAM},
}

@inproceedings{pohjola_pancake_2023,
	address = {New York, NY, USA},
	series = {{PLOS} '23},
	title = {Pancake: {Verified} {Systems} {Programming} {Made} {Sweeter}},
	isbn = {979-8-4007-0404-8},
	url = {https://doi.org/10.1145/3623759.3624544},
	doi = {10.1145/3623759.3624544},
	abstract = {We introduce Pancake, a new language for verifiable, low-level systems programming, especially device drivers. Pancake eschews complex type systems to make the language attractive to systems programmers, while at the same time aiming to ease the formal verification of code. We describe the design of the language and its verified compiler, and examine its usability, performance and current limitations through case studies of device drivers and related systems components for an seL4-based operating system.},
	booktitle = {Proceedings of the 12th {Workshop} on {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Pohjola, Johannes Åman and Syeda, Hira Taqdees and Tanaka, Miki and Winter, Krishnan and Sau, Tsun Wang and Nott, Benjamin and Ung, Tiana Tsang and McLaughlin, Craig and Seassau, Remy and Myreen, Magnus O. and Norrish, Michael and Heiser, Gernot},
	year = {2023},
	pages = {1--9},
}

@inproceedings{wu_method_2023,
	address = {New York, NY, USA},
	series = {{ICCSIE} '23},
	title = {A {Method} for {Identifying} {Encrypted} {Webshell} {Traffic}},
	isbn = {979-8-4007-0880-0},
	url = {https://doi.org/10.1145/3617184.3630160},
	doi = {10.1145/3617184.3630160},
	abstract = {Abstract: With the widespread application of web service technologies, implanting Webshells on target servers has become one of the most common attack methods. To evade detection, many WebShells are encrypted, increasing the difficulty of Webshell detection. This paper presents a method for recognizing traffic from encrypted webshells, extracting and filtering encrypted request and response packets in traffic files, and identifying the encryptions and keys used in the Webshell files. Based on this method, this paper has implemented an encrypted traffic identification system, decrypting the ciphertext, keys, and passwords in malicious traffic. Experimental results show that the system can effectively identify a large variety of encrypted WebShells, ensuring concurrent stability while being functionally comprehensive.},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Cyber} {Security} and {Information} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Qi and Wen, Shuo and Liu, Boliang},
	year = {2023},
	keywords = {decrypt, Godzilla encryption, traffic identification, Webshell},
	pages = {341--349},
}

@inproceedings{shirani_towards_2023,
	address = {USA},
	series = {{CASCON} '23},
	title = {Towards {Cross}-{Architecture} {Binary} {Code} {Vulnerability} {Detection}},
	abstract = {Today’s Internet of Things (IoT) environments are heterogeneous as they are typically comprised of devices equipped with various CPU architectures and software platforms. Therefore, in defending IoT environments against security threats, the capability of crossarchitecture vulnerability detection is of paramount importance. In this paper, we propose BinX, a deep learning-based approach for code similarity detection in binaries that are obtained through different compilers and optimization levels for various architectures. Our research is guided by a key idea that involves leveraging the Ghidra decompiler to generate the decompiled C code and the high p-code intermediate representation and pre-train transformerbased model, specifically BERT and CodeBERT, to accurately generate semantic embeddings. These embeddings are then utilized as inputs to an RNN Siamese neural network, enhancing the learning process for code similarity detection. The effectiveness of our approach is demonstrated through several experiments and comparisons with existing methods. Our results showcase the potential of BinX in enabling cross-architecture vulnerability detection in cross-architecture cross-compiled binaries, contributing to the advancement of security in IoT environments.},
	booktitle = {Proceedings of the 33rd {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Shirani, Paria and Bhatt, Sagar and Hailane, Asmaa and Jourdan, Guy-Vincent},
	year = {2023},
	keywords = {vulnerability detection, Binary code analysis, machine learning.},
	pages = {191--196},
}

@inproceedings{kong_defitainter_2023,
	address = {New York, NY, USA},
	series = {{ISSTA} 2023},
	title = {{DeFiTainter}: {Detecting} {Price} {Manipulation} {Vulnerabilities} in {DeFi} {Protocols}},
	isbn = {979-8-4007-0221-1},
	url = {https://doi.org/10.1145/3597926.3598124},
	doi = {10.1145/3597926.3598124},
	abstract = {DeFi protocols are programs that manage high-value digital assets on blockchain. The price manipulation vulnerability is one of the common vulnerabilities in DeFi protocols, which allows attackers to gain excessive profits by manipulating token prices. In this paper, we propose DeFiTainter, an inter-contract taint analysis framework for detecting price manipulation vulnerabilities. DeFiTainter features two innovative mechanisms to ensure its effectiveness. The first mechanism is to construct a call graph for inter-contract taint analysis by restoring call information, not only from code constants but also from contract storage and function parameters. The second mechanism is a high-level semantic induction tailored for detecting price manipulation vulnerabilities, which accurately identifies taint sources and sinks and tracks taint data across contracts. Extensive evaluation of real-world incidents and high-value DeFi protocols shows that DeFiTainter outperforms existing approaches and achieves state-of-the-art performance with a precision of 96\% and a recall of 91.3\% in detecting price manipulation vulnerabilities. Furthermore, DeFiTainter uncovers three previously undisclosed price manipulation vulnerabilities.},
	booktitle = {Proceedings of the 32nd {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Kong, Queping and Chen, Jiachi and Wang, Yanlin and Jiang, Zigui and Zheng, Zibin},
	year = {2023},
	keywords = {smart contract, vulnerability detection, taint analysis},
	pages = {1144--1156},
}

@inproceedings{liu_exploring_2023,
	address = {New York, NY, USA},
	series = {{ISSTA} 2023},
	title = {Exploring {Missed} {Optimizations} in {WebAssembly} {Optimizers}},
	isbn = {979-8-4007-0221-1},
	url = {https://doi.org/10.1145/3597926.3598068},
	doi = {10.1145/3597926.3598068},
	abstract = {The prosperous trend of deploying complex applications to web browsers has boosted the development of WebAssembly (wasm) compilation toolchains. Software written in different high-level programming languages are compiled into wasm executables, which can be executed fast and safely in a virtual machine. The performance of wasm executables depends highly on compiler optimizations. Despite the prosperous use of wasm executables, recent research has indicated that real-world wasm applications are slower than anticipated, suggesting deficiencies in wasm optimizations. This paper aims to present the first systematic and in-depth understanding of the status quo of wasm optimizations. To do so, we present DITWO, a differential testing framework to uncover missed optimizations (MO) of wasm optimizers. DITWO compiles a C program into both native x86 executable and wasm executable, and differentiates optimization indication traces (OITraces) logged by running each executable to uncover MO. Each OITrace is composed with global variable writes and function calls, two performance indicators that practically and systematically reflect the optimization degree across wasm and native executables. Our analysis of the official wasm optimizer, wasm-opt, successfully identifies 1,293 inputs triggering MO of wasm-opt. With extensive manual effort, we identify nine root causes for all MO, and we estimate that fixing discovered MO can result in a performance improvement of at least 17.15\%. We also summarize four lessons from our findings to deliver better wasm optimizations.},
	booktitle = {Proceedings of the 32nd {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhibo and Xiao, Dongwei and Li, Zongjie and Wang, Shuai and Meng, Wei},
	year = {2023},
	keywords = {Compiler Optimization, Software Testing, WebAssembly},
	pages = {436--448},
}

@inproceedings{cong_towards_2023,
	address = {New York, NY, USA},
	series = {{PEPM} 2023},
	title = {Towards a {Reflection} for {Effect} {Handlers}},
	isbn = {979-8-4007-0011-8},
	url = {https://doi.org/10.1145/3571786.3573015},
	doi = {10.1145/3571786.3573015},
	abstract = {A reflection is a relationship between compiling and decompiling functions. This concept has been studied as a means to ensure correctness of compilers, in particular, those for languages featuring control effects. We aim to develop a reflection for algebraic effects and handlers. As a first step towards this goal, we investigate what we obtain by following the existing recipe for control operators. We show that, if we use the simplest CPS translation as the compiling function, we can prove most but not all theorems required of a reflection. From this result, we identify two conditions of the CPS translation that would lead to a reflection for effect handlers.},
	booktitle = {Proceedings of the 2023 {ACM} {SIGPLAN} {International} {Workshop} on {Partial} {Evaluation} and {Program} {Manipulation}},
	publisher = {Association for Computing Machinery},
	author = {Cong, Youyou and Asai, Kenichi},
	year = {2023},
	keywords = {algebraic effects and handlers, CPS translation, direct style translation, reflection},
	pages = {55--65},
}

@inproceedings{mittal_solving_2022,
	address = {New York, NY, USA},
	series = {{MODELS} '22},
	title = {Solving the instance model-view update problem in {AADL}},
	isbn = {978-1-4503-9466-6},
	url = {https://doi.org/10.1145/3550355.3552396},
	doi = {10.1145/3550355.3552396},
	abstract = {The Architecture Analysis and Design Language (AADL) is a rich language for modeling embedded systems through several constructs such as component extension and refinement to promote modularity of component declarations. To ease processing AADL models, OSATE, the reference tool for AADL, defines another model (namely 'instance' model) computed from a base 'declarative' model/s. An instance model is a simple object tree where all information from the declarative model is flattened so that tools can easily use this information to analyze the system. However for modifications, they have to make changes in the complex declarative model since there is no automated backward transformation (deinstantiation) from instance to declarative models. Since the instance model is a 'view' of the declarative model, this is a view-update problem. In this paper, we propose the OSATE Declarative-Instance Mapping Tool (OSATE-DIM1), an Eclipse plugin for deinstantiation of AADL models implementing a solution of this view-update problem. We evaluate OSATE-DIM with a benchmark of existing AADL model processing tools and verify the correctness of our deinstantiation transformations. We also discuss how our approach could be useful for decompilation of Object-Oriented languages' intermediate representations.},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Model} {Driven} {Engineering} {Languages} and {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mittal, Rakshit and Blouin, Dominique and Bhobe, Anish and Bandyopadhyay, Soumyadip},
	year = {2022},
	keywords = {embedded systems, AADL, cyber-physical systems, model-driven engineering, view-update problem},
	pages = {55--65},
}

@inproceedings{pauck_scaling_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Scaling {Arbitrary} {Android} {App} {Analyses}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3561339},
	doi = {10.1145/3551349.3561339},
	abstract = {More apps are published every day and the functionality of each app increases steadily as well. Consequently app analyses are often overwhelmed when confronted with up-to-date, real-world apps. One of the biggest issues originates from the scalability of analyses with respect to libraries. Analyses, more precisely the tools implementing them, cannot distinguish the app’s code from the code of a library. Always analyzing the whole code base is the result. However, this is usually not necessary, for example, when a security property is checked, trusted libraries must not be analyzed. We propose an approach to differentiate an app’s code from a library’s code. The approach is based on clone detection and implemented in our prototype APK-Simplifier. As the evaluation shows APK-Simplifier can be employed in a cooperative analysis to remove library code and to enhance arbitrary analysis tools’ scalability. In fact, five analysis tools have been enabled to analyze five up-to-date, real-world apps they could not analyze before. Still, it is alerting that the majority of such apps remains not analyzable as also shown during evaluation.},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Pauck, Felix},
	year = {2023},
	keywords = {Android, security, taint analysis, clone detection, cooperative analysis, software analysis},
}

@inproceedings{lv_fastbot2_2023,
	address = {New York, NY, USA},
	series = {{ASE} '22},
	title = {Fastbot2: {Reusable} {Automated} {Model}-based {GUI} {Testing} for {Android} {Enhanced} by {Reinforcement} {Learning}},
	isbn = {978-1-4503-9475-8},
	url = {https://doi.org/10.1145/3551349.3559505},
	doi = {10.1145/3551349.3559505},
	abstract = {We introduce a reusable automated model-based GUI testing technique for Android apps to accelerate the testing cycle. Our key insight is that the knowledge of event-activity transitions from the previous testing runs, i.e., executing which events can reach which activities, is valuable for guiding the follow-up testing runs to quickly cover major app functionalities. To this end, we propose (1) a probabilistic model to memorize and leverage this knowledge during testing, and (2) design a model-based guided testing strategy (enhanced by a reinforcement learning algorithm). We implemented our technique as an automated testing tool named Fastbot2. The evaluation on two popular industrial apps (with billions of user installations), Douyin and Toutiao, shows that Fastbot2 outperforms the state-of-the-art testing tools (Monkey, Ape and Stoat) in both activity coverage and fault detection in the context of continuous testing. To date, Fastbot2 has been deployed in the CI pipeline at ByteDance for nearly two years, and 50.8\% of the developer-fixed crash bugs were reported by Fastbot2, which significantly improves app quality. Fastbot2 has been made publicly available to benefit the community at: https://github.com/bytedance/Fastbot\_Android.},
	booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Lv, Zhengwei and Peng, Chao and Zhang, Zhao and Su, Ting and Liu, Kai and Yang, Ping},
	year = {2023},
}

@inproceedings{liu_pg-vulnet_2022,
	address = {New York, NY, USA},
	series = {{ESEM} '22},
	title = {{PG}-{VulNet}: {Detect} {Supply} {Chain} {Vulnerabilities} in {IoT} {Devices} using {Pseudo}-code and {Graphs}},
	isbn = {978-1-4503-9427-7},
	url = {https://doi.org/10.1145/3544902.3546240},
	doi = {10.1145/3544902.3546240},
	abstract = {Background: With the boosting development of IoT technology, the supply chains of IoT devices become more powerful and sophisticated, and the security issues introduced by code reuse are becoming more prominent. Therefore, the detection and management of vulnerabilities through code similarity detection technology is of great significance for protecting the security of IoT devices. Aim: We aim to propose a more accurate, parallel-friendly, and realistic software supply chain vulnerability detection solution for IoT devices. Method: This paper presents PG-VulNet, standing for Vulnerability-detection Network based on Pseudo-code Graphs. It is a ”multi-model” cross-architecture vulnerability detection solution based on pseudo-code and Graph Matching Network (GMN). PG-VulNet extracts both behavioral and structural features of pseudo-code to build customized feature graphs and then uses GMN to detect supply chain vulnerabilities based on these graphs. Results: The experiments show that PG-VulNet achieves an average detection accuracy of 99.14\%, significantly higher than existing approaches like Gemini, VulSeeker, FIT, and Asteria. In addition to this, PG-VulNet also excels in detection overhead and false alarms. In the real-world evaluation, PG-VulNet detected 690 known vulnerabilities in 1,611 firmwares. Conclusions: PG-VulNet can effectively detect the vulnerabilities introduced by software supply chain in IoT firmwares and is well suited for large-scale detection. Compared with existing approaches, PG-VulNet has significant advantages.},
	booktitle = {Proceedings of the 16th {ACM} / {IEEE} {International} {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Xin and Wu, Yixiong and Yu, Qingchen and Song, Shangru and Liu, Yue and Zhou, Qingguo and Zhuge, Jianwei},
	year = {2022},
	keywords = {Vulnerability Detection, Graph Neural Network, Binary Code Similarity, IoT Software Supply Chain},
	pages = {205--215},
}

@inproceedings{jeng_novel_2022,
	address = {New York, NY, USA},
	series = {{ICCCM} '22},
	title = {A {Novel} {Deep} {Learning} {Based} {Attention} {Mechanism} for {Android} {Malware} {Detection} and {Explanation}},
	isbn = {978-1-4503-9634-9},
	url = {https://doi.org/10.1145/3556223.3556257},
	doi = {10.1145/3556223.3556257},
	abstract = {With the popularity of Android mobile devices and the increase of related applications, hackers regard it as the primary attack target. Therefore, malware detection is essential nowadays, and many of these studies employ deep learning techniques. In recent years, the attention mechanism provides corresponding attention weights for different hidden states, and it is widely used in many fields, such as machine translation and image markup. However, no research has applied the attention mechanism to Android malware analysis. Hence, this paper completes the goal of malware family classification based on the static features of Android applications. We compare the difference between the original convolutional neural network (CNN) and the addition of the attention mechanism. The final experimental results show that the attention mechanism improves the accuracy of the existing CNN model by 1.99\% in static opcode images. In addition, we further adopt the occlusion sensitivity method to try to explain the classification model proposed in this paper. Finally, the experimental results of model interpretation show that the classification model can effectively identify the threat behavior of malware.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Computer} and {Communications} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Jeng, Tzung-Han and Chang, Ying-Ching and Yang, Hui-Hsuan and Chen, Li-Kai and Chen, Yi-Ming},
	year = {2022},
	keywords = {Deep Learning, Android Malware Classification, Attention Mechanism, CNN, Model Interpretation},
	pages = {226--232},
}

@inproceedings{liao_smartdagger_2022,
	address = {New York, NY, USA},
	series = {{ISSTA} 2022},
	title = {{SmartDagger}: a bytecode-based static analysis approach for detecting cross-contract vulnerability},
	isbn = {978-1-4503-9379-9},
	url = {https://doi.org/10.1145/3533767.3534222},
	doi = {10.1145/3533767.3534222},
	abstract = {With the increasing popularity of blockchain, automatically detecting vulnerabilities in smart contracts is becoming a significant problem. Prior research mainly identifies smart contract vulnerabilities without considering the interactions between multiple contracts. Due to the lack of analyzing the fine-grained contextual information during cross-contract invocations, existing approaches often produced a large number of false positives and false negatives. This paper proposes SmartDagger, a new framework for detecting cross-contract vulnerability through static analysis at the bytecode level. SmartDagger integrates a set of novel mechanisms to ensure its effectiveness and efficiency for cross-contract vulnerability detection. Particularly, SmartDagger effectively recovers the contract attribute information from the smart contract bytecode, which is critical for accurately identifying cross-contract vulnerabilities. Besides, instead of performing the typical whole-program analysis which is heavy-weight and time-consuming, SmartDagger selectively analyzes a subset of functions and reuses the data-flow results, which helps to improve its efficiency. Our further evaluation over a manually labelled dataset showed that SmartDagger significantly outperforms other state-of-the-art tools (i.e., Oyente, Slither, Osiris, and Mythril) for detecting cross-contract vulnerabilities. In addition, running SmartDagger over a randomly selected dataset of 250 smart contracts in the real-world, SmartDagger detects 11 cross-contract vulnerabilities, all of which are missed by prior tools.},
	booktitle = {Proceedings of the 31st {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Liao, Zeqin and Zheng, Zibin and Chen, Xiao and Nan, Yuhong},
	year = {2022},
	keywords = {static analysis, smart contract, bug finding, interprocedure analysis},
	pages = {752--764},
}

@inproceedings{oconnor_toward_2022,
	address = {New York, NY, USA},
	series = {{ITiCSE} '22},
	title = {Toward an {Automatic} {Exploit} {Generation} {Competition} for an {Undergraduate} {Binary} {Reverse} {Engineering} {Course}},
	isbn = {978-1-4503-9201-3},
	url = {https://doi.org/10.1145/3502718.3524744},
	doi = {10.1145/3502718.3524744},
	abstract = {Analyzing binary programs without source code is critical for cybersecurity professionals. This paper presents an undergraduate binary reverse engineering course design that culminates with a comprehensive binary exploitation competition. Our approach challenges students to develop tools that automatically detect and exploit program vulnerabilities. We hypothesize that this competition presents a unique opportunity to exercise the core competencies of binary reverse engineering. We share our detailed design, labs, experiences, and lessons learned from this course for others to build on our initial success.},
	booktitle = {Proceedings of the 27th {ACM} {Conference} on on {Innovation} and {Technology} in {Computer} {Science} {Education} {Vol}. 1},
	publisher = {Association for Computing Machinery},
	author = {OConnor, TJ and Mann, Carl and Petersen, Tiffanie and Thomas, Isaiah and Stricklan, Chris},
	year = {2022},
	keywords = {reverse engineering, cybersecurity education, vulnerability research},
	pages = {442--448},
}

@article{xing_h-container_2022,
	address = {New York, NY, USA},
	title = {H-{Container}: {Enabling} {Heterogeneous}-{ISA} {Container} {Migration} in {Edge} {Computing}},
	volume = {39},
	issn = {0734-2071},
	url = {https://doi.org/10.1145/3524452},
	doi = {10.1145/3524452},
	abstract = {Edge computing is a recent computing paradigm that brings cloud services closer to the client. Among other features, edge computing offers extremely low client/server latencies. To consistently provide such low latencies, services should run on edge nodes that are physically as close as possible to their clients. Thus, when the physical location of a client changes, a service should migrate between edge nodes to maintain proximity. Differently from cloud nodes, edge nodes integrate CPUs of different Instruction Set Architectures (ISAs), hence a program natively compiled for a given ISA cannot migrate to a server equipped with a CPU of a different ISA. This hinders migration to the closest node.We introduce H-Container, a system that migrates natively compiled containerized applications across compute nodes featuring CPUs of different ISAs. H-Container advances over existing heterogeneous-ISA migration systems by being (a) highly compatible – no user’s source-code nor compiler toolchain modifications are needed; (b) easily deployable – fully implemented in user space, thus without any OS or hypervisor dependency, and (c) largely Linux-compliant – it can migrate most Linux software, including server applications and dynamically linked binaries. H-Container targets Linux and its already-compiled executables, adopts LLVM, extends CRIU, and integrates with Docker. Experiments demonstrate that H-Container adds no overheads during program execution, while 10–100\&nbsp;ms are added during migration. Furthermore, we show the benefits of H-Container in real-world scenarios, demonstrating, for example, up to 94\% increase in Redis throughput when client/server proximity is maintained through heterogeneous container migration.},
	number = {1–4},
	journal = {ACM Trans. Comput. Syst.},
	publisher = {Association for Computing Machinery},
	author = {Xing, Tong and Barbalace, Antonio and Olivier, Pierre and Karaoui, Mohamed L. and Wang, Wei and Ravindran, Binoy},
	month = jul,
	year = {2022},
	keywords = {containers, Edge, heterogeneous ISA, migration},
}

@inproceedings{kim_reverse_2022,
	address = {New York, NY, USA},
	series = {{MobiSys} '22},
	title = {Reverse engineering and retrofitting robotic aerial vehicle control firmware using dispatch},
	isbn = {978-1-4503-9185-6},
	url = {https://doi.org/10.1145/3498361.3538938},
	doi = {10.1145/3498361.3538938},
	abstract = {Unmanned Aerial Vehicles as a service (UAVaaS) has increased the field deployment of Robotic Aerial Vehicles (RAVs) for different services such as transportation and terrain exploration. These RAVs are controlled by firmware, which is often closed-source, developed by vendors, and flashed into the ROM. While these binary blobs enable off-the-shelf management of RAVs, end users (individuals or organizations) have no idea if the control firmware is designed and implemented correctly, and can only rely on firmware updates from vendors when any vulnerability is discovered. This paper proposes DisPatch, the first reverse engineering and patching framework for understanding and improving controller design and implementation within RAV firmware. DisPatch first decompiles binary instructions and recovers controller functions and core controller variables by combining control theory with program analysis using symbolic execution and data flow analysis. End users can then write a patch in a domain-specific language (DSL), which will be translated and injected into the binary firmware by DisPatch automatically. We have applied DisPatch to two instances of commodity firmware from3DR IRIS+ and MantisQ RAVs and demonstrated 100\% and 80.7\% accuracy respectively in the controller decompilation. We have also shown the ability to prevent severe controller performance degradation by patching two real-world bugs with in the firmware and without breaking other functionality. Finally, we show that DisPatch introduces less than 0.53\% of space overhead and 1.48\% of runtime overhead without violating the soft real-time deadlines. DisPatch provides the first step towards an RAV binary firmware reverse engineering and patching system to customize controller design and implementation.},
	booktitle = {Proceedings of the 20th {Annual} {International} {Conference} on {Mobile} {Systems}, {Applications} and {Services}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Taegyu and Ding, Aolin and Etigowni, Sriharsha and Sun, Pengfei and Chen, Jizhou and Garcia, Luis and Zonouz, Saman and Xu, Dongyan and Tian, Dave (Jing)},
	year = {2022},
	keywords = {binary analysis, security, firmware analysis, robotic aerial vehicle},
	pages = {69--83},
}

@inproceedings{sun_two-path_2022,
	address = {New York, NY, USA},
	series = {{ITCC} '22},
	title = {Two-path {Android} {Malware} {Detection} {Based} on {N}-gram {Feature} {Weighting}},
	isbn = {978-1-4503-9682-0},
	url = {https://doi.org/10.1145/3548636.3548651},
	doi = {10.1145/3548636.3548651},
	abstract = {In recent years, with the full popularity of Android system and applications, the types and number of Android malicious applications also show explosive growth, and more efficient detection technology is urgently needed to identify malicious software. In view of the current research on N-gram features is relatively single, in order to make more comprehensive use of N-gram features and explore the potential relationship between features and attributes of applications, this paper proposes a two-path Android malware detection model based on N-gram feature weighting, and achieves N-gram feature extraction in two different ways by setting an application file threshold. Finally, Neural network is used to classify the fused features. Testing results of 1205 malicious samples and 1084 benign samples shows that the detection accuracy of the model was up to 99.2\%. At the same time, this experiment further verify the effectiveness of relevant improvements, and the results show that compared with traditional machine learning algorithms, this model has higher adaptability and accuracy.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Information} {Technology} and {Computer} {Communications}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Min and Zhang, Danni},
	year = {2022},
	pages = {99--104},
}

@inproceedings{armengol-estape_exebench_2022,
	address = {New York, NY, USA},
	series = {{MAPS} 2022},
	title = {{ExeBench}: an {ML}-scale dataset of executable {C} functions},
	isbn = {978-1-4503-9273-0},
	url = {https://doi.org/10.1145/3520312.3534867},
	doi = {10.1145/3520312.3534867},
	abstract = {Machine-learning promises to transform compilation and software engineering, yet is frequently limited by the scope of available datasets. In particular, there is a lack of runnable, real-world datasets required for a range of tasks ranging from neural program synthesis to machine learning-guided program optimization. We introduce a new dataset, ExeBench, which attempts to address this. It tackles two key issues with real-world code: references to external types and functions and scalable generation of IO examples. ExeBench is the first publicly available dataset that pairs real-world C code taken from GitHub with IO examples that allow these programs to be run. We develop a toolchain that scrapes GitHub, analyzes the code, and generates runnable snippets of code. We analyze our benchmark suite using several metrics, and show it is representative of real-world code. ExeBench contains 4.5M compilable and 700k executable C functions. This scale of executable, real functions will enable the next generation of machine learning-based programming tasks.},
	booktitle = {Proceedings of the 6th {ACM} {SIGPLAN} {International} {Symposium} on {Machine} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Brauckmann, Alexander and Magalhães, José Wesley de Souza and O'Boyle, Michael F. P.},
	year = {2022},
	keywords = {C, Code Dataset, Compilers, Machine Learning for Code, Mining Software Repositories, Program Synthesis},
	pages = {50--59},
}

@inproceedings{verbeek_formally_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Formally verified lifting of {C}-compiled x86-64 binaries},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523702},
	doi = {10.1145/3519939.3523702},
	abstract = {Lifting binaries to a higher-level representation is an essential step for decompilation, binary verification, patching and security analysis. In this paper, we present the first approach to provably overapproximative x86-64 binary lifting. A stripped binary is verified for certain sanity properties such as return address integrity and calling convention adherence. Establishing these properties allows the binary to be lifted to a representation that contains an overapproximation of all possible execution paths of the binary. The lifted representation contains disassembled instructions, reconstructed control flow, invariants and proof obligations that are sufficient to prove the sanity properties as well as correctness of the lifted representation. We apply this approach to Linux Foundation and Intel’s Xen Hypervisor covering about 400K instructions. This demonstrates our approach is the first approach to provably overapproximative binary lifting scalable to commercial off-the-shelf systems. The lifted representation is exportable to the Isabelle/HOL theorem prover, allowing formal verification of its correctness. If our technique succeeds and the proofs obligations are proven true, then – under the generated assumptions – the lifted representation is correct.},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Verbeek, Freek and Bockenek, Joshua and Fu, Zhoulai and Ravindran, Binoy},
	year = {2022},
	keywords = {Binary Analysis, Disassembly, Formal Verification},
	pages = {934--949},
}

@inproceedings{lehmann_finding_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Finding the {Dwarf}: {Recovering} {Precise} {Types} from {WebAssembly} {Binaries}},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523449},
	doi = {10.1145/3519939.3523449},
	abstract = {The increasing popularity of WebAssembly creates a demand for understanding and reverse engineering WebAssembly binaries. Recovering high-level function types is an important part of this process. One method to recover types is data-flow analysis, but it is complex to implement and may require manual heuristics when logical constraints fall short. In contrast, this paper presents SnowWhite, a learning-based approach for recovering precise, high-level parameter and return types for WebAssembly functions. It improves over prior work on learning-based type recovery by representing the types-to-predict in an expressive type language, which can describe a large number of complex types, instead of the fixed, and usually small type vocabulary used previously. Thus, recovery of a single type is no longer a classification task but sequence prediction, for which we build on the success of neural sequence-to-sequence models. We evaluate SnowWhite on a new, large-scale dataset of 6.3 million type samples extracted from 300,905 WebAssembly object files. The results show the type language is expressive, precisely describing 1,225 types instead the 7 to 35 types considered in previous learning-based approaches. Despite this expressiveness, our type recovery has high accuracy, exactly matching 44.5\% (75.2\%) of all parameter types and 57.7\% (80.5\%) of all return types within the top-1 (top-5) predictions.},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Lehmann, Daniel and Pradel, Michael},
	year = {2022},
	keywords = {reverse engineering, machine learning, WebAssembly, type recovery, corpus, dataset, debugging information, DWARF, neural networks, type prediction},
	pages = {410--425},
}

@inproceedings{mantovani_convergence_2022,
	address = {New York, NY, USA},
	series = {{ASIA} {CCS} '22},
	title = {The {Convergence} of {Source} {Code} and {Binary} {Vulnerability} {Discovery} – {A} {Case} {Study}},
	isbn = {978-1-4503-9140-5},
	url = {https://doi.org/10.1145/3488932.3497764},
	doi = {10.1145/3488932.3497764},
	abstract = {Decompilers are tools designed to recover a high-level language representation (typically in C code) from program binaries. Over the past five years, decompilers have improved enormously, not only in terms of the readability of the produced pseudocode, but also in terms of similarity of the recovered representation to the original source code. Albeit decompilers are routinely used by reverse engineers in different disciplines (e.g., to support vulnerability discovery or malware analysis), they are not yet adopted to produce input for source-code static analysis tools. In particular, source code vulnerability discovery and binary vulnerability discovery remain today two very different areas of research, despite the fact that decompilers could potentially bridge this gap and enable source-code analysis on binary files.In this paper, we conducted a number of experiments on real world vulnerabilities to evaluate the feasibility of this approach. In particular, our measurements are intended to show how the differences between original and decompiled code impact the accuracy of static analysis tools.Remarkably, our results show that in 71\% of the cases, the same vulnerabilities can be detected by running the static analyzers on the decompiled code, even though for several cases we observe a steep increment in the number of false positives. To understand the reasons behind these differences, we manually investigated all cases and we identified a number of root causes that affected the ability of static tools to 'understand' the generated code.},
	booktitle = {Proceedings of the 2022 {ACM} on {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Mantovani, Alessandro and Compagna, Luca and Shoshitaishvili, Yan and Balzarotti, Davide},
	year = {2022},
	keywords = {decompiler, vulnerability, reversing, sast},
	pages = {602--615},
}

@inproceedings{mahmud_acid_2022,
	address = {New York, NY, USA},
	series = {{ICSE} '22},
	title = {{ACID}: an {API} compatibility issue detector for {Android} apps},
	isbn = {978-1-4503-9223-5},
	url = {https://doi.org/10.1145/3510454.3516854},
	doi = {10.1145/3510454.3516854},
	abstract = {Android API is frequently updated, and compatibility issues may be induced when the API level supported by the device differs from the API level targeted by app developers. This paper presents ACID, an API compatibility issue detector for Android apps. ACID utilizes API differences and static analysis of Android apps to detect both API invocation compatibility issues and API callback compatibility issues. Our evaluation on 20 benchmark apps from previous studies shows that ACID is more accurate and faster in detecting compatibility issues than state-of-the-art techniques. We also ran ACID on 35 more real-world apps to demonstrate ACID's practical applicability. ACID is available at https://github.com/TSUMahmud/acid and the demonstration video of ACID is available at https://youtu.be/XUNBPMIx2q4.},
	booktitle = {Proceedings of the {ACM}/{IEEE} 44th {International} {Conference} on {Software} {Engineering}: {Companion} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Mahmud, Tarek and Che, Meiru and Yang, Guowei},
	year = {2022},
	keywords = {Android, API callback compatibility issues, API evolution, API invocation compatibility issues},
	pages = {1--5},
}

@inproceedings{zhang_pitracker_2022,
	address = {New York, NY, USA},
	series = {{WiSec} '22},
	title = {{PITracker}: {Detecting} {Android} {PendingIntent} {Vulnerabilities} through {Intent} {Flow} {Analysis}},
	isbn = {978-1-4503-9216-7},
	url = {https://doi.org/10.1145/3507657.3528555},
	doi = {10.1145/3507657.3528555},
	abstract = {Intent is an essential inter-component communication mechanism of Android OS, which can be used to request an action from another app component. The security of its design and implementation attracts lots of attention. However, the security of PendingIntent, a kind of delayed-triggered Intent, was neglected by most previous research, and the related analysis techniques are still imperfect. In this paper, we design a novel automated tool, PITracker, to detect the PendingIntent vulnerabilities in Android apps. It achieves the Intent flow tracking technique proposed by us, figuring out how an Intent is created and where it goes. In the real-world evaluations, PITracker discovered 2,939 potential threats in 10,000 third-party apps and 214 in 1,412 pre-installed apps. Among them, 11 exploitable vulnerabilities have been confirmed and acknowledged by the corresponding vendors.},
	booktitle = {Proceedings of the 15th {ACM} {Conference} on {Security} and {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chennan and Li, Shuang and Diao, Wenrui and Guo, Shanqing},
	year = {2022},
	keywords = {vulnerability detection, android, pendingintent},
	pages = {20--25},
}

@inproceedings{otsuki_overcoming_2022,
	address = {New York, NY, USA},
	series = {{ICSIM} '22},
	title = {Overcoming the obfuscation method of the dynamic name resolution},
	isbn = {978-1-4503-9551-9},
	url = {https://doi.org/10.1145/3520084.3520103},
	doi = {10.1145/3520084.3520103},
	abstract = {Using unevaluated obfuscation methods has a significant risk since the methods might have some vulnerabilities. One evaluation for obfuscation is de-obfuscation which discloses the hidden information by the obfuscation. This paper proposed the de-obfuscation method against for DNR (dynamic name resolution) obfuscation method. DNR hides system-defined names by encrypting them and resolves names dynamically during runtime. This paper clarifies the steps of de-obfuscation and proposes static and dynamic manners to de-obfuscate DNR. Through the case study, two ways both succeed in disclosing the hidden information of DNR.},
	booktitle = {Proceedings of the 2022 5th {International} {Conference} on {Software} {Engineering} and {Information} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Otsuki, Naruaki and Tamada, Haruaki},
	year = {2022},
	keywords = {obfuscation, de-obfuscation, dynamic name resolution, resilience of the obfuscation method, reverse-transformation},
	pages = {118--124},
}

@article{alrabaee_survey_2022,
	address = {New York, NY, USA},
	title = {A {Survey} of {Binary} {Code} {Fingerprinting} {Approaches}: {Taxonomy}, {Methodologies}, and {Features}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3486860},
	doi = {10.1145/3486860},
	abstract = {Binary code fingerprinting is crucial in many security applications. Examples include malware detection, software infringement, vulnerability analysis, and digital forensics. It is also useful for security researchers and reverse engineers since it enables high fidelity reasoning about the binary code such as revealing the functionality, authorship, libraries used, and vulnerabilities. Numerous studies have investigated binary code with the goal of extracting fingerprints that can illuminate the semantics of a target application. However, extracting fingerprints is a challenging task since a substantial amount of significant information will be lost during compilation, notably, variable and function naming, the original data and control flow structures, comments, semantic information, and the code layout. This article provides the first systematic review of existing binary code fingerprinting approaches and the contexts in which they are used. In addition, it discusses the applications that rely on binary code fingerprints, the information that can be captured during the fingerprinting process, and the approaches used and their implementations. It also addresses limitations and open questions related to the fingerprinting process and proposes future directions.},
	number = {1},
	journal = {ACM Comput. Surv.},
	publisher = {Association for Computing Machinery},
	author = {Alrabaee, Saed and Debbabi, Mourad and Wang, Lingyu},
	month = jan,
	year = {2022},
	keywords = {reverse engineering, Binary code analysis, software security},
}

@inproceedings{wood_novel_2021,
	address = {USA},
	series = {{CASCON} '21},
	title = {A novel technique for control flow obfuscation in {JVM} applications using {InvokeDynamic} with native bootstrapping},
	abstract = {Protecting the intellectual property of end-user software is a challenging industry problem. Modern obfuscation techniques aim to prevent reverse engineering and unauthorized use or modification to software. Obfuscation algorithms are especially necessary in JVM applications due to the large amount of contextual information stored in JVM bytecode. However, many commercial-grade obfuscation methods can be easily be undone by deobfuscation software such that it can later be decompiled and refactored for illegitimate use. In this paper, we propose a method of control flow obfuscation using the InvokeDynamic instruction with native call site bootstrapping. The proposed method prevents JVM byte-code from leaking call site information in function invocations. We evaluate the proposed technique against a series of benchmarks comparing original software with its obfuscated form. To this end, we observe an insignificant difference in application running time.},
	booktitle = {Proceedings of the 31st {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp.},
	author = {Wood, Bradley and Azim, Akramul},
	year = {2021},
	keywords = {reverse engineering, obfuscation, control-flow, Java virtual machine},
	pages = {232--236},
}

@article{ullah_iot-based_2021,
	address = {New York, NY, USA},
	title = {{IoT}-based {Cloud} {Service} for {Secured} {Android} {Markets} using {PDG}-based {Deep} {Learning} {Classification}},
	volume = {22},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3418206},
	doi = {10.1145/3418206},
	abstract = {Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones.Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48\% among clones from different application stores.},
	number = {2},
	journal = {ACM Trans. Internet Technol.},
	publisher = {Association for Computing Machinery},
	author = {Ullah, Farhan and Naeem, Muhammad Rashid and Bajahzar, Abdullah S. and Al-Turjman, Fadi},
	month = oct,
	year = {2021},
	keywords = {deep learning, Internet of Things, Clone detection, cloud services, program dependency graph},
}

@inproceedings{wu_android_2022,
	address = {New York, NY, USA},
	series = {{MLMI} '21},
	title = {An {Android} {Malware} {Detection} and {Malicious} {Code} {Location} {Method} {Based} on {Graph} {Neural} {Network}},
	isbn = {978-1-4503-8424-7},
	url = {https://doi.org/10.1145/3490725.3490733},
	doi = {10.1145/3490725.3490733},
	abstract = {In recent years, enormously Android malware poses a significant threat to Android platform security. To detect malicious applications, researchers have done a lot of work, in which finding and locating malicious code segments is an important research content. In the previous research, most detection methods cannot directly locate malicious code, and some methods with the locate ability can only find some specific types of malicious operations. This paper proposed a graph convolution algorithm and weighted mechanism to find malicious nodes implied in the Android application function call graph and provided a general method for malicious code location. We analyzed the sub-graph structural differences between benign code and malicious payload in the function call graph, constructed graph convolution operation to make the nodes in the graph learn the surrounding sub-graph structure, designed the weighting mechanism to set the malicious score to every code node, and filtered out the nodes with the highest malicious score to locate the malicious code fragments. On the dataset composed of 2650 malicious and 2650 benign applications, the accuracy of malware detection is 92.6\%, and the accuracy of malicious code location is between 72.6\% and 88.1\%, indicating that our method is accurate and efficient.},
	booktitle = {Proceedings of the 2021 4th {International} {Conference} on {Machine} {Learning} and {Machine} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Qing and Sun, Peng and Hong, Xueshu and Zhu, Xueling and Liu, Bo},
	year = {2022},
	keywords = {Machine learning, Android, Malware detection, Graph Neural Network, Malicious code localization},
	pages = {50--56},
}

@article{bibi_secure_2021,
	address = {New York, NY, USA},
	title = {Secure {Distributed} {Mobile} {Volunteer} {Computing} with {Android}},
	volume = {22},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3428151},
	doi = {10.1145/3428151},
	abstract = {Volunteer Computing provision of seamless connectivity that enables convenient and rapid deployment of greener and cheaper computing infrastructure is extremely promising to complement next-generation distributed computing systems. Undoubtedly, without tactile Internet and secure VC ecosystems, harnessing its full potentials and making it an alternative viable and reliable computing infrastructure is next to impossible. Android-enabled smart devices, applications, and services are inevitable for Volunteer computing. Contrarily, the progressive developments of sophisticated Android malware may reduce its exponential growth. Besides, Android malwares are considered the most potential and persistent cyber threat to mobile VC systems. To secure Android-based mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural Network) multi-vector Android malware threat detection framework. The proposed mechanism is highly scalable with well-coordinated infrastructure and self-optimizing capabilities to proficiently tackle fast-growing dynamic variants of sophisticated malware threats and attacks with 99.01\% detection accuracy. For a comprehensive evaluation, the authors employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo) with standard performance evaluation metrics. Moreover, MulDroid is compared with our constructed contemporary hybrid DL-driven architectures and benchmark algorithms. Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly show unbiased results.},
	number = {1},
	journal = {ACM Trans. Internet Technol.},
	publisher = {Association for Computing Machinery},
	author = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
	month = sep,
	year = {2021},
	keywords = {android malware, deep learning (DL), tactile internet, Volunteer computing (VC)},
}

@inproceedings{biernacki_reflectingnbspstackednbspcontinuations_2021,
	address = {New York, NY, USA},
	series = {{PPDP} '21},
	title = {Reflecting\&nbsp;{Stacked}\&nbsp;{Continuations} in\&nbsp;a\&nbsp;{Fine}-{Grained}\&nbsp;{Direct}-{Style}\&nbsp;{Reduction}\&nbsp;{Theory}},
	isbn = {978-1-4503-8689-0},
	url = {https://doi.org/10.1145/3479394.3479399},
	doi = {10.1145/3479394.3479399},
	abstract = {The delimited-control operator shift0 has been formally shown to capture the operational semantics of deep handlers for algebraic effects. Its CPS translation generates λ-terms in which continuation composition is not expressed in terms of nested function calls, as is typical of other delimited-control operators, e.g. shift, but with function applications consuming a sequence of continuations one at a time, as if they formed a stack. We present a novel reduction theory for Moggi’s computational λ-calculus extended with shift0 and a control delimiter dollar, which models the capture of evaluation contexts in a fine-grained manner as an interaction between the let-expressions and the delimiter. We establish a connection between our reduction theory and the existing theories of shif0 and dollar. Moreover, we develop a CPS translation for our calculus along with a direct-style translation that together form a reflection, i.e. the translations preserve reductions and the direct-style translation is a right inverse of the CPS translation. This construction relies on the invariant that CPS root terms are in η-head-normal form. The results of this work could potentially be used for compiler optimisations and lead to a similar development for algebraic effects.},
	booktitle = {Proceedings of the 23rd {International} {Symposium} on {Principles} and {Practice} of {Declarative} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Biernacki, Dariusz and Pyzik, Mateusz and Sieczkowski, Filip},
	year = {2021},
	keywords = {reflection, continuation-passing style, delimited control, lambda calculus},
}

@inproceedings{pei_stateformer_2021,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2021},
	title = {{StateFormer}: fine-grained type recovery from binaries using generative state modeling},
	isbn = {978-1-4503-8562-6},
	url = {https://doi.org/10.1145/3468264.3468607},
	doi = {10.1145/3468264.3468607},
	abstract = {Binary type inference is a critical reverse engineering task supporting many security applications, including vulnerability analysis, binary hardening, forensics, and decompilation. It is a difficult task because source-level type information is often stripped during compilation, leaving only binaries with untyped memory and register accesses. Existing approaches rely on hand-coded type inference rules defined by domain experts, which are brittle and require nontrivial effort to maintain and update. Even though machine learning approaches have shown promise at automatically learning the inference rules, their accuracy is still low, especially for optimized binaries. We present StateFormer, a new neural architecture that is adept at accurate and robust type inference. StateFormer follows a two-step transfer learning paradigm. In the pretraining step, the model is trained with Generative State Modeling (GSM), a novel task that we design to teach the model to statically approximate execution effects of assembly instructions in both forward and backward directions. In the finetuning step, the pretrained model learns to use its knowledge of operational semantics to infer types. We evaluate StateFormer's performance on a corpus of 33 popular open-source software projects containing over 1.67 billion variables of different types. The programs are compiled with GCC and LLVM over 4 optimization levels O0-O3, and 3 obfuscation passes based on LLVM. Our model significantly outperforms state-of-the-art ML-based tools by 14.6\% in recovering types for both function arguments and variables. Our ablation studies show that GSM improves type inference accuracy by 33\%.},
	booktitle = {Proceedings of the 29th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Pei, Kexin and Guan, Jonas and Broughton, Matthew and Chen, Zhongtian and Yao, Songchen and Williams-King, David and Ummadisetty, Vikas and Yang, Junfeng and Ray, Baishakhi and Jana, Suman},
	year = {2021},
	keywords = {Reverse Engineering, Type Inference, Machine Learning for Program Analysis, Transfer Learning},
	pages = {690--702},
}

@inproceedings{casolare_steaelergon_2021,
	address = {New York, NY, USA},
	series = {{ARES} '21},
	title = {{SteælErgon}: {A} {Framework} for {Injecting} {Colluding} {Malicious} {Payload} in {Android} {Applications}},
	isbn = {978-1-4503-9051-4},
	url = {https://doi.org/10.1145/3465481.3470077},
	doi = {10.1145/3465481.3470077},
	abstract = {Mobile malware is growing in number and its complexity is constantly increasing. Malware authors are continuously looking new ways to elude anti-malware controls. Anti-malware are not able to detect zero-day malware, because to detect malicious behaviour they need to know its signature, but to have this information the malware must already be widespread. Furthermore, anti-malware are able to scan one application at a time: for this reason a type of malware characterized by the colluding attack, where the malicious action is split in two (or more) applications, can not be recognised. To demonstrate the ineffectiveness of current anti-malware mechanisms in recognizing colluding attacks, in this paper we propose SteælErgon, a framework aimed to inject a malicious payload in two or more different Android applications. Clearly the malicious payload will be executed once all the applications composing the collusive attacks are installed into the infected device. In detail, SteælErgon is able to inject a collusive malicious payload attacking the external storage, allowing the attacker to catch sensitive and private information stored into the infected device. We perform an experimental analysis by submitting the generated colluding application to different 79 anti-malware, by showing that current detection mechanism are not able to detect this kind of threat. To boost research in focusing the attention in colluding attacks we freely release SteælErgon, is available for research purposes at the following url: https://github.com/vigimella/StealErgon.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Casolare, Rosangela and Ciaramella, Giovanni and Martinelli, Fabio and Mercaldo, Francesco and Santone, Antonella},
	year = {2021},
	keywords = {malware, Android, security, mobile, colluding, covert channel, detection},
}

@inproceedings{singh_multi-view_2021,
	address = {New York, NY, USA},
	series = {{ARES} '21},
	title = {Multi-{View} {Learning} for {Repackaged} {Malware} {Detection}},
	isbn = {978-1-4503-9051-4},
	url = {https://doi.org/10.1145/3465481.3470040},
	doi = {10.1145/3465481.3470040},
	abstract = {Repackaging refers to the core process of unpacking a software package, then repackaging it after a probable modification to the decompiled code and/or to other resource files. In the case of repackaged malware, benign apps are injected with a malicious payload. Repackaged malware pose a serious threat to the Android ecosystem. Moreover, repackaged malware and benign apps share more than 80\% of their features, which makes detection a challenging problem. This paper presents a novel technique based on multi-view learning to address this challenge of detecting repackaged malware. Multi-View Learning is a technique where data from multiple distinct feature groups, referred to as views, are fused to improve the model’s generalization performance. In the context of Android, we define views as different components of the app, such as permissions, APIs, sensor usage, etc. We analyzed 15,297 repackaged app pairs and extracted seven different views to represent an app. We perform an ablation study to identify which view(s) contribute more to the classification. The model was trained end-to-end to jointly learn appropriate features and to perform the classification. We show that our approach achieves accuracy and an F1-score of 97.46\% and 0.98, respectively.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Availability}, {Reliability} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Singh, Shirish and Chaturvedy, Kushagra and Mishra, Bharavi},
	year = {2021},
	keywords = {Malware detection, Mobile apps, Multi-view learning, Repackaged malware},
}

@article{abuhamad_large-scale_2021,
	address = {New York, NY, USA},
	title = {Large-scale and {Robust} {Code} {Authorship} {Identification} with {Deep} {Feature} {Learning}},
	volume = {24},
	issn = {2471-2566},
	url = {https://doi.org/10.1145/3461666},
	doi = {10.1145/3461666},
	abstract = {Successful software authorship de-anonymization has both software forensics applications and privacy implications. However, the process requires an efficient extraction of authorship attributes. The extraction of such attributes is very challenging, due to various software code formats from executable binaries with different toolchain provenance to source code with different programming languages. Moreover, the quality of attributes is bounded by the availability of software samples to a certain number of samples per author and a specific size for software samples. To this end, this work proposes a deep Learning-based approach for software authorship attribution, that facilitates large-scale, format-independent, language-oblivious, and obfuscation-resilient software authorship identification. This proposed approach incorporates the process of learning deep authorship attribution using a recurrent neural network, and ensemble random forest classifier for scalability to de-anonymize programmers. Comprehensive experiments are conducted to evaluate the proposed approach over the entire Google Code Jam (GCJ) dataset across all years (from 2008 to 2016) and over real-world code samples from 1,987 public repositories on GitHub. The results of our work show high accuracy despite requiring a smaller number of samples per author. Experimenting with source-code, our approach allows us to identify 8,903 GCJ authors, the largest-scale dataset used by far, with an accuracy of 92.3\%. Using the real-world dataset, we achieved an identification accuracy of 94.38\% for 745 C programmers on GitHub. Moreover, the proposed approach is resilient to language-specifics, and thus it can identify authors of four programming languages (e.g., C, C++, Java, and Python), and authors writing in mixed languages (e.g., Java/C++, Python/C++). Finally, our system is resistant to sophisticated obfuscation (e.g., using C Tigress) with an accuracy of 93.42\% for a set of 120 authors. Experimenting with executable binaries, our approach achieves 95.74\% for identifying 1,500 programmers of software binaries. Similar results were obtained when software binaries are generated with different compilation options, optimization levels, and removing of symbol information. Moreover, our approach achieves 93.86\% for identifying 1,500 programmers of obfuscated binaries using all features adopted in Obfuscator-LLVM tool.},
	number = {4},
	journal = {ACM Trans. Priv. Secur.},
	publisher = {Association for Computing Machinery},
	author = {Abuhamad, Mohammed and Abuhmed, Tamer and Mohaisen, David and Nyang, Daehun},
	month = jul,
	year = {2021},
	keywords = {deep learning identification, program features, Software authorship identification, software forensics},
}

@inproceedings{kalhauge_logical_2021,
	address = {New York, NY, USA},
	series = {{PLDI} 2021},
	title = {Logical bytecode reduction},
	isbn = {978-1-4503-8391-2},
	url = {https://doi.org/10.1145/3453483.3454091},
	doi = {10.1145/3453483.3454091},
	abstract = {Reducing a failure-inducing input to a smaller one is challenging for input with internal dependencies because most sub-inputs are invalid. Kalhauge and Palsberg made progress on this problem by mapping the task to a reduction problem for dependency graphs that avoids invalid inputs entirely. Their tool J-Reduce efficiently reduces Java bytecode to 24 percent of its original size, which made it the most effective tool until now. However, the output from their tool is often too large to be helpful in a bug report. In this paper, we show that more fine-grained modeling of dependencies leads to much more reduction. Specifically, we use propositional logic for specifying dependencies and we show how this works for Java bytecode. Once we have a propositional formula that specifies all valid sub-inputs, we run an algorithm that finds a small, valid, failure-inducing input. Our algorithm interleaves runs of the buggy program and calls to a procedure that finds a minimal satisfying assignment. Our experiments show that we can reduce Java bytecode to 4.6 percent of its original size, which is 5.3 times better than the 24.3 percent achieved by J-Reduce. The much smaller output is more suitable for bug reports.},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Kalhauge, Christian Gram and Palsberg, Jens},
	year = {2021},
	keywords = {input reduction, type-safe code transformation},
	pages = {1003--1016},
}

@inproceedings{ringer_proof_2021,
	address = {New York, NY, USA},
	series = {{PLDI} 2021},
	title = {Proof repair across type equivalences},
	isbn = {978-1-4503-8391-2},
	url = {https://doi.org/10.1145/3453483.3454033},
	doi = {10.1145/3453483.3454033},
	abstract = {We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to suggested tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in Pumpkin Pi, an extension to the Pumpkin Patch Coq plugin suite for proof repair. We demonstrate Pumpkin Pi’s flexibility on eight case studies, including supporting a benchmark from a user study,easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ringer, Talia and Porter, RanDair and Yazdani, Nathaniel and Leo, John and Grossman, Dan},
	year = {2021},
	keywords = {proof engineering, proof repair, proof reuse},
	pages = {112--127},
}

@inproceedings{liu_igscript_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {{IGScript}: {An} {Interaction} {Grammar} for {Scientific} {Data} {Presentation}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445535},
	doi = {10.1145/3411764.3445535},
	abstract = {Most of the existing scientific visualizations toward interpretive grammar aim to enhance customizability in either the computation stage or the rendering stage or both, while few approaches focus on the data presentation stage. Besides, most of these approaches leverage the existing components from the general-purpose programming languages (GPLs) instead of developing a standalone compiler, which pose a great challenge about learning curves for the domain experts who have limited knowledge about programming. In this paper, we propose IGScript, a novel script-based interaction grammar tool, to help build scientific data presentation animations for communication. We design a dual-space interface and a compiler which converts natural language-like grammar statements or scripts into a data story animation to make an interactive customization on script-driven data presentations, and then develop a code generator (decompiler) to translate the interactive data exploration animations back into script codes to achieve statement parameters. IGScript makes the presentation animations editable, e.g., it allows to cut, copy, paste, append, or even delete some animation clips. We demonstrate the usability, customizability, and flexibility of IGScript by a user study, four case studies conducted by using four types of commonly-used scientific data, and performance evaluations.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Richen and Gao, Min and Ye, Shunlong and Zhang, Jiang},
	year = {2021},
	keywords = {data presentation, interaction grammar, scientific visualization},
}

@inproceedings{wang_falx_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {Falx: {Synthesis}-{Powered} {Visualization} {Authoring}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445249},
	doi = {10.1145/3411764.3445249},
	abstract = {Modern visualization tools aim to allow data analysts to easily create exploratory visualizations. When the input data layout conforms to the visualization design, users can easily specify visualizations by mapping data columns to visual channels of the design. However, when there is a mismatch between data layout and the design, users need to spend significant effort on data transformation. We propose Falx, a synthesis-powered visualization tool that allows users to specify visualizations in a similarly simple way but without needing to worry about data layout. In Falx, users specify visualizations using examples of how concrete values in the input are mapped to visual channels, and Falx automatically infers the visualization specification and transforms the data to match the design. In a study with 33 data analysts on four visualization tasks involving data transformation, we found that users can effectively adopt Falx to create visualizations they otherwise cannot implement.},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Chenglong and Feng, Yu and Bodik, Rastislav and Dillig, Isil and Cheung, Alvin and Ko, Amy J},
	year = {2021},
}

@inproceedings{kang_obfus_2021,
	address = {New York, NY, USA},
	series = {{CODASPY} '21},
	title = {{OBFUS}: {An} {Obfuscation} {Tool} for {Software} {Copyright} and {Vulnerability} {Protection}},
	isbn = {978-1-4503-8143-7},
	url = {https://doi.org/10.1145/3422337.3450321},
	doi = {10.1145/3422337.3450321},
	abstract = {In this paper, we propose OBFUS, a web-based tool that can easily apply obfuscation techniques to high-level and low-level programming languages. OBFUS's high-level obfuscator parses and obfuscates the source code, overlaying the obfuscation to produce more complex results. OBFUS's low-level obfuscator decompiles binary programs into LLVM IR. This LLVM IR pro-gram is obfuscated and the LLVM IR program is recompiled to become an obfuscated binary program.},
	booktitle = {Proceedings of the {Eleventh} {ACM} {Conference} on {Data} and {Application} {Security} and {Privacy}},
	publisher = {Association for Computing Machinery},
	author = {Kang, Seoyeon and Lee, Sujeong and Kim, Yumin and Mok, Seong-Kyun and Cho, Eun-Sun},
	year = {2021},
	keywords = {software protection, obuscation},
	pages = {309--311},
}

@inproceedings{hilbig_empirical_2021,
	address = {New York, NY, USA},
	series = {{WWW} '21},
	title = {An {Empirical} {Study} of {Real}-{World} {WebAssembly} {Binaries}: {Security}, {Languages}, {Use} {Cases}},
	isbn = {978-1-4503-8312-7},
	url = {https://doi.org/10.1145/3442381.3450138},
	doi = {10.1145/3442381.3450138},
	abstract = {WebAssembly has emerged as a low-level language for the web and beyond. Despite its popularity in different domains, little is known about WebAssembly binaries that occur in the wild. This paper presents a comprehensive empirical study of 8,461 unique WebAssembly binaries gathered from a wide range of sources, including source code repositories, package managers, and live websites. We study the security properties, source languages, and use cases of the binaries and how they influence the security of the WebAssembly ecosystem. Our findings update some previously held assumptions about real-world WebAssembly and highlight problems that call for future research. For example, we show that vulnerabilities that propagate from insecure source languages potentially affect a wide range of binaries (e.g., two thirds of the binaries are compiled from memory unsafe languages, such as C and C++) and that 21\% of all binaries import potentially dangerous APIs from their host environment. We also show that cryptomining, which once accounted for the majority of all WebAssembly code, has been marginalized (less than 1\% of all binaries found on the web) and gives way to a diverse set of use cases. Finally, 29\% of all binaries on the web are minified, calling for techniques to decompile and reverse engineer WebAssembly. Overall, our results show that WebAssembly has left its infancy and is growing up into a language that powers a diverse ecosystem, with new challenges and opportunities for security researchers and practitioners. Besides these insights, we also share the dataset underlying our study, which is 58 times larger than the largest previously reported benchmark.},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Hilbig, Aaron and Lehmann, Daniel and Pradel, Michael},
	year = {2021},
	pages = {2696--2708},
}

@book{noauthor_pepm_2021,
	address = {New York, NY, USA},
	title = {{PEPM} 2021: {Proceedings} of the 2021 {ACM} {SIGPLAN} {Workshop} on {Partial} {Evaluation} and {Program} {Manipulation}},
	isbn = {978-1-4503-8305-9},
	abstract = {We are pleased to present the proceedings of the 2021 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation (PEPM 2021), held January 18–19th, 2021, in aliation with the annual Symposium on Principles of Programming Languages (POPL 2021). (Originally POPL 2021 was originally to be held in Copenhagen, Denmark, but due to the COVID-19 pandemic, it was moved entirely online.) PEPM has a history dating back to 1991, and originates in the discoveries of practically useful automated techniques for evaluating programs with only partial input. Over the years, the scope of PEPM has expanded to include a variety of research areas centered around the theme of semantics-based program manipulation — the systematic exploitation of treating programs not only as subject to black-box execution, but also as data structures that can be generated, analysed, and transformed whilst establishing or maintaining important semantic properties. Relevant topics range from refactoring, partial evaluation, supercompilation, staged programming, fusion, and other meta-programming to model-driven development, program analysis, inductive programming, decompilation, program generation, and abstract interpretation.},
	publisher = {Association for Computing Machinery},
	year = {2021},
}

@inproceedings{wang_automated_2021,
	address = {New York, NY, USA},
	series = {{ASE} '20},
	title = {An automated assessment of {Android} clipboards},
	isbn = {978-1-4503-6768-4},
	url = {https://doi.org/10.1145/3324884.3418905},
	doi = {10.1145/3324884.3418905},
	abstract = {Since the new privacy feature in iOS enabling users to acknowledge which app is reading or writing to his or her clipboard through prompting notifications was updated, a plethora of top apps have been reported to frequently access the clipboard without user consent. However, the lack of monitoring and control of Android application's access to the clipboard data leave Android users blind to their potential to leak private information from Android clipboards, raising severe security and privacy concerns. In this preliminary work, we envisage and investigate an approach to (i) dynamically detect clipboard access behaviour, and (ii) determine privacy leaks via static data flow analysis, in which we enhance the results of taint analysis with call graph concatenation to enable leakage source backtracking. Our preliminary results indicate that the proposed method can expose clipboard data leakage as substantiated by our discovery of a popular app, i.e., Sogou Input, directly monitoring and transferring user data in a clipboard to backend servers.},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wei and Sun, Ruoxi and Xue, Minhui and Ranasinghe, Damith C.},
	year = {2021},
	pages = {1249--1251},
}

@article{albert_taming_2020,
	address = {New York, NY, USA},
	title = {Taming callbacks for smart contract modularity},
	volume = {4},
	url = {https://doi.org/10.1145/3428277},
	doi = {10.1145/3428277},
	abstract = {Callbacks are an effective programming discipline for implementing event-driven programming, especially in environments like Ethereum which forbid shared global state and concurrency. Callbacks allow a callee to delegate the execution back to the caller. Though effective, they can lead to subtle mistakes principally in open environments where callbacks can be added in a new code. Indeed, several high profile bugs in smart contracts exploit callbacks. We present the first static technique ensuring modularity in the presence of callbacks and apply it to verify prominent smart contracts. Modularity ensures that external calls to other contracts cannot affect the behavior of the contract. Importantly, modularity is guaranteed without restricting programming. In general, checking modularity is undecidable—even for programs without loops. This paper describes an effective technique for soundly ensuring modularity harnessing SMT solvers. The main idea is to define a constructive version of modularity using commutativity and projection operations on program segments. We believe that this approach is also accessible to programmers, since counterexamples to modularity can be generated automatically by the SMT solvers, allowing programmers to understand and fix the error. We implemented our approach in order to demonstrate the precision of the modularity analysis and applied it to real smart contracts, including a subset of the 150 most active contracts in Ethereum. Our implementation decompiles bytecode programs into an intermediate representation and then implements the modularity checking using SMT queries. Overall, we argue that our experimental results indicate that the method can be applied to many realistic contracts, and that it is able to prove modularity where other methods fail.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Albert, Elvira and Grossman, Shelly and Rinetzky, Noam and Rodríguez-Núñez, Clara and Rubio, Albert and Sagiv, Mooly},
	month = jan,
	year = {2020},
	keywords = {program analysis, smart contracts, blockchain, invariants, logic and verification, program verification},
}

@inproceedings{erinfolami_devil_2020,
	address = {New York, NY, USA},
	series = {{CCS} '20},
	title = {Devil is {Virtual}: {Reversing} {Virtual} {Inheritance} in {C}++ {Binaries}},
	isbn = {978-1-4503-7089-9},
	url = {https://doi.org/10.1145/3372297.3417251},
	doi = {10.1145/3372297.3417251},
	abstract = {The complexities that arise from the implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious of virtual inheritance. The conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10\% on Linux and 12.5\% on Windows) of real-world C++ programs including Mysql and Libstdc++. (2) We show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These falses either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95\% and 95.5\% (GCC -O0) and a minimum of 77.5\% and 73.8\% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Erinfolami, Rukayat Ayomide and Prakash, Aravind},
	year = {2020},
	keywords = {class inheritance recovery, software reverse engineering, virtual inheritance recovery},
	pages = {133--148},
}

@article{grech_madmax_2020,
	address = {New York, NY, USA},
	title = {{MadMax}: analyzing the out-of-gas world of smart contracts},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3416262},
	doi = {10.1145/3416262},
	abstract = {Ethereum is a distributed blockchain platform, serving as an ecosystem for smart contracts: full-fledged intercommunicating programs that capture the transaction logic of an account. A gas limit caps the execution of an Ethereum smart contract: instructions, when executed, consume gas, and the execution proceeds as long as gas is available.Gas-focused vulnerabilities permit an attacker to force key contract functionality to run out of gas—effectively performing a permanent denial-of-service attack on the contract. Such vulnerabilities are among the hardest for programmers to protect against, as out-of-gas behavior may be uncommon in nonattack scenarios and reasoning about these vulnerabilities is nontrivial.In this paper, we identify gas-focused vulnerabilities and present MadMax: a static program analysis technique that automatically detects gas-focused vulnerabilities with very high confidence. MadMax combines a smart contract decompiler and semantic queries in Datalog. Our approach captures high-level program modeling concepts (such as "dynamic data structure storage" and "safely resumable loops") and delivers high precision and scalability. MadMax analyzes the entirety of smart contracts in the Ethereum blockchain in just 10 hours and flags vulnerabilities in contracts with a monetary value in billions of dollars. Manual inspection of a sample of flagged contracts shows that 81\% of the sampled warnings do indeed lead to vulnerabilities.},
	number = {10},
	journal = {Commun. ACM},
	publisher = {Association for Computing Machinery},
	author = {Grech, Neville and Kong, Michael and Jurisevic, Anton and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	month = sep,
	year = {2020},
	pages = {87--95},
}

@inproceedings{yan_eshield_2020,
	address = {New York, NY, USA},
	series = {{ISSTA} 2020},
	title = {{EShield}: protect smart contracts against reverse engineering},
	isbn = {978-1-4503-8008-9},
	url = {https://doi.org/10.1145/3395363.3404365},
	doi = {10.1145/3395363.3404365},
	abstract = {Smart contracts are the back-end programs of blockchain-based applications and the execution results are deterministic and publicly visible. Developers are unwilling to release source code of some smart contracts to generate randomness or for security reasons, however, attackers still can use reverse engineering tools to decompile and analyze the code. In this paper, we propose EShield, an automated security enhancement tool for protecting smart contracts against reverse engineering. EShield replaces original instructions of operating jump addresses with anti-patterns to interfere with control flow recovery from bytecode. We have implemented four methods in EShield and conducted an experiment on over 20k smart contracts. The evaluation results show that all the protected smart contracts are resistant to three different reverse engineering tools with little extra gas cost.},
	booktitle = {Proceedings of the 29th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Yan, Wentian and Gao, Jianbo and Wu, Zhenhao and Li, Yue and Guan, Zhi and Li, Qingshan and Chen, Zhong},
	year = {2020},
	keywords = {Reverse Engineering, Program Analysis, Ethereum, Smart Contract, Blockchain},
	pages = {553--556},
}

@inproceedings{seraj_novel_2020,
	address = {New York, NY, USA},
	series = {{WIMS} 2020},
	title = {A {Novel} {Dataset} for {Fake} {Android} {Anti}-{Malware} {Detection}},
	isbn = {978-1-4503-7542-9},
	url = {https://doi.org/10.1145/3405962.3405980},
	doi = {10.1145/3405962.3405980},
	abstract = {Today in the world people are able to get all types of Android applications (apps) from the app store or various sources over the Internet. A large number of apps is being produced daily, some of which are infected with malware. Thus, the use of anti-malware identification tools is essential. At the same time, a number of attackers who exploit a number of anti-malwares have been doing obtaining information from mobile phones in various ways, such as decompiling or infecting anti-malware. Therefore, in this paper, we developed a classification dataset from collected anti-malware data looking for fraudulent anti-malware products. Additionally, we applied various machine learning algorithms and we propose a combination of algorithms which provides high accuracy over various evaluation tests, showing that our approach is both practical and effective.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Web} {Intelligence}, {Mining} and {Semantics}},
	publisher = {Association for Computing Machinery},
	author = {Seraj, Saeed and Pavlidis, Michalis and Polatidis, Nikolaos},
	year = {2020},
	keywords = {Malware, Machine learning, Android, Anti-malware, Cyber security, Fake anti-malware detection},
	pages = {205--209},
}

@inproceedings{nandi_synthesizing_2020,
	address = {New York, NY, USA},
	series = {{PLDI} 2020},
	title = {Synthesizing structured {CAD} models with equality saturation and inverse transformations},
	isbn = {978-1-4503-7613-6},
	url = {https://doi.org/10.1145/3385412.3386012},
	doi = {10.1145/3385412.3386012},
	abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
	year = {2020},
	keywords = {Decompilation, Program Synthesis, Computer-Aided Design, Equality Saturation},
	pages = {31--44},
}

@inproceedings{vasileiadis_revealing_2019,
	address = {New York, NY, USA},
	series = {{SSPREW9} '19},
	title = {Revealing malicious remote engineering attempts on {Android} apps with magic numbers},
	isbn = {978-1-4503-7746-1},
	url = {https://doi.org/10.1145/3371307.3371312},
	doi = {10.1145/3371307.3371312},
	abstract = {Malicious reverse engineering is a prominent activity conducted by attackers to plan their code tampering attacks. Android apps are particularly exposed to malicious reverse engineering, because their code can be easily analyzed and decompiled, or monitored using debugging tools, that were originally meant to be used by developers.In this paper, we propose a solution to identify attempts of malicious reverse engineering on Android apps. Our approach is based on a series of periodic checks on the execution environment (i.e., Android components) and on the app itself. The check outcome is encoded into a Magic Number and send to a sever for validation. The owner of the app is then supposed to take countermeasures and react, by disconnecting or banning the apps under attack.Our empirical validation suggests that the execution overhead caused by our periodic checks is acceptable, because its resource consumption is compatible with the resources commonly available in smartphones.},
	booktitle = {Proceedings of the 9th {Workshop} on {Software} {Security}, {Protection}, and {Reverse} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Vasileiadis, Leonidas and Ceccato, Mariano and Corradini, Davide},
	year = {2019},
	keywords = {code tampering, malicious reverse engineering, remote attestation},
}

@inproceedings{rohleder_hands-ghidra_2019,
	address = {New York, NY, USA},
	series = {{SPRO}'19},
	title = {Hands-{On} {Ghidra} - {A} {Tutorial} about the {Software} {Reverse} {Engineering} {Framework}},
	isbn = {978-1-4503-6835-3},
	url = {https://doi.org/10.1145/3338503.3357725},
	doi = {10.1145/3338503.3357725},
	abstract = {In this tutorial, the Ghidra software reverse engineering framework will be presented, its characteristics highlighted and its features to the hitherto industry standard in reverse engineering tools, IDA Pro - the interactive disassembler, compared against. This framework was released on March the 5th 2019, by the National Security Agency under the Apache v2 license and brought with it a powerful decompiler for many different architectures (X86 16/32/64, ARM/AARCH64, Java/DEX bytecode, ...), which will be presented and its underlying intermediate language p-code and the corresponding SLEIGH-format explained. Further, hands-on demonstrations will follow, including the aforementioned SLEIGH-format, the plugin-system and the standalone-mode, showcased on different reverse engineering tasks like binary diffing, code lifting, deobfuscation and patching.},
	booktitle = {Proceedings of the 3rd {ACM} {Workshop} on {Software} {Protection}},
	publisher = {Association for Computing Machinery},
	author = {Rohleder, Roman},
	year = {2019},
	keywords = {decompilation, reverse engineering, code lifting, disassembly, framework},
	pages = {77--78},
}

@inproceedings{xylogiannopoulos_text_2020,
	address = {New York, NY, USA},
	series = {{ASONAM} '19},
	title = {Text mining for malware classification using multivariate all repeated patterns detection},
	isbn = {978-1-4503-6868-1},
	url = {https://doi.org/10.1145/3341161.3350841},
	doi = {10.1145/3341161.3350841},
	abstract = {Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Xylogiannopoulos, Konstantinos F. and Karampelas, Panagiotis and Alhajj, Reda},
	year = {2020},
	keywords = {Malware, Mobile handsets, Androids, Android malware detection, ARPaD, Humanoid robots, LERP-RSA, malware family classification, Payloads, Social network services, text mining, Text mining},
	pages = {887--894},
}

@inproceedings{kalhauge_binary_2019,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2019},
	title = {Binary reduction of dependency graphs},
	isbn = {978-1-4503-5572-8},
	url = {https://doi.org/10.1145/3338906.3338956},
	doi = {10.1145/3338906.3338956},
	abstract = {Delta debugging is a technique for reducing a failure-inducing input to a small input that reveals the cause of the failure. This has been successful for a wide variety of inputs including C programs, XML data, and thread schedules. However, for input that has many internal dependencies, delta debugging scales poorly. Such input includes C\#, Java, and Java bytecode and they have presented a major challenge for input reduction until now. In this paper, we show that the core challenge is a reduction problem for dependency graphs, and we present a general strategy for reducing such graphs. We combine this with a novel algorithm for reduction called Binary Reduction in a tool called J-Reduce for Java bytecode. Our experiments show that our tool is 12x faster and achieves more reduction than delta debugging on average. This enabled us to create and submit short bug reports for three Java bytecode decompilers.},
	booktitle = {Proceedings of the 2019 27th {ACM} {Joint} {Meeting} on {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Kalhauge, Christian Gram and Palsberg, Jens},
	year = {2019},
	keywords = {Debugging, dependencies, reduction},
	pages = {556--566},
}

@inproceedings{kumar_android_2019,
	address = {New York, NY, USA},
	series = {{AINTEC} '19},
	title = {Android {Malware} {Prediction} {Using} {Extreme} {Learning} {Machine} with {Different} {Kernel} {Functions}},
	isbn = {978-1-4503-6849-0},
	url = {https://doi.org/10.1145/3340422.3343639},
	doi = {10.1145/3340422.3343639},
	abstract = {Android is currently the most popular smartphone platform which occupied 88\% of global sale by the end of 2nd quarter 2018. With the popularity of these applications, it is also inviting cybercriminals to develop malware application for accessing important information from smartphones. The major objective of cybercriminals to develop Malware apps or Malicious apps to threaten the organization privacy data, user privacy data, and device integrity. Early identification of such malware apps can help the android user to save private data and device integrity. In this study, features extracted from intermediate code representations obtained using decompilation of APK file are used for providing requisite input data to develop the models for predicting android malware applications. These models are trained using extreme learning with multiple kernel functions ans also compared with the model trained using most frequently used classifiers like linear regression, decision tree, polynomial regression, and logistic regression. This paper also focuses on the effectiveness of data sampling techniques for balancing data and feature selection methods for selecting right sets of significant uncorrelated metrics. The high-value of accuracy and AUC confirm the predicting capability of data sampling, sets of metrics, and training algorithms to malware and normal applications.},
	booktitle = {Proceedings of the 15th {Asian} {Internet} {Engineering} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kumar, Lov and Hota, Chinmay and Mahindru, Arvind and Neti, Lalita Bhanu Murthy},
	year = {2019},
	keywords = {Artificial neural network, Genetics algorithm, Maintainability, Object-Oriented Metrics, Parallel Computing},
	pages = {33--40},
}

@inproceedings{erinfolami_declassifier_2019,
	address = {New York, NY, USA},
	series = {Asia {CCS} '19},
	title = {{DeClassifier}: {Class}-{Inheritance} {Inference} {Engine} for {Optimized} {C}++ {Binaries}},
	isbn = {978-1-4503-6752-3},
	url = {https://doi.org/10.1145/3321705.3329833},
	doi = {10.1145/3321705.3329833},
	abstract = {Recovering class inheritance from C++ binaries has several security benefits including in solving problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining—or worse exclusion—due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5\% and 71.4\% true positive directed edges in the class hierarchy tree (CHT) under O0 and O2 optimizations respectively, (2) a combination of constructor-destructor (ctor-dtor) analysis provides a substantial improvement in inheritance inference than constructor-only (ctor-only) analysis.},
	booktitle = {Proceedings of the 2019 {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Erinfolami, Rukayat Ayomide and Prakash, Aravind},
	year = {2019},
	keywords = {software reverse engineering, class hierarchy recovery},
	pages = {28--40},
}

@article{raychev_predicting_2019,
	address = {New York, NY, USA},
	title = {Predicting program properties from 'big code'},
	volume = {62},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3306204},
	doi = {10.1145/3306204},
	abstract = {We present a new approach for predicting program properties from large codebases (aka "Big Code"). Our approach learns a probabilistic model from "Big Code" and uses this model to predict properties of new, unseen programs.The key idea of our work is to transform the program into a representation that allows us to formulate the problem of inferring program properties as structured prediction in machine learning. This enables us to leverage powerful probabilistic models such as Conditional Random Fields (CRFs) and perform joint prediction of program properties.As an example of our approach, we built a scalable prediction engine called JSNICE for solving two kinds of tasks in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63\% of name identifiers and its type annotation predictions are correct in 81\% of cases. Since its public release at http://jsnice.org, JSNice has become a popular system with hundreds of thousands of uses.By formulating the problem of inferring program properties as structured prediction, our work opens up the possibility for a range of new "Big Code" applications such as de-obfuscators, decompilers, invariant generators, and others.},
	number = {3},
	journal = {Commun. ACM},
	publisher = {Association for Computing Machinery},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	month = feb,
	year = {2019},
	pages = {99--107},
}

@inproceedings{roessle_formally_2019,
	address = {New York, NY, USA},
	series = {{CPP} 2019},
	title = {Formally verified big step semantics out of x86-64 binaries},
	isbn = {978-1-4503-6222-1},
	url = {https://doi.org/10.1145/3293880.3294102},
	doi = {10.1145/3293880.3294102},
	abstract = {This paper presents a methodology for generating formally proven equivalence theorems between decompiled x86-64 machine code and big step semantics. These proofs are built on top of two additional contributions. First, a robust and tested formal x86-64 machine model containing small step semantics for 1625 instructions. Second, a decompilation-into-logic methodology supporting both x86-64 assembly and machine code at large scale. This work enables black-box binary verification, i.e., formal verification of a binary where source code is unavailable. As such, it can be applied to safety-critical systems that consist of legacy components, or components whose source code is unavailable due to proprietary reasons. The methodology minimizes the trusted code base by leveraging machine-learned semantics to build a formal machine model. We apply the methodology to several case studies, including binaries that heavily rely on the SSE2 floating-point instruction set, and binaries that are obtained by compiling code that is obtained by inlining assembly into C code.},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {International} {Conference} on {Certified} {Programs} and {Proofs}},
	publisher = {Association for Computing Machinery},
	author = {Roessle, Ian and Verbeek, Freek and Ravindran, Binoy},
	year = {2019},
	keywords = {semantics, theorem proving, x86-64},
	pages = {181--195},
}

@article{grech_madmax_2018,
	address = {New York, NY, USA},
	title = {{MadMax}: surviving out-of-gas conditions in {Ethereum} smart contracts},
	volume = {2},
	url = {https://doi.org/10.1145/3276486},
	doi = {10.1145/3276486},
	abstract = {Ethereum is a distributed blockchain platform, serving as an ecosystem for smart contracts: full-fledged inter-communicating programs that capture the transaction logic of an account. Unlike programs in mainstream languages, a gas limit restricts the execution of an Ethereum smart contract: execution proceeds as long as gas is available. Thus, gas is a valuable resource that can be manipulated by an attacker to provoke unwanted behavior in a victim's smart contract (e.g., wasting or blocking funds of said victim). Gas-focused vulnerabilities exploit undesired behavior when a contract (directly or through other interacting contracts) runs out of gas. Such vulnerabilities are among the hardest for programmers to protect against, as out-of-gas behavior may be uncommon in non-attack scenarios and reasoning about it is far from trivial. In this paper, we classify and identify gas-focused vulnerabilities, and present MadMax: a static program analysis technique to automatically detect gas-focused vulnerabilities with very high confidence. Our approach combines a control-flow-analysis-based decompiler and declarative program-structure queries. The combined analysis captures high-level domain-specific concepts (such as "dynamic data structure storage" and "safely resumable loops") and achieves high precision and scalability. MadMax analyzes the entirety of smart contracts in the Ethereum blockchain in just 10 hours (with decompilation timeouts in 8\% of the cases) and flags contracts with a (highly volatile) monetary value of over \$2.8B as vulnerable. Manual inspection of a sample of flagged contracts shows that 81\% of the sampled warnings do indeed lead to vulnerabilities, which we report on in our experiment.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Grech, Neville and Kong, Michael and Jurisevic, Anton and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	month = oct,
	year = {2018},
	keywords = {Security, Program Analysis, Smart Contracts, Blockchain},
}

@inproceedings{feichtner_automated_2018,
	address = {New York, NY, USA},
	series = {{WiSec} '18},
	title = {Automated {Binary} {Analysis} on {iOS}: {A} {Case} {Study} on {Cryptographic} {Misuse} in {iOS} {Applications}},
	isbn = {978-1-4503-5731-9},
	url = {https://doi.org/10.1145/3212480.3212487},
	doi = {10.1145/3212480.3212487},
	abstract = {A wide range of mobile applications for Apple's iOS platform process sensitive data and, therefore, rely on protective mechanisms natively provided by the operating system. A wrong application of cryptography or security-critical APIs, however, exposes secrets to unrelated parties and undermines the overall security.We introduce an approach for uncovering cryptographic misuse in iOS applications. We present a way to decompile 64-bit ARM binaries to their LLVM intermediate representation (IR). Based on the reverse-engineered code, static program slicing is applied to determine the data flow in relevant code segments. For this analysis to be most accurate, we propose an adapted version of Andersen's pointer analysis, capable of handling decompiled LLVM IR code with type information recovered from the binary. To finally highlight the improper usage of cryptographic APIs, a set of predefined security rules is checked against the extracted execution paths. As a result, we are not only able to confirm the existence of problematic statements in iOS applications but can also pinpoint their origin.To evaluate the applicability of our solution and to disclose possible weaknesses, we conducted a manual and automated inspection on a set of iOS applications that include cryptographic functionality. We found that 343 out of 417 applications (82\%) are subject to at least one security misconception. Among the most common flaws are the usage of non-random initialization vectors and constant encryption keys as input to cryptographic primitives.},
	booktitle = {Proceedings of the 11th {ACM} {Conference} on {Security} \&amp; {Privacy} in {Wireless} and {Mobile} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Feichtner, Johannes and Missmann, David and Spreitzer, Raphael},
	year = {2018},
	keywords = {Reverse Engineering, Program Analysis, iOS, Cryptographic Misuse},
	pages = {236--247},
}

@article{classen_anatomy_2018,
	address = {New York, NY, USA},
	title = {Anatomy of a {Vulnerable} {Fitness} {Tracking} {System}: {Dissecting} the {Fitbit} {Cloud}, {App}, and {Firmware}},
	volume = {2},
	url = {https://doi.org/10.1145/3191737},
	doi = {10.1145/3191737},
	abstract = {Fitbit fitness trackers record sensitive personal information, including daily step counts, heart rate profiles, and locations visited. By design, these devices gather and upload activity data to a cloud service, which provides aggregate statistics to mobile app users. The same principles govern numerous other Internet-of-Things (IoT) services that target different applications. As a market leader, Fitbit has developed perhaps the most secure wearables architecture that guards communication with end-to-end encryption. In this article, we analyze the complete Fitbit ecosystem and, despite the brand's continuous efforts to harden its products, we demonstrate a series of vulnerabilities with potentially severe implications to user privacy and device security. We employ a range of techniques, such as protocol analysis, software decompiling, and both static and dynamic embedded code analysis, to reverse engineer previously undocumented communication semantics, the official smartphone app, and the tracker firmware. Through this interplay and in-depth analysis, we reveal how attackers can exploit the Fitbit protocol to extract private information from victims without leaving a trace, and wirelessly flash malware without user consent. We demonstrate that users can tamper with both the app and firmware to selfishly manipulate records or circumvent Fitbit's walled garden business model, making the case for an independent, user-controlled, and more secure ecosystem. Finally, based on the insights gained, we make specific design recommendations that can not only mitigate the identified vulnerabilities, but are also broadly applicable to securing future wearable system architectures.},
	number = {1},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher = {Association for Computing Machinery},
	author = {Classen, Jiska and Wegemer, Daniel and Patras, Paul and Spink, Tom and Hollick, Matthias},
	month = mar,
	year = {2018},
	keywords = {firmware reverse engineering, health, Nexmon, Wearables},
}

@inproceedings{senthivel_denial_2018,
	address = {New York, NY, USA},
	series = {{CODASPY} '18},
	title = {Denial of {Engineering} {Operations} {Attacks} in {Industrial} {Control} {Systems}},
	isbn = {978-1-4503-5632-9},
	url = {https://doi.org/10.1145/3176258.3176319},
	doi = {10.1145/3176258.3176319},
	abstract = {We present a new type of attack termed denial of engineering operations in which an attacker can interfere with the normal cycle of an engineering operation leading to a loss of situational awareness. Specifically, the attacker can deceive the engineering software during attempts to retrieve the ladder logic program from a programmable logic controller (PLC) by manipulating the ladder logic on the PLC, such that the software is unable to process it while the PLC continues to execute it successfully. This attack vector can provide sufficient cover for the attacker»s actual scenario to play out while the owner tries to understand the problem and reestablish positive operational control. To enable the forensic analysis and, eventually, eliminate the threat, we have developed the first decompiler for ladder logic programs.Ladder logic is a graphical programming language for PLCs that control physical processes such as power grid, pipelines, and chemical plants; PLCs are a common target of malicious modifications leading to the compromise of the control behavior (and potentially serious consequences). Our decompiler, Laddis, transforms a low-level representation to its corresponding high-level original representation comprising of graphical symbols and connections. The evaluation of the accuracy of the decompiler on the program of varying complexity demonstrates perfect reconstruction of the original program. We present three new attack scenarios on PLC-deployed ladder logic and demonstrate the effectiveness of the decompiler on these scenarios.},
	booktitle = {Proceedings of the {Eighth} {ACM} {Conference} on {Data} and {Application} {Security} and {Privacy}},
	publisher = {Association for Computing Machinery},
	author = {Senthivel, Saranyan and Dhungana, Shrey and Yoo, Hyunguk and Ahmed, Irfan and Roussev, Vassil},
	year = {2018},
	keywords = {disassembler, forensics, industrial control system, ladder logic, plc, protocol reverse engineering, scada},
	pages = {319--329},
}

@inproceedings{dutu_improving_2025,
	title = {Improving {iOS} {Sandbox} {Profile} {Decompilation} {Accuracy}},
	issn = {2247-5443},
	doi = {10.1109/RoEduNet68395.2025.11208400},
	abstract = {Mobile devices have become ubiquitous, with iOS being the second most popular mobile operating system on the market [1]. One method iOS uses to ensure the security of its apps is through sandboxing. This mechanism is implemented as a set of rules compiled into binary files that lie inside the OS firmware and which are not made public by Apple. Thus, security engineers require third-party tools to decompile and then visualize the contents of the profiles mentioned above. This paper presents a validation framework for iOS sandbox profile decompilers, specifically targeting the SandBlaster tool. Our approach represents sandbox profiles as dependency graphs and compares decompiled profiles with reference implementations compiled from Sandbox Profile Language (SBPL) representations using SandScout. We evaluated our framework in iOS versions 7–10, analyzing both individual profiles and bundled profile collections. The results demonstrate 100\% precision and recall for iOS 7–8 profiles, 90-100\% for iOS 9, and 75-100\% for iOS 10. We also optimised a performance bottleneck in SandBlaster's node matching algorithm, reducing decompilation time from over 7 hours to under 5 minutes.},
	booktitle = {2025 24th {RoEduNet} {Conference}: {Networking} in {Education} and {Research} ({RoEduNet})},
	author = {Duţu, Teodor-Ştefan and Deaconescu, Adrian-Răzvan and Peca, Ludmila},
	month = sep,
	year = {2025},
	keywords = {Security, Accuracy, Operating systems, Visualization, Smart phones, Education, iOS, Microprogramming, Mobile communication, Sandboxing},
	pages = {1--6},
}

@inproceedings{mauthe_large-scale_2021,
	title = {A {Large}-{Scale} {Empirical} {Study} of {Android} {App} {Decompilation}},
	issn = {1534-5351},
	doi = {10.1109/SANER50967.2021.00044},
	abstract = {Decompilers are indispensable tools in Android malware analysis and app security auditing. Numerous academic works also employ an Android decompiler as the first step in a program analysis pipeline. In such settings, decompilation is frequently regarded as a "solved" problem, in that it is simply expected that source code can be accurately recovered from an app. While a large proportion of methods in an app can typically be decompiled successfully, it is common that at least some methods fail to decompile. In order to better understand the practical applicability of techniques in which decompilation is used as part of an automated analysis, it is important to know the actual expected failure rate of Android decompilation. To this end, we have performed what is, to the best of our knowledge, the first large-scale study of Android decompilation failure rates. We have used three sets of apps, consisting of, respectively, 3,018 open-source apps, 13,601 apps from a recent crawl of Google Play, and a collection of 24,553 malware samples. In addition to the state-of-the-art Dalvik bytecode decompiler jadx, we used three popular Java decompilers. While jadx achieves an impressively low failure rate of only 0.02\% failed methods per app on average, we found that it manages to recover source code for all methods in only 21\% of the Google Play apps.We have also sought to better understand the degree to which in-the-wild obfuscation techniques can prevent decompilation. Our empirical evaluation, complemented with an indepth manual analysis of a number of apps, indicate that code obfuscation is quite rarely encountered, even in malicious apps. Moreover, decompilation failures mostly appear to be caused by technical limitations in decompilers, rather than by deliberate attempts to thwart source-code recovery by obfuscation. This is an encouraging finding, as it indicates that near-perfect Android decompilation is, at least in theory, achievable, with implementation-level improvements to decompilation tools.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Mauthe, Noah and Kargén, Ulf and Shahmehri, Nahid},
	month = mar,
	year = {2021},
	keywords = {Tools, Process control, Malware, decompilation, Java, reverse engineering, obfuscation, malware, Android, Pipelines, Conferences, Manuals, mobile apps},
	pages = {400--410},
}

@inproceedings{pavel_technique_2021,
	title = {A {Technique} for {Detecting} the {Substitution} of a {Java}-{Module} of an {Information} {System} {Prone} to {Pharming} with {Using} a {Hidden} {Embedding} of a {Digital} {Watermark} {Resistant} to {Decompilation}},
	issn = {2157-023X},
	doi = {10.1109/ICUMT54235.2021.9631736},
	abstract = {The relevance of the problem under study is due to the rapid development of the java programming language, a large number of commercial applications written in this programming language. The purpose of the article is to develop a methodology that allows you to determine the java-module of the information system, which does not contain a hidden digital watermark. It is necessary that the digital watermark is not damaged during the decompilation of the entire system in order to replace the java module, and that the class files and the entire information system function unchanged. Thus, at first, a digital watermark is hidden in all java-modules of the information system. Next, we check the resistance of the digital watermark to decompilation attacks with block substitution. The materials of the article can be useful in the implementation of hidden embedding of a digital watermark in the class files of a large java module.},
	booktitle = {2021 13th {International} {Congress} on {Ultra} {Modern} {Telecommunications} and {Control} {Systems} and {Workshops} ({ICUMT})},
	author = {Pavel, Sharikov and Andrey, Krasov and Artem, Gelfand and Ernest, Birikh},
	month = oct,
	year = {2021},
	keywords = {Codes, decompilation, Java, bytecode, Computer languages, decompilation attack, digital watermark, Resistance, Watermarking, intruder model, java, java module substitution, java-module, Virtual machining, Writing},
	pages = {219--223},
}

@incollection{domas_decompilation_2024,
	title = {Decompilation and {Architecture}},
	isbn = {978-1-394-19990-7},
	url = {https://ieeexplore.ieee.org/document/10649756},
	doi = {10.1002/9781394277131.ch1},
	abstract = {Summary {\textless}p{\textgreater}This chapter explores the steps necessary to get started reverse engineering an application. Decompilation is crucial to transforming an application from machine code to something that can be read and understood by humans. For many programming languages, full decompilation is impossible. These languages build code directly to machine code, and some information, such as variable names, is lost in the process. JIT compilation also makes reverse engineering these applications much easier. Unlike true machine code programs, JIT\&\#x2010;compiled programs can often be converted to source code. All high\&\#x2010;level languages are eventually converted into a series of bits called machine code. Assembly code is designed to be a human\&\#x2010;readable version of machine code. A microarchitecture describes how a particular ISA is implemented on a processor. Reduced instruction set computing architectures define a small number of simpler instructions.{\textless}/p{\textgreater}},
	booktitle = {x86 {Software} {Reverse}-{Engineering}, {Cracking}, and {Counter}-{Measures}},
	publisher = {Wiley},
	author = {Domas, Stephanie and Domas, Christopher},
	year = {2024},
	keywords = {Codes, Source coding, Reverse engineering, Java, Computer architecture, Computer languages, Computers},
	pages = {1--12},
}

@inproceedings{liu_empirical_2023,
	title = {An {Empirical} {Study} of {Smart} {Contract} {Decompilers}},
	issn = {2640-7574},
	doi = {10.1109/SANER56733.2023.00011},
	abstract = {Smart contract decompilers, converting smart contract bytecode into smart contract source code, have been used extensively in many scenarios such as binary code analysis, reverse engineering, and security studies. However, existing studies, as well as industrial engineering practices, all assumed that smart contract decompilers are reliable and trustworthy, to generate correct and semantically equivalent source code from binaries. Unfortunately, whether such an assumption truly holds in practice is still unknown.In this paper, we conduct, to the best of our knowledge, the first and most comprehensive large-scale empirical study of smart contract decompilers, to gain an understanding of the reliability, limitations, and remaining research challenges of state-of-the-art smart contract decompilation tools. We first designed and implemented a software prototype SOLINSIGHT, then used it to study 5 state-of-the-art smart contract decompilers. We obtained important findings and insights from empirical results, such as: 1) we proposed 3 root causes leading to decompiler failures; 2) we revealed 2 reasons hurting performance; 3) we identified 3 root causes affecting decompilation effectiveness; 4) we proposed a measurement metric for completeness; and 5) we investigated the resilience of contract decompilers against program transformations. We suggest that: 1) decompiler builders should enhance decompilers in terms of effectiveness, performance, and completeness; and 2) security researchers should select appropriate decompilers based on the suggestions in this study. We believe these findings and suggestions will help decompiler builders, contract developers, and security researchers, by providing better guidelines for contract decompiler studies.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Liu, Xia and Hua, Baojian and Wang, Yang and Pan, Zhizhong},
	month = mar,
	year = {2023},
	keywords = {Source coding, Software, Decompilation, Reverse engineering, Prototypes, Measurement, Smart contracts, Empirical study, Reliability engineering},
	pages = {1--12},
}

@inproceedings{broukhis_notes_2020,
	title = {Notes on {Simulating} the {BESM}-6 and {Decompiling} {Pascal} {Programs}},
	doi = {10.1109/SORUCOM51654.2020.9464996},
	abstract = {The article describes the author's source of interest in the BESM-6 computer, its software, and the history of development of simulators of the BESM-6 architecture, and provides comments and observations regarding decompilation of a Pascal compiler for the BESM-6 and of the strategy game Kalah.},
	booktitle = {2020 {Fifth} {International} {Conference} “{History} of {Computing} in the {Russia}, former {Soviet} {Union} and {Council} for {Mutual} {Economic} {Assistance} countries” ({SORUCOM})},
	author = {Broukhis, Leonid A.},
	month = oct,
	year = {2020},
	keywords = {ALGOL-60, BESM-6, DISPAK, Dubna monitor system, FORTRAN, History, Optimization, Pascal, Program processors, simulation, Software, Software algorithms, Software measurement, Task analysis},
	pages = {67--71},
}

@inproceedings{yoo_digital_2019,
	title = {Digital {Forensic} {Artifact} {Collection} {Technique} using {Application} {Decompilation}},
	doi = {10.1109/PlatCon.2019.8668959},
	abstract = {Nowadays, many applications tend to collect user profile, such as location, usage trace and so on, even if it is not malicious. This information can be important clues in the criminal investigation. So, the technique is needed which extract artifacts from applications using decompilation. We describe a method for selecting and analyzing forensic artifacts from the Android application with a share of over 80\% of mobile devices. Based on the static analysis method, we propose a method for automatically collecting forensic artifact. The effectiveness of the proposed idea is proved by simulation.},
	booktitle = {2019 {International} {Conference} on {Platform} {Technology} and {Service} ({PlatCon})},
	author = {Yoo, Dongkyun and Shin, Yeonghun and Kim, SungJin and Kim, HyunJin and Kwon, SungMoon and Shon, Taeshik},
	month = jan,
	year = {2019},
	keywords = {Reverse Engineering, Tools, Java, Static analysis, Smart phones, APK Decompilation, Digital forensics, Servers},
	pages = {1--3},
}

@inproceedings{katz_using_2018,
	title = {Using recurrent neural networks for decompilation},
	doi = {10.1109/SANER.2018.8330222},
	abstract = {Decompilation, recovering source code from binary, is useful in many situations where it is necessary to analyze or understand software for which source code is not available. Source code is much easier for humans to read than binary code, and there are many tools available to analyze source code. Existing decompilation techniques often generate source code that is difficult for humans to understand because the generated code often does not use the coding idioms that programmers use. Differences from human-written code also reduce the effectiveness of analysis tools on the decompiled source code. To address the problem of differences between decompiled code and human-written code, we present a novel technique for decompiling binary code snippets using a model based on Recurrent Neural Networks. The model learns properties and patterns that occur in source code and uses them to produce decompilation output. We train and evaluate our technique on snippets of binary machine code compiled from C source code. The general approach we outline in this paper is not language-specific and requires little or no domain knowledge of a language and its properties or how a compiler operates, making the approach easily extensible to new languages and constructs. Furthermore, the technique can be extended and applied in situations to which traditional decompilers are not targeted, such as for decompilation of isolated binary snippets; fast, on-demand decompilation; domain-specific learned decompilation; optimizing for readability of decompilation; and recovering control flow constructs, comments, and variable or function names. We show that the translations produced by this technique are often accurate or close and can provide a useful picture of the snippet's behavior.},
	booktitle = {2018 {IEEE} 25th {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Katz, Deborah S. and Ruchti, Jason and Schulte, Eric},
	month = mar,
	year = {2018},
	keywords = {Binary codes, Data models, Recurrent neural networks, Tools, Training, decompilation, Decoding, deep learning, Natural languages, recurrent neural networks, translation},
	pages = {346--356},
}

@inproceedings{harrand_strengths_2019,
	title = {The {Strengths} and {Behavioral} {Quirks} of {Java} {Bytecode} {Decompilers}},
	issn = {2470-6892},
	doi = {10.1109/SCAM.2019.00019},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84\% of classes of our dataset and semantically equivalent code output for 78\% of classes.},
	booktitle = {2019 19th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit},
	month = sep,
	year = {2019},
	keywords = {Program processors, Semantics, Distortion, decompilation, Measurement, Java, Java bytecode, reverse engineering, source code analysis, Syntactics, Uniform resource locators},
	pages = {92--102},
}

@inproceedings{wiedemeier_pylingual_2025,
	title = {{PyLingual}: {Toward} {Perfect} {Decompilation} of {Evolving} {High}-{Level} {Languages}},
	issn = {2375-1207},
	doi = {10.1109/SP61157.2025.00052},
	abstract = {Python is one of the most popular programming languages among both industry developers and malware authors. Despite demand for Python decompilers, community efforts to maintain automatic Python decompilation tools have been hindered by Python's aggressive language improvements and unstable bytecode specification. Every year, language features are added, code generation undergoes significant changes, and opcodes are added, deleted, and modified. Our research aims to integrate Natural Language Processing (NLP) techniques with classical Programming Language (PL) theory to create a Python decompiler that accomodates evolving language features and changes to the bytecode specification with minimal human maintenance effort. PyLINGUAL plugs in data-driven NLP components to a version-agnostic core to automatically absorb superficial bytecode and compiler changes, while leveraging programmatic components for abstract control flow reconstruction. To establish trust in the decompilation results, we introduce a stringent correctness measure based on “perfect decompilation”, a statically verifiable refinement of semantic equivalence. We demonstrate the efficacy of our approach with extensive real-world datasets of benign and malicious Python source code and their corresponding compiled PYC binaries. Our research makes three major contributions: (1) we present PyLINGUAL, a scalable, data-driven decompilation framework with state-of-the-art support for Python versions 3.6 through 3.12, improving the perfect decompilation rate by an average of 45\% over the best results of existing decompiler across four datasets; (2) we provide a Python decompiler evaluation framework that verifies decompilation results with perfect decompilation; and (3) we launch PyLINGUAL as a public online service.},
	booktitle = {2025 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Wiedemeier, Josh and Tarbet, Elliot and Zheng, Max and Ko, Sangsoo and Ouyang, Jessica and Cha, Sang Kil and Jee, Kangkook},
	month = may,
	year = {2025},
	keywords = {Codes, Security, Source coding, Program processors, Reverse engineering, Semantics, Translation, reverse engineering, Syntactics, Natural language processing, decompiler, nlp, python, Python},
	pages = {2976--2994},
}

@inproceedings{han_queryx_2023,
	title = {{QueryX}: {Symbolic} {Query} on {Decompiled} {Code} for {Finding} {Bugs} in {COTS} {Binaries}},
	issn = {2375-1207},
	doi = {10.1109/SP46215.2023.10179314},
	abstract = {Extensible static checking tools, such as Sys and CodeQL, have successfully discovered bugs in source code. These tools allow analysts to write application-specific rules, referred to as queries. These queries can leverage the domain knowledge of analysts, thereby making the analysis more accurate and scalable. However, the majority of these tools are inapplicable to binary-only analysis. One exception, joern, translates a binary code into decompiled code and feeds the decompiled code into an ordinary C code analyzer. However, this approach is not sufficiently precise for symbolic analysis, as it overlooks the unique characteristics of decompiled code. While binary analysis platforms, such as angr, support symbolic analysis, analysts must understand their intermediate representations (IRs) although they are mostly working with decompiled code.In this paper, we propose a precise and scalable symbolic analysis called fearless symbolic analysis that uses intuitive queries for binary code and implement this in QueryX. To make the query intuitive, QueryX enables analysts to write queries on top of decompiled code instead of IRs. In particular, QueryX supports callbacks on decompiled code, using which analysts can control symbolic analysis to discover bugs in the code. For precise analysis, we lift decompiled code into our IR named DNR and perform symbolic analysis on DNR while considering the characteristics of the decompiled code. Notably, DNR is only used internally such that it allows analysts to write queries regardless of using DNR. For scalability, QueryX automatically reduces control-flow graphs using callbacks and ordering dependencies between callbacks that are specified in the queries. We applied QueryX to the Windows kernel, the Windows system service, and an automotive binary. As a result, we found 15 unique bugs including 10 CVEs and earned \$180,000 from the Microsoft bug bounty program.},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Han, HyungSeok and Kyea, JeongOh and Jin, Yonghwi and Kang, Jinoh and Pak, Brian and Yun, Insu},
	month = may,
	year = {2023},
	keywords = {Binary codes, Codes, Computer bugs, Privacy, Scalability, Security, Source coding},
	pages = {3279--3295},
}

@inproceedings{ahad_pyfet_2023,
	title = {Pyfet: {Forensically} {Equivalent} {Transformation} for {Python} {Binary} {Decompilation}},
	issn = {2375-1207},
	doi = {10.1109/SP46215.2023.10179370},
	abstract = {Decompilation is a crucial capability in forensic analysis, facilitating analysis of unknown binaries. The recent rise of Python malware has brought attention to Python decompilers that aim to obtain source code representation from a Python binary. However, Python decompilers fail to handle various binaries, limiting their capabilities in forensic analysis.This paper proposes a novel solution that transforms a decompilation error-inducing Python binary into a decompilable binary. Our key intuition is that we can resolve the decompilation errors by transforming error-inducing code blocks in the input binary into another form. The core of our approach is the concept of Forensically Equivalent Transformation (FET) which allows non-semantic preserving transformation in the context of forensic analysis. We carefully define the FETs to minimize their undesirable consequences while fixing various error-inducing instructions that are difficult to solve when preserving the exact semantics. We evaluate the prototype of our approach with 17,117 real-world Python malware samples causing decompilation errors in five popular decompilers. It successfully identifies and fixes 77,022 errors. Our approach also handles anti-analysis techniques, including opcode remapping, and helps migrate Python 3.9 binaries to 3.8 binaries.},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Ahad, Ali and Jung, Chijung and Askar, Ammar and Kim, Doowon and Kim, Taesoo and Kwon, Yonghwi},
	month = may,
	year = {2023},
	keywords = {Privacy, Source coding, Decompilation, Semantics, Prototypes, Binary-Transformation, Field effect transistors, Forensics, Limiting, Python-Malware, Reverse-Engineering},
	pages = {3296--3313},
}

@article{sateanpattanakul_java_2022,
	title = {Java {Bytecode} {Control} {Flow} {Classification}: {Framework} for {Guiding} {Java} {Decompilation}},
	volume = {18},
	issn = {1550-4654},
	doi = {10.13052/jmm1550-4646.1822},
	abstract = {Decompilation is the main process of software development, which is very important when a program tries to retrieve lost source codes. Although decompiling Java bytecode is easier than bytecode, many Java decompilers cannot recover originally lost sources, especially the selection statement, i.e., if statement. This deficiency affects directly decompilation performance. In this paper, we propose the methodology for guiding Java decompiler to deal with the aforementioned problem. In the framework, Java bytecode is transformed into two kinds of features called frame feature and latent semantic feature. The former is extracted directly from the bytecode. The latter is achieved by two-step transforming the Java bytecode to bigram and then term frequency-inverse document frequency (TFIDF). After that, both of them are fed to the genetic algorithm to reduce their dimensions. The proposed feature is achieved by converting the selected TFIDF to a latent semantic feature and concatenating it with the selected frame feature. Finally, KNN is used to classify the proposed feature. The experimental results show that the decompilation accuracy is 93.68 percent, which is obviously better than Java Decompiler.},
	number = {2},
	journal = {Journal of Mobile Multimedia},
	author = {Sateanpattanakul, Siwadol and Jetpipattanapong, Duangpen and Mathulaprangsan, Seksan},
	month = mar,
	year = {2022},
	keywords = {Source coding, Decompilation, Semantics, Accuracy, Feature extraction, Nearest neighbor methods, Software development management, Java, feature selection, genetic algorithm, Genetic algorithms, Indexing, latent semantic indexing},
	pages = {179--202},
}

@inproceedings{sharikov_methodology_2025,
	title = {Methodology for {Embedding} a {Digital} {Watermark} in {Java} {Application} {Class} {Files} {Resistant} to {Decompilation} {Attacks} {Aimed} at {Its} {Destruction}},
	doi = {10.1109/SmartIndustryCon65166.2025.10986108},
	abstract = {This paper examines the development of a methodology for embedding a digital watermark in class files by replacing bytecode opcodes and adding certain Java programming language constructs that hinder decompilation. The possible actions of an intruder are considered, along with the methods by which an attacker might obtain a class file or a Java application. An analysis of existing decompilers is conducted, with examples of programming language constructs that lead to decompilation errors or incorrect decompilation results. The principle of the methodology is demonstrated, and experiments are conducted to confirm that after embedding the digital watermark using the developed approach, it remains resilient to decompilation attacks aimed at its removal. The compiled class file containing the modified bytecode is analyzed. It is proven that the logic of the Java application remains unchanged after editing the bytecode of the class file and recompiling it. Certain constructs and code design patterns are examined, demonstrating that specific constructs in the source code increases the likelihood of incorrect decompilation of Java application class files, the inability to recompile the decompiled code.},
	booktitle = {2025 {International} {Russian} {Smart} {Industry} {Conference} ({SmartIndustryCon})},
	author = {Sharikov, Pavel and Krasov, Andrey and Chechulin, Andrey},
	month = mar,
	year = {2025},
	keywords = {Codes, Source coding, Software, Java, obfuscation, bytecode, Logic, Resilience, class file, Computer languages, decompilation attack, digital watermark, Resistance, Usability, Watermarking},
	pages = {906--911},
}

@inproceedings{lacomis_dire_2019,
	title = {{DIRE}: {A} {Neural} {Approach} to {Decompiled} {Identifier} {Naming}},
	issn = {2643-1572},
	doi = {10.1109/ASE.2019.00064},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = jan,
	year = {2019},
	keywords = {Software, Decompilation, Reverse engineering, Analytical models, Deep learning, Recurrent neural networks, Tools, Training},
	pages = {628--639},
}

@inproceedings{al-kaswan_extending_2023,
	title = {Extending {Source} {Code} {Pre}-{Trained} {Language} {Models} to {Summarise} {Decompiled} {Binaries}},
	issn = {2640-7574},
	doi = {10.1109/SANER56733.2023.00033},
	abstract = {Binary reverse engineering is used to understand and analyse programs for which the source code is unavailable. Decompilers can help, transforming opaque binaries into a more readable source code-like representation. Still, reverse engineering is difficult and costly, involving considering effort in labelling code with helpful summaries. While the automated summarisation of decompiled code can help reverse engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise de-compiled binary functions. Further-more, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by removing identifiers, and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82 and, 44.21 for summarising source, decompiled, and obfuscated decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie},
	month = mar,
	year = {2023},
	keywords = {Binary codes, Source coding, Optimization, Software, Task analysis, Binary, CodeT5, Data models, Decompilation, Deep Learning, Labeling, Pre-trained Language Models, Reverse engineering, Reverse Engineering, Summarization, Synthetic data, Transformers},
	pages = {260--271},
}

@inproceedings{alsabbagh_control_2021,
	title = {A {Control} {Injection} {Attack} against {S7} {PLCs} -{Manipulating} the {Decompiled} {Code}},
	issn = {2577-1647},
	doi = {10.1109/IECON48115.2021.9589721},
	abstract = {In this paper, we discuss an approach which allows an attacker to modify the control logic program that runs in S7 PLCs in its high-level decompiled format. Our full attack-chain compromises the security measures of PLCs, retrieves the machine bytecode of the target device, and employs a decompiler to convert the stolen compiled bytecode (low-level) to its decompiled version (high-level) e.g. Ladder Diagram LAD. As the LAD code exposes the structure and semantics of the control logic, our attack also manipulates the LAD code based on the attacker’s understanding to the physical process causing abnormal behaviors of the system that we target. Finally, it converts the infected LAD code to its executable version i.e. machine bytecode that can run on the PLC using a compiler before pushing the malicious code back to the PLC. For a real scenario, we implemented our full attack-chain on a small industrial setting using real S7-300 PLCs, and built the database (for our decompiler and compiler) using 108 different control logic programs of varying complexity, ranging from simple programs consisting of a few instructions to more complex ones including multi functions, sub-functions and data blocks. We tested and evaluated the accuracy of our decompiler and compiler on 5 random programs written for real industrial applications. Our experimental results showed that an external adversary is able to infect S7 PLCs successfully. We eventually suggest some potential mitigation approaches to secure systems against such a threat.},
	booktitle = {{IECON} 2021 – 47th {Annual} {Conference} of the {IEEE} {Industrial} {Electronics} {Society}},
	author = {Alsabbagh, Wael and Langendörfer, Peter},
	month = oct,
	year = {2021},
	keywords = {Codes, Program processors, Compiler, Control Injection Attack, Decompiler, Electric potential, Industrial electronics, Ladder Diagram, Process control, Programmable Logic Controllers (PLCs), Programmable logic devices, Semantics},
	pages = {1--8},
}

@article{liao_augmenting_2025,
	title = {Augmenting {Smart} {Contract} {Decompiler} {Output} {Through} {Fine}-{Grained} {Dependency} {Analysis} and {LLM}-{Facilitated} {Semantic} {Recovery}},
	volume = {51},
	issn = {1939-3520},
	doi = {10.1109/TSE.2025.3623325},
	abstract = {Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate function identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA’s accuracy in control and data flow analysis and LLM’s capability in semantic prediction. More specifically, SmartHalo constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract functions shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o mini with SmartHalo further enhances its performance, achieving a precision of 91.32\% and a recall of 87.38\% for function boundaries, a precision of 90.40\% and a recall of 88.82\% for variable types, and a precision of 80.66\% and a recall of 91.78\% for contract attributes.},
	number = {12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Liao, Zeqin and Nan, Yuhong and Gao, Zixu and Liang, Henglong and Hao, Sicheng and Ren, Peifan and Zheng, Zibin},
	month = feb,
	year = {2025},
	keywords = {Codes, Source coding, Optimization, Training, Semantics, Accuracy, decompilation, large language model, Large language models, Static analysis, Annotations, static analysis, Smart contracts, Smart contract},
	pages = {3574--3590},
	annote = {This is the author version of the article accepted for publication in IEEE Transactions on Software Engineering},
}

@inproceedings{she_wadec_2024-1,
	title = {{WaDec}: {Decompiling} {WebAssembly} {Using} {Large} {Language} {Model}},
	issn = {2643-1572},
	abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm’s binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art’s 116.94\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%. In summary, WaDec enhances understanding of the code’s structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},
	booktitle = {2024 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
	month = oct,
	year = {2024},
	keywords = {Codes, Security, Source coding, Optimization, Training, decompilation, binary, Faces, finetune, large language model, Large language models, llm, Measurement, readability, Self-supervised learning, Software engineering, wasm, webassembly},
	pages = {481--492},
}

@article{park_static_2023,
	title = {Static {Analysis} of {JNI} {Programs} via {Binary} {Decompilation}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2023.3241639},
	abstract = {JNI programs are widely used thanks to the combined benefits of C and Java programs. However, because understanding the interaction behaviors between two different programming languages is challenging, JNI program development is difficult to get right and vulnerable to security attacks. Thus, researchers have proposed static analysis of JNI program source code to detect bugs and security vulnerabilities in JNI programs. Unfortunately, such source code analysis is not applicable to compiled JNI programs that are not open-sourced or open-source JNI programs containing third-party binary libraries. While JN-SAF, the state-of-the-art analyzer for compiled JNI programs, can analyze binary code, it has several limitations due to its symbolic execution and summary-based bottom-up analysis. In this paper, we propose a novel approach to statically analyze compiled JNI programs without their source code using binary decompilation. Unlike JN-SAF that analyzes binaries directly, our approach decompiles binaries and analyzes JNI programs with the decompiled binaries using an existing JNI program analyzer for source code. To decompile binaries to compilable C source code with precise JNI-interoperation-related types, we improve an existing decompilation tool by leveraging the characteristics of JNI programs. Our evaluation shows that the approach is precise as almost the same as the state-of-the-art JNI program analyzer for source code, and more precise than JN-SAF.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Park, Jihee and Lee, Sungho and Hong, Jaemin and Ryu, Sukyoung},
	month = may,
	year = {2023},
	keywords = {Codes, Security, Source coding, Java, Static analysis, Computer architecture, static analysis, Libraries, binary decompilation, Java native interface},
	pages = {3089--3105},
}

@inproceedings{yang_human_2025,
	title = {A {Human} {Study} of {Automatically} {Generated} {Decompiler} {Annotations}},
	issn = {2158-3927},
	doi = {10.1109/DSN64029.2025.00026},
	abstract = {Reverse engineering is a crucial technique in software security, enabling professionals to analyze malware, identify vulnerabilities, and patch legacy software without access to source code. Although decompilers attempt to reconstruct high-level code from binaries, essential information, such as variable names and types, is often dissimilar from the original version, hindering readability and comprehension.Recent advancements have employed AI to enhance decompiler output by recovering original variable names and types. Traditional evaluation of recovery techniques relies on measuring similarity between original and recovered names, assuming that higher similarity enhances readability. However, studies suggest that these "intrinsic" metrics may not accurately predict "extrinsic" outcomes like user comprehension or task performance, revealing a gap in understanding readability and cognitive load in reverse engineering.This paper presents an extrinsic evaluation of machine-generated variable and type names, focusing on their impact on reverse engineers’ comprehension of decompiled code. We conducted a user study with 40 participants—including students and professionals—to assess code comprehension both with and without AI-generated variable and type name assistance. Our findings indicate a lack of correlation between traditional machine learning metrics and actual comprehension gains, highlighting limitations in current evaluation techniques. Despite this, participants showed a preference for AI-augmented decompiler outputs. These insights contribute to understanding the effectiveness of automatic recovery techniques in enhancing reverse engineering tasks and underscore the need for comprehensive, user-centered evaluation frameworks.},
	booktitle = {2025 55th {Annual} {IEEE}/{IFIP} {International} {Conference} on {Dependable} {Systems} and {Networks} ({DSN})},
	author = {Yang, Yuwei and Grandel, Skyler and Lacomis, Jeremy and Schwartz, Edward and Vasilescu, Bogdan and Le Goues, Claire and Leach, Kevin},
	month = jun,
	year = {2025},
	keywords = {Codes, Security, Source coding, Decompilation, Reverse engineering, Malware, Measurement, Machine learning, Binary Reverse Engineering, Cognitive load, Correlation, Focusing, Human Study},
	pages = {129--142},
}

@inproceedings{zhang_optimizing_2024,
	title = {Optimizing {Decompiler} {Output} by {Eliminating} {Redundant} {Data} {Flow} in {Self}-{Recursive} {Inlining}},
	issn = {2576-3148},
	doi = {10.1109/ICSME58944.2024.00015},
	abstract = {Decompilation, which aims to lift a binary to a high-level language such as C, is one of the most common approaches software security analysts use for analyzing binary code. Recovering decompiled code with high readability is essential, as humans must understand the code's functionality correctly. However, some compilation optimization strategies will introduce obfuscation into the binary code, thereby reducing the readability of decompiled code. Among them, the function inlining related optimization strategies combine functions, causing the original function's code volume and complexity to multiply. Especially with self-recursive inlining optimization, it transforms initially simple functions into ones with significantly increased code volume and complex logic, greatly hindering the understanding of security engineers. In this paper, we present Erase, the first approach to reverse the self-recursive inlining optimization technique. We compare Erase with state-of-the-art decompilers Ghidra and Hex-Rays to evaluate ERASE's improvement for the functions affected by self-recursive inlining. Experimental results show that Erase's output is 78.4\% and 88.9\% more compact (fewer lines of code) than Ghidra and Hex-Rays, respectively. Moreover, reverse engineers spend 88.5\% less time analyzing ERASE's output than analyzing Ghidra and 90.4\% less time than analyzing Hex-Rays, and the accuracy of analyzing Erase's output is 2.75 times higher than both Ghidra and Hex-Rays.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Zhang, Runze and Cao, Ying and Liang, Ruigang and Hu, Peiwei and Chen, Kai},
	month = oct,
	year = {2024},
	keywords = {Binary codes, Security, Optimization, Decompilation, Reverse Engineering, Accuracy, Compiler Optimization, Complexity theory, High level languages, Logic, Program Analysis, Self-Recursive Inlining, Software maintenance, Systematics, Transforms},
	pages = {38--49},
}

@inproceedings{armengol-estape_slade_2024-1,
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	issn = {2643-2838},
	doi = {10.1109/CGO57630.2024.10444788},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	booktitle = {2024 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F.P.},
	month = mar,
	year = {2024},
	keywords = {Codes, Security, Optimization, Task analysis, Transformers, Standards, decompilation, Engines, neural decompilation, language models, type inference, Transformer},
	pages = {67--80},
}

@inproceedings{pal_len_2024,
	title = {"{Len} or index or count, anything but v1": {Predicting} {Variable} {Names} in {Decompilation} {Output} with {Transfer} {Learning}},
	issn = {2375-1207},
	doi = {10.1109/SP54263.2024.00152},
	abstract = {Binary reverse engineering is an arduous and tedious task performed by skilled and expensive human analysts. Information about the source code is irrevocably lost in the compilation process. While modern decompilers attempt to generate C-style source code from a binary, they cannot recover lost variable names. Prior works have explored machine learning techniques for predicting variable names in decompiled code. However, the state-of-the-art systems, DIRE and DIRTY, generalize poorly to functions in the testing set that are not included in the training set—31.8\% for DIRE on DIRTY’s data set and 36.9\% for DIRTY on DIRTY’s data set.In this paper, we present VarBERT, a Bidirectional Encoder Representations from Transformers (BERT) to predict meaningful variable names in decompilation output. An advantage of VarBERT is that we can pre-train on human source code and then fine-tune the model to the task of predicting variable names. We also create a new data set VarCorpus, which significantly expands the size and variety of the data set. Our evaluation of VarBERT on VarCorpus, demonstrates a significant improvement in predicting the developer’s original variable names for O2 optimized binaries achieving accuracies of 54.43\% for IDA and 54.49\% for Ghidra. VarBERT is strictly better than state-of-the-art techniques: On a subset of VarCorpus, VarBERT could predict the developer’s original variable names 50.70\% of the time, while DIRE and DIRTY predicted original variable names 35.94\% and 38.00\% of the time, respectively.},
	booktitle = {2024 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Pal, Kuntal Kumar and Bajaj, Ati Priya and Banerjee, Pratyay and Dutcher, Audrey and Nakamura, Mutsumi and Basque, Zion Leonahenahe and Gupta, Himanshu and Sawant, Saurabh Arjun and Anantheswaran, Ujjwala and Shoshitaishvili, Yan and Doupé, Adam and Baral, Chitta and Wang, Ruoyu},
	month = may,
	year = {2024},
	keywords = {Codes, Privacy, Source coding, Decompilation, Reverse engineering, Transformers, Training, Machine learning and computer security, Program and binary analysis, Transfer learning},
	pages = {4069--4087},
}

@article{reiter_automatically_2025,
	title = {Automatically {Mitigating} {Vulnerabilities} in {Binary} {Programs} via {Partially} {Recompilable} {Decompilation}},
	volume = {22},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2024.3482413},
	abstract = {Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD) to help automate the process. PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically succeed on an entire binary, our approach does because it is limited to a few functions, such as those identified by our binary fault localization. We evaluate the assumptions underlying our approach and find that, without any grammar or compilation restrictions, up to 79\% of individual functions are successfully decompiled and recompiled. In comparison, only 1.7\% of the full C-binaries succeed. When recompilation succeeds, PRD produces test-equivalent binaries 93.0\% of the time. We evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; and human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, perform as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with the same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, the evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries.},
	number = {3},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Reiter, Pemma and Tay, Hui Jun and Weimer, Westley and Doupé, Adam and Wang, Ruoyu and Forrest, Stephanie},
	month = may,
	year = {2025},
	keywords = {Codes, Computer bugs, Source coding, Software, Prototypes, Measurement, Software engineering, C++ languages, Grammar, Location awareness, Maintenance engineering, software maintenance, C plus plus languages},
	pages = {2270--2282},
}

@article{sang_control_2024,
	title = {From {Control} {Application} to {Control} {Logic}: {PLC} {Decompile} {Framework} for {Industrial} {Control} {System}},
	volume = {19},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2024.3402117},
	abstract = {Industrial Control System (ICS) depends on the underlying Programmable Logical Controllers (PLCs) to run. As such, the security of the internal control logic of the PLCs is the top concern of ICS. Reversing analysis and forensic work against PLC require extracting control logic from the control application running inside PLC, which is still an unresolved problem. To address the challenge, we propose a PLC decompile framework named CLEVER, which can analyze the control application and extract the control logic. First, we propose a simulation execution based code extraction method, which is utilized to filter the control logic related data. Then, to normalize the control application decompile process, an intermediate representation (IR) is designed, which can simplify the analysis process and enhance the extensibility of CLEVER. Finally, a heuristic data flow analysis algorithm is proposed to find variable dependency, and a sequential parsing method is utilized to reconstruct the source code from the control application. To evaluate our work, real world PLC hardware and programming software are used for the experiment. We use 22 real-world, 58 hand-written, and 150 auto-generated control applications to demonstrate the usability, correctness, and operational efficiency of CLEVER.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Sang, Chao and Wu, Jun and Li, Jianhua and Guizani, Mohsen},
	year = {2024},
	keywords = {Codes, Software, Reverse engineering, Process control, Feature extraction, program analysis, Data mining, industrial control system, Assembly, network forensics, PLC, Registers},
	pages = {8685--8700},
}

@inproceedings{korencik_symbolic_2020,
	title = {On {Symbolic} {Execution} of {Decompiled} {Programs}},
	doi = {10.1109/QRS51102.2020.00044},
	abstract = {In this paper, we present a combination of existing and new tools that together make it possible to apply formal verification methods to programs in the form of ×86\_64 machine code. Our approach first uses a decompilation tool (remill) to extract low-level intermediate representation (LLVM) from the machine code. This step consists of instruction translation (i.e. recovery of operation semantics), control flow extraction and address identification.The main contribution of this paper is the second step, which builds on data flow analysis and refinement of indirect (i.e. data-dependent) control flow. This step makes the processed bitcode much more amenable to formal analysis.To demonstrate the viability of our approach, we have compiled a set of benchmark programs into native executables and analysed them using two LLVM-based tools: DIVINE, a software model checker and KLEE, a symbolic execution engine. We have compared the outcomes to direct analysis of the same programs.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Korenčik, Lukáš and Ročkai, Petr and Lauko, Henrich and Barnat, Jiří},
	month = feb,
	year = {2020},
	keywords = {Security, Tools, Semantics, decompilation, binary analysis, Engines, Formal verification, llvm, program analysis, Software quality, Software reliability, symbolic execution},
	pages = {265--272},
}

@inproceedings{magin_heros_2025,
	title = {Heros in {Action}: {Analyzing} {Objective}-{C} {Binaries} through {Decompilation} and {IFDS}},
	doi = {10.1109/STATIC66697.2025.00005},
	abstract = {This paper demonstrates static taint analysis on Objective-C binaries through the integration of the Ghidra framework with the Heros IFDS solver, achieving an inter-procedural, field-sensitive and flow-sensitive analysis. Our contributions include two plugins: one extending Ghidra Objective-C capabilities to improve decompilation accuracy, and another integrating the Heros framework for inter-procedural taint analysis on Ghidra Intermediate Representation (IR).To assess our approach, we introduce a new benchmark suite tailored to Objective-C, covering diverse dataflow challenges and promoting further community-driven research. By leveraging existing frameworks, this work demonstrates how established static analysis techniques can be adapted to binary targets, laying a groundwork for advancements in Objective-C binary analysis.},
	booktitle = {2025 {IEEE}/{ACM} 1st {International} {Workshop} on {Advancing} {Static} {Analysis} for {Researchers} and {Industry} {Practitioners} in {Software} {Engineering} ({STATIC})},
	author = {Magin, Florian and Patat, Gwendal and Scherf, Fabian},
	month = apr,
	year = {2025},
	keywords = {Decompilation, Accuracy, Software engineering, Static analysis, Conferences, Benchmark testing, Heros, IFDS, Industries, Objective-C},
	pages = {1--6},
}

@inproceedings{izrailov_gremc_2024,
	title = {{GREMC}: {Genetic} {Reverse}-{Engineering} of {Machine} {Code} to {Search} {Vulnerabilities} in {Software} for {Industry} 4.0. {Predicting} the {Size} of the {Decompiling} {Source} {Code}},
	doi = {10.1109/SmartIndustryCon61328.2024.10515515},
	abstract = {The article is devoted to the problem of security of cyber-physical systems as part of production according to the Industry 4.0 concept. For this purpose, the author's approach of “Genetic Reverse Engineering of Machine Code” (GREMC) is proposed. The essence of this approach lies in the use of artificial intelligence in the field of genetic algorithms to restore the source code of software executed in the form of machine code on cyber-physical devices for Industry 4.0. The resulting code can then be analysed for vulnerabilities by an expert. One issue that arises during genetic reverse engineering is predicting the size of the source code based on its machine representation (i.e., in an object or executable file). The article is devoted to solving this problem for functions in the C programming language. To do this, a method for obtaining a relationship between the sizes of the source and machine code of individual functions is described, which consists of steps such as loading a dataset with C-functions, isolating the function code in it, preprocessing them, compiling them into machine code, calculating the required sizes, building dependencies between each source code size and the corresponding machine code sizes, generating the final table and determining the dependency formula. An experiment is carried out using a software prototype that implements the method; ExeBench with 83 thousand C-functions is taken as a dataset. Justifications are given regarding the appearance of abnormal machine code sizes and their impact on the dependence formula.},
	booktitle = {2024 {International} {Russian} {Smart} {Industry} {Conference} ({SmartIndustryCon})},
	author = {Izrailov, Konstantin},
	month = mar,
	year = {2024},
	keywords = {Codes, Source coding, Software, Reverse engineering, decompilation, Genetics, machine code, Production, Prototypes, reverse-engineering, size dependence, source code},
	pages = {622--628},
}

@inproceedings{cao_revisiting_2023,
	title = {Revisiting {Deep} {Learning} for {Variable} {Type} {Recovery}},
	issn = {2643-7171},
	doi = {10.1109/ICPC58990.2023.00042},
	abstract = {Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.},
	booktitle = {2023 {IEEE}/{ACM} 31st {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Cao, Kevin and Leach, Kevin},
	month = may,
	year = {2023},
	keywords = {Source coding, Reverse engineering, Transformers, Training, Semantics, Static analysis, Machine Learning, Computer architecture, Ghidra, Hex-Rays, Symbols},
	pages = {275--279},
}

@article{ahmed_learning_2022,
	title = {Learning to {Find} {Usages} of {Library} {Functions} in {Optimized} {Binaries}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2021.3106572},
	abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries’ behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, “natural” source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to “decompile” binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},
	month = oct,
	year = {2022},
	keywords = {Optimization, Reverse engineering, Tools, Training, Malware, deep learning, Databases, Libraries, software modeling},
	pages = {3862--3876},
	annote = {Journal reference: Transactions on Software Engineering (2021)},
}

@inproceedings{liu_sok_2022,
	title = {{SoK}: {Demystifying} {Binary} {Lifters} {Through} the {Lens} of {Downstream} {Applications}},
	issn = {2375-1207},
	doi = {10.1109/SP46214.2022.9833799},
	abstract = {Binary lifters convert executables into an intermediate representation (IR) of a compiler framework. The recovered IR code is generally deemed “analysis friendly,” bridging low-level code analysis with well-established compiler infrastructures. With years of development, binary lifters are becoming increasingly popular for use in various security, systems, and software (re)-engineering applications. Recent studies have also reported highly promising results that suggest binary lifters can generate LLVM IR code with correct functionality, even for complex cases.This paper conducts an in-depth study of binary lifters from an orthogonal and highly demanding perspective. We demystify the “expressiveness” of binary lifters, and reveal how well the lifted LLVM IR code can support critical downstream applications in security analysis scenarios. To do so, we generate two pieces of LLVM IR code by compiling C/C++ programs or by lifting the corresponding executables. We then feed these two pieces of LLVM IR code to three keystone downstream applications (pointer analysis, discriminability analysis, and decompilation) and determine whether inconsistent analysis results are generated. We study four popular static and dynamic LLVM IR lifters that were developed by the industry or academia from a total of 252,063 executables generated by various compilers and optimizations and on different architectures. Our findings show that modern binary lifters afford IR code that is highly suitable for discriminability analysis and decompilation, and suggest that such binary lifters can be applied in common similarity- or code comprehension-based security analysis (e.g., binary diffing). However, the lifted IR code appears unsuited to rigorous static analysis (e.g., pointer analysis). To obtain a more comprehensive view of the utility of binary lifters, we also compare the performance of lifter-enabled approaches with that of binary-only tools in three security tasks, i.e., sanitization, binary diffing, and C decompilation. We summarize our findings and make suggestions for the correct use and further enhancement of binary lifters. We also explored practical ways to enhance the accuracy of pointer analysis using lifted IR code, by using and augmenting Debin, a tool for predicting debug information.},
	booktitle = {2022 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Liu, Zhibo and Yuan, Yuanyuan and Wang, Shuai and Bao, Yuyan},
	month = may,
	year = {2022},
	keywords = {Codes, Privacy, Security, Software, reverse-engineering, Static analysis, Industries, Feeds, software-security},
	pages = {1100--1119},
}

@inproceedings{slawinski_applications_2019,
	title = {Applications of {Graph} {Integration} to {Function} {Comparison} and {Malware} {Classification}},
	doi = {10.1109/ICSRS48664.2019.8987703},
	abstract = {We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled. NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms. 11Code available at https://github.com/gtownrocks/grafuple},
	booktitle = {2019 4th {International} {Conference} on {System} {Reliability} and {Safety} ({ICSRS})},
	author = {Slawinski, Michael and Wortman, Andy},
	month = jan,
	year = {2019},
	keywords = {Training, Malware, decompilation, Syntactics, abstract syntax tree, Aerospace electronics, classification, Glass box, graph integration, machine learning, malware, NET, pagerank, Reliability, Robust control, Safety, Taxonomy, Vectors},
	pages = {16--24},
}

@inproceedings{smith_irene_2024,
	title = {{IRENE}: {A} {Toolchain} for {High}-level {Micropatching} through {Recompilable} {Sub}-function {Regions}},
	issn = {2155-7586},
	doi = {10.1109/MILCOM61039.2024.10773695},
	abstract = {Devices with long life-cycles extend the duration of a software product’s exposure. Vulnerabilities in the device’s software may be discovered after the toolchain for building software for that device has been deprecated, the source-code for the program that is running on the device is unavailable, or both. Current solutions for patching these latent vulnerabilities are insufficient, either requiring deep technical expertise and manual effort or producing a large set of changes to the target binary. The large number of changes in the target program makes validating the patch difficult.We present IRENE, a decompiler that produces recompilable decompilation for patching. IRENE decompiles sub-function regions of a binary program to separate recompilable regions of code. The IRENE compiler replaces the original binary region with recompiled assembly, representing the user’s patch. IRENE allows developers to compose patches by modifying a high-level representation of the target program, but produces targeted micropatches. We evaluate the size of IRENE produced micropatches against patches produced both by recompilation and manual assembly patching in a case study. This evaluation shows a patch size reduction of 4x compared with recompilation and a 21x size reduction when compared with recompiling the patch with a newer toolchain version.},
	booktitle = {{MILCOM} 2024 - 2024 {IEEE} {Military} {Communications} {Conference} ({MILCOM})},
	author = {Smith, Ian and Bertolaccini, Francesco and Tan, William and Brown, Michael D.},
	month = oct,
	year = {2024},
	keywords = {Codes, Software, binary analysis, Manuals, Buildings, Assembly, binary patching, binary rewriting, Military communication, recompilation},
	pages = {1064--1069},
}

@inproceedings{ahmad_ali_qureshi_apk_2024,
	title = {{APK} {Insight}: {Revolutionizing} {Forensic} {Analysis} with a {User}-{Friendly} {Approach}},
	doi = {10.1109/ICECT61618.2024.10581196},
	abstract = {This research paper introduces a novel forensic tool named ‘A2forensicsKit’, developed for non-technical users to proficiently analyze and decompile Android Application Package (APK) files [7]. With the increasing prevalence of mobile applications, the need for accessible forensic solutions has become imperative. Our tool focuses on simplifying the complex process of APK analysis, enabling investigators, law enforcement, and other non-technical users to extract valuable insights from mobile applications. The tool employs an intuitive user interface, providing a seamless experience for users with limited technical expertise. Through a combination of automated processes and user-friendly features, our tool allows for the swift identification of malicious code, potential vulnerabilities, and unauthorized activities within APK files. Its effectiveness lies in its ability to bridge the gap between intricate forensic procedures and the diverse user base involved in digital investigations. Key functionalities include static analysis and decompilationof APK files, all seamlessly integrated into a unified platform. The tool's efficiency is underscored by its capacity to generate comprehensive reports, helping investigators in presenting findings in a clear and understandable manner. In conclusion, our research presents a user-friendly forensic tool that empowers non-technical users to conduct thorough analysis and decompilation of APK files. By addressing the accessibility gap in digital forensics, this tool contributes to enhancing the efficiency and inclusivity of mobile application investigations.},
	booktitle = {2024 {International} {Conference} on {Engineering} \& {Computing} {Technologies} ({ICECT})},
	author = {Ahmad Ali Qureshi, Muhammad and Munawar Gill, Arsham and Sadaf, Memoona},
	month = may,
	year = {2024},
	keywords = {Training, Static analysis, Transforms, Operating systems, Cybersecurity, Digital forensics, Android Application Package, APK Analysis, Digital Forensics, Forensic Tools, Law enforcement, Mobile Application Security, Technological innovation},
	pages = {1--6},
}

@inproceedings{enders_jump-table-agnostic_2025,
	title = {A {Jump}-{Table}-{Agnostic} {Switch} {Recovery} on {ASTs}},
	issn = {2576-3148},
	doi = {10.1109/ICSME64153.2025.00028},
	abstract = {Recovering high-level control-flow structures is a crucial part of modern reverse engineering, especially in fields like binary analysis. Here, analysts often use decompilers to convert functions of binary programs into a more humanreadable C -like representation. Among these control-flow structures, switch statements have unique significance because of their ability to represent complex decision-making and branching behavior in a concise and readable manner. Consequently, the successful recovery of switch statements during decompilation can greatly enhance the readability of the resulting output, making it a highly desired goal in the field of reverse engineering. In this paper, we present a new technique for identifying abstract syntax tree components that can be transformed into semantically equivalent switches, thus improving code readability. In contrast to other approaches, we do not rely on jump tables that have or have not been emitted during compilation. Instead, we identify clusters of comparisons involving the same expression but with varying constant values within the abstract syntax tree to be transformed into switch constructs. Because this approach is inherently linked to the semantic definition of a switch statements, it only generates meaningful switches by design. We evaluated our approach on the coreutils-9.3 dataset and compared it to the leading decompilers Ghidra and Hex-Rays, both of which attempt to recover switch statements as well. Our evaluation results indicate that our approach outperforms both Ghidra and Hex-Rays by successfully recovering more than twice as many switch constructs in the given dataset.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	author = {Enders, Steffen and Behner, Eva-Maria C. and Padilla, Elmar},
	month = sep,
	year = {2025},
	keywords = {Source coding, Reverse engineering, Semantics, decompilation, binary analysis, reverse engineering, Syntactics, Complexity theory, Software maintenance, Transforms, Control systems, control-flow structuring, Decision making, jump tables, switch recovery, Switches},
	pages = {1--12},
}

@inproceedings{li_stan_2020,
	title = {{STAN}: {Towards} {Describing} {Bytecodes} of {Smart} {Contract}},
	doi = {10.1109/QRS51102.2020.00045},
	abstract = {More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1\% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named Stan to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, Stan can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that Stan can generate adequate, accurate and readable descriptions for contract’s bytecodes, which have practical value for users.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Li, Xiaoqi and Chen, Ting and Luo, Xiapu and Zhang, Tao and Yu, Le and Xu, Zhou},
	month = feb,
	year = {2020},
	keywords = {Security, Tools, Semantics, Software quality, Software reliability, Natural language processing, Smart contracts, Smart contract, Ethereum, Program comprehension},
	pages = {273--284},
}

@inproceedings{bodei_language-independent_2018,
	title = {Language-{Independent} {Synthesis} of {Firewall} {Policies}},
	doi = {10.1109/EuroSP.2018.00015},
	abstract = {Configuring and maintaining a firewall configuration is notoriously hard. Policies are written in low-level, platform-specific languages where firewall rules are inspected and enforced along non trivial control flow paths. Further difficulties arise from Network Address Translation (NAT), since filters must be implemented with addresses translations in mind. In this work, we study the problem of decompiling a real firewall configuration into an abstract specification. This abstract version throws the low-level details away by exposing the meaning of the configuration, i.e., the allowed connections with possible address translations. The generated specification makes it easier for system administrators to check if: (i) the intended security policy is actually implemented; (ii) two configurations are equivalent; (iii) updates have the desired effect on the firewall behavior. The peculiarity of our approach is that is independent of the specific target firewall system and language. This independence is obtained through a generic intermediate language that provides the typical features of real configuration languages and that separates the specification of the rulesets, determining the destiny of packets, from the specification of the platform-dependent steps needed to elaborate packets. We present a tool that decompiles real firewall configurations from different systems into this intermediate language and uses the Z3 solver to synthesize the abstract specification that succinctly represents the firewall behavior and the NAT. Tests on real configurations show that the tool is effective: it synthesizes complex policies in a matter of minutes and, and it answers to specific queries in just a few seconds. The tool can also point out policy differences before and after configuration updates in a simple, tabular form.},
	booktitle = {2018 {IEEE} {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Bodei, Chiara and Degano, Pierpaolo and Galletta, Letterio and Focardi, Riccardo and Tempesta, Mauro and Veronese, Lorenzo},
	month = apr,
	year = {2018},
	keywords = {Tools, Standards, Operating systems, Firewall configuration, Firewalls (computing), Network address translation, Network Security, Policy Synthesis, Proposals},
	pages = {92--106},
}

@inproceedings{zubair_control_2022,
	title = {Control {Logic} {Obfuscation} {Attack} in {Industrial} {Control} {Systems}},
	doi = {10.1109/CSR54599.2022.9850326},
	abstract = {Industrial control systems (ICS) are essential for safe and efficient operations of critical infrastructures such as power grids, pipelines, and water treatment facilities. Attackers target ICS, mainly programmable logic controllers (PLC), to sabotage underlying infrastructure. A PLC controls a physical process through connected sensors and actuators. It runs a control-logic program that specifies monitoring and controlling a physical process and is a common target of cyberattacks. A vendor-provided proprietary engineering software is typically used to investigate the infected control logic. This paper shows that an attacker can use control-logic obfuscation as an anti-forensics technique to hinder the investigations and incident response. The control-logic obfuscation subverts the engineering software’s decompilation function; therefore, we call it a denial-of-decompilation attack. The attack exploits a fundamental design principle of creating compiled control logic in engineering software, thereby affecting the engineering software of multiple vendors in the industry.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cyber} {Security} and {Resilience} ({CSR})},
	author = {Zubair, Nauman and Ayub, Adeen and Yoo, Hyunguk and Ahmed, Irfan},
	month = jul,
	year = {2022},
	keywords = {Software, Process control, Programmable logic devices, Pipelines, Control systems, Control-logic attacks, digital forensics, industrial control system (ICS), Integrated circuits, Power grids, programmable logic controller (PLC)},
	pages = {227--232},
}

@inproceedings{semenov_obfuscated_2019,
	title = {Obfuscated {Code} {Quality} {Measurement}},
	doi = {10.1109/MMA.2019.8936022},
	abstract = {Nowadays, number of cyber-attacks aimed at software increases. Thus, improving the quality of software security services becomes acute. Confidentiality is one of the basic security services. It is provided, including through the use of obfuscation mechanism. Code quality measurement Methods has been investigated. Particular attention is paid to methods aimed at decompiled code, where standard metrics do not take in account. The method of decompiled code static analysis has been improved. This method takes into account the features of the compile procedure for languages with intermediate code.},
	booktitle = {2019 {XXIX} {International} {Scientific} {Symposium} "{Metrology} and {Metrology} {Assurance}" ({MMA})},
	author = {Semenov, Serhii and Davydov, Viacheslav and Voloshyn, Denys},
	month = sep,
	year = {2019},
	keywords = {Codes, Security, Software, Standards, Atmospheric measurements, code quality, Cyberattack, immediate code, Metrology, obfuscation, Particle measurements, security service, Static analysis},
	pages = {1--6},
}

@inproceedings{enders_pidarci_2021,
	title = {{PIdARCI}: {Using} {Assembly} {Instruction} {Patterns} to {Identify}, {Annotate}, and {Revert} {Compiler} {Idioms}},
	doi = {10.1109/PST52912.2021.9647781},
	abstract = {Analysis of binary code is a building block of computer security. Especially in malware or firmware analysis where source code oftentimes is not available, techniques like decompilation are utilized to Figure out the functionality of binaries. During the optimization phase in modern compilers, human-readable expressions are often transformed into instruction sequences (compiler idioms or idioms) that may be more efficient in terms of speed or size than the direct translation. However, these transformations are often considerably worse in terms of readability for the analyst. Such compiler specific sequences are not only significantly longer than the apparent translation of the original high-level language operation but also have no trivial correlation to the original expression’s semantics. Modern decompilers address this issue by reverting idioms using static, manually crafted rules. In this paper, we introduce a novel approach to find and annotate arithmetic idioms with their corresponding high-level language expressions to significantly simplify manual analysis. In contrast to previous approaches, our method does not require manual work to create the patterns for matching idioms and significantly less manual labour to derive the transformation rules to calculate the original constants. In our evaluation, we compared the results of PIdARCI against the current academic and commercial state-of-the-art Ghidra, RetDec, and Hex Rays / IDA Pro. We show that PIdARCI matches more than 99\% of all considered idioms, exceeding the matching rate of the other approaches.},
	booktitle = {2021 18th {International} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Enders, Steffen and Rybalka, Mariia and Padilla, Elmar},
	month = feb,
	year = {2021},
	keywords = {Semantics, Malware, High level languages, Databases, Manuals, Matched filters, Pattern matching},
	pages = {1--7},
}

@inproceedings{chen_investigating_2023,
	title = {Investigating {Neural}-based {Function} {Name} {Reassignment} from the {Perspective} of {Binary} {Code} {Representation}},
	issn = {2643-4202},
	doi = {10.1109/PST58708.2023.10320193},
	abstract = {Building a model to reassign descriptive names for binary functions is considerable assistance for reverse engineering. Existing methods proposed for this issue are based on the low-level representation of binary code (e.g., assembly code), and especially the recent approaches employed neural-based models on instruction sequences. However, their performance is still unsatisfactory. Meanwhile, modern decompilers provide lifted representations of binary code, and their effectiveness has not been adequately studied. This paper further explores the issue of function name reassignment from the perspective of binary code representation. Specifically, we present a general and flexible NEural-based function name Reassignment framework NER, which leverages a decompiler to obtain a specific representation and applies the corresponding serialization strategy on it. NER then uses an alternative neural network to make predictions. Three levels of representation are investigated, including assembly code, Intermediate Representation (IR), and pseudo-code. We observe the binary code representations are significant for the final performance. It demonstrates that the pseudo-code is the most effective one. Based on these findings, we leverage the framework to implement a reassignment model NER-pc, which has 25\% and 10\% F1 score improvements against the state-of-the-art methods. Besides, more experiments are conducted to verify the design of NER and the effectiveness of NER-pc.},
	booktitle = {2023 20th {Annual} {International} {Conference} on {Privacy}, {Security} and {Trust} ({PST})},
	author = {Chen, Guoqiang and Gao, Han and Zhang, Jie and He, Yanru and Cheng, Shaoyin and Zhang, Weiming},
	month = aug,
	year = {2023},
	keywords = {Binary codes, Privacy, Security, Reverse engineering, Neural networks, Predictive models, Buildings, Binary analysis, Binary code representation, Function name prediction},
	pages = {1--11},
}

@inproceedings{yu_bytecode_2022,
	title = {Bytecode {Obfuscation} for {Smart} {Contracts}},
	issn = {2640-0715},
	doi = {10.1109/APSEC57359.2022.00083},
	abstract = {Ethereum smart contracts face serious security problems, which not only cause huge economic losses, but also destroy the Ethereum credit system. To solve this problem, code obfuscation techniques are applied to smart contracts to improve their complexity and security. However, the current source code obfuscation methods have insufficient anti-decompilation ability. Therefore, we propose a novel bytecode obfuscation approach called BOSC based on four kinds of bytecode obfuscation techniques, which is directed at solidity. The experimental results show that, after the bytecode obfuscation, the failure rate of decompilation tools is over 99\% and only a small amount of gas is consumed.},
	booktitle = {2022 29th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Yu, Qifan and Zhang, Pengcheng and Dong, Hai and Xiao, Yan and Ji, Shunhui},
	month = feb,
	year = {2022},
	keywords = {Codes, Security, Source coding, Faces, Complexity theory, Smart contracts, Ethereum, Smart Contract, Bytecode Obfusca-tion, Economics},
	pages = {566--567},
}

@inproceedings{lee_identifying_2023,
	title = {Identifying {Code} {Tampering} {Using} {A} {Bytecode} {Comparison} {Analysis} {Tool}},
	issn = {2770-8209},
	doi = {10.1109/SERA57763.2023.10197775},
	abstract = {The issues related to SolarWinds attacks point out a large concern with modern software development projects in that there are fundamental flaws with existing security infrastructure. The purpose of this research is to investigate to what extent can the SootDiff analysis tool, a bytecode comparison tool, be used to determine if an application has been tampered with by comparing a known good version with a version that is unknown. The compiled and decompiled bytecodes as Jimple representations were compared to analyze the unique differences in identifying code tempering. The results showed that the scope of the variable is important in whether the change was detected. Variables with a scope that was entirely contained within one method could have their names changed without triggering a warning, but global variables to objects could not. The parameter variable and the local variable behave differently. Since the parameter is in the publicly available part of the method Java treats it the same way as it does the global variable. The local variable is strictly private to the method and not made available to the outside. Such findings can support the analysis tool which is useful for identifying potential breaches to detect meaningful changes in code even if it is decompiled.},
	booktitle = {2023 {IEEE}/{ACIS} 21st {International} {Conference} on {Software} {Engineering} {Research}, {Management} and {Applications} ({SERA})},
	author = {Lee, Young and McDonald, Arlen and Yang, Jeong},
	month = may,
	year = {2023},
	keywords = {Codes, Security, Software, Software engineering, Java, bytecode, Jimple, software supply chain, software supply chain security, SolarWinds, SootDiff, Supply chains},
	pages = {69--76},
}

@inproceedings{behner_sok_2025,
	title = {{SoK}: {No} {Goto}, {No} {Cry}? {The} {Fairy} {Tale} of {Flawless} {Control}-{Flow} {Structuring}},
	issn = {2995-1356},
	doi = {10.1109/EuroSP63326.2025.00032},
	abstract = {Decompilers play a crucial role in the detailed analysis of malware or firmware, particularly because control-flow structuring allows the recovery of high-level code that is more readable to human analysts. Despite the ongoing debate over their usage of gotos to work around constraints during control-flow structuring, pattern-matching approaches remain prevalent among both commercial and open-source decompilers. With the emergence of pattern-independent restructuring techniques, various attempts have been made to overcome readability limitations, especially concerning the use of gotos. However, despite these advances, recent approaches often fail to thoroughly address several inherent challenges of control-flow structuring, thereby affecting output quality or practicality.In this paper, we systematize the intrinsic challenges of control-flow structuring that every approach must address. In addition, we review existing methods, comparing them, while highlighting both their advantages and limitations with respect to these challenges. Specifically, we emphasize the practicability issues of current pattern-independent restructuring techniques and discuss whether and how future methods might overcome them. Finally, we explore the theoretical potential to mitigate some of these challenges by suggesting methodology ideas for various aspects of control-flow structuring. Overall, this paper enables other researchers to make informed decisions when developing or enhancing control-flow structuring methods, thereby preventing negative side-effects arising from the interdependence of challenges.},
	booktitle = {2025 {IEEE} 10th {European} {Symposium} on {Security} and {Privacy} ({EuroS}\&{P})},
	author = {Behner, Eva-Maria C. and Enders, Steffen and Padilla, Elmar},
	month = jun,
	year = {2025},
	keywords = {Codes, Reverse engineering, Malware, decompilation, reverse engineering, Static analysis, Microprogramming, static analysis, control-flow structuring, control-flow recovery, Reviews},
	pages = {411--431},
}

@article{peng_bctd-ics_2025,
	title = {{BCTD}-{ICS}: {A} {Blockchain}-{Aided} {Framework} for {Trusted} {Detection} of {Industrial} {Control} {System} {Components}},
	volume = {12},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2025.3583304},
	abstract = {The cybersecurity threats targeting industrial control systems (ICSs) are evolving with increasing sophistication. Addressing the detection blind spots in existing source code analysis techniques, this study reveals a dual security paradox arising from code sensitivity: privacy leakage risks caused by decompilation techniques and integrity verification deficiencies in reverse engineering. This article investigates three critical challenges: 1) what are the component flow process and detection elements of ICS component source code? 2) how can high-performance and reliable tracing and traceability be provided for ICS component source code exceptions and routine detection? and 3) how can privacy enhancement and trusted detection of ICS component source code with high sensitivity be achieved? This article proposes a blockchain-integrated trusted detection framework for ICS (BCTD-ICS), delivering groundbreaking solutions: 1) establishing a lifecycle circulation model that systematically maps component types, stakeholders, and detection parameters; 2) developing a tripartite collaborative architecture [blockchain- identification resolution zero-knowledge proofs (ZKPs)], featuring a traceability mechanism with trusted identification codes (resolution efficiency: 40 ms/105 queries) to eliminate decompilation-induced privacy risks; and 3) creating an industrial-oriented privacy enhancement system utilizing DBSCAN clustering for intelligent sampling (26\% compression rate on BCN3D Moveo) and optimizing ZK-SNARK protocols through Shamir’s secret sharing, establishing a backdoor-resistant distributed parameter generation system (time delay increment {\textless} 100 ms). Experimentally verified, our solution enables ICS component code detection supply-chain-wise without sensitive data leakage in real-world industries. This work establishes a novel trusted detection paradigm for ICS, advancing detection efficiency and credibility under strict privacy preservation requirements, meeting Industry 4.0 security demands.},
	number = {18},
	journal = {IEEE Internet of Things Journal},
	author = {Peng, Xiangzhen and Ma, Tianyu and Zheng, Chengliang and Shen, Zhidong and Cui, Xiaohui},
	month = sep,
	year = {2025},
	keywords = {Codes, Privacy, Security, Source coding, Protocols, Supply chains, Internet of Things, blockchain, Artificial intelligence technology, Blockchains, identification resolution technology, industrial control systems (ICSs), Object recognition, source code detection, Telecommunication traffic, zero-knowledge proofs (ZKPs)},
	pages = {37552--37570},
}

@inproceedings{lu_research_2021,
	title = {A {Research} on {ELF} {File} {Protection} {Schemes} of {IOT} {Application} in {Electric} {Power} {Industry}},
	doi = {10.1109/CISCE52179.2021.9445905},
	abstract = {As more and more IOT Applications implemented in electric power industry, source code leaking gradually becomes an important topic for discussion. Some researches have proved that IOT APP could be cracked down and source code (C/C++, JAVA, etc) is likely to be decompiled by attackers with expert skills. And this trend is going to be irreversible with a variety of auto-cracking tools published, which means that the cost of time and cracking technique are unprecedentedly decreased. To avoid source code leaking, some ways of protecting native code in ELF file such as code obfuscation or ELF shell (special in Android) were provided and then applied widely. But these schemes can not meanwhile completely satisfy the performance and secure rate in Linux platform. This paper originally proposes a scheme that protects native code from being decompiled and stolen in situation where ELF file is called by JAVA in Linux. The SM4 encryption algorithm and look-up table are creatively used to achieve the goals of apparently elevating the security within a limited performance price. At the posterior of the paper, a control experiment between one famous open source project and this scheme is introduced to supply the data supporting.},
	booktitle = {2021 {International} {Conference} on {Communications}, {Information} {System} and {Computer} {Engineering} ({CISCE})},
	author = {Lu, Ziang and Shao, Zhipeng},
	month = may,
	year = {2021},
	keywords = {Tools, Java, component, ELF(SO) file protection, Geophysical measurement techniques, Ground penetrating radar, IOT application, Linux, look-up table, native code, SM4, Table lookup, Technical requirements},
	pages = {105--108},
}

@inproceedings{r_designing_2025,
	title = {Designing a {Static} {Malware} {Analysis} {Framework} for {Detecting} {Malicious} {Malware} {Code} with {Ghidra}},
	doi = {10.1109/ICSSAS66150.2025.11081372},
	abstract = {Malware analysis is an integral part of cybersecurity, however traditional signature-based detection techniques are inadequate for advanced obfuscation techniques. This paper proposes a static malware analysis framework for identifying malicious code, using Ghidra. The proposed malware analysis framework decompiles malware samples automatically in order to extract features, while reviewing control flow, scanning opcodes, or extracting embedded strings. Additionally, the system uses integrated tools such as VirusTotal API and PEview to validate or classify signatures and analyze file structure. An experimental evaluation of the proposed framework showed an 89\% success rate of malware detection that outperformed the performance of traditional signature based methods (72\%) and had a lower false-positive rate (7\%) than heuristic based methods (15\%) under specific conditions. Results suggest that the proposed framework is effective towards the identification of obfuscated malware while being reliable. Unlike earlier traditional techniques, the system user-friendly utilizes Ghidra's improved, advanced capabilities of decompilation and scripts to offer more precision and automation. The solution provides improvement in cybersecurity with an effective, scalable, and automated, static approach to malware analysis.},
	booktitle = {2025 3rd {International} {Conference} on {Self} {Sustainable} {Artificial} {Intelligence} {Systems} ({ICSSAS})},
	author = {R, Siva Surya and R, Varuneshan and C, Heltin Genitha},
	month = jun,
	year = {2025},
	keywords = {Codes, Feature extraction, Malware, Reliability, Machine learning, Ghidra, Cybersecurity, Automation, Computer security, Computer viruses, Dynamic scheduling, Heuristic Detection, Inspection, Opcode Inspection, PEview, Static Malware Analysis, VirusTotal API},
	pages = {1696--1701},
}

@inproceedings{tahtaci_android_2020,
	title = {Android {Malware} {Detection} {Using} {Machine} {Learning}},
	doi = {10.1109/ASYU50717.2020.9259834},
	abstract = {The usage of mobile devices is increasing exponentially. There were lots of critical applications such as banking to health applications are available on mobile devices through mobile applications. This penetration and spread of mobile applications brings some threats. Malicious software(Malware) is one of these dangers. Malware has the potential to cause damage to various scales such as theft of sensitive data, identity and credit card. To reduce the effects of these threats, antiviruses have been developed and malware analysis teams have been established, but human effort may be insufficient in the rapidly growing malware market. For this reason, automated malware scanning solutions should be developed by making use of machine learning algorithms. In this study, machine learning models were created by using the n-gram features of the smali files, which are the decompiled Android packages. The trained models are combined with different feature extraction and feature selection methods and as a result their performances are reported.},
	booktitle = {2020 {Innovations} in {Intelligent} {Systems} and {Applications} {Conference} ({ASYU})},
	author = {TAHTACI, Burak and CANBAY, Beyzanur},
	month = oct,
	year = {2020},
	keywords = {Feature extraction, Malware, Artificial Neural Neworks, Covariance matrices, Forestry, Internet, Machine learning, Machine Learning, Malware Detection, Mobile Malwares, Random Forest, Static Code Analysis, Support vector machines},
	pages = {1--6},
}

@inproceedings{aminuddin_android_2020,
	title = {Android {Assets} {Protection} {Using} {RSA} and {AES} {Cryptography} to {Prevent} {App} {Piracy}},
	doi = {10.1109/ICOIACT50329.2020.9331988},
	abstract = {Android is the major operating system for mobile devices. The presence of the Google Play Store creates an ecosystem between the app developers and users. As the ecosystem grows, some pirated apps start to show up. This is possible due to the nature of the Android Application that easily can be extracted and decompiled to reveal the source code and the assets file. This research proposed a methodology to protect the application assets from piracy using the cryptographic algorithm. The assets are encrypted during the compile-time using the Gradle build system provided by the Android Studio. While the decryption is performed in the Android device during the application run-time. The proposed algorithm is the pair of asymmetric algorithm called RSA and the symmetric algorithm called AES. This research shows that RSA-AES gives the best security in protecting the assets of Android applications. Besides, the performance of the algorithm is evaluated based on the speed of the encryption and decryption that reach 106.82 MB/s and 44.42 MB/s respectively.},
	booktitle = {2020 3rd {International} {Conference} on {Information} and {Communications} {Technology} ({ICOIACT})},
	author = {Aminuddin, Afrig},
	month = jan,
	year = {2020},
	keywords = {Internet, Android, Cryptography, Decryption, Ecosystems, Encryption, Mobile handsets, Operating systems, Performance evaluation, Public key},
	pages = {461--465},
}

@inproceedings{han_binary_2022,
	title = {Binary vulnerability mining technology based on neural network feature fusion},
	doi = {10.1109/AEMCSE55572.2022.00058},
	abstract = {The high complexity of software and the diversity of security vulnerabilities have brought severe challenges to the research of software security vulnerabilities Traditional vulnerability mining methods are inefficient and have problems such as high false positives and high false negatives, which can not meet the growing needs of software security. To solve the above problems, this paper proposes a binary vulnerability mining technology based on neural network feature fusion. Firstly, this method constructs binary vulnerability data sets containing multiple vulnerability types, then decompile them to the pcode intermediate language level, and then extracts relevant feature vectors from binary vulnerability data sets according to Bert fine tuning model and bilstm model respectively. In order to fully obtain the semantic information of vulnerabilities, this method standardized the two, fused them, and carried out relevant experiments. The experimental results show that the accuracy of vulnerability detection on SARD data set is 96.92\%, which is higher than other binary vulnerability detection methods based on neural network.},
	booktitle = {2022 5th {International} {Conference} on {Advanced} {Electronic} {Materials}, {Computers} and {Software} {Engineering} ({AEMCSE})},
	author = {Han, Wenjie and Pang, Jianmin and Zhou, Xin and Zhu, Di},
	month = apr,
	year = {2022},
	keywords = {Software, Data models, Deep learning, Semantics, Feature extraction, Neural networks, Data mining, Computers, Decompilation Technology, Neural network, Vulnerability mining},
	pages = {257--261},
}

@inproceedings{xiao_xvmp_2023,
	title = {{xVMP}: {An} {LLVM}-based {Code} {Virtualization} {Obfuscator}},
	issn = {2640-7574},
	doi = {10.1109/SANER56733.2023.00082},
	abstract = {Obfuscation techniques are widely used to protect the digital copyright and intellectual property rights of software. Among them, code virtualization is one of the most powerful obfuscation techniques, which hides both the control flow and the data flow of the code, thereby preventing code from being decompiled. However, existing code virtualization solutions are not well-resistant to de-obfuscation techniques (e.g., symbolic execution and frequency analysis), and only target limited program languages and architectures, which are challenging to integrate into the process of software development and maintenance.In this paper, We propose an LLVM-based code virtualization tool, namely xVMP to fulfill a scalable and virtualized instruction-hardened obfuscation. To mask the effects of multiple program languages and architectures, xVMP incorporates the obfuscation process of code virtualization into the compilation, and generates virtualized code based on LLVM intermediate representation (IR). After virtualization, it embeds the interpreter of virtualized code into the IR and compiles to an executable. To enhance the security, xVMP encrypts virtualized instructions in each basic block and decrypts them at runtime to enhance the security of obfuscation. In addition, it supports specified function obfuscation. xVMP identifies the function annotations marked by the developer in the source code to locate the function to protect. We implement the prototype of xVMP, and evaluate it with a microbenchmark and three real-world programs. The experimental results show that xVMP can be more difficult to crack than the state-of-the-art obfuscators, and it can support more source code types and architectures, and can be applied to real-world software. Source Code: https://github.com/GANGE666/xVMP.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Xiao, Xuangan and Wang, Yizhuo and Hu, Yikun and Gu, Dawu},
	month = mar,
	year = {2023},
	keywords = {Codes, Source coding, Reverse Engineering, Prototypes, Annotations, Code Virtualization, Computer architecture, Intellectual property, Obfuscation, Runtime},
	pages = {738--742},
}

@inproceedings{maalla_improves_2020,
	title = {Improves the {Operation} and {Maintenance} {Technology} of {DC} {Transmission} {Based} on the {Algorithm} of {VBE}},
	doi = {10.1109/ITOEC49072.2020.9141591},
	abstract = {At present, VBE is widely used in DC transmission research, and its performance directly affects the actual operation process of DC transmission systems. This research improves the operation and maintenance technology of field personnel and the reliability of DC transmission by studying the program of the VBE system and analyzing its weak links. Our research is to simulate the operating conditions of the VBE system and use the platform to test the VBE to ensure that it is in good working condition. Therefore, research decompiles the VBE system, builds a test platform, sorts out its weak links, and proposes anti-accident measures for research. Both in the theoretical method research and engineering application prospects are in the domestic leading position.},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	author = {Maalla, Allam},
	month = jun,
	year = {2020},
	keywords = {Converter Valve, Cooling, DC Transmission, Monitoring, Optical fibers, Optical receivers, Single-Ship CPU, Threshold voltage, Valves, VBE},
	pages = {1813--1817},
}

@inproceedings{zhu_similarity_2021,
	title = {Similarity {Measure} for {Smart} {Contract} {Bytecode} {Based} on {CFG} {Feature} {Extraction}},
	doi = {10.1109/CISAI54367.2021.00113},
	abstract = {As the mainstream of smart contract research, most Ethereum smart contracts do not open their source code, and the bytecode of smart contracts has attracted the attention of researchers. Based on the similarity measurement of smart contract bytecode, a series of tasks such as vulnerability mining, contract upgrading and malicious contract detection can be carried out. This paper proposes a method to measure the similarity of smart contract bytecode. Firstly, the key opcode combination of smart contract is summarized. When traversing the CFG(control flow graph) constructed by decompilation of smart contract bytecode, the opcodes in the basic block are pattern matched, and the features between the basic blocks are extracted according to the in-out degree, so as to enhance the similarity measurement effect of contract semantics in vector space. The experimental results show that the proposed method is greatly improved compared with the baseline.},
	booktitle = {2021 {International} {Conference} on {Computer} {Information} {Science} and {Artificial} {Intelligence} ({CISAI})},
	author = {Zhu, Di and Pang, Jianmin and Zhou, Xin and Han, Wenjie},
	month = sep,
	year = {2021},
	keywords = {Semantics, Feature extraction, feature extraction, Smart contracts, Flow graphs, basic block, bytecode similarity, CFG, Euclidean distance, Extraterrestrial measurements, Information science},
	pages = {558--562},
}

@inproceedings{wahaz_is_2021,
	title = {Is {WhatsApp} {Plus} {Malicious}? {A} {Review} {Using} {Static} {Analysis}},
	doi = {10.1109/IWBIS53353.2021.9631860},
	abstract = {For cybersecurity activists, reviewing whether an application, including modified applications, is malicious or not is a challenging job. WhatsApp Plus is a messenger application modified from the official WhatsApp application. Comparing the source code of the WhatsApp Plus with the official WhatsApp is one way to review its security or malice. Considering that WhatsApp is very popular and has many users, the results of this investigation are very useful for users to avoid malicious applications. In this study, we have conducted an exploration of the source code of the WhatsApp and WhatsApp Plus applications to find out whether or not WhatsApp Plus has been inserted with malware, spyware, or other malicious code. The exploration used the static analysis method, where the source code of the two applications were decompiled, compared, and analyzed. The de-compilation is done using the MobSF tool and the comparison using the extension of Visual Studio Code called Compare Folders. The differences in the source code found are then analyzed for possible behavior to determine whether it can cause harm, for example stealing user credentials. Although no malicious code was found on WhatsApp Plus, in our study, users must stay alert since they remain vigilant in installing and using WhatsApp Plus because the developer may add malicious code to the next version update.},
	booktitle = {2021 6th {International} {Workshop} on {Big} {Data} and {Information} {Security} ({IWBIS})},
	author = {Wahaz, Rizaldi and Harmana, Rakha Nadhifa and Amiruddin, Amiruddin and Suryadinata, Ardya},
	month = oct,
	year = {2021},
	keywords = {Codes, Deep learning, Static analysis, Databases, Freeware, Information security, Malicious, Source code, Static Analysis, Visualization, WhatsApp, WhatsApp Plus},
	pages = {91--96},
}

@article{nghi_phu_efficient_2019,
	title = {An {Efficient} {Algorithm} to {Extract} {Control} {Flow}-{Based} {Features} for {IoT} {Malware} {Detection}},
	volume = {64},
	issn = {1460-2067},
	doi = {10.1093/comjnl/bxaa087},
	abstract = {Control flow-based feature extraction method has the ability to detect malicious code with higher accuracy than traditional text-based methods. Unfortunately, this method has been encountered with the NP-hard problem, which is infeasible for the large-sized and high-complexity programs. To tackle this, we propose a control flow-based feature extraction dynamic programming algorithm for fast extraction of control flow-based features with polynomial time O(N$^{\textrm{2}}$), where N is the number of basic blocks in decompiled executable codes. From the experimental results, it is demonstrated that the proposed algorithm is more efficient and effective in detecting malware than the existing ones. Applying our algorithm to an Internet of Things dataset gives better results on three measures: Accuracy = 99.05\%, False Positive Rate = 1.31\% and False Negative Rate = 0.66\%.},
	number = {1},
	journal = {The Computer Journal},
	author = {Nghi Phu, Tran and Dai Tho, Nguyen and Huy Hoang, Le and Ngoc Toan, Nguyen and Ngoc Binh, Nguyen},
	month = jan,
	year = {2019},
	keywords = {CFD, control flow-based features, dynamic programming, embedded malware, IoT malware detection},
	pages = {599--609},
}

@inproceedings{feng_research_2020,
	title = {Research on {Information} {Security} {Technology} of {Mobile} {Application} in {Electric} {Power} {Industry}},
	doi = {10.1109/IPEC49694.2020.9115191},
	abstract = {With the continuous popularization of smart terminals, Android and IOS systems are the most mainstream mobile operating systems in the market, and their application types and application numbers are constantly increasing. As an open system, the security issues of Android application emerge in endlessly, such as the reverse decompilation of installation package, malicious code injection, application piracy, interface hijacking, SMS hijacking and input monitoring. These security issues will also appear on mobile applications in the power industry, which will not only result in the embezzlement of applied knowledge copyrights but also lead to serious leakage of users' information and even economic losses. It may even result in the remote malicious control of key facilities, which will cause serious social issues. Under the background of the development of smart grid information construction, also with the application and promotion of power services in mobile terminals, information security protection for mobile terminal applications and interactions with the internal system of the power grid has also become an important research direction. While analyzing the risks faced by mobile applications, this article also enumerates and analyzes the necessary measures for risk resolution.},
	booktitle = {2020 {Asia}-{Pacific} {Conference} on {Image} {Processing}, {Electronics} and {Computers} ({IPEC})},
	author = {Feng, Li and Tao, Chen and Bin, Wang and Jianye, Zhang and Song, Qing},
	month = apr,
	year = {2020},
	keywords = {information security, mobile application, power industry, security reinforcement},
	pages = {51--54},
}

@inproceedings{han_binary_2020,
	title = {Binary software vulnerability detection method based on attention mechanism},
	doi = {10.1109/ICMCCE51767.2020.00320},
	abstract = {Aiming at the stack overflow vulnerability in binary software, this paper proposes a binary vulnerability detection method based on the attention mechanism. First, this paper analyze the basic characteristics of stack overflow vulnerabilities, and perform data preprocessing on the decompiled files to make the neural network better adapt to the characteristics of stack overflow vulnerabilities, then formulate instruction specifications at the assembly language level, and finally input the data into the fusion attention mechanism Learning in the neural network. This paper compares and analyzes three kinds of neural networks on the CWE121 data set. The experimental results show that after neural network training, the detection method based on the attention mechanism can be effective and accurately discover whether the target area has stack overflow vulnerabilities, thereby greatly improving the detection efficiency.},
	booktitle = {2020 5th {International} {Conference} on {Mechanical}, {Control} and {Computer} {Engineering} ({ICMCCE})},
	author = {Han, Wenjie and Pang, Jianmin and Zhou, Xin and Zhu, Di},
	month = feb,
	year = {2020},
	keywords = {Software, Binary, Data models, Training, machine learning, attention, Computational modeling, Data preprocessing, Force, Neural networks, software vulnerability},
	pages = {1462--1466},
}

@inproceedings{kim_pildroid_2019,
	title = {{PILDroid}: {A} {System} for {Detecting} the {Leakage} of {Privacy} {Information} using the {JNI}},
	doi = {10.1109/ICICE49024.2019.9117398},
	abstract = {We live in a period of explosive growth of smart device applications. Specifically, the growth rate of Android applications is amazing. And these Android applications often use JNI (Java native interface). However, research on the leakage of private information using JNI is lacking. In this paper, we propose a system for detecting the leakage of private information using JNI. Our system named PILDroid adopts tainted analysis based on static method. PILDroid can perform the analysis more easily and effectively than the assembly-based analysis systems because it decompiles a JNI into LLVM IR instead of assembly language. And this paper demonstrates an experiment to verify the precision and performance. For the test data, we selected five Android applications: three of which are well known malware, two of which are malware made by us. And experiment results, PILDroid determined that there has a leak of privacy information flow from five malware.},
	booktitle = {2019 8th {International} {Conference} on {Innovation}, {Communication} and {Engineering} ({ICICE})},
	author = {Kim, Yeoneo and Kim, Jinseob and Liu, Xiao and Cheon, Junseok and Woo, Gyun},
	month = oct,
	year = {2019},
	keywords = {Android, static analysis, taint analysis, privacy leakage},
	pages = {153--156},
}

@inproceedings{fang_research_2021,
	title = {Research on {Multi}-model {Android} {Malicious} {Application} {Detection} {Based} on {Feature} {Fusion}},
	doi = {10.1109/RCAE53607.2021.9638928},
	abstract = {With the widespread use of the Android operating system, the number of applications on the platform is increasing, and malicious applications are also emerging. How to effectively identify android malware applications to prevent and protect the security of the mobile terminal is a crucial issue. This paper uses the feature fusion method and directly call the library function to extract the permissions and API features of the APK file, then decompile the APK file to obtain the opcode features and merge the three features with multiple features to generate a feature vector. Finally it use a multi-model neural network HYDRA to learn fusion feature vector, so that it can identify and detect malware. The work also compare it with other single-feature machine learning algorithms to verify its effect. Experimental results show that the accuracy of the multi-model neural network detection method based on feature fusion reaches 98.92\%, which is better than other single-model feature methods.},
	booktitle = {2021 4th {International} {Conference} on {Robotics}, {Control} and {Automation} {Engineering} ({RCAE})},
	author = {Fang, Zhan and Liu, Jun and Huang, Ribian and Chen, Peng and Li, Xin and Chen, Xiao},
	month = jan,
	year = {2021},
	keywords = {Feature extraction, Malware, Operating systems, Neural networks, Libraries, Automation, android malicious applications, decompile, feature fusion, Machine learning algorithms, neural network},
	pages = {147--151},
}

@inproceedings{huang_runtime-environment_2019,
	title = {Runtime-{Environment} {Testing} {Method} for {Android} {Applications}},
	doi = {10.1109/QRS-C.2019.00111},
	abstract = {One of the key problems in app testing is to improve the test coverage of apps. However, current testing techniques, whether in code coverage or activity coverage, are not satisfactory. To address this limitation, we present an algorithm for generating test runtime-environment-set to exercise mobile apps. Our approach is based on code analysis to systematically test the targeted code of the Android apps. It analyzes the decompiled code that identifies the code related to Android SDK version, generating the corresponding test cases with the runtime-environment set. We also implement our approach on Android, and validate the method with the existing widely used strategies. An empirical study of the practical usefulness of the technique has been presented on 6 widely-used industrial apps. 18 unique crashes have been found, and the method coverage has been increased by far 9.8\% to 130.4\% on those apps.},
	booktitle = {2019 {IEEE} 19th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} {Companion} ({QRS}-{C})},
	author = {Huang, Song and Yang, Sen and Hui, Zhanwei and Yao, Yongming and Chen, Lele and Liu, Jialuo and Chen, Qiang},
	month = jul,
	year = {2019},
	keywords = {Tools, Computer crashes, Google, Runtime environment, Smart phones, Testing, android version, code analysis, Mobile testing, runtime environment set generation},
	pages = {534--535},
}

@inproceedings{adamec_malware_2025,
	title = {Malware {Detection} with {LLM}},
	doi = {10.1109/KIT67756.2025.11205440},
	abstract = {The increasing sophistication of malware poses critical challenges to traditional detection techniques, particularly in the face of polymorphic and evasive threats. Recent advances in Natural Language Processing (NLP), specifically through the deployment of large language models (LLMs), offer promising capabilities for addressing these challenges. This study evaluates the effectiveness of three LLMs-GPT-2, T5, and CodeBERT-in detecting malware from decompiled.c code derived from Portable Executable (PE) files. The models were tested on a balanced dataset containing real-world malware and benign samples, preprocessed using a custom tokenization and classification pipeline. Experimental results indicate that GPT-2 and T5 achieved strong performance. The study confirms the viability of transformerbased LLMs for source-code-based malware detection, particularly GPT-2 and T5, and emphasizes the importance of model selection and dataset quality. Future work will focus on refining these architectures using larger, more diverse training sets and incorporating class-weighted loss functions to further improve detection balance and reduce false positives.},
	booktitle = {2025 {Communication} and {Information} {Technologies} ({KIT})},
	author = {Adamec, Matej and Turčaník, Michal},
	month = oct,
	year = {2025},
	keywords = {Codes, Transformers, Training, Malware, Faces, Large language models, Malware Detection, CodeBERT, GPT 2, Information and communication technology, LLM, Pipelines, Refining, T5, Tokenization},
	pages = {1--7},
}

@inproceedings{corcalciuc_low-level_2018,
	title = {Low-{Level} {Control}-{Flow} {Manipulation} {Techniques}},
	doi = {10.1109/ROLCG.2018.8572021},
	abstract = {Contrary to interacting with software remotely at runtime, commercial software may be altered by an attacker that is able to to change the meaning of the program at will by decompiling and recompiling the program. An attacker is able to coerce control-flow, manipulate predicates in order to lead the program into a favourable state. The aim of this paper is to present a strictly limited set of low-level attack patterns and to draw a parallel to exploits carried out against system software that cannot be tampered with.},
	booktitle = {2018 {Conference} {Grid}, {Cloud} \& {High} {Performance} {Computing} in {Science} ({ROLCG})},
	author = {Corcalciuc, Horia V.},
	month = oct,
	year = {2018},
	keywords = {Taxonomy, Runtime, control flow, Physics, predicate logic, security, Software packages, software protection, Software protection, UML, Unified modeling language},
	pages = {1--4},
}

@inproceedings{lv_research_2022,
	title = {Research {Based} on {LLVM} {Code} {Obfuscation} {Technology}},
	doi = {10.1109/IIoTBDSC57192.2022.00039},
	abstract = {Computer programs are executed continuously in a certain relatively fixed order of instructions. Replacing these instructions with another more complex order or with different instructions with the same meaning without changing the result of the program will change the logic of the original instructions without affecting the result of the operation, which is the core principle of code obfuscation. In the compilation process, before the compiler compiles the code into the target machine code, it will transform the code into a kind of intermediate code, and then generate the target code after Control Flow Flattening and string encryption obfuscation of the intermediate code, which will make the decompilation work more difficult, thus realizing a code obfuscation system, and through some test cases, the effectiveness and feasibility of the system is demonstrated.},
	booktitle = {2022 {International} {Conference} on {Industrial} {IoT}, {Big} {Data} and {Supply} {Chain} ({IIoTBDSC})},
	author = {Lv, Di and Zhao, Liang and Chen, Bin},
	month = sep,
	year = {2022},
	keywords = {Codes, Source coding, Reverse engineering, Process control, Supply chains, Transforms, Operating systems, code obfuscation, code safety, Control Flow Flattening obfuscation, encryption process, String encryption},
	pages = {163--167},
}

@inproceedings{li_research_2023,
	title = {Research on {Recognition} of {Android} {Counterfeit} {Application} {Based} on {Siamese} {Network}},
	doi = {10.1109/ICIPNP62754.2023.00052},
	abstract = {In recent years, with the popularity of mobile phones and mobile Internet, the download volume and application rate of third-party applications for smart phones, namely APP, have increased rapidly, and criminals have taken advantage of users' trust in well-known apps to produce similar interfaces and functions, induce users to download and install, steal users' personal information, property or spread malware, not only causing losses to users, but also disrupting normal market order. In order to identify counterfeit APP more quickly and effectively, this paper proposes a counterfeit APP recognition model based on twin network. The model first decompiles and extracts the name, package name, icon, and signature information of the APP, then filters out suspected counterfeit applications by calculating the editing distance of the name, and finally calculates the icon similarity based on the twin network model to determine whether the application is counterfeit. In this paper, datasets containing multiple types of phishing applications are used for experiments, and the effects of VGG16, ResNet50, and ViT algorithms on the recognition results are compared as the basic feature extraction networks. The results show that the accuracy of the ViT-based twin network architecture reaches 85.12\%, which can effectively identify counterfeit applications.},
	booktitle = {2023 {International} {Conference} on {Information} {Processing} and {Network} {Provisioning} ({ICIPNP})},
	author = {Li, Kun and Liu, Yanyan and Wang, Xuchen and Li, Guopeng and Ma, Ziyue},
	month = oct,
	year = {2023},
	keywords = {Training, Accuracy, Feature extraction, Malware, deep learning, Smart phones, Residual neural networks, Counterfeit APP, Knowledge engineering, Network architecture, Phishing, Probability distribution, Twin networks},
	pages = {218--222},
}

@inproceedings{bhattacharjee_hybrid_2025,
	title = {Hybrid {Multi}-{Stage} {Framework} for {Advanced} {Malware} {Detection}: {Enhancing} {Accuracy} \& {Resilience}},
	issn = {2832-8973},
	doi = {10.1109/ICEPE65965.2025.11139736},
	abstract = {Malware detection is one of the parts of the modern cybersecurity system that requires precision and scalability against the ever-evolving threats. This paper will present a multi-staged hybrid framework for malware detection that combines static analysis, machine learning, and rule-based approaches. The files are processed through a sequence of stages: hash-based matching, machine learning classification, YARA rule evaluation, and decompiled code analysis. This further helps in accuracy and reduces the false prediction. The last decision will aggregate the outputs of all the stages using weighted scoring and achieving the final overall accuracy as 94.8\% with a dataset of 8,932 samples. A layered approach ensures that a comprehensive analysis is provided along with a strong, scalable solution for malware detection. Our findings indicate that the multiplexing of various methods for detection may significantly boost the accuracy of detection along with minimizing the shortcomings inherent in each technique.},
	booktitle = {2025 7th {International} {Conference} on {Energy}, {Power} and {Environment} ({ICEPE})},
	author = {Bhattacharjee, Srijita and Patil, Param and Patil, Vishal and Nage, Pranav},
	month = may,
	year = {2025},
	keywords = {Codes, Scalability, Accuracy, Feature extraction, Malware, Static analysis, Machine learning, Databases, AI-driven analysis, behavioral analysis, de-compilation for malware analysis, enhancing malware detection, feature extraction, machine learning based, Malware detection, multi staged, Multiplexing, Radare2, Resilience, signature based, YARA rules},
	pages = {1--6},
}

@inproceedings{yama_machine_2025,
	title = {Machine {Learning} {Approach} to {Malware} {Classification} {Using} {Byte} {N}-{Grams} on {IoT} {Devices}},
	doi = {10.1109/IMCOM64595.2025.10857519},
	abstract = {Malware for IoT devices has become popular in recent years. Many detection methods have been proposed to detect the malware, mainly using machine learning, but they are not designed to work on IoT devices and are often implemented using Python. Running a Python program requires the installation of a package, which is impractical given the memory and storage size of IoT devices. In addition, methods that require decompilation, static and dynamic analysis of test samples are difficult to run on IoT devices. Therefore, in this research, we propose a method implemented in C/C++ so that malware detection using byte n-grams and machine learning can be run on IoT devices, and then resource consumption can be measured. Byte n-gram methods are good approaches to detect malware without knowing the execution environment. Moreover, the Top-L approach to information gain can be effectively applied to reduce storage and memory consumption. As a result of the evaluation, when using an SVM model with Top-50000 that maintains effective classification accuracy, the memory consumption was 71.91MB and the storage consumption was 1.3GB, which can be implemented in IoT devices.},
	booktitle = {2025 19th {International} {Conference} on {Ubiquitous} {Information} {Management} and {Communication} ({IMCOM})},
	author = {Yama, Yuto and Uda, Ryuya},
	month = jan,
	year = {2025},
	keywords = {Accuracy, Malware, Radio frequency, machine learning, Machine learning, Support vector machines, malware detection, Internet of Things, IoT, Python, Information management, Memory management, Virtual environments},
	pages = {1--6},
}

@incollection{domas_advanced_2024,
	title = {Advanced {Techniques}},
	isbn = {978-1-394-19990-7},
	url = {https://ieeexplore.ieee.org/document/10649762},
	doi = {10.1002/9781394277131.ch16},
	abstract = {Summary {\textless}p{\textgreater}This chapter describes at a high level some advanced techniques and tools on the cutting edge of reverse engineering. Timeless debugging is also known as reverse debugging. Binary instrumentation is when security professionals inject code to watch or modify a process as it executes. This can be useful for finding memory leaks, tracing key checks, performing anti\&\#x2010;anti\&\#x2010;debugging, etc. Normally, for reversing and cracking, it's necessary to learn and write tools for each new architecture. The idea of intermediate representations is to translate all assembly code for all architectures to the same language. The idea of decompiling is to recover original source code from advanced automated analysis of assembly code. Automatic structure recovery involves automatically finding patterns and links in memory to make inferences about the data types used. Visualization can be used to deepen the understanding of file structure and execution. Theorem provers use mathematics to analyze code, including reduction, deobfuscation, boundaries, inputs, etc.{\textless}/p{\textgreater}},
	booktitle = {x86 {Software} {Reverse}-{Engineering}, {Cracking}, and {Counter}-{Measures}},
	publisher = {Wiley},
	author = {Domas, Stephanie and Domas, Christopher},
	year = {2024},
	keywords = {Codes, Software, Engines, Computer architecture, Data visualization, Debugging, Instruments},
	pages = {245--249},
}

@inproceedings{wu_malicious_2024,
	title = {A {Malicious} {Code} {Detection} {Strategy} {Based} on {Feature} {Fusion}},
	doi = {10.1109/ICETCI61221.2024.10594168},
	abstract = {Due to its weak characteristics, the general malware software detection technology has the weakness of inaccurate detection and inefficiency. Therefore, a malicious application or software detection mechanism is designed based on feature fusion. The detection mechanism based on OPC X-gram and malicious applications or software is improved, and the malicious applications or software detection mechanism based on OPC X-gram with multi-X value combination can mine the expressive logic sequence of malicious applications or software and improve the detection ability of malicious application or software. The potential feature representation mechanism of OPC X-gram is designed, and the decompiling component is used to get the source of the procedure to be tested. Then the source parts to be analyzed is extracted. Combined with the OPC X-gram sequences with multiple X values, the recognizable sequences are screened out by using the content feature recognition method, and the malicious application or software classification training is carried out on the OPC X-gram logic sequences with different X values by using KNN and RF classification algorithms, and finally the OPC X-gram will be used. The inferring sequence will be f made up by logic analysis again. By the experiment analysis, the proposed multi-X value OPC X-gram approach is better than the bin-file X-gram sequence logic and the single-X value OPC X-gram in terms of the accuracy of the potential threat application or software classification.},
	booktitle = {2024 {IEEE} 4th {International} {Conference} on {Electronic} {Technology}, {Communication} and {Information} ({ICETCI})},
	author = {Wu, Liang},
	month = may,
	year = {2024},
	keywords = {Software, Training, Accuracy, Code detection, Decompiling, Feature extraction, Malware, Nearest neighbor methods, Radio frequency},
	pages = {1502--1506},
}

@inproceedings{chen_mobile_2021,
	title = {Mobile application reinforcement method based on control flow and data flow confusion},
	issn = {2157-1481},
	doi = {10.1109/ICMTMA52658.2021.00080},
	abstract = {At present, mobile applications are developing rapidly, and the methods of attacking mobile applications by reverse engineering to obtain the core logic of the program are becoming more and more intense. In the face of malicious tampering, permission bypassing or obtaining core intellectual property rights through reverse engineering of mobile application, this paper proposes a mobile application reinforcement method based on control flow and data flow confusion, and factors affecting the accuracy and complexity of program analysis extracted for mobile applications, analyze its own characteristics and the relationship between them, detailed analysis of the implementation principle and code of the variable target compiler, in the syntax analysis and semantic analysis stage, the corresponding code obfuscation algorithm is implemented. Through obfuscation, the compiled program is strengthened at the three levels of grammar, control flow, and data flow. Experiments show that even if the source code of mobile application reinforced by this method is obtained through decompilation, the internal logic of the application cannot be known, and the next attack cannot be initiated.},
	booktitle = {2021 13th {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation} ({ICMTMA})},
	author = {Chen, Lu and Ma, Yuanyuan and Shao, Zhipeng and Chen, Mu},
	month = jan,
	year = {2021},
	keywords = {Program processors, Reverse engineering, Semantics, Java, Syntactics, Intellectual property, control flow, mobile application, security reinforcement, data flow, Mechatronics},
	pages = {345--348},
}

@inproceedings{yu_android_2019,
	title = {Android {Malicious} {Code} {Detection} {Based} on {Secondary} {Pruning} {Optimization}},
	doi = {10.1109/CISP-BMEI48845.2019.8965960},
	abstract = {In this paper, we propose a detection algorithm for Android malicious code based on integrated multi-feature. By decompiling and processing APK files, the multi-class behavior features of Android application are extracted, and the classification is achieved through an integrated learning framework based on quadratic pruning optimization. The prototype system automatically detects the malicious code of the Android platform and analyzes the validity of the algorithm through experimental verification.},
	booktitle = {2019 12th {International} {Congress} on {Image} and {Signal} {Processing}, {BioMedical} {Engineering} and {Informatics} ({CISP}-{BMEI})},
	author = {Yu, Qing and Ma, Kuolang and Wang, Zuohua},
	month = oct,
	year = {2019},
	keywords = {Security, Optimization, Feature extraction, Malware, Android, Classification algorithms, Multi-feature, Predictive models, Pruning},
	pages = {1--5},
}

@inproceedings{johri_amadel_2025,
	title = {{AMADel}: {Android} {Malware} {Analysis} {Using} {Deep} {Learning}},
	issn = {2769-2884},
	doi = {10.1109/ICRITO66076.2025.11241815},
	abstract = {Malware Detection is a nuanced activity that often requires human intervention and complex sandboxing methodology to detect complex and obfuscated malware. Human intervention is not practical for a large sample size of untested applications. Android, being the most used and heavily built-upon mobile operating system, is no stranger to having malware embedded in its packaged applications. This paper proposes an innovative ensemble deep learning model designed to classify Android applications as benign or malicious, known as AMADel. AMADeL includes three Convolutional Neural Networks (CNNs) models ensembled together, which were trained on images generated using DEX bytecode obtained by decompiling APKs using Androguard. The APK files were sourced from the AndroZoo dataset. The dataset was prepared and used to train various models. The standalone CNNs gave the highest accuracy of 86.14\%. Pre-trained models, such as ResNet50 and VGG16, achieved accuracies of 81.19\% and 82.18\%, respectively. However, the proposed AMADel attained an accuracy of over 90\%.},
	booktitle = {2025 12th {International} {Conference} on {Reliability}, {Infocom} {Technologies} and {Optimization} ({Trends} and {Future} {Directions}) ({ICRITO})},
	author = {Johri, Siddharth and Bose, Adyot and Jha, Shaaswat K and Kakkar, Misha and Mehrotra, Deepti},
	month = sep,
	year = {2025},
	keywords = {Optimization, Deep Learning, Deep learning, Accuracy, Malware, Reliability, Malware Detection, Android, Operating systems, Neural networks, Convolution Neural Networks, Convolutional neural networks, DEX bytecode, Market research, Residual neural networks, ResNet50, VGG19},
	pages = {1--5},
}

@inproceedings{krishnaswamy_mbased_2024,
	title = {{MBASED}: {Practical} {Simplifications} of {Mixed} {Boolean}-{Arithmetic} {Obfuscation}},
	doi = {10.1109/URTC65039.2024.10937558},
	abstract = {Mixed Boolean-Arithmetic (MBA) obfuscation is a technique that complicates boolean expressions by combining arithmetic and boolean operations. It obstructs the process of reverse engineering by making code more difficult to analyze. This paper presents the Binary Ninja plugin, MBASED (Mixed Boolean-Arithmetic Simplification Engine for Deobfuscation), which performs MBA deobfuscation on C programs. It utilizes novel simplification methods that take ideas from compiler construction and simplification performed on parse trees. MBASED is the first practical implementation of MBA deobfuscation in an industry-grade decompiler. Its extensible framework allows users to incorporate their own simplification passes.},
	booktitle = {2024 {IEEE} {MIT} {Undergraduate} {Research} {Technology} {Conference} ({URTC})},
	author = {Krishnaswamy, Nitin and Mandadi, Sanjana and Nelson, Micah and Slater, Timothy and Liu, Benson},
	month = oct,
	year = {2024},
	keywords = {Codes, Security, Reverse engineering, Malware, Engines, reverse engineering, security, Libraries, decompiler, Arithmetic, deobfuscation, Mixed Boolean-Arithmetic},
	pages = {1--5},
}

@inproceedings{zhao_research_2024,
	title = {Research on {Security} {Protection} {Mechanism} of {Android} {APP}},
	doi = {10.1109/ICICSE61805.2024.10625666},
	abstract = {Based on the idea of a digital signature, an Android program protection scheme is proposed. First, check whether there is a security file when the program starts, download it from the server if there is no security file, and perform subsequent verification if there is; Secondly, the gatekeeper mechanism is used to determine whether the installation address of the software is from the specified server by asking, and if it is, the subsequent verification is made, and if it is not, the program is directly exited; Then, the signature authentication is performed based on the server, and the hash value in the security file is compared with the decrypted hash value. If the hash value is consistent, the file is not tampered with; Finally, the integrity of the file is verified, and the installation is allowed if every value in the security file is verified. The application protection method can identify the installation files from unknown sources and prevent the installation. The whole process adopts the method of JNI call, the application core code is placed in the Java layer, the digital signature mechanism and integrity verification are placed in the Native layer, and the Java layer is packaged into the.so library, the Java layer calls the.so library through the JNI, which can effectively prevent decompilation.},
	booktitle = {2024 4th {International} {Conference} on {Information} {Communication} and {Software} {Engineering} ({ICICSE})},
	author = {Zhao, Shu-Han and Li, Yong-Zhen and Wang, Zhen-Zhen and Jin, Zhe-Xue},
	month = may,
	year = {2024},
	keywords = {Security, Malware, Java, Operating systems, Logic gates, Libraries, Servers, “Gatekeeper mechanism”, Integrity verification, JNI technology, Security file, Server-based digital signature},
	pages = {35--38},
}

@article{chu_security_2019,
	title = {Security and {Privacy} {Analyses} of {Internet} of {Things} {Children}’s {Toys}},
	volume = {6},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2018.2866423},
	abstract = {This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many Internet of Things toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks.},
	number = {1},
	journal = {IEEE Internet of Things Journal},
	author = {Chu, Gordon and Apthorpe, Noah and Feamster, Nick},
	month = feb,
	year = {2019},
	keywords = {Privacy, Mobile applications, Internet of Things, Servers, Authentication, Data security, Internet of Things (IoT), privacy, Toy manufacturing industry},
	pages = {978--985},
	annote = {8 pages, 8 figures; publication version},
}

@inproceedings{bragagnolo_smartinspect_2018,
	title = {{SmartInspect}: solidity smart contract inspector},
	doi = {10.1109/IWBOSE.2018.8327566},
	abstract = {Solidity is a language used for smart contracts on the Ethereum blockchain. Smart contracts are embedded procedures stored with the data they act upon. Debugging smart contracts is a really difficult task since once deployed, the code cannot be reexecuted and inspecting a simple attribute is not easily possible because data is encoded. In this paper, we address the lack of inspectability of a deployed contract by analyzing contract state using decompilation techniques driven by the contract structure definition. Our solution, SmartInspect, also uses a mirror-based architecture to represent locally object responsible for the interpretation of the contract state. SmartInspect allows contract developers to better visualize and understand the contract stored state without needing to redeploy, nor develop any ad-hoc code.},
	booktitle = {2018 {International} {Workshop} on {Blockchain} {Oriented} {Software} {Engineering} ({IWBOSE})},
	author = {Bragagnolo, Santiago and Rocha, Henrique and Denker, Marcus and Ducasse, Stephane},
	month = mar,
	year = {2018},
	keywords = {Tools, Debugging, Indexes, Smart Contracts, Blockchain, Inspection, Contracts, Inspecting, Solidity},
	pages = {9--18},
}

@inproceedings{akarsh_deep_2019,
	title = {Deep {Learning} {Framework} and {Visualization} for {Malware} {Classification}},
	issn = {2575-7288},
	doi = {10.1109/ICACCS.2019.8728471},
	abstract = {In this paper we propose a deep learning framework for classification of malware. There has been an enormous increase in the volume of malware generated lately which represents a genuine security danger to organizations and people. So as to battle the expansion of malwares, new strategies are needed to quickly identify and classify malware. Malimg dataset, a publicly available benchmark data set was used for the experimentation. The architecture used in this work is a hybrid cost-sensitive network of one-dimensional Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) network which obtained an accuracy of 94.4\%, an increase in performance compared to work done by [1] which got 84.9\%. Hyper parameter tuning is done on deep learning architecture to set the parameters. A learning rate of 0.01 was taken for all experiments. Train-test split of 70-30\% was done during experimentation. This facilitates to find how well the models perform on imbalanced data sets. Usual methods like disassembly, decompiling, de-obfuscation or execution of the binary need not be done in this proposed method. The source code and the trained models are made publicly available for further research.},
	booktitle = {2019 5th {International} {Conference} on {Advanced} {Computing} \& {Communication} {Systems} ({ICACCS})},
	author = {Akarsh, S. and Simran, K. and Poornachandran, Prabaharan and Menon, Vijay Krishna and Soman, K.P.},
	month = mar,
	year = {2019},
	keywords = {Deep learning, Feature extraction, Malware, deep learning, machine learning, Computer architecture, Convolution, cost-sensitive learning, image processing, Logic gates},
	pages = {1059--1063},
}

@article{gao_malware_2022,
	title = {Malware {Detection} by {Control}-{Flow} {Graph} {Level} {Representation} {Learning} {With} {Graph} {Isomorphism} {Network}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3215267},
	abstract = {With society’s increasing reliance on computer systems and network technology, the threat of malicious software grows more and more serious. In the field of information security, malware detection has been a key problem that academia and industry are committed to solving. Machine learning is an effective method for processing large-scale data, such as the Gradient Boosting Decision Tree (GBDT) and deep neural network technology. Although these types of detection methods can deal with cyber threats, most feature extraction methods are based on the statistical information features of portable executable (PE) files and thus lack the decompiled code and execution flow structure of the PE samples. Therefore, we propose a Control-Flow Graph (CFG)- and Graph Isomorphism Network (GIN)-based malware classification system. The feature vectors of CFG basic blocks are generated using the large-scale pre-trained language model MiniLM, which is beneficial for the GIN to further learn and compress the CFG-based representation, and classified with multi-layer perceptron. In addition, we evaluated the effectiveness of the representation under different dimensions and classifiers. To evaluate our method, we set up a CFG-based malware detection graph dataset from a PE file of the Blue Hexagon Open Dataset for Malware Analysis (BODMAS), which we call the Malware Geometric Binary Dataset (MGD-BINARY) and collected the experimental results of CFG representation in different dimensions and classifier settings. The evaluation results show that our proposal has proved an Accuracy metric of 0.99160 and achieved 0.99148 Area Under the Curve (AUC) results.},
	journal = {IEEE Access},
	author = {Gao, Yun and Hasegawa, Hirokazu and Yamaguchi, Yukiko and Shimada, Hajime},
	year = {2022},
	keywords = {Codes, Analytical models, Feature extraction, Malware, machine learning, Machine learning, Malware detection, Data mining, graph classification, Graphics, Natural language processing, static analysis},
	pages = {111830--111841},
}

@inproceedings{zhang_osprey_2021,
	title = {{OSPREY}: {Recovery} of {Variable} and {Data} {Structure} via {Probabilistic} {Analysis} for {Stripped} {Binary}},
	issn = {2375-1207},
	doi = {10.1109/SP40001.2021.00051},
	abstract = {Recovering variables and data structure information from stripped binary is a prominent challenge in binary program analysis. While various state-of-the-art techniques are effective in specific settings, such effectiveness may not generalize. This is mainly because the problem is inherently uncertain due to the information loss in compilation. Most existing techniques are deterministic and lack a systematic way of handling such uncertainty. We propose a novel probabilistic technique for variable and structure recovery. Random variables are introduced to denote the likelihood of an abstract memory location having various types and structural properties such as being a field of some data structure. These random variables are connected through probabilistic constraints derived through program analysis. Solving these constraints produces the posterior probabilities of the random variables, which essentially denote the recovery results. Our experiments show that our technique substantially outperforms a number of state-of-the-art systems, including IDA, Ghidra, Angr, and Howard. Our case studies demonstrate the recovered information improves binary code hardening and binary decompilation.},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Zhang, Zhuo and Ye, Yapeng and You, Wei and Tao, Guanhong and Lee, Wen-chuan and Kwon, Yonghwi and Aafer, Yousra and Zhang, Xiangyu},
	month = may,
	year = {2021},
	keywords = {Binary codes, Privacy, Systematics, Reverse-Engineering, Binary-Analysis, Data structures, Probabilistic logic, Probabilistic-Analysis, Random variables, Type-Inference, Uncertainty},
	pages = {813--832},
}

@article{costa_lightweight_2023,
	title = {A {Lightweight} and {Multi}-{Stage} {Approach} for {Android} {Malware} {Detection} {Using} {Non}-{Invasive} {Machine} {Learning} {Techniques}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3296606},
	abstract = {Android has been a constant target of cybercriminals that try to attack one of the most used operating systems, commonly using malicious applications (denominated malware) that, once installed on a device, can harm users in several ways. Existing malware detection solutions are usually invasive as they obtain classification features by performing reverse engineering, decompilation, or disassembly of the analyzed application, which infringes licenses and terms of use of applications. In addition, these solutions often employ a single machine learning (ML) model to detect various types of malware, resulting in several false alarms. In this context, we propose an approach to detect Android malware consisting of a set of specific-type detectors in which each one performs a multi-stage analysis, based on rules and ML techniques, in different phases of the application cycle (before and after its installation). Our approach also differs from state-of-the-art solutions by being non-invasive, since it leverages a process to obtain application’s features that does not infringe licenses and terms of use of applications. In addition, according to experiments performed on a real Android smartphone, our proposal presents the following additional advantages over state-of-the-art solutions: a more efficient process to classify applications that is three times faster and requires ten times less CPU usage in some cases (saving device energy); and a better detection performance, with higher balanced accuracy, nine times less false positive cases, and ten times less false negative cases.},
	journal = {IEEE Access},
	author = {Costa, Leonardo da and Moia, Vitor},
	year = {2023},
	keywords = {Feature extraction, Malware, machine learning, Android, Operating systems, Smart phones, Proposals, malware detection, Androids, Behavioral sciences, Detectors, multi-stage analysis, non-invasive feature extraction, Noninvasive treatment},
	pages = {73127--73144},
}

@inproceedings{alsabbagh_stealth_2021,
	title = {A {Stealth} {Program} {Injection} {Attack} against {S7}-300 {PLCs}},
	volume = {1},
	doi = {10.1109/ICIT46573.2021.9453483},
	abstract = {Industrial control systems (ICSs) consist of programmable logic controllers (PLCs) which communicate with an engineering station on one side, and control a certain physical process on the other side. Siemens PLCs, particularly S7-300 controllers, are widely used in industrial systems, and modern critical infrastructures heavily rely on them. But unfortunately, security features are largely absent in such devices or ignored/disabled because security is often at odds with operations. As a consequence of the already reported vulnerabilities, it is possible to leverage PLCs and perhaps even the corporate IT network. In this paper we show that S7-300 PLCs are vulnerable and demonstrate that exploiting the execution process of the logic program running in a PLC is feasible. We discuss a replay attack that compromises the password protected PLCs, then we show how to retrieve the Bytecode from the target and decompile the Bytecode to STL source code. Afterwards we present how to conduct a typical injection attack showing that even a very tiny modification in the code is sufficient to harm the target system. Finally we combine the replay attack with the injection approach to achieve a stronger attack – the stealth program injection attack – which can hide the previous modification by engaging a fake PLC, impersonating the real infected device. For real scenarios, we implemented all our attacks on a real industrial setting using S7-300 PLC. We eventually suggest mitigation approaches to secure systems against such threats.},
	booktitle = {2021 22nd {IEEE} {International} {Conference} on {Industrial} {Technology} ({ICIT})},
	author = {Alsabbagh, Wael and Langendörfer, Peter},
	month = mar,
	year = {2021},
	keywords = {Security, Process control, Programmable logic devices, Conferences, Critical infrastructure, Fake PLC, Injection Attack, Password, Replay Attack, S7-300 PLCs, Stealthy Attack},
	pages = {986--993},
}

@inproceedings{zhao_deep_2018,
	title = {Deep {Neural} {Network} {Based} on {Android} {Mobile} {Malware} {Detection} {System} {Using} {Opcode} {Sequences}},
	issn = {2576-7828},
	doi = {10.1109/ICCT.2018.8600052},
	abstract = {Malware detection is more challenging due to the increase in android malicious programs and the current problems of android malicious detection. This paper proposes an android mobile malware detection system based on deep neural network, a novel malware detection method which uses optimized deep Convolutional Neural Network to learn from opcode sequences. In the proposed detection system, the optimized Convolutional Neural Network is trained multiple times by the raw operation code sequence extracted from the decompiled android file, so that the feature information can be effectively learned and the malicious program can be detected more accurately. More critically, the k-max pooling method with better results was adopted in the pooling operation phase, and which improves the detection effect of the proposed method. The experimental results show that the detection system achieved the accuracy of 99\%, which is 2\%-11 \% higher than the accuracy of the machine learning detection algorithms when using the same dataset. It also ensures that the indicators such as Fl-score, Recall and Precision are maintained above 97\%.},
	booktitle = {2018 {IEEE} 18th {International} {Conference} on {Communication} {Technology} ({ICCT})},
	author = {Zhao, Lichao and Li, Dan and Zheng, Guangcong and Shi, Wenbo},
	month = oct,
	year = {2018},
	keywords = {Feature extraction, Malware, Smart phones, Convolutional neural networks, Data mining, Convolutional Neural Network, k-max pooling, malware detection, Matrix converters, opcode},
	pages = {1141--1147},
}

@article{rodriguez-bazan_android_2023,
	title = {Android {Ransomware} {Analysis} {Using} {Convolutional} {Neural} {Network} and {Fuzzy} {Hashing} {Features}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3328314},
	abstract = {Most of the time, cybercriminals look for new ways to bypass security controls by improving their attacks. In the 1980s, attackers developed malware to kidnap user data by requesting payments. Malware is called a ransomware. Recently, they have demanded payment in Bitcoin or any other cryptocurrency. Ransomware is one of the most dangerous threats on the Internet, and this type of malware could affect almost all devices. Malware cipher device data, making them inaccessible to users. In this study, a new method for Android ransomware classification was proposed. This method implements a Convolutional Neural Network (CNN) for malware classification based on images. This paper presents a novel method for transforming an Android Application Package (APK) into a grayscale image. The image creation relies on using Natural Language Processing (NLP) techniques for text cleaning and Fuzzy Hashing to represent the decompiled code from the APK in a set of hashes after preprocessing using NLP techniques. The image is composed of n fuzzy hashes that represent the APK. The method was tested using a dataset of 7,765 Android ransomware samples obtained from external researchers and public sources. The accuracy of the proposed method was higher than that of other methods in the literature.},
	journal = {IEEE Access},
	author = {Rodriguez-Bazan, Horacio and Sidorov, Grigori and Escamilla-Ambrosio, Ponciano Jorge},
	year = {2023},
	keywords = {Deep learning, deep learning, Operating systems, Classification algorithms, Convolutional neural networks, Natural language processing, Android ransomware, Androids, convolutional neural network, fuzzy hashing, Fuzzy systems, malware classification, Matched filters, Metadata, ransomware, Ransomware, XML},
	pages = {121724--121738},
}

@article{nethala_deep_2025,
	title = {A {Deep} {Learning}-{Based} {Ensemble} {Framework} for {Robust} {Android} {Malware} {Detection}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3551152},
	abstract = {The exponential growth of Android applications has resulted in a surge of malware threats, posing severe risks to user privacy and data security. To address these challenges, this study introduces a novel malware detection approach utilizing an ensemble of Convolutional Neural Networks (CNNs) for enhanced classification accuracy. The methodology incorporates a multi-phase process, starting with the extraction and preprocessing of APK (Android app) files. The preprocessing phase involves decompressing, decompiling, and transforming the APK files into bytecode and Dex files. The extracted byte data is converted into 1D vectors and reshaped into 2D grayscale images, enabling efficient feature learning through CNNs. The proposed ensemble of CNN-based models undergoes comprehensive training, validation, and evaluation, demonstrating superior performance compared to existing approaches. We used two popular Android datasets to evaluate the performance of our proposed model. Specifically, the model achieves an accuracy of 98.65\%, F1-score of 96.43\% on the Drebin dataset and attains 97.91\% accuracy, 96.73\% of F1-score on the AMD dataset. These results confirm the mode’s ability to effectively identify Android malware with high precision and reliability, outperforming traditional techniques. This research not only underscores the potential of our proposed approach in malware detection but also sets a foundation for future advancements. Future efforts will focus on real-time malware detection, integration with mobile security frameworks, and evaluation across diverse datasets to ensure adaptability to emerging malware threats.},
	journal = {IEEE Access},
	author = {Nethala, Sainag and Chopra, Pronoy and Kamaluddin, Khaja and Alam, Shahid and Alharbi, Soltan and Alsaffar, Mohammad},
	year = {2025},
	keywords = {Deep learning, Accuracy, Feature extraction, Malware, deep learning, Static analysis, machine learning, Machine learning, Support vector machines, Computational modeling, malware classification, Android malware detection, attention mechanism, convolutional neural networks, ensemble learning, Meta-CNN, Random forests, Real-time systems},
	pages = {46673--46696},
}

@inproceedings{wang_smart_2022,
	title = {Smart {Contract} {Vulnerability} {Detection} for {Educational} {Blockchain} {Based} on {Graph} {Neural} {Networks}},
	doi = {10.1109/IEIR56323.2022.10050059},
	abstract = {With the development of blockchain technology, more and more attention has been paid to the intersection of blockchain and education, and various educational evaluation systems and E-learning systems are developed based on blockchain technology. Among them, Ethereum smart contract is favored by developers for its “event-triggered” mechanism for building education intelligent trading systems and intelligent learning platforms. However, due to the immutability of blockchain, published smart contracts cannot be modified, so problematic contracts cannot be fixed by modifying the code in the educational blockchain. In recent years, security incidents due to smart contract vulnerabilities have caused huge property losses, so the detection of smart contract vulnerabilities in educational blockchain has become a great challenge. To solve this problem, this paper proposes a graph neural network (GNN) based vulnerability detection for smart contracts in educational blockchains. Firstly, the bytecodes are decompiled to get the opcode. Secondly, the basic blocks are divided, and the edges between the basic blocks according to the opcode execution logic are added. Then, the control flow graphs (CFG) are built. Finally, we designed a GNN-based model for vulnerability detection. The experimental results show that the proposed method is effective for the vulnerability detection of smart contracts. Compared with the traditional approaches, it can get good results with fewer layers of the GCN model, which shows that the contract bytecode and GCN model are efficient in vulnerability detection.},
	booktitle = {2022 {International} {Conference} on {Intelligent} {Education} and {Intelligent} {Research} ({IEIR})},
	author = {Wang, Zhifeng and Wu, Wanxuan and Zeng, Chunyan and Yao, Jialong and Yang, Yang and Xu, Hongmin},
	month = feb,
	year = {2022},
	keywords = {Source coding, Semantics, bytecode, Education, Predictive models, educational blockchain, Graph neural networks, Image edge detection, smart contract, Smart contracts, vulnerability detection},
	pages = {8--14},
}

@article{zhang_bian_2023,
	title = {{BiAn}: {Smart} {Contract} {Source} {Code} {Obfuscation}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2023.3298609},
	abstract = {With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174\% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8\% and 40.5\% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50\% of cases for ten widely-used static analysis detection tools.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhang, Pengcheng and Yu, Qifan and Xiao, Yan and Dong, Hai and Luo, Xiapu and Wang, Xiao and Zhang, Meng},
	month = sep,
	year = {2023},
	keywords = {Codes, Security, Source coding, source code, obfuscation, Complexity theory, Intellectual property, smart contract, Smart contracts, Layout, Ethereum, Blockchain},
	pages = {4456--4476},
}

@article{zhang_fault_2022,
	title = {Fault {Diagnosis} of {Power} {Transformer} {Based} on {SSA}—{MDS} {Pretreatment}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3202982},
	abstract = {Aiming at the problems of coupling between transformer input characteristics and low accuracy of transformer fault diagnosis, SSA-MDS and other soft technologies are used to analyze the key characteristics of transformer faults, so as to improve the accuracy of transformer fault diagnosis. The SSA algorithm cascade MDS algorithm to process the DGA data is proposed. Subsequently, the TSSA-RF model is introduced to classify the DGA data. The DGA data is first mapped to a high-dimensional space. Next, the optimal feature subset is encoded using the SSA algorithm to reduce irrelevant and redundant features. In this study, the correlation between the optimal feature dimension and the transformer fault diagnosis accuracy is investigated. the expression of the optimal feature subset is obtained by decompiling the SSA operator. The pre-processed data are classified using the RF model, and the TSSA -RF model for classifying the DGA data is found with the highest accuracy through the comparison of different optimization algorithms. After the RF model is optimized using the TSSA algorithm, its accuracy increases by 7.89\%, and the accuracy of the TSSA -RF model is obtained as 92.11\%. The example results show that compared with the original data, the proposed data processing algorithm improves the diagnostic accuracy of transformer by 11.97 \% in the RF model. Compared with multiple preprocessing methods, SSA-MDS has the highest accuracy. Compared with the original data, the accuracy of TSSA-RF model increases by 11.64 \%.},
	journal = {IEEE Access},
	author = {Zhang, Mei and Chen, Wanli},
	year = {2022},
	keywords = {Optimization, Data models, feature extraction, Classification algorithms, Classification tree analysis, fault diagnosis, Fault diagnosis, Oil insulation, Power transformer, Power transformer insulation, TSSA algorithm, RF model, RF∼model},
	pages = {92505--92515},
}

@inproceedings{han_research_2021,
	title = {Research on {APT} {Attack} {Detection} {Technology} {Based} on {DenseNet} {Convolutional} {Neural} {Network}},
	doi = {10.1109/CISAI54367.2021.00091},
	abstract = {The detection of malicious code and variants of advanced persistent threat(APT) attacks is the main way to deal with APT attacks at this stage. APT attack organizations usually use code deformation, shelling, obfuscation and other methods to avoid detection to bypass APT attack malicious code detection. Aiming at the status quo, this paper proposes an APT attack detection scheme based on DenseNet convolutional neural network. First, the binary sample of the malicious code of the APT attack are preprocessed with some operations such as decompression and decompilation. APT attack malicious code samples are running in a sandbox with anti-code escaping technology, and the acquired data are converted into grayscale images. Then, we perform feature extraction and family clustering operations on the pre-processed image. Finally, the DenseNet convolutional neural network model is trained and tested on the sample data of the APT attack malicious code of eight families. The experimental results show that the average accuracy of the proposed scheme for the detection of APT attack malicious code and its variants can reach 98.84\%. While cutting off the APT attack chain, it has a high detection accuracy.},
	booktitle = {2021 {International} {Conference} on {Computer} {Information} {Science} and {Artificial} {Intelligence} ({CISAI})},
	author = {Han, Xiang and Li, Chao and Li, Xin and Lu, Tianliang},
	month = sep,
	year = {2021},
	keywords = {Data models, Feature extraction, Malware, Organizations, deep learning, Data preprocessing, Information science, advanced persistent threat attack, DenseNet convolutional neural network, Gray-scale, gray-scale image, malicious code},
	pages = {440--448},
}

@inproceedings{zhang_novel_2024,
	title = {A {Novel} {Approach} to {Malicious} {Code} {Detection} {Using} {CNN}-{BiLSTM} and {Feature} {Fusion}},
	doi = {10.1109/RICAI64321.2024.10911787},
	abstract = {With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness.This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware.In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability.The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.},
	booktitle = {2024 6th {International} {Conference} on {Robotics}, {Intelligent} {Control} and {Artificial} {Intelligence} ({RICAI})},
	author = {Zhang, Lixia and Liu, Tianxu and Shen, Kaihui and Chen, Cheng},
	month = feb,
	year = {2024},
	keywords = {Deep Learning, Deep learning, Training, Accuracy, Feature extraction, Malware, Malware Detection, Support vector machines, Transfer learning, Adaptation models, Technological innovation, Explainable AI, Feature Fusion},
	pages = {745--755},
}

@inproceedings{hua_mmguard_2021,
	title = {{MMGuard}: {Automatically} {Protecting} {On}-{Device} {Deep} {Learning} {Models} in {Android} {Apps}},
	doi = {10.1109/SPW53761.2021.00019},
	abstract = {On-device deep learning models have shown growing popularity in mobile apps, which allows offline model inference while preserving user privacy. However, on-device deep learning models also introduce security challenges, i.e., the trained models can be easily stolen or even be tampered by attackers. Recent studies suggested that most of the on-device models are lacking of sufficient protection, i.e., can be stolen by decompiling the apps directly. In this work, we present MMGUARD, an automated framework for building mutual authentication between Android apps and deep neural network models, which can thus protect on-device models from being easily attacked (piracy and tampering). Unlike existing model protect methods, our approach does not require model re-training or any prior knowledge of the training data. The key idea of MMGUARD is to verify the deep learning model in the app before inference, i.e., feeding owner- and apprelated information to it, which can greatly increase the effort of model hacking. We evaluate MMGUARD on 5 popular image classification DNNs and 43 real world Android apps from Google Play. Experiment results suggest that MMGUARD introduces negligible latency on models and can be automatically applied to real world apps.},
	booktitle = {2021 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Hua, Jiayi and Li, Yuanchun and Wang, Haoyu},
	month = may,
	year = {2021},
	keywords = {Privacy, Data models, Deep learning, Training, User experience, deep learning, Neural networks, mobile app, model attack., Model protection, on-device model, Training data},
	pages = {71--77},
}

@article{chen_android_2024,
	title = {Android {Malware} {Family} {Clustering} {Based} on {Multiple} {Features}},
	volume = {73},
	issn = {1558-1721},
	doi = {10.1109/TR.2023.3332090},
	abstract = {Familiar analysis for malware plays an important role in comprehending the diversity of malicious behaviors and identifying the emerging security threats. Existing studies mainly focus on classifying malware into known families by supervised learning. However, these methods face two main challenges, 1) the lack of a large amount of labeled data and 2) the poor effectiveness in identifying unknown families of malware. To overcome these challenges, we propose a new method called multiple features (MulFC) based on unsupervised learning. In the method, we first leverage a decompiling tool to extract multiple features, including manifest features, application programming interface (API) features, and opcode features. Then, the opcode features are preprocessed to filter out the redundant ones to reduce the calculation cost. After that, we adopt the Jaccard index to calculate the similarities between malware and construct a malware network. Finally, InfoMap is applied to perform the clustering on the basis of the malware network. Overall, MulFC does not require the use of labeled data and can identify unknown families of malware. Experiments are conducted on two datasets for the performance evaluation of MulFC. The experimental results show that MulFC achieves 0.810 in terms of normalized mutual information, 0.576 in terms of adjusted rand index, 0.620 in terms of the Fowlkes–Mallows index, and 0.805 in terms of V-measure on average, and outperforms the state-of-the-art baseline method by 0.060, 0.054, 0.038, and 0.065, respectively.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Chen, Xin and Yu, Dongjin and Cai, Xinxin and Jiang, He and Yu, Haihua},
	month = jun,
	year = {2024},
	keywords = {Feature extraction, Malware, Operating systems, Data mining, Androids, Android malware, Costs, Indexes, InfoMap, malware family clustering, multiple features, unsupervised learning, Unsupervised learning},
	pages = {1202--1215},
}

@inproceedings{s_reverse_2023,
	title = {Reverse {Engineering} techniques for {Android} systems: {A} {Systematic} approach},
	doi = {10.1109/GCON58516.2023.10183629},
	abstract = {The Google Play Store has over 3 million Android apps available for download, making it the largest app store in the world. With such a vast number of apps available, there is a growing need for security and analysis, which has led to an increase in the use of reverse engineering tools and techniques. In this paper we provide an overview of the main tools used in Android reverse engineering, which are used for various activities including decompiling apk files, converting Java into Intermediate Languages (IL), and inspecting an application's Java code. The study's finding shows App Cloner and Mt manager tools have the highest precision and capabilities in optimizing and analyzing the security of the applications. This paper concludes with a discussion of the most effective reverse engineering tools and the future prospects of improvements possible for these tools.},
	booktitle = {2023 {IEEE} {Guwahati} {Subsection} {Conference} ({GCON})},
	author = {S, Hrushik Raj and P, Thejaswini and Nandi, Sukumar},
	month = jun,
	year = {2023},
	keywords = {Codes, Reverse engineering, Java, Systematics, Internet, android application, code obfuscation, reverse engineering tools, Software systems, Surveys},
	pages = {1--6},
}

@inproceedings{tirkey_anatomizing_2019,
	title = {Anatomizing {Android} {Malwares}},
	issn = {2640-0715},
	doi = {10.1109/APSEC48747.2019.00067},
	abstract = {Android OS being the popular choice of majority users also faces the constant risk of breach of confidentiality, integrity and availability (CIA). Effective mitigation efforts needs to identified in order to protect and uphold the CIA triad model, within the android ecosystem. In this paper, we propose a novel method of android malware classification using Object-Oriented Software Metrics and machine learning algorithms. First, android apps are decompiled and Object-Oriented Metrics are obtained. VirusTotal service is used to tag an app either as malware or benign. Object-Oriented Metrics and malware tag are clubbed together into a dataset. Eighty different machine-learned models are trained over five thousand seven hundred and seventy four android apps. We evaluate the performance and stability of these models using it's malware classification accuracy and AUC (area under ROC curve) values. Our method yields an accuracy and AUC of 99.83\% and 1.0 respectively.},
	booktitle = {2019 26th {Asia}-{Pacific} {Software} {Engineering} {Conference} ({APSEC})},
	author = {Tirkey, Anand and Mohapatra, Ramesh Kumar and Kumar, Lov},
	month = feb,
	year = {2019},
	keywords = {Tools, Feature extraction, Malware, Measurement, machine learning, Support vector machines, Neural networks, malware detection, android, object oriented metrics, Object oriented modeling},
	pages = {450--457},
}

@inproceedings{kurniawan_hybrid_2024,
	title = {Hybrid {Machine} {Learning} {Model} for {Anticipating} {Cyber} {Crime} {Malware} in {Android}: {Work} on {Progress}},
	doi = {10.1109/EECSI63442.2024.10776359},
	abstract = {Improvements in information technology bring new challenges in cyber security, especially on the Android platform which is the main target of malware attacks. The National Cyber and Crypto Agency (BSSN) as the national cybersecurity institute recorded millions of attacks involving the Android Package Kit (.apk) application for electronic wedding invitations in Indonesia. This research aims to develop a hybrid machine learning model to detect and anticipate malware on Android devices, using algorithms such as Support Vector Machine (SVM), Random Forest (RF), and K-nearest neighbors (KNN). The main challenges are the limited number of representative datasets and the lack of effective detection techniquesThis research utilizes a primary server with a virtual machine (VM) to analyze the security of Android applications using mobSF and Frida, involving decompilation, TLS testing, and sensitive feature extraction. The system also depends on an Android emulator and Android Debug Bridge (ADB) for simulation, along with PostgreSQL, Job Scheduler, and Remote Desktop Protocol (RDP) for data management and task scheduling. The dataset comprises 1314 malware samples and 770 benign samples, totaling 2048 Android applications, with an average analysis time of 35 minutes per application. Through data collection and pre-processing, as well as model training and evaluation, it is hoped that the proposed framework can improve the accuracy of malware detection, making a significant contribution in protecting Android users from ever-evolving cyber threats. After performing a comparison using the algorithms mentioned above, it was found that the Random Forest algorithm showed the best performance with an accuracy of 97.11\%, a precision of 97.22\%, and a recall of 97.11\%.},
	booktitle = {2024 11th {International} {Conference} on {Electrical} {Engineering}, {Computer} {Science} and {Informatics} ({EECSI})},
	author = {Kurniawan, Fandi and Stiawan, Deris and Antoni, Darius and Heriyanto, Ahmad and Idris, Mohd. Yazid and Budiarto, Rahmad},
	month = sep,
	year = {2024},
	keywords = {Accuracy, Feature extraction, Malware, Nearest neighbor methods, Radio frequency, machine learning, Support vector machines, Operating systems, malware detection, Random forests, Machine learning algorithms, Computer crime, hybrid analysis, Malware android},
	pages = {499--505},
}

@inproceedings{ogwara_mobdroid_2020,
	title = {{MOBDroid}: {An} {Intelligent} {Malware} {Detection} {System} for {Improved} {Data} {Security} in {Mobile} {Cloud} {Computing} {Environments}},
	issn = {2474-154X},
	doi = {10.1109/ITNAC50341.2020.9315052},
	abstract = {We propose an intelligent malware detection system (MOBDroid) that aims to protect the end-user's mobile device (MD) in mobile cloud computing (MCC) environment. MOBDroid utilizes the Android Operating System (OS) permission-based security system. The APK files of 28,306 benign and malicious applications (apps) collected from the AndroZoo and RmvDroid malware repositories were used in the system development process. The apps were decompiled in order to extract their manifest files and construct a dataset comprising the permissions requested by each of the apps. We identified some unique permissions that could be used to distinguish between malicious and benign apps and performed a series of experiments using a machine learning (ML) model; the model drew on the ML.net library and was implemented in C\#.net. In the experiments conducted, we obtained classification accuracy of 96.89\%, a detection rate of 98.65\%, and false negative rate of 1.35\%. The results indicate that our model compares very favorably to other models reported in the extant literature.},
	booktitle = {2020 30th {International} {Telecommunication} {Networks} and {Applications} {Conference} ({ITNAC})},
	author = {Ogwara, Noah Oghenefego and Petrova, Krassie and Bobby Yang, Mee Loong},
	month = jan,
	year = {2020},
	keywords = {Data models, Feature extraction, Malware, machine learning, Classification algorithms, malware detection, Cloud computing, Communications technology, data security, MOBDroid, Mobile applications, mobile cloud computing, mobile devices},
	pages = {1--6},
}

@inproceedings{xia_bcontext2name_2023,
	title = {{BContext2Name}: {Naming} {Functions} in {Stripped} {Binaries} with {Multi}-{Label} {Learning} and {Neural} {Networks}},
	issn = {2693-8928},
	doi = {10.1109/CSCloud-EdgeCom58631.2023.00037},
	abstract = {Conducting binary function naming helps reverse engineers understand the internal workings of the code and perform malicious code analysis without accessing the source code. However, the loss of debugging information poses the challenge of insufficient high-level semantic information description for stripping binary code function naming. Meanwhile, the existing binary function naming scheme has one function label for only one sample. The long-tail effect of function labels for a single sample makes the machine learning-based prediction models face the challenge. To obtain a function correlation label and improve the propensity score of uncommon tail labels, we propose a multi-label learning-based binary function naming model BContext2Name. This model automatically generates relevant labels for binary function naming by function context information with the help of PfastreXML model. The experimental results show that BContext2Name can enrich function labels and alleviate the long-tail effect that exists for a single sample class. To obtain high-level semantics of binary functions, we align pseudocode and basic blocks based on disassembly and decompilation, identify concrete or abstract values of API parameters by variable tracking, and construct API-enhanced control flow graphs. Finally, a seq2seq neural network translation model with attention mechanism is constructed between function multi-label learning and enhanced control flow graphs. Experiments on the dataset reveal that the F1 values of the BContext2Name model improve by 3.55\% and 15.23\% over the state-of-the-art XFL and Nero, respectively. This indicates that function multi-label learning can provide accurate labels for binary functions and can help reverse analysts understand the inner working mechanism of binary code. Code and data for this evaluation are available at https://github.com/CSecurityZhongYuan/BContext2Name.},
	booktitle = {2023 {IEEE} 10th {International} {Conference} on {Cyber} {Security} and {Cloud} {Computing} ({CSCloud})/2023 {IEEE} 9th {International} {Conference} on {Edge} {Computing} and {Scalable} {Cloud} ({EdgeCom})},
	author = {Xia, Bing and Ge, Yunxiang and Yang, Ruinan and Yin, Jiabin and Pang, Jianmin and Tang, Chongjun},
	month = jul,
	year = {2023},
	keywords = {Binary codes, Source coding, Semantics, Malware, Neural networks, Cloud computing, Binary code, Multi-Label learning, Neural network translation, Program analysis, Software reverse engineering, Tail},
	pages = {167--172},
}

@inproceedings{ayub_charlie_2024,
	title = {Charlie, {Charlie}, {Charlie} on {Industrial} {Control} {Systems}: {PLC} {Control} {Logic} {Attacks} by {Design}, {Not} by {Chance}},
	issn = {2765-8406},
	doi = {10.1109/HOST55342.2024.10545392},
	abstract = {Programmable logic controllers (PLCs) in industrial control systems (ICS) run a control logic program to monitor and control critical infrastructures in real-time, such as nuclear plants and power grids. Attackers target PLC control logic remotely to sabotage or disrupt physical processes. Network intrusion detection systems (IDS) are increasingly used to detect malicious control logic. This paper demonstrates that standard IDS features in a protocol message header and payload are not resilient for detecting (control logic) binary programs, such as entropy, n-gram, and decompilation. It identifies and utilizes a PLC design feature, redundant address pins (RAP), unexplored in the literature, to bypass IDS for injecting a small piece of programmable malicious code (PMC) into a PLC's control logic as an initial attack vector, allowing it to execute with every scan cycle. We propose three unique attack methods (GizmoSplit, BuffWarp, and EnigmaFlow) using PMC as a proof of concept that blends control logic with network traffic via payload encoding, small-size payloads, or sparse memory addressing. The GizmoSplit attack divides the control logic into small gadgets and writes them in random memory locations in a PLC; PMC modifies the stack with the location of the gadgets to execute them as return-oriented programming. The BuffWarp attack employs a small-size buffer where the attacker writes malicious code periodically to bypass stateful inspection at the payload level; PMC, in turn, keeps moving the buffer content to consecutive memory locations to execute. The EnigmaFlow attack encodes control logic and sends it to a PLC's typically unused memory region, which PMC decodes and executes. The evaluation results indicate that these attacks are stealthy and can subvert IDS utilizing standard message header and payload features. This work points to a research gap in intrusion detection that caters to control logic attacks exploiting PLC design features.},
	booktitle = {2024 {IEEE} {International} {Symposium} on {Hardware} {Oriented} {Security} and {Trust} ({HOST})},
	author = {Ayub, Adeen and Jo, Wooyeon and Ahmed, Irfan},
	month = may,
	year = {2024},
	keywords = {Codes, Process control, Feature extraction, Malware, Encoding, Control systems, Telecommunication traffic, control logic, ICS attacks, industrial control systems, intrusion detection systems, programmable logic controllers},
	pages = {182--193},
}

@inproceedings{mk_comprehensive_2025,
	title = {Comprehensive {Research} on {Mobile} {Application} {Security} {Assessment}},
	doi = {10.1109/ICCRTEE64519.2025.11052915},
	abstract = {The increasing use of mobile apps has raised generalized concerns regarding security threats, like malware and leakage of data. Most malicious applications masquerade as normal apps to steal private information from users. Conventional security approaches, both static analysis and dynamic analysis, have natural limitations when utilized individually. For the purpose of enhancing mobile security, this work suggests a Comprehensive Mobile Application Security Analysis Framework which combines static analysis, dynamic analysis, malware analysis, and reverse engineering techniques in the interest of identifying security defects in Android applications.The framework employs MoBSF for static analysis, Frida for dynamic instrumentation, VirusTotal for malware detection, and Apktool and Quark for reverse engineering. Apktool enables low-level decompilation of smali code, while Quark focuses on malware behavior detection through API call analysis. All these tools facilitate a comprehensive analysis of APK vulnerabilities, runtime threats, and malicious payloads.In this research paper, we explore various methodologies to analyze and improve mobile application security accuracy through static analysis, dynamic analysis, malware detection, and reverse engineering.This study shows how an integrated approach combining all four techniques improves detection accuracy, reducing false positives and uncovering hidden security threats. Results show that the hybrid approach effectively detects both known and unknown malware, making it an efficient approach for mobile application security assessments. Future enhance automation; explore machine learning enhancements for improved zero-day threat detection.},
	booktitle = {2025 {International} {Conference} on {Computational} {Robotics}, {Testing} and {Engineering} {Evaluation} ({ICCRTEE})},
	author = {MK, Nagarajan and Dinesh, Rayavarapu Lakshmi Narasimha and S, Ratna Selvan and Nagamaiah, Mothadaka and Anjaneyulu, Kolakani Venkata Prasanna},
	month = may,
	year = {2025},
	keywords = {Security, Reverse engineering, Reverse Engineering, Accuracy, Malware, Static analysis, Malware Detection, Runtime, Mobile applications, Dynamic Analysis, Computer viruses, Android security, APK security, Mobile security, Mobile Security, Threat assessment},
	pages = {1--6},
}

@inproceedings{bosse_tiny_2023,
	title = {Tiny {Machine} {Learning} {Virtualization} for {IoT} and {Edge} {Computing} using the {REXA} {VM}},
	doi = {10.1109/FiCloud58648.2023.00026},
	abstract = {Tiny Machine Learning is a new approach that is being used for data-driven prediction classification and regression on microcontrollers using local sensor data. The models are typically learned off-line and sent to the microcontroller for use as binary objects or frozen and converted static data. This approach is not universal or flexible. The REXA VM, which can virtualize embedded systems and sensor nodes and includes a general machine learning framework that supports arbitrary dynamic artificial neural network and decision tree models, is introduced in this study. The models are delivered as text files with highly compressed program code that are enclosed in code frames with embedded data (model parameters). The VM offers fundamental computations for ANN and DT models (Microservices). Using a decompiler, models can be updated (retrained) and sent to other nodes (mobile models). It can be demonstrated that virtualization using a bytecode machine and just-in-time compiler is still appropriate and effective for extremely low-resource processors.},
	booktitle = {2023 10th {International} {Conference} on {Future} {Internet} of {Things} and {Cloud} ({FiCloud})},
	author = {Bosse, Stefan and Polle, Christoph},
	month = aug,
	year = {2023},
	keywords = {Codes, Data models, Computational modeling, Internet of Things, Artificial neural networks, Embedded Systems, Microcontrollers, Microservices, Sensor Networks, Tiny ML, Virtual Machines, Virtualization},
	pages = {122--129},
}

@inproceedings{xia_firmware_2023,
	title = {A {Firmware} {Vulnerability} {Detection} {Method} {Based} on {Feature} {Filtering}},
	doi = {10.1109/ISPA-BDCloud-SocialCom-SustainCom59178.2023.00172},
	abstract = {Open source code reuse and code cross-platform deployment accelerate the spread of software vulnerabilities, and pose challenges for accurate detection of cross-platform vulnerabilities. The binary vulnerability similarity detection method based on machine learning lacks a fine-grained vulnerability similarity feature filtering mechanism, and cannot solve cross-function vulnerabilities triggered by data dependencies, resulting in a high vulnerability false positive rate in existing solutions, which limits practical applications. Software code vulnerabilities are usually caused by function key statements. Therefore, based on binary function decompilation, we identify the key statement that causes the vulnerability, analyze the cross-function data dependency of the key statements, extract the code similarity features and code vulnerability features of the key statements, and propose a feature filtering-based Binary vulnerability detection method SimFilter. SimFilter calculates the similarity of key statements by comparing the characteristics of function key statements, and infers the similarity of binary function vulnerabilities based on the similarity of key statements. The experimental results show that the vulnerability semantic features extracted by SimFilter are highly interpretable, and the accuracy rate is increased by 10.8\% compared with other models. At the same time, we analyzed and evaluated the impact of different similarity features on the accuracy of SimFilter, and gave the best empirical value of the SimFilter model in firmware vulnerability detection.},
	booktitle = {2023 {IEEE} {Intl} {Conf} on {Parallel} \& {Distributed} {Processing} with {Applications}, {Big} {Data} \& {Cloud} {Computing}, {Sustainable} {Computing} \& {Communications}, {Social} {Computing} \& {Networking} ({ISPA}/{BDCloud}/{SocialCom}/{SustainCom})},
	author = {Xia, Bing and Tang, Chongjun and Liu, Wenbo and Chu, Shihao and Dong, Yu},
	month = feb,
	year = {2023},
	keywords = {Codes, Source coding, Analytical models, Semantics, Feature extraction, Machine learning, Vulnerability Detection, Binary Similarity, Data Dependency, Filtering, Internet Of Things, Open Source Software},
	pages = {1069--1076},
}

@inproceedings{adams_blockchain_2024,
	title = {A {Blockchain} {Smart} {Contract} {Framework} {Using} {Interpreted} {Programming} {Languages} and {Decentralized} {Storage}},
	issn = {1558-058X},
	doi = {10.1109/SoutheastCon52093.2024.10500201},
	abstract = {We present a model for an alternative to Ethereum Virtual Machine (EVM) smart contract blockchain networks which relies on an interpreted programming language and a distributed file system. Specifically we present the model by integrating the Interplanetary File System (IPFS) for decentralized files storage and Python as the interpreted programming language. IPFS ensures that contract code does not change by using content/hash-based addressing. The address of the file can be passed between nodes to recreate the files locally as long as nodes are running an IPFS node as well. In this implementation, we design the model to work with an existing prototype blockchain which was built with IPFS integration. The use of Python to write the code helps ensure transparency as the code does not need to be compiled and thus would not need to be run through a decompiler to make deployed code human-readable. We describe how the data is extracted and validated from a transaction before the code is executed locally and it's respective state updated and stored on the blockchain.},
	booktitle = {{SoutheastCon} 2024},
	author = {Adams, Shawn C. and Zheng, Yuliang},
	month = mar,
	year = {2024},
	keywords = {Codes, Prototypes, Smart contracts, smart contracts, blockchain, Virtual machining, python, Blockchains, File systems, InterPlanetary File System, IPFS},
	pages = {222--230},
}

@inproceedings{borrello_customprocessingunit_2023,
	title = {{CustomProcessingUnit}: {Reverse} {Engineering} and {Customization} of {Intel} {Microcode}},
	issn = {2770-8411},
	doi = {10.1109/SPW59333.2023.00031},
	abstract = {Microcode provides an abstraction layer over the instruction set to decompose complex instructions into simpler micro-operations that can be more easily implemented in hardware. It is an essential optimization to simplify the design of x86 processors. However, introducing an additional layer of software beneath the instruction set poses security and reliability concerns. The microcode details are confidential to the manufacturers, preventing independent auditing or customization of the microcode. Moreover, microcode patches are signed and encrypted to prevent unauthorized patching and reverse engineering. However, recent research has recovered decrypted microcode and reverse-engineered read/write debug mechanisms on Intel Goldmont (Atom), making analysis and customization of microcode possible on a modern Intel microarchitecture. In this work, we present the first framework for static and dynamic analysis of Intel microcode. Building upon prior research, we reverse-engineer Goldmont microcode semantics and reconstruct the patching primitives for microcode customization. For static analysis, we implement a Ghidra processor module for decompilation and analysis of decrypted microcode. For dynamic analysis, we create a UEFI application that can trace and patch microcode to provide complete microcode control on Goldmont systems. Leveraging our framework, we reverse-engineer the confidential Intel microcode update algorithm and perform the first security analysis of its design and implementation. In three further case studies, we illustrate the potential security and performance benefits of microcode customization. We provide the first x86 Pointer Authentication Code (PAC) microcode implementation and its security evaluation, design and implement fast software breakpoints that are more than 1000x faster than standard breakpoints, and present constant-time microcode division, illustrating the potential security and performance benefits of microcode customization.},
	booktitle = {2023 {IEEE} {Security} and {Privacy} {Workshops} ({SPW})},
	author = {Borrello, Pietro and Easdon, Catherine and Schwarzl, Martin and Czerny, Roland and Schwarz, Michael},
	month = may,
	year = {2023},
	keywords = {Software, Software algorithms, Reverse engineering, Static analysis, Instruction sets, Hardware, Microarchitecture},
	pages = {285--297},
}

@inproceedings{lin_reifunc_2024,
	title = {{ReIFunc}: {Identifying} {Recurring} {Inline} {Functions} in {Binary} {Code}},
	issn = {2640-7574},
	doi = {10.1109/SANER60148.2024.00074},
	abstract = {Function inlining, although a common phenomenon, can greatly hinder the readability of the binary code obtained through decompilation. Identifying inline functions in the binary code is additionally challenging as there is no clear boundary between an inlined function and its caller function, the instructions of the same function might differ during inline expansion, and existing graph-schema methods for inline function identification cannot handle the vast number of functions involved due to their complexity. To address the challenge, in this paper, we propose an effective inline function identification solution named ReIFunc, which combines subgraph isomorphism and deep learning to identify these recurring inline functions (RIFs). Our evaluation shows that ReIFunc can effectively match functions within a broad candidate set with a high precision rate exceeding 99\% while maintaining an acceptable recall, thus getting rid of the constraints imposed by the limited size of the candidate set.},
	booktitle = {2024 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Lin, Wei and Guo, Qingli and Yu, DongSong and Yin, Jiawei and Gong, Qi and Gong, Xiaorui},
	month = mar,
	year = {2024},
	keywords = {Binary codes, Optimization, Software, Analytical models, Deep learning, binary analysis, Buildings, inline function identification, representation learning model, subgraph isomorphism},
	pages = {670--680},
}

@inproceedings{seixas_high-accuracy_2025,
	title = {High-{Accuracy} {Android} {Ransomware} {Detection} {Using} {SMALI} {Code}-{Based} {Analysis}},
	doi = {10.1109/ACDSA65407.2025.11166547},
	abstract = {Ransomware targeting the Android platform continues to pose a serious threat to users and organizations, with increasing sophistication and widespread impact. This paper introduces a high-accuracy detection framework that leverages static analysis of SMALI code—a low-level representation generated during APK decompilation—to identify Android ransomware. Unlike approaches that rely on Java-level reconstruction or dynamic behavior, our method extracts meaningful features such as permission usage, API calls, cryptographic operations, and component structure directly from SMALI and manifest files. The framework constructs a structured dataset from these features and applies various machine learning classifiers to distinguish ransomware from benign applications. Experiments conducted on two publicly available datasets, including AMD and CI-CAndMal2017, demonstrate the framework’s effectiveness. The Random Forest classifier achieved a peak accuracy of 98.89\%, with the ensemble and model also performing competitively. The results highlight the promise of SMALI-based static analysis for efficient, scalable, and interpretable ransomware detection.},
	booktitle = {2025 {International} {Conference} on {Artificial} {Intelligence}, {Computer}, {Data} {Sciences} and {Applications} ({ACDSA})},
	author = {Seixas, Vinicius and Elish, Karim},
	month = aug,
	year = {2025},
	keywords = {Accuracy, Feature extraction, Organizations, Static analysis, machine learning, Android, Cryptography, Operating systems, Computational modeling, static analysis, ransomware, Ransomware, Random forests},
	pages = {1--6},
}

@inproceedings{luo_binary_2025,
	title = {Binary {Code} {Similarity} {Detection} by {Fusing} {Graph} {Attention} {Network} and {Abstract} {Syntax} {Tree}},
	doi = {10.1109/ICSP65755.2025.11086752},
	abstract = {Since the introduction of deep learning-based binary code similarity detection methods, there has been a proliferation of methods using graph neural networks to learn semantic information from intermediate representations such as control flow graphs. Graph Attention Network (GAT) is a graph neural network model based on the attention mechanism, the core idea of which is to dynamically aggregate information from adjacent nodes by adaptively learning the importance weights between nodes in the graph, which can enhance the training effect of the model by incorporating the attention mechanism. The structured representation advantage of Abstract Syntax Trees (AST) is reflected in the hierarchical decomposition of program syntax structures, which effectively captures the logical semantics and contextual dependencies of the code, a feature that makes AST robust against the obfuscation of cross-architecture binary code. However, there has not yet been an approach that fuses graph attention networks and abstract syntax trees in the field of binary code similarity detection, so we propose a cross-architecture binary code similarity detection approach that fuses abstract syntax trees and graph attention networks. We use the open-source dataset provided by Asteria, decompile the binary code using the binary analysis tool IDA pro to extract the abstract syntax tree as the feature source of the neural network, and deeply optimize GAT for the limitations of processing tree data structure to improve its ability to learn the semantic information of AST. We trained TreeLSTM, plain GAT, and deeply optimized GAT neural network models respectively, and compared them. The experimental results show that the training speed of our proposed method is improved by 54.63\% compared to the TreeLSTM-based method, and the AUC and Accuracy metrics are basically comparable.},
	booktitle = {2025 10th {International} {Conference} on {Intelligent} {Computing} and {Signal} {Processing} ({ICSP})},
	author = {Luo, Rui and Ge, Hui and Jin, Xiao and Qin, Zhendong and Liu, Lihua},
	month = may,
	year = {2025},
	keywords = {Binary codes, Training, Semantics, Feature extraction, Syntactics, abstract syntax tree, Data mining, Graph neural networks, Attention mechanisms, binary code similarity, Fuses, graph attention network, Tree data structures},
	pages = {1247--1253},
}

@inproceedings{verbeek_formally_2025-1,
	title = {Formally {Verified} {Binary}-{Level} {Pointer} {Analysis}},
	issn = {1558-1225},
	doi = {10.1109/ICSE55347.2025.00231},
	abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows “meaningful” precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
	booktitle = {2025 {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
	month = apr,
	year = {2025},
	keywords = {Scalability, Software, Accuracy, binary analysis, Software engineering, Testing, formal methods, pointer analysis},
	pages = {42--53},
}

@article{ghimire_survey_2025,
	title = {A {Survey} on {Application} of {AI} on {Reverse} {Engineering} for {Software} {Analysis} and {Security}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3593456},
	abstract = {Reverse engineering process serves essential functions in software analysis and security auditing and malware detection but requires significant time and effort. Researchers and practitioners now investigate how Artificial Intelligence (AI) technology can automate and improve different reverse engineering procedures. This survey provides an extensive evaluation of recent AI-based reverse engineering techniques which focus on software decompilation and function identification as well as control flow recovery and vulnerability analysis. The paper presents a classification system for existing methods while comparing them through an analysis of their development from traditional rule-based systems to contemporary deep learning frameworks. The research examines fundamental datasets together with field tools and evaluation metrics. This paper establishes a fundamental understanding of AI integration in reverse engineering for software security while discussing future development directions.},
	journal = {IEEE Access},
	author = {Ghimire, Ashutosh and Lingala, Sahasra Rao and Zhang, Junjie and Alsulami, Faris and Amsaad, Fathi},
	year = {2025},
	keywords = {Codes, Security, Source coding, Software, Reverse engineering, Malware, Static analysis, Logic, malware detection, Surveys, software security, anomalies, Artificial intelligence, threat analysis},
	pages = {152903--152913},
}

@inproceedings{rajendran_ai-based_2025,
	title = {{AI}-{Based} {Malware} {Classification} in {Android} {Apps} {Using} {Vision} {Transformer} ({ViT}) on {Bytecode} {Images}},
	doi = {10.1109/iTechSECOM64750.2025.11307212},
	abstract = {The rapid proliferation of Android applications has made mobile platforms a prime target for malware attacks. To address the growing concern of malicious apps, this study presents an AI-based malware classification approach using Vision Transformer (ViT) on bytecode images of Android applications. Traditional malware detection techniques, including signature-based and heuristic methods, often fail to detect novel and obfuscated malware due to limited generalization and static rule-based systems. These limitations have necessitated the need for more intelligent and adaptive solutions that can handle complex and evolving malware patterns. To overcome these challenges, we propose a novel framework: Real-Time Malware Detection in App Stores using Bytecode Image Generation and Vision Transformer-Based Classification (RAMViD). In this approach, Android APK files are first decompiled to extract Dalvik bytecode, which is then transformed into grayscale images representing the byte patterns of the application. These images are fed into a finetuned Vision Transformer model that learns intricate visual features of benign and malicious patterns for accurate classification. The proposed method can be integrated into app store infrastructures to perform real-time malware analysis during the app submission process. This ensures proactive filtering of malicious applications before they reach end-users, enhancing platform security without manual intervention. Experimental results demonstrate that the ViT-based classification framework outperforms conventional CNN-based models in accuracy and robustness. The system achieves high detection rates with reduced false positives, highlighting its effectiveness in identifying both known and previously unseen malware. This solution offers a scalable, automated, and intelligent alternative for securing Android ecosystems.},
	booktitle = {2025 {Second} {International} {Conference} on {Intelligent} {Technologies} for {Sustainable} {Electric} and {Communications} {Systems} ({iTech} {SECOM})},
	author = {Rajendran, Megala and Al-Nussairi, Ahmed Kateb J. and Abdulhasan, Maki Mahdi and Sreseh, Saleh Naji and Owaied, Huda Qasim},
	month = oct,
	year = {2025},
	keywords = {Security, Deep Learning, Transformers, Accuracy, Malware, Static analysis, Operating systems, Static Analysis, Visualization, Real-time systems, Android Malware Detection, Bytecode Image, Computer vision, Mobile App Security, Real-Time Analysis, Robustness, Vision Transformer},
	pages = {1--6},
}

@inproceedings{wang_typeforge_2025,
	title = {{TypeForge}: {Synthesizing} and {Selecting} {Best}-{Fit} {Composite} {Data} {Types} for {Stripped} {Binaries}},
	issn = {2375-1207},
	doi = {10.1109/SP61157.2025.00193},
	abstract = {Static binary analysis is a widely used approach for ensuring the security of closed-source software. However, the absence of type information in stripped binaries, particularly for composite data types, poses significant challenges for both static analyzers and reverse engineering experts in achieving efficient and accurate analysis. Existing methods often struggle with inaccuracies and scalability limitations when dealing with such data types. To address these problems, we present Typeforge, a novel approach inspired by the workflow of reverse engineering experts, which uses a two-stage synthesis-selection strategy to automate the recovery of composite data types from stripped binaries. We design a new graph structure, the Type Flow Graph (TFG) to represent type information within stripped binaries. In the first stage, TFG-based Type Synthesis focuses on efficiently and accurately building constraints and synthesizing possible composite type declarations from the stripped binaries. In the second stage, we propose an LLM-assisted double-elimination framework to select the best-fit type declaration from the candidates by assessing the readability of the decompiled code. Our comparison with state-of-the-art approaches demonstrates that TYPEFORGE achieves F1 scores of 81.7\% and 88.2\% in Composite Data Type Identification and Layout Recovery, respectively, substantially outperforming existing methods. Additionally, TYPEFORGE achieves an F1 score of 72.1\% in Relationship Recovery, a particularly challenging task for previous approaches. Furthermore, TYPEFORGE has significantly lower time overhead, requiring only about 3.8\% of the time taken by OSPREY, the best-performing existing approach, making it a promising solution for various real-world reverse engineering tasks.},
	booktitle = {2025 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Wang, Yanzhong and Liang, Ruigang and Li, Yilin and Hu, Peiwei and Chen, Kai and Zhang, Bolun},
	month = may,
	year = {2025},
	keywords = {Codes, Privacy, Scalability, Security, Software, Reverse engineering, Accuracy, Buildings, Flow graphs, Layout},
	pages = {1--18},
}

@article{huang_firmccf_2026,
	title = {{FirmCCF}: {Detecting} {Custom} {Cryptographic} {Function} {Vulnerabilities} {Through} {Query}-{Driven} {Approaches}},
	volume = {13},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2025.3631834},
	abstract = {Cryptographic techniques are widely used to safeguard software against privacy breaches. Efficiently detecting encryption algorithms in software to determine whether they meet security requirements is a critical task. However, traditional static and dynamic detection methods often suffer from high false alarm rates or low efficiency, as they cannot fully capture the structural and semantic features of cryptographic algorithms. In this article, we propose FirmCCF, a vulnerability detection tool for custom cryptographic functions in Internet of Things (IoT) devices. FirmCCF leverages an improved deep learning encoder–decoder classification model, CodeT5-cate, to identify and classify cryptographic functions in source code and decompiled firmware. It then outputs highly structured meta-level attributes of cryptographic functions via a large language model (LLM) and detects vulnerabilities through a query-driven approach. FirmCCF achieves 99.97\% accuracy, 99.72\% recall, and 99.86\% F1 -score in detecting cryptographic functions from binary files. We further define seven security rules, encode them as queries, and use them to uncover seven categories of vulnerabilities. An evaluation of 40902 function codes revealed 46 vulnerabilities, including eight previously unknown issues. Our work highlights the urgent need for systematic assessment solutions to detect and mitigate vulnerabilities in custom cryptographic functions.},
	number = {2},
	journal = {IEEE Internet of Things Journal},
	author = {Huang, Jing and Wang, Min and Hu, Yupeng},
	month = jan,
	year = {2026},
	keywords = {Codes, Software, Software algorithms, Feature extraction, Cryptography, Encryption, CodeT5-cate, custom cryptographic function, Heuristic algorithms, Internet of Things, query-driven, Salt, Threat modeling},
	pages = {2988--2999},
}

@inproceedings{wang_research_2024,
	title = {Research on {Code} {Virtualization} {Methods} for {Cloud} {Applications}},
	doi = {10.1109/NaNA63151.2024.00054},
	abstract = {With the vigorous development of technologies such as the Internet, mobile devices, and the Internet of Things, cloud services have gained a crucial position in our life and have become an essential part of daily operations for modern enterprises and organizations. However, existing cloud application security solutions have failed to address the problem of runtime binary code security, focusing only on vulnerabilities inherent to the applications themselves. Code virtualization, also known as virtualization obfuscation, is a technique that protects software from malicious analysis by obfuscating the code. It hides the code’s control flow and data flow, thereby preventing the code from being decompiled. In this paper, we introduce a code virtualization method, VMENP, which splits the code into multiple modules and uses polymorphic and modular encryption mechanisms to secure binary code at runtime. Specifically, VMENP divides the protected program into several modules, designs a virtual instruction set along with its corresponding interpreter, encrypts each instruction individually after converting it into intermediate representations, and then encrypts it again at the basic block level. After compiling the segmented code into polymorphic bytecode sequences, it embeds the virtual code’s interpreter into the IR. During runtime, the protected code is dynamically decrypted and executed by the basic block level, with each basic block’s storage and execution in memory in a short time window. VMENP identifies functions annotated in the source code to locate the functions that need protection. This paper uses a large amount of {\textbackslash}mathrmC / {\textbackslash}mathrmC++ code to perform tests in a real cloud environment. Experimental results show that VMENP achieves a good balance between the granularity of protection and the performance overhead of the protected programs, while successfully protecting {\textbackslash}mathrmC / {\textbackslash}mathrmC++ code and programs running on X86/64 and ARM32/64 architectures.},
	booktitle = {2024 {International} {Conference} on {Networking} and {Network} {Applications} ({NaNA})},
	author = {Wang, Zhoukai and Xu, Zuoyan and Zhang, Yaling and Song, Xin and Wang, Yichuan},
	month = aug,
	year = {2024},
	keywords = {Binary codes, Codes, Reverse engineering, reverse engineering, Computer architecture, Runtime, code obfuscation, Cloud computing, Cloud application security, code virtualization, Instruction sets, polymorphic Routines},
	pages = {287--292},
}

@inproceedings{chen_software_2024,
	title = {Software {Diversification} {Protection} {Methods} for {Binary} {Programs}},
	doi = {10.1109/ICSP62122.2024.10743227},
	abstract = {Software diversification is an effective software protection method against reverse engineering and code reuse attacks, which can provide heterogeneous redundant execution bodies for mimetic defense mechanisms. Most existing software diversification methods require access to the source code, which can provide defenders with more valuable information for devising effective defense strategies. However, due to commercial copyright and the purpose of preventing software piracy, developers often keep their software closed-source, making it difficult to access the source code. Therefore, a method called R2BF (ReCooking and Randomizing Binary File) is proposed, which combines decompilation and diversification compilation techniques to address the difficulty of obtaining the source code. This method involves diversifying the source code through compilation to achieve software diversification for binary executable files. Security and performance testing of diversified C programs has shown that diversified binary executable files can mitigate vulnerabilities resulting from code reuse, validating that this method can enhance the security of binary programs and is feasible for software protection. The diversified binary executable files are nearly identical to the original, non-diversified files in terms of file size and execution time, and may even exhibit acceleration and optimization effects in certain scenarios.},
	booktitle = {2024 9th {International} {Conference} on {Intelligent} {Computing} and {Signal} {Processing} ({ICSP})},
	author = {Chen, Yingchao and Wang, Junchao and Zhou, Xin and Pang, Jianmin},
	month = apr,
	year = {2024},
	keywords = {Codes, Security, Source coding, Optimization, Software, Software algorithms, Reverse engineering, decompilation, component, Software protection, diversification compilation, program protection, Signal processing algorithms, software diversification, Software testing},
	pages = {285--291},
}

@article{priambodo_malqwen_2025,
	title = {{MalQwen}: {Fine} {Tuned} {LLM} for {Static} {Android} {Malware} {Analysis} {Report}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3637047},
	abstract = {The Android operating system continues to face escalating security challenges, primarily due to its open-source nature and the rapid proliferation of applications from untrusted sources. Traditional static analysis tools lack the flexibility to capture evolving malware behaviors, limiting their interpretability and scalability. Large Language Models (LLMs) are now applied in cybersecurity for malware detection, phishing classification, and cyber threat intelligence. However, their use has not been extended to producing detailed and interpretable Android malware analysis reports. This study integrates LLMs into Android malware analysis by creating a dataset for instruction tuning and fine-tuning the Qwen-7B model using the LoRA method. The model MalQwen is developed by fine-tuning Qwen 2.5-7B with 429 malware samples containing decompiled code and expert labeled security reports. MalQwen outperforms models like Gemini and LLaMA, achieving a BERTscore of 0.84 for SMS malware and a Perplexity score of 3.30 for Scareware. These findings confirm MalQwen’s superior performance in generating precise malware reports, validating LLMs as a powerful new method for Android malware analysis.},
	journal = {IEEE Access},
	author = {Priambodo, Tegar Ganang Satrio and Prabowo, Angela Oryza and Puspitarini, Annisa Dwi and Winarso, Raihan Adam Handoyo and Aisyah, Nur and Pratama, Mohammad Yoga and Purwitasari, Diana and Pratomo, Baskoro Adi},
	year = {2025},
	keywords = {Codes, Security, Training, Feature extraction, Malware, large language model, Static analysis, Operating systems, Data mining, static analysis, Adaptation models, Android malware analysis, Cyber threat intelligence, LoRA fine-tuning, report generation},
	pages = {208483--208497},
}

@inproceedings{guo_android_2024,
	title = {Android {Application} {Reinforcement} {Method} {Based} on {5G} {Terminal} {Device}},
	doi = {10.1109/IDS62739.2024.00017},
	abstract = {This paper presents a novel Android application reinforcement method designed for 5G smartphones, addressing the security vulnerabilities inherent in Android's open-source na-ture. The method involves extracting key functions from Android applications, decompiling them into Smali code, and then trans-forming them into C++ code. This code is virtualized at compile time and repackaged into a new APK file, offering a more efficient reinforcement approach compared to traditional methods. It uses less computational power and storage while meeting the high safety standards of 5G technology. This approach is particularly effective against automated, low-threshold reverse engineering tools, which have challenged previous hardening methods. In our experiments, we analyzed six common Android shell tools on enhanced applications, demonstrating the ineffectiveness of these tools against our method. Our reinforcement overcomes the easy recovery issue of traditional methods and shows strong resistance to general reverse-engineering tools. We evaluated the performance pre- and post-hardening in terms of CPU utilization, size, and runtime memory usage. For instance, CPU utilization post-hardening increased by 0.9\% for Gaud Map, 4.8\% for WPS, and 1.3 \% for public comments. The use of native Layer code and function localization enhances program efficiency, offsetting the performance overhead of fusion, thus maintaining overall performance stability. Our method's robustness against common reverse-engineering tools, with minimal impact on CPU usage, application size, and memory usage, confirms its practicality for deployment in the 5G ecosystem.},
	booktitle = {2024 10th {IEEE} {International} {Conference} on {Intelligent} {Data} and {Security} ({IDS})},
	author = {Guo, Yaqiong and Chen, Jingnan and Xu, Zichao},
	month = may,
	year = {2024},
	keywords = {Codes, Decompilation, Static analysis, Android, Operating systems, Performance evaluation, Code Virtualization, C++ languages, 5G mobile communication, 5G Technology, Application Reinforcement, Resists, Smali Code},
	pages = {53--58},
}

@article{hartman_cross-architecture_2025,
	title = {Cross-{Architecture} {Binary} {Function} {Fingerprinting}},
	volume = {23},
	issn = {1558-4046},
	doi = {10.1109/MSEC.2024.3468153},
	abstract = {By combining the SLEIGH decompiler in Ghidra with an machine learning-based technique we can fingerprint reused functions across processor architectures with high accuracy. This opens the door for reverse engineers and antivirus tools to more effectively identify vulnerable and malware code.},
	number = {2},
	journal = {IEEE Security \& Privacy},
	author = {Hartman, Corey M. and Rimal, Bhaskar P. and de Leon, Daniel Conte and Budhathoki, Nirmal},
	month = mar,
	year = {2025},
	keywords = {Codes, Source coding, Optimization, Accuracy, Malware, Machine learning, Libraries, Internet of Things, Object recognition, Fingerprint recognition, Systems architecture},
	pages = {71--80},
}

@inproceedings{naliapara_maia_2025,
	title = {{MAIA} - {Malware} {Analysis} and {Intelligence} {Assistant}},
	doi = {10.1109/MPSecICETA64837.2025.11118682},
	abstract = {As cyber threats become more complex, the need for innovative tools in cybersecurity is paramount. MAIA (Malware Analysis and Intelligence Assistant), addresses this demand by automating malware reverse engineering to enhance threat analysis capabilities. It integrates advanced unpacking techniques, decompilation using Ghidra, and artificial intelligence analysis allowing for an indepth understanding of its structure, behaviors, and concealed functions. By enabling detailed static analysis and AI-driven insights, it assists cybersecurity teams in rapidly identifying and countering malicious tactics. Through comprehensive report generation and an interactive user interface, it equips cybersecurity professionals with actionable data to strengthen digital defenses. MAIA exemplifies the integration of AI with traditional malware analysis, aiming to bolster proactive threat mitigation and incident response efforts.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Emerging} {Technologies} and {Applications} ({MPSec} {ICETA})},
	author = {Naliapara, Heli and Singhvi, Puru and Shukla, Pratham and Mirkar, Sulalah},
	month = feb,
	year = {2025},
	keywords = {Codes, Reverse engineering, Reverse Engineering, Malware, Static analysis, Static Code Analysis, Manuals, Malware Analysis, Automation, Computer security, Artificial intelligence, AI-driven Cybersecurity Solutions, Artificial Intelligence, Automated Malware Analysis, Cyber Defense Tools, Cyber Threat Mitigation, Incident Response, Prevention and mitigation, Threat Intelligence, User interfaces},
	pages = {1--6},
}

@inproceedings{izrailov_reengineering_2025,
	title = {Reengineering {Modern} {Industrial} {Software} to {Find} {Vulnerabilities} {Based} on {Genetic} {Algorithms}},
	issn = {2993-4060},
	doi = {10.1109/ICIEAM65163.2025.11028410},
	abstract = {Modern industry is built, among other things, on software, the presence of vulnerabilities in which is a significant problem. It is more rational to search for vulnerabilities in those representations of the program (source code, algorithms, architecture, etc.) on the basis of which they were developed. However, as a rule, there is difficult-to-analyze machine code available. Obtaining higher-level representations is possible by reverse engineering, carried out in various ways, such as expert, algorithmic, intelligent, enumeration and logging. The qualitative comparison of these representations is given. The current study is devoted to a new method of reverse engineering based on the use of genetic algorithms. The course of the research and the following main scientific results are briefly described: the methodology of reverse engineering of a software system, the model of the life cycle of a program with multi-level vulnerabilities, the concept of genetic de-evolution of program representations, scientific, methodological and algorithmic instrumentation for genetic decompilation, an architectural block for conducting genetic de-evolution of representations with functionality for searching for multi-level vulnerabilities. All results are novel, as well as theoretically and practically significant.},
	booktitle = {2025 {International} {Conference} on {Industrial} {Engineering}, {Applications} and {Manufacturing} ({ICIEAM})},
	author = {Izrailov, Konstantin and Kotenko, Igor and Buinevich, Mikhail},
	month = may,
	year = {2025},
	keywords = {Source coding, Software algorithms, Reverse engineering, Genetics, reverse engineering, Information security, Instruments, Software systems, Genetic algorithms, vulnerability, Industries, information security, genetic algorithms, Manufacturing, software},
	pages = {918--923},
}

@inproceedings{hossain_mollah_enhanced_2025,
	title = {An {Enhanced} {Hybrid} {Deep} {Learning} {Architecture} for {Android} {Malware} {Detection} {Using} {CFG} and {DeepWalk} {Embeddings}},
	doi = {10.1109/IICAIET67254.2025.11264981},
	abstract = {Android malware propagation poses increasing security risks to mobile ecosystems, with conventional detection techniques fighting an uphill battle against advanced evasion strategies and polymorphic variants that take advantage of static analysis limitations. Existing approaches including signature-based detection and conventional machine learning techniques have moderate performance against advanced obfuscation techniques and cannot capture complex structural relationships inherent in malicious code execution behavior. These limitations necessitate advanced techniques capable of carrying out in-depth structural analysis and dynamic pattern detection for malware detection. This paper proposes a new hybrid deep learning technique that combines Control Flow Graph (CFG) analysis with Deep Graph Convolutional Neural Networks (DeepGraphCNNs) and Temporal Convolutional Networks (TCN) to overcome current limitations. Our technique meticulously reverse-engineers APK packages, decompiles Dalvik bytecode to extract CFG representations and utilizes sophisticated graph embedding methods (DeepWalk, Node2Vec, Word2Vec) in the feature transformation process. The experiment is conducted on 2,349 samples from CICMalAnal2017 and MalwareBazar. Our DeepWalk-DeepGraphCNNs and TCN model demonstrated superior performance with 95.10\% accuracy and 93.67\% AUC-ROC. Our experimental results ensure the success of the technique for addressing contemporary Android security issues.},
	booktitle = {2025 {IEEE} {International} {Conference} on {Artificial} {Intelligence} in {Engineering} and {Technology} ({IICAIET})},
	author = {Hossain Mollah, Mohammad Sarwar and Bin Marhusin, Mohd Fadzli and Omar, Syaril Nizam},
	month = aug,
	year = {2025},
	keywords = {Deep learning, Accuracy, Feature extraction, Malware, Static analysis, Ecosystems, Malware detection, Convolutional neural networks, Graph neural networks, Flow graphs, CFG, Android security, Mobile security, Graph embedding, Hybrid deep learning, Mobile cybersecurity},
	pages = {455--460},
}

@article{feng_interactive_2025,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Interactive {End}-to-{End} {Decompilation} via {Large} {Language} {Models}},
	volume = {14},
	issn = {2079-9292},
	doi = {10.3390/electronics14224442},
	abstract = {The goal of decompilation is to convert compiled low-level code (e.g., assembly code) back into high-level programming languages, enabling analysis in scenarios where source code is unavailable. This task supports various reverse engineering applications, such as vulnerability identification, malware analysis, and legacy software migration. The end-to-end decompilation method based on large language models (LLMs) reduces reliance on additional tools and minimizes manual intervention due to its inherent properties. However, previous end-to-end methods often lose critical information necessary for reconstructing control flow structures and variables when processing binary files, making it challenging to accurately recover the program's logic. To address these issues, we propose the ReF Decompile method, which incorporates the following innovations: (1) The Relabeling strategy replaces jump target addresses with labels, preserving control flow clarity. (2) The Function Call strategy infers variable types and retrieves missing variable information from binary files. Experimental results on the Humaneval-Decompile Benchmark demonstrate that ReF Decompile surpasses comparable baselines and achieves state-of-the-art (SOTA) performance of 61.43\%.},
	language = {English},
	number = {22},
	journal = {ELECTRONICS},
	publisher = {MDPI},
	author = {Feng, Yunlong and Li, Bohan and Shi, Xiaoming and Zhu, Qingfu and Che, Wanxiang},
	month = nov,
	year = {2025},
	note = {Type: Article},
	keywords = {large language model, code generation},
}

@article{aljebreen_binary-source_2025,
	address = {19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND},
	title = {Binary-{Source} {Code} {Matching} {Based} on {Decompilation} {Techniques} and {Graph} {Analysis}},
	volume = {16},
	issn = {2158-107X},
	abstract = {Recent approaches to binary-source code matching often operate at the intermediate representation (IR) level, with some applying the matching process at the binary level by compiling the source code to binary and then matching it directly with the binary code. Others, though less common, perform matching at the decompiler-generated pseudo-code level by first decompiling the binary code into pseudo-code and then comparing it with the source code. However, all these approaches are limited by the loss of semantic information in the original source code and the introduction of noise during compilation and decompilation, making accurate matching challenging and often requiring specialized expertise. To address these limitations, this study introduces a system for binary-source code matching based on decompilation techniques and Graph analysis (BSMDG) that matches binary code with source code at the source code level. Our method utilizes the Ghidra decompiler in conjunction with a custom-built transpiler to reconstruct highlevel C++ source code from binary executables. Subsequently, call graphs (CGs) and control flow graphs (CFGs) are generated for both the original and translated code to evaluate their structural and semantic similarities. To evaluate our system, we used a curated dataset of C++ source code and corresponding binary files collected from the AtCoder website for training and testing. Additionally, a case study was conducted using the widely recognized POJ-104 benchmark dataset to assess the system's generalizability. The results demonstrate the effectiveness of combining decompilation with graph-based analysis, with our system achieving 90\% accuracy on POJ-104, highlighting its potential in code clone detection, vulnerability identification, and reverse engineering tasks.},
	language = {English},
	number = {5},
	journal = {INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS},
	publisher = {SCIENCE \& INFORMATION SAI ORGANIZATION LTD},
	author = {Aljebreen, Ghader and Alnanih, Reem and Eassa, Fathy and Khemakhem, Maher and Jambi, Kamal and Ashraf, Muhammed Usman},
	year = {2025},
	note = {Type: Article},
	keywords = {decompiler, Binary-source code matching, call graphs, code clone detection, control flow graphs},
	pages = {253--268},
}

@article{li_iradt_2024,
	address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
	title = {{IRaDT}: {LLVM} {IR} as {Target} for {Efficient} {Neural} {Decompilation}},
	volume = {34},
	issn = {0218-1940},
	doi = {10.1142/S0218194024500463},
	abstract = {Decompilation is a widely utilized technique in reverse engineering, aimed at restoring binary code to human-readable high-level language code. However, the readability of the output from traditional decompilers is often poor. With advancements in language models, several learning-based decompilation methods have emerged. Nevertheless, the probabilistic nature of language models leads to outputs whose correctness cannot be guaranteed, necessitating further analysis by engineers to identify the corresponding functionality of the code. Inspired by compiler toolchains, we propose a novel approach to enhance the effectiveness of language models in decompilation tasks. Traditional rule-based methods and learning-based techniques are fused together in our approach, drawing insights from both paradigms. Specifically, we present a pre-trained sequence-to-sequence model called IRaDT tailored to refine decompilation outputs at the intermediate representation level. Through this hybridization, we aim to address the limitations of existing methodologies and achieve more accurate and robust decompilation. We construct a diverse decompilation dataset targeting IR and evaluated IRaDT based on this dataset. The experimental results indicate that IRaDT has the ability to improve the readability of IR while ensuring its compileability, achieving a 74\% improvement compared to RetDec and a 93\% improvement compared to ChatGPT.},
	language = {English},
	number = {12},
	journal = {INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING},
	publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
	author = {Li, Yuzhang and Xu, Tao and Wang, Chunlu},
	month = feb,
	year = {2024},
	note = {Type: Article},
	keywords = {Decompilation, LLVM immediate representation, neural machine translation, pre-trained language model},
	pages = {1971--1992},
}

@article{alcocer_use_2024,
	address = {125 London Wall, London, ENGLAND},
	title = {On the use of statistical machine translation for suggesting variable names for decompiled code: {The} {Pharo} case},
	volume = {79},
	issn = {2590-1184},
	doi = {10.1016/j.cola.2024.101271},
	abstract = {Adequately selecting variable names is a difficult activity for practitioners. In 2018, Jaffe et al. proposed the use of statistical machine translation (SMT) to suggest descriptive variable names for decompiled code. A large corpus of decompiled C code was used to train the SMT model. Our paper presents the results of a partial replication of Jaffe's experiment. We apply the same technique and methodology to a dataset made of code written in the Pharo programming language. We selected Pharo since its syntax is simple - it fits on half of a postcard - and because the optimizations performed by the compiler are limited to method scope. Our results indicate that SMT may recover between 8.9\% and 69.88\% of the variable names depending on the training set. Our replication concludes that: (i) the accuracy depends on the code similarity between the training and testing sets; (ii) the simplicity of the Pharo syntax and the satisfactory decompiled code alignment have a positive impact on predicting variable names; and (iii) a relatively small code corpus is sufficient to train the SMT model, which shows the applicability of the approach to less popular programming languages. Additionally, to assess SMT's potential in improving original variable names, ten Pharo developers reviewed 400 SMT name suggestions, with four reviews per variable. Only 15 suggestions (3.75\%) were unanimously viewed as improvements, while 45 (11.25\%) were perceived as improvements by at least two reviewers, highlighting SMT's limitations in providing suitable alternatives.},
	language = {English},
	journal = {JOURNAL OF COMPUTER LANGUAGES},
	publisher = {ELSEVIER SCI LTD},
	author = {Alcocer, Juan Pablo Sandoval and Camacho-Jaimes, Harold and Galindo-Gutierrez, Geraldine and Neyem, Andres and Bergel, Alexandre and Ducasse, Stephane},
	month = jun,
	year = {2024},
	note = {Type: Article},
	keywords = {Decompiled code, Identifiers, Readability, Statistical machine translation, Variable names},
}

@article{gribkov_analysis_2023,
	address = {TROPIC ISLE BLDG, PO BOX 3331. ROAD TOWN, TORTOLA, BRITISH VIRGIN ISL},
	title = {Analysis of {Decompiled} {Program} {Code} {Using} {Abstract} {Syntax} {Trees}},
	volume = {57},
	issn = {0146-4116},
	doi = {10.3103/S0146411623080060},
	abstract = {This article proposes a method for preprocessing fragments of binary program codes for subsequent detection of their similarity using machine learning methods. The method is based on the analysis of pseudocode obtained as a result of decompiling fragments of binary codes. The analysis is performed using attributed abstract syntax trees (AASTs). As part of the study, testing and comparative analysis of the effectiveness of the developed method are carried out. This method makes it possible to increase the efficiency of detecting functionally similar fragments of program code, compared to analogs, by using the semantic context of vertices in abstract syntax trees.},
	language = {English},
	number = {8},
	journal = {AUTOMATIC CONTROL AND COMPUTER SCIENCES},
	publisher = {PLEIADES PUBLISHING LTD},
	author = {Gribkov, N. A. and Ovasapyan, T. D. and Moskvin, D. A.},
	month = feb,
	year = {2023},
	note = {Type: Article},
	keywords = {abstract syntax tree, binary code, code clones, pseudocode, semantic similarity, syntactic similarity},
	pages = {958--967},
}

@article{gregorio_e-apk_2023,
	address = {125 London Wall, London, ENGLAND},
	title = {E-{APK}: {Energy} pattern detection in decompiled android applications},
	volume = {76},
	issn = {2665-9182},
	doi = {10.1016/j.cola.2023.101220},
	abstract = {Energy efficiency is a non-functional requirement that developers must consider, particularly when building software for battery-operated devices like mobile ones: a long-lasting battery is an essential requirement for an enjoyable user experience.In previous studies, it has been shown that many mobile applications include inefficiencies that cause battery to be drained faster than necessary. Some of these inefficiencies result from software patterns that have been catalogued, and for which more energy-efficient alternatives are also known.The existing catalogues, however, assume as a fundamental requirement that one has access to the source code of an application in order to be able to analyse it. This requirement makes independent energy analysis challenging, or even impossible, e.g. for a mobile user or, most significantly, an App Store trying to provide information on how efficient an application being submitted for publication is.We study the viability of looking for known energy patterns in applications by decompiling them and analysing the resulting code. For this, we decompiled and analysed 420 open-source applications by extending an existing tool, which is now capable of transparently decompiling and analysing android applications. With the collected data, we performed a comparative study of the presence of four energy patterns between the source code and the decompiled code.We performed two types of analysis: (i) comparing the total number of energy pattern detections; (ii) comparing the similarity between energy pattern detections. When comparing the total number of detections in source code against decompiled code, we found that 79.29\% of the applications reported the same number of energy pattern detections.To test the similarity between source code and APKs, we calculated, for each application, a similarity score based on our four implemented detectors. Of all applications, 35.76\% achieved a perfect similarity score of 4, and 89.40\% got a score of 3 or more out of 4. Furthermore, only two applications got a score of 0.When viewed in tandem, the results of the two analyses we performed point in a promising direction. They provide initial evidence that static analysis techniques, typically used in source code, can be a viable method to inspect APKs when access to source code is restricted, and further research in this area is worthwhile.},
	language = {English},
	journal = {JOURNAL OF COMPUTER LANGUAGES},
	publisher = {ELSEVIER SCI LTD},
	author = {Gregorio, Nelson and Bispo, Joao and Fernandes, Joao Paulo and de Medeiros, Sergio Queiroz},
	month = aug,
	year = {2023},
	note = {Type: Article},
	keywords = {Decompiler, Static analysis, Android, Compilers, Code patterns, Energy efficiency, Metaprogramming, Mobile},
}

@article{dramko_dire_2023-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{DIRE} and its {Data}: {Neural} {Decompiled} {Variable} {Renamings} with {Respect} to {Software} {Class}},
	volume = {32},
	issn = {1049-331X},
	doi = {10.1145/3546946},
	abstract = {The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE's overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23\% more frequently than the original DIRE model.},
	language = {English},
	number = {2},
	journal = {ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Dramko, Luke and Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Ed and Allamanis, Miltiadis and Neubig, Graham and Vasilescu, Bogdan and Le Goues, Claire},
	month = apr,
	year = {2023},
	note = {Type: Article},
	keywords = {decompilation, Machine learning, data provenance},
}

@article{kargen_android_2023,
	address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
	title = {Android decompiler performance on benign and malicious apps: an empirical study},
	volume = {28},
	issn = {1382-3256},
	doi = {10.1007/s10664-022-10281-9},
	abstract = {Decompilers are indispensable tools in Android malware analysis and app security auditing. Numerous academic works also employ an Android decompiler as the first step in a program analysis pipeline. In such settings, decompilation is frequently regarded as a “solved” problem, in that it is simply expected that source code can be accurately recovered from an app. On the other hand, it is known that, e.g, obfuscation can negatively impact a decompiler's effectiveness. Therefore, in order to better understand potential failure modes of, e.g., automated analysis pipelines involving decompilation, it is important to characterize the performance of decompilers on both benign and malicious apps. To this end, we have performed what is, to the best of our knowledge, the first large-scale study of Android decompilation failure rates, using three sets of apps; namely, 3,018 open-source apps, 13,601 apps crawled from Google Play, and an existing collection of 24,553 malware samples. In addition to the state-of-the-art Dalvik bytecode decompiler Jadx, we also studied the performance of three popular Java decompilers. Furthermore, this paper also presents the findings from a follow-up study on 54,945 malware apps, where we additionally performed an analysis of the reasons for decompilation failures. Our study revealed that decompilers generally have very low failure rates, and that few failures on benign apps appear to be related to obfuscation. On malware, however, obfuscation appears to be a more prominent cause of failures, although the vast majority of malicious apps could still be fully decompiled by an ensemble of decompilers.},
	language = {English},
	number = {2},
	journal = {EMPIRICAL SOFTWARE ENGINEERING},
	publisher = {SPRINGER},
	author = {Kargen, Ulf and Mauthe, Noah and Shahmehri, Nahid},
	month = mar,
	year = {2023},
	note = {Type: Article},
	keywords = {Decompilation, Reverse engineering, Malware, Android, Obfuscation, Mobile apps},
}

@article{wang_binvuldet_2023,
	address = {OXFORD FULFILLMENT CENTRE THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
	title = {{BinVulDet}: {Detecting} vulnerability in binary program via decompiled pseudo code and {BiLSTM}-attention},
	volume = {125},
	issn = {0167-4048},
	doi = {10.1016/j.cose.2022.103023},
	abstract = {Static detection of security vulnerabilities in binary programs is an important research field in software supply chain security. However, existing vulnerability detection methods based on code similarity can only detect known vulnerabilities. Vulnerability features generated by vulnerability pattern-based detec-tion methods are low robust due to the influence of manually defined patterns, compiler diversity, and irrelevant function instructions. In this paper, we propose BinVulDet, which is a binary level vulnerability detection tool for accurate known and unknown vulnerability detection. BinVulDet uses decompilation techniques to obtain pseudo code containing high-level semantic information against the impact of com-pilation diversity. Then the program slicing technique is used to extract the statements with data depen-dencies and control dependencies related to the vulnerability. A BiLSTM-attention neural network is used to extract rich contextual semantic information from slice codes to generate more robust vulnerability patterns to detect vulnerabilities. The experimental results show that BinVulDet outperforms the state-of-the-art binary vulnerability detection methods. The FPR and FNR of BinVulDet are 1.04\% and 0.89\% on average, respectively, which are 3.93\% and 22.86\% lower than the baseline model on average. BinVulDet can effectively against the influence of compilation diversity and successfully be used for real-world vul-nerability detection by being evaluated in three CVE vulnerability projects.(c) 2022 Elsevier Ltd. All rights reserved.},
	language = {English},
	journal = {COMPUTERS \& SECURITY},
	publisher = {ELSEVIER ADVANCED TECHNOLOGY},
	author = {Wang, Yan and Jia, Peng and Peng, Xi and Huang, Cheng and Liu, Jiayong},
	month = feb,
	year = {2023},
	note = {Type: Article},
	keywords = {BiLSTM-attention, Binary program, Decompile, Program slicing, Vulnerability detection},
}

@article{liang_neutron_2021,
	address = {CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND},
	title = {Neutron: an attention-based neural decompiler},
	volume = {4},
	issn = {2523-3246},
	doi = {10.1186/s42400-021-00070-0},
	abstract = {Decompilation aims to analyze and transform low-level program language (PL) codes such as binary code or assembly code to obtain an equivalent high-level PL. Decompilation plays a vital role in the cyberspace security fields such as software vulnerability discovery and analysis, malicious code detection and analysis, and software engineering fields such as source code analysis, optimization, and cross-language cross-operating system migration. Unfortunately, the existing decompilers mainly rely on experts to write rules, which leads to bottlenecks such as low scalability, development difficulties, and long cycles. The generated high-level PL codes often violate the code writing specifications. Further, their readability is still relatively low. The problems mentioned above hinder the efficiency of advanced applications (e.g., vulnerability discovery) based on decompiled high-level PL codes.In this paper, we propose a decompilation approach based on the attention-based neural machine translation (NMT) mechanism, which converts low-level PL into high-level PL while acquiring legibility and keeping functionally similar. To compensate for the information asymmetry between the low-level and high-level PL, a translation method based on basic operations of low-level PL is designed. This method improves the generalization of the NMT model and captures the translation rules between PLs more accurately and efficiently. Besides, we implement a neural decompilation framework called Neutron. The evaluation of two practical applications shows that Neutron's average program accuracy is 96.96\%, which is better than the traditional NMT model.},
	language = {English},
	number = {1},
	journal = {CYBERSECURITY},
	publisher = {SPRINGERNATURE},
	author = {Liang, Ruigang and Cao, Ying and Hu, Peiwei and Chen, Kai},
	month = mar,
	year = {2021},
	note = {Type: Article},
	keywords = {Decompilation, Translation, Attention, LSTM},
}

@article{harrand_java_2020,
	address = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
	title = {Java decompiler diversity and its application to meta-decompilation},
	volume = {168},
	issn = {0164-1212},
	doi = {10.1016/j.jss.2020.110645},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84\%, respectively 78\%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6\% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler. (C) 2020 Published by Elsevier Inc.},
	language = {English},
	journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
	publisher = {ELSEVIER SCIENCE INC},
	author = {Harrand, Nicolas and Soto-Valero, Cesar and Monperrus, Martin and Baudry, Benoit},
	month = oct,
	year = {2020},
	note = {Type: Article},
	keywords = {Decompilation, Reverse engineering, Java bytecode, Source code analysis},
	annote = {arXiv admin note: substantial text overlap with arXiv:1908.06895},
}

@article{mateless_decompiled_2020,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Decompiled {APK} based malicious code classification},
	volume = {110},
	issn = {0167-739X},
	doi = {10.1016/j.future.2020.03.052},
	abstract = {Due to the increasing growth in the variety of Android malware, it is important to distinguish between the unique types of each. In this paper, we introduce the use of a decompiled source code for malicious code classification. This decompiled source code provides deeper analysis opportunities and understanding of the nature of malware. Malicious code differs from text due to syntax rules of compilers and the effort of attackers to evade potential detection. Hence, we adapt Natural Language Processing-based techniques under some constraints for malicious code classification. First, the proposed methodology decompiles the Android Package Kit files, then API calls, keywords, and non-obfuscated tokens are extracted from the source code and categorized to stop-tokens, feature-tokens, and long-tail-tokens. We also introduce the use of generalized N-tokens to represent tokens that are typically less frequent. Our approach was evaluated, in comparison to the use of API calls and permissions for features, as a baseline, and their combination, as well as in comparison to the use of neural network architectures based on decompiled Android Package Kits. A rigorous evaluation of comprehensive public real-world Android malware datasets, including 24,553 apps that were categorized to 71 families for the malicious families classification, and 60,000 apps for malicious code detection was performed. Our approach outperformed the baselines in both tasks. (C) 2020 Elsevier B.V. All rights reserved.},
	language = {English},
	journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
	publisher = {ELSEVIER},
	author = {Mateless, Roni and Rejabek, Daniel and Margalit, Oded and Moskovitch, Robert},
	month = sep,
	year = {2020},
	note = {Type: Article},
	keywords = {Android malware, Source code analysis, Malicious code},
	pages = {135--147},
}

@article{qasim_control_2020,
	address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
	title = {Control {Logic} {Forensics} {Framework} using {Built}-in {Decompiler} of {Engineering} {Software} in {Industrial} {Control} {Systems}},
	volume = {33},
	doi = {10.1016/j.fsidi.2020.301013},
	abstract = {In industrial control systems (ICS), attackers inject malicious control-logic into programmable logic controllers (PLCs) to sabotage physical processes, such as nuclear plants, traffic-light signals, elevators, and conveyor belts. For instance, Stuxnet operates by transfering control logic to Siemens S7-300 PLCs over the network to manipulate the motor speed of centrifuges. These devestating attacks are referred to as control-logic injection attacks. Their network traffic, if captured, contains malicious control logic that can be leveraged as a forensic artifact. In this paper, we present Reditus to recover control logic from a suspicious ICS network traffic. Reditus is based on the observation that an engineering software has a built-in decompiler that can transform the control logic into its source-code. Reditus integrates the decompiler with a (previously-captured) set of network traffic from a control-logic to recover the source code of the binary control-logic automatically. We evaluate Reditus on the network traffic of 40 control logic programs transferred from the SoMachine Basic engineering software to a Modicon M221 PLC. Our evaluation successfully demonstrates that Reditus can recover the source-code of a control logic from its network traffic. (C) 2020 The Author(s). Published by Elsevier Ltd on behalf of DFRWS. All rights reserved.},
	language = {English},
	number = {S},
	journal = {FORENSIC SCIENCE INTERNATIONAL-DIGITAL INVESTIGATION},
	publisher = {ELSEVIER SCI LTD},
	author = {Qasim, Syed Ali and Smith, Jared M. and Ahmed, Irfan},
	month = jul,
	year = {2020},
	note = {Backup Publisher: DFRWS USA
Type: Article; Proceedings Paper},
	keywords = {Forensics, Control logic, Industrial control systems, SCADA},
	annote = {20th Annual DFRWS USA Conference, ELECTR NETWORK, JUL 20-24, 2020},
}

@article{kumar_malsff_2026,
	address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
	title = {{MalSFF}: {Multi}-architecture malware detection using multi-static feature fusion based on image visualization and learning methods},
	volume = {129},
	issn = {0045-7906},
	doi = {10.1016/j.compeleceng.2025.110834},
	abstract = {Malware detection is a necessity in the modern digital world. This research presents a novel MalSFF: Multi-Architecture Malware Detection Using Multi-Static Feature Fusion Based on visual image analysis and transfer learning. Firstly, it decompiles binary programs to extract bytecodes and assembly code (ASM) through reverse-engineering before transforming them into grayscale images. This research strategically fine-tunes the MobileNet models (V1, V2, V3-Small, and V3-Large) for feature extraction of both file types. Thereafter, it performs feature stacking through early fusion, late fusion, and ensemble voting to obtain a single feature map, and then utilizes a filter-based feature selection algorithm. Finally, the MalSFF employs six different classifiers, with optimized hyperparameters using an automated grid-search algorithm. For better generalization, this study uses four different datasets: (i) Microsoft BIG, (ii) MalImg, (iii) Dumpware10, and (iv) Real-world samples. The MalSFF achieved 98.72\% accuracy for the MalImg and 96.93\% accuracy, 97\% precision, 97\% recall, and 97\% F1-score, 0.012 ms of response time for the BIG dataset. For memory-resident malware, it achieved 93.84\% accuracy and a 91\% F1-score, with a response time of only 0.05 s. The MalSFF demonstrates resilience against FGSM, PGD, and DeepFool adversarial attacks. The MalSFF is a lightweight and computationally efficient, well-suited for resource-constrained IIoT networks.},
	language = {English},
	number = {B},
	journal = {COMPUTERS \& ELECTRICAL ENGINEERING},
	publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
	author = {Kumar, Sanjeev and Kumar, Anil},
	month = jan,
	year = {2026},
	note = {Type: Article},
	keywords = {Malware detection, Cybersecurity, Artificial intelligence, Adversarial attacks, Convolution neural network, Feature fusion, Image visualization},
}

@article{prasad_andromd_2025,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {{AndroMD}: {An} {Android} malware detection framework based on source code analysis and permission scanning},
	volume = {28},
	doi = {10.1016/j.rineng.2025.107050},
	abstract = {The rapid growth of Android-based mobile and IoT applications has significantly increased the attack surface for malicious actors. These adversaries often exploit apps and social engineering to deliver malware that compromises device security and user privacy. To address this ongoing threat, we present AndroMD, an intelligent and scalable Android malware detection framework that combines automated dataset construction, optimal feature selection, and ensemble-based classification. The proposed framework is built on three core components. First, an automated pipeline processes over 600,000 APKs to extract static features from more than 140 million Java files and 600,000 manifest files, resulting in three distinct datasets: KeyCount, ZeroOne, and MNF. These datasets are constructed using keys and patterns derived from a detailed analysis of real decompiled malware code, ensuring semantic relevance. Second, we introduce the AndroMD Optimal Feature Selection (AOFS) method, which selects compact, high-performing feature subsets using iterative evaluation based on ensemble feedback. Third, an ensemble detection model combines Random Forest, Decision Tree, and Bagging classifiers, with a threshold-based aggregation mechanism that allows fine-grained control over detection sensitivity. Extensive evaluation demonstrates AndroMD's strong performance, achieving up to 99.88\% accuracy on internal datasets and 91.66\% accuracy in live testing, including detection of custom and zero-day malware samples. AndroMD also identifies threats overlooked by VirusTotal, showcasing its real-world applicability. The framework, along with sample datasets and code, is made publicly available to support reproducibility and further research on Android security.},
	language = {English},
	journal = {RESULTS IN ENGINEERING},
	publisher = {ELSEVIER},
	author = {Prasad, Arvind and Chandra, Shalini and Alenazy, Wael Mohammad and Ali, Gauhar and Shah, Sajid and ElAffendi, Mohammed},
	month = feb,
	year = {2025},
	note = {Type: Article},
	keywords = {Machine learning, Cybersecurity, Android malware dataset, Feature selection, Malware analysis},
}

@article{zhao_feature_2026,
	address = {2455 ℡LER RD, THOUSAND OAKS, CA 91320 USA},
	title = {Feature fusion-based squeeze and excitation ({FFSE})-bidirectional temporal convolutional network ({BiTCN}): {A} hybrid malware detection model},
	volume = {34},
	issn = {0926-227X},
	doi = {10.1177/0926227X251370259},
	abstract = {Cybersecurity threats continue to escalate with the rapid advancement of internet technology, with malicious code posing a particularly daunting challenge. Traditional detection methods based on feature code matching struggle to keep pace with evolving anti-detection techniques and the proliferation of malicious code variants. Current malware detection often relies on extracting opcode sequences or converting binary files into grayscale maps for analysis based on deep learning. However, text-based methods for malware detection face decompilation errors due to obfuscation, which compromise feature extraction accuracy, and the limitation of n-gram in capturing global behavior patterns beyond local opcode sequences. Image-based methods, on the other hand, risk losing code structure and semantics during image conversion, and the need for fixed-size inputs in convolutional neural networks can lead to feature information loss during resizing or cropping. To address these challenges, this paper proposes feature fusion-based squeeze and excitation (FFSE)-bidirectional temporal convolutional network (BiTCN), a novel hybrid malware detection model that integrates the advantages of FFSE and BiTCN. The FFSE is utilized to extract multi-scale features and fuse global and local features with a channel attention mechanism, while the BiTCN is adopted to capture temporal evolution of malware behavior and integrate features of different levels with a pooling fusion mechanism. Experimental results on the BIG2015 and DataCon datasets demonstrate that the proposed model outperforms existing malware detection methods in terms of Accuracy , Recall , and F1-score . It proves the robustness and generalization of the proposed model and contributes significantly to the field of malware detection.},
	language = {English},
	number = {1},
	journal = {JOURNAL OF COMPUTER SECURITY},
	publisher = {SAGE PUBLICATIONS INC},
	author = {Zhao, Jinxiong and Niu, Ji'en and Yang, Yong and Li, Zhiru and Liu, Dongqing and Zhang, Lei},
	month = jan,
	year = {2026},
	note = {Type: Article},
	keywords = {malware detection, BiTCN, channel attention mechanism, FFSE, pooling fusion mechanism},
	pages = {29--45},
}

@article{bai_nature_2025,
	address = {PO BOX 564, 1001 LAUSANNE, SWITZERLAND},
	title = {From nature to engineering: {Coupling} bioinspired gradient pores and composite synergy in negative {Poisson} aramid/cellulose aerogels for oil-water separation and thermal insulation},
	volume = {522},
	issn = {1385-8947},
	doi = {10.1016/j.cej.2025.167139},
	abstract = {Natural hierarchical porous materials, inspired by biological systems like plant vascular networks, hold trans-formative potential for energy-efficient and environmental applications. However, translating these natural designs into engineered aerogels with precise multiscale pore control and multifunctional performance remains a critical challenge. To address this gap, we propose a dual-innovation strategy integrating bioinspired structural engineering and composite property synergy. First, we developed an aramid-fiber decompilation/reconstruction process using ultrasound-assisted hydrothermal cracking to produce platelet nanofibers. Second, a dynamic icetemplating technique was employed to create annular temperature gradients, enabling directional self-assembly of nanofibers into density-gradient pore architectures. This bioinspired based design not only replicates the structural complexity of natural systems and realizes the interfacial synergy of the components (embodied by a negative Poisson's ratio of-0.30 and an ultra-high porosity of 94.20 \%), but also confers an excellent fatigue resistance to Si@ANF/CNF aerogel, which maintains a strain retention rate of 83.06 \% after 100 cycles of 50 \% strain. The resulting composite system demonstrates exceptional multifunctionality, including excellent oil-water separation capability (49-102 g/g, 99.76 \% water repellency) and superior thermal insulation (0.044 W/m \& sdot;K at 160 degrees C). By demonstrating that bioinspired structural precision inherently enables multifunctional optimization, this work bridges the gap between natural design principles and scalable engineering solutions, offering a sustainable platform for advanced aerogel applications.},
	language = {English},
	journal = {CHEMICAL ENGINEERING JOURNAL},
	publisher = {ELSEVIER SCIENCE SA},
	author = {Bai, Yu and Zhang, Wanqi and Zhang, Zhen and Zhang, Xiaotao and Wang, Ximing and Wu, Yiqiang},
	month = oct,
	year = {2025},
	note = {Type: Article},
	keywords = {Aramid nanofibers, Bioinspired aerogels, Hierarchical pores, Negative Poisson's ratio, Oil-water separation},
}

@article{karamitas_recover_2025,
	address = {22 RUE DE PALESTRO, PARIS, 75002, FRANCE},
	title = {{REcover}: towards recovering object files from stripped binary executables},
	volume = {21},
	issn = {2263-8733},
	doi = {10.1007/s11416-025-00565-1},
	abstract = {Reverse engineering complex proprietary software is a tedious and time consuming task. A fair amount of the overall effort is usually devoted to locating those software components which are responsible for the functionality of interest (e.g., a proprietary encryption algorithm). To aid this process, several tools, available in the public domain, can be used, implementing sophisticated algorithms for control-flow recovery, stack frame recovery, data type inference, decompilation etc. However, the important problem of decomposing a binary executable to its constituent object files (or compile-units) has not been, in our opinion, sufficiently studied. In this paper we present novel techniques for estimating the number, as well as the boundaries, of compile-units in binary executables. We present algorithms which recover information that improves the precision degree in reverse engineering tasks. In addition, our algorithms can be used to reduce the effort of locating, recovering and understanding specific functionalities of closed-source software. We evaluate our algorithms on the public DeepBinDiff ELF dataset, consisting of {\textbackslash}textbackslashdocumentclass[12pt]\{minimal\} {\textbackslash}textbackslashusepackage\{amsmath\} {\textbackslash}textbackslashusepackage\{wasysym\} {\textbackslash}textbackslashusepackage\{amsfonts\} {\textbackslash}textbackslashusepackage\{amssymb\} {\textbackslash}textbackslashusepackage\{amsbsy\} {\textbackslash}textbackslashusepackage\{mathrsfs\} {\textbackslash}textbackslashusepackage\{upgreek\} {\textbackslash}textbackslashsetlength\{{\textbackslash}textbackslashoddsidemargin\}\{-69pt\} {\textbackslash}textbackslashbegin\{document\}{\textbackslash}textbackslashapprox{\textbackslash}textback slashend\{document\} 2000 binaries, as well as two larger executables of GNU GDB, built for ARM and AArch64 and show that they consistently approach the ground-truth, with an average recovery precision close to 75\%. Furthermore, we show how our research can aid in binary diffing applications, by comparing the recovered compile-unit structure of a pair of Microsoft Windows kernel images. We make our prototype implementation, written in Python and named REcover, publicly available for further evaluation by the reverse engineering community.},
	language = {English},
	number = {1},
	journal = {JOURNAL OF COMPUTER VIROLOGY AND HACKING TECHNIQUES},
	publisher = {SPRINGER FRANCE},
	author = {Karamitas, Chariton and Kehagias, Athanasios},
	month = aug,
	year = {2025},
	note = {Type: Article},
	keywords = {Reverse engineering, Clustering methods, Community networks, Software design},
}

@article{bojic_modernizing_2025,
	address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
	title = {Modernizing 90s {Era} {Software} to a {New} {Language} and {Environment} {Using} {LLMs} - {An} {Empirical} {Investigation}},
	volume = {35},
	issn = {0218-1940},
	doi = {10.1142/S021819402550024X},
	abstract = {Legacy software, particularly from the 1990s, often becomes obsolete due to aging hardware and outdated software environments. Traditionally, software modernization required extensive manual effort, involving reverse engineering, code rewriting, and re-architecting. However, advancements in large language models (LLMs) have introduced new possibilities for automating software translation and modernization. This paper explores the feasibility of using LLMs for modernizing 90s-era Windows applications, specifically migrating legacy C and C++ code to Python. Our methodology includes decompilation, source code analysis, automated translation using ChatGPT, and user interface reconstruction. We empirically evaluate three software projects by analyzing LLM-based translation accuracy across different code structures, including algorithmic logic, file handling, and graphical interfaces. Results indicate that while LLMs achieve high translation accuracy (-88\%) for structured code, challenges persist in handling decompiled code and user interface generation. The study provides insights into the effectiveness and limitations of LLMs in real-world software renovation, offering guidelines for leveraging machine learning in legacy system modernization. These findings contribute to both academic research and practical applications, suggesting a pathway for cost-effective and scalable legacy software migration.},
	language = {English},
	number = {08},
	journal = {INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING},
	publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
	author = {Bojic, Dragan and Draskovic, Drazen},
	month = aug,
	year = {2025},
	note = {Type: Article},
	keywords = {Decompilation, machine learning, software translation},
	pages = {1099--1119},
}

@article{peng_decentralized_2026,
	address = {125 London Wall, London, ENGLAND},
	title = {A decentralized defense model for covert zero-dynamic attacks in industrial control systems},
	volume = {265},
	issn = {0951-8320},
	doi = {10.1016/j.ress.2025.111483},
	abstract = {The architecture of Industrial Control Systems (ICS) has evolved into an integrated cyber-physical system, introducing covert, zero-dynamic cyberattack vectors that threaten the reliability of ICS. We address two issues: (1) how to use decentralized solutions to defend resource-constrained ICS against highly covert zero-dynamic attacks; (2) how to improve performance (meeting real-time, high-frequency interaction demands) while enhancing reliability via decentralization. We propose a defense model integrating blockchain, zero-knowledge proofs, and smart contract (SC) obfuscation to bolster ICS resilience. A customized zk-SNARK algorithm enables efficient identity authentication, completed in under 3 ms. The Garble framework obfuscates SCs, concealing ICS device IP addresses and disrupting attack chains. A blockchain acts as a secure intermediary between the engineer workstation (EW) and programmable logic controller (PLC). To reduce blockchain overhead, we refine a proportional-integral-derivative (PID)-based roulette wheel algorithm. Obfuscated SCs resist decompilation by tools such as Objdump, ensuring robust protection. Our node selection mechanism balances security and diversity, mitigating systemic biases like the Matthew effect. By leveraging blockchain to supply computational power for encrypting and protecting ICS data flows, we offer new insights into defending ICS against highly covert cyberattacks. Experimental evaluation validates the model's effectiveness under real-world ICS scenarios.},
	language = {English},
	number = {A},
	journal = {RELIABILITY ENGINEERING \& SYSTEM SAFETY},
	publisher = {ELSEVIER SCI LTD},
	author = {Peng, Xiangzhen and Zheng, Chengliang and Shi, Jianyu and Cui, Xiaohui},
	month = jan,
	year = {2026},
	note = {Type: Article},
	keywords = {Cyber security, Reliability engineering, Industrial control systems, Blockchain smart contracts, Zero-dynamic attacks},
}

@article{liu_compiler_2025,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Compiler {Identification} with {Divisive} {Analysis} and {Support} {Vector} {Machine}},
	volume = {17},
	doi = {10.3390/sym17060867},
	abstract = {Compilers play a crucial role in software development, as most software must be compiled into binaries before release. Analyzing the compiler version from binary files is of great importance in software reverse engineering, maintenance, traceability, and information security. In this work, we propose a novel framework for compiler version identification. Firstly, we generated 1000 C language source codes using CSmith and subsequently compiled them into 16,000 binary files using 16 distinct versions of compilers. The symmetric distribution of the dataset among different compiler versions may ensure unbiased model training. Then, IDA Pro was used to decompile the binary files into assembly instruction sequences. From these sequences, we extracted frequency-based features via the Bag-of-Words (BOW) model and sequence-based features derived from the grey-level co-occurrence matrix (GLCM). Finally, we introduced a divide-and-conquer framework (DIANA-SVM) to effectively classify compiler versions. The experimental results demonstrate that traditional Support Vector Machine (SVM) models struggle to accurately identify compiler versions using compiled executable files. In contrast, DIANA-SVM's symmetric data separation approach enhances performance, achieving an accuracy of 94\% (+/- 0.375\%). This framework enables precise identification of high-risk compiler versions, offering a reliable tool for software supply chain security. Theoretically, our GLCM-based sequence modeling and divide-and-conquer framework advance feature extraction methodologies for binary files, offering a scalable solution for similar classification tasks beyond compiler identification.},
	language = {English},
	number = {6},
	journal = {SYMMETRY-BASEL},
	publisher = {MDPI},
	author = {Liu, Changlan and Zhang, Yingsong and Zuo, Peng and Wang, Peng},
	month = jun,
	year = {2025},
	note = {Type: Article},
	keywords = {decompilation, compiler identification, DIANA-SVM, frequency-based features, sequence-based features},
}

@article{vu_minh_static_2025,
	address = {Floor 5, Northspring 21-23 Wellington Street, Leeds, W YORKSHIRE, ENGLAND},
	title = {A static method for detecting android malware based on directed {API} call},
	volume = {21},
	issn = {1744-0084},
	doi = {10.1108/IJWIS-03-2024-0095},
	abstract = {Purpose- The openness of the Android operating system offers users convenience but also exposes them to a multitude of malicious applications. Consequently, analyzing applications before installation has become a crucial research area in mobile security. Static analysis, known for its accuracy and low cost, is a prominent method within this field. This paper aims to propose an ML/DL-based approach to detect benign and malicious applications in APK format. Design/methodology/approach- The analysis method, detailed further in the paper, consists of five steps. Step 1, each APK file in the sample set undergoes decompilation to convert it into source code. Then, directed API call graph (DACG) generator is used to analyze the decompiled source code from Step 1 and extract API calls. After that, the authors apply the graph2vec method to convert the DACG data set into characteristic subgraphs. Next, saving the necessary features that each model needs to learn from the vector set. This helps reduce the vector dimensionality for each model type and reduces time and noise by eliminating unnecessary features. Finally, training and evaluating the ability to detect Android malware based on popular machine learning algorithms such as Random Forest, support vector machine, K-nearest neighbor, logistic regression and one of the most powerful machine learning algorithms currently available, gradient boosting regression. Findings- The authors come to the conclusion, feature graphs based on API call graphs are effective in detecting Android malware. Experimental results demonstrate the proposed method's superiority over existing detection methods on a data set of 7,000 samples, achieving TPR {\textgreater} 97\%, FPR {\textless} 1\% and AUC similar to 0.98. Following these steps, a final classification will determine the safety of the tested application, aiding users in avoiding malware installation. Research limitations/implications- Although some limitations remain to be addressed, the DACG construction method holds significant potential for further exploration. Future research will focus on integrating dynamic analysis techniques to broaden the detectable and classifiable Android malware categories. In addition, the authors aim to adapt the methodology for broader applicability to other system types, including the widely used ELF systems in Linux. Originality/value- In the study, the authors addressed the issue of generating graph-based feature for Android malware detection in a meaningful, practical and efficient way. The results can be used as a pattern for similar scenarios and applications.},
	language = {English},
	number = {3},
	journal = {INTERNATIONAL JOURNAL OF WEB INFORMATION SYSTEMS},
	publisher = {EMERALD GROUP PUBLISHING LTD},
	author = {Vu Minh, Manh and Nguyen, Huy-Trung and Le, H. Viet and Nguyen, Tri Duc and Do, Xuan Cho},
	month = may,
	year = {2025},
	note = {Type: Article},
	keywords = {Static analysis, Machine learning, Android malware, API call, Directed graph},
	pages = {183--204},
}

@article{ren_low_2025,
	address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
	title = {Low complexity decompression method for {FPGA} bitstreams},
	volume = {24},
	issn = {1615-5262},
	doi = {10.1007/s10207-025-00982-9},
	abstract = {FPGA bitstream is a binary file containing chip configuration information. It is often compressed to conserve storage space and reduce configuration time. However, if the bitstream file is compressed, hardware attacks cannot be detected because FPGA reverse-engineering tools can only decompile uncompressed and unencrypted bitstream. This study proposes a low complexity decompression method for commercial FPGA bitstreams, which is a crucial step for assessing the security of FPGAs. By comparing the structure of compressed and uncompressed bitstream files, we identify the compression region, extract the compressed data, and construct a database for the mapping information. Then, we introduce a decoding algorithm that utilizes a sliding window approach based on the characteristics of the compressed data and implement the algorithm on an Intel Cyclone V FPGA. Finally, the proposed framework is validated on 24 different types of Intel FPGAs from 6 families, with a perfect accuracy rate of 100\%. To the best of our knowledge, this is the first paper to disclose a decompression method for commercial FPGAs.},
	language = {English},
	number = {1},
	journal = {INTERNATIONAL JOURNAL OF INFORMATION SECURITY},
	publisher = {SPRINGER},
	author = {Ren, Lingrui and Zhang, Xingcan and Wang, Jian},
	month = feb,
	year = {2025},
	note = {Type: Article},
	keywords = {Bitstream, Decompression method, FPGA, Hardware security},
}

@article{thabit_detecting_2025,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Detecting {Malicious} .{NET} {Executables} {Using} {Extracted} {Methods} {Names}},
	volume = {6},
	doi = {10.3390/ai6020020},
	abstract = {The .NET framework is widely used for software development, making it a target for a significant number of malware attacks by developing malicious executables. Previous studies on malware detection often relied on developing generic detection methods for Windows malware that were not tailored to the unique characteristics of .NET executables. As a result, there remains a significant knowledge gap regarding the development of effective detection methods tailored to .NET malware. This work introduces a novel framework for detecting malicious .NET executables using statically extracted method names. To address the lack of datasets focused exclusively on .NET malware, a new dataset consisting of both malicious and benign .NET executable features was created. Our approach involves decompiling .NET executables, parsing the resulting code, and extracting standard .NET method names. Subsequently, feature selection techniques were applied to filter out less relevant method names. The performance of six machine learning models-XGBoost, random forest, K-nearest neighbor (KNN), support vector machine (SVM), logistic regression, and na \& iuml;ve Bayes-was compared. The results indicate that XGBoost outperforms the other models, achieving an accuracy of 96.16\% and an F1-score of 96.15\%. The experimental results show that standard .NET method names are reliable features for detecting .NET malware.},
	language = {English},
	number = {2},
	journal = {AI},
	publisher = {MDPI},
	author = {Thabit, Hamdan and Ahmad, Rami and Abdullah, Ahmad and Abualkishik, Abedallah Zaid and Alwan, Ali A.},
	month = feb,
	year = {2025},
	note = {Type: Article},
	keywords = {machine learning, static analysis, malware detection, malware analysis, .NET, windows},
}

@article{cui_sensitive_2025,
	address = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
	title = {Sensitive {Target}-{Guided} {Directed} {Fuzzing} for {IoT} {Web} {Services}},
	volume = {83},
	issn = {1546-2218},
	doi = {10.32604/cmc.2025.063592},
	abstract = {The development of the Internet of Things (IoT) has brought convenience to people's lives, but it also introduces significant security risks. Due to the limitations of IoT devices themselves and the challenges of re-hosting technology, existing fuzzing for IoT devices is mainly conducted through black-box methods, which lack effective execution feedback and are blind. Meanwhile, the existing static methods mainly rely on taint analysis, which has high overhead and high false alarm rates. We propose a new directed fuzz testing method for detecting bugs in web service programs of IoT devices, which can test IoT devices more quickly and efficiently. Specifically, we identify external input entry points using multiple features. Then we quickly find sensitive targets and paths affected by external input sources based on sensitive data flow analysis of decompiled code, treating them as testing objects. Finally, we perform a directed fuzzing test. We use debugging interfaces to collect execution feedback and guide the program to reach sensitive targets based on program pruning techniques. We have implemented a prototype system, AntDFuzz, and evaluated it on firmware from ten devices across five well-known manufacturers. We discovered twelve potential vulnerabilities, seven of which were confirmed and assigned bug id by China National Vulnerability Database (CNVD). The results show that our approach has the ability to find unknown bugs in real devices and is more efficient compared to existing tools.},
	language = {English},
	number = {3},
	journal = {CMC-COMPUTERS MATERIALS \& CONTINUA},
	publisher = {TECH SCIENCE PRESS},
	author = {Cui, Xiongwei and Wang, Yunchao and Wei, Qiang},
	year = {2025},
	note = {Type: Article},
	keywords = {IoT, vulnerabilities, directed fuzzing, sensitive targets},
	pages = {4939--4959},
}

@article{yu_crosscode2vec_2025,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {{CrossCode2Vec}: {A} unified representation across source and binary functions for code similarity detection},
	volume = {620},
	issn = {0925-2312},
	doi = {10.1016/j.neucom.2024.129238},
	abstract = {Code similarity detection identifies code by analyzing similarities in syntax, semantics, and structure, which includes types of tasks: source-to-source, binary-to-binary, and source-to-binary. Due to encoding and representation disparities between source and binary code, existing methods have mainly focused on individual tasks, without providing a universal solution. Additionally, current source-to-binary tasks only achieve one-to-one matching between source code and binary functions, neglecting the one-to-many relationship inherent between source code and its cross-compiled binaries. In this paper, we propose CrossCode2Vec, a unified framework for representing code in both source and binary functions, which aims to bridge the gap in original coding features and provide a standardized similarity measurement across three code similarity detection tasks. For source code and its corresponding compiled binary, we first design an enhanced Abstract Path Context data preprocessing method, construct an abstract syntax tree (AST) from both source code functions and decompiled binary functions, and implement the function embedding followed by the pre-trained Word2vec model. Then we propose a task-specific data sampling strategy. We establish a one-to-one correspondence between source and binary functions through symbol tables and create a one-to-many relationship between source functions and their cross-compiled binaries based on sampling rules. Finally, we employ a hierarchical LSTM-attention network to facilitate the representation and similarity measurement of functions. We conduct both extrinsic and intrinsic evaluations to confirm the effectiveness of CrossCode2Vec in code representation and code similarity tasks, validating its superiority in model architecture and data processing methods. CrossCode2Vec demonstrates stable and exceptional performance across multiple experiments, reinforcing its ability to bridge the gap between source and binary code representations while effectively measuring their similarities.},
	language = {English},
	journal = {NEUROCOMPUTING},
	publisher = {ELSEVIER},
	author = {Yu, Gaoqing and An, Jing and Lyu, Jiuyang and Huang, Wei and Fan, Wenqing and Cheng, Yixuan and Sui, Aina},
	month = mar,
	year = {2025},
	note = {Type: Article},
	keywords = {Code Similarity Detection, Cross-modal code matching, Representation learning},
}

@article{balogh_using_2024,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Using {Generative} {AI} {Models} to {Support} {Cybersecurity} {Analysts}},
	volume = {13},
	issn = {2079-9292},
	doi = {10.3390/electronics13234718},
	abstract = {One of the tasks of security analysts is to detect security vulnerabilities and ongoing attacks. There is already a large number of software tools that can help to collect security-relevant data, such as event logs, security settings, application manifests, and even the (decompiled) source code of potentially malicious applications. The analyst must study these data, evaluate them, and properly identify and classify suspicious activities and applications. Fast advances in the area of Artificial Intelligence have produced large language models that can perform a variety of tasks, including generating text summaries and reports. In this article, we study the potential black-box use of LLM chatbots as a support tool for security analysts. We provide two case studies: the first is concerned with the identification of vulnerabilities in Android applications, and the second one is concerned with the analysis of security logs. We show how LLM chatbots can help security analysts in their work, but point out specific limitations and security concerns related to this approach.},
	language = {English},
	number = {23},
	journal = {ELECTRONICS},
	publisher = {MDPI},
	author = {Balogh, Stefan and Mlyncek, Marek and Vranak, Oliver and Zajac, Pavol},
	month = feb,
	year = {2024},
	note = {Type: Article},
	keywords = {LLM, cybersecurity, vulnerability assessment},
}

@article{xiang_fine-grained_2025,
	address = {OXFORD FULFILLMENT CENTRE THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
	title = {A fine-grained approach for {Android} taint analysis based on labeled taint value graphs},
	volume = {148},
	issn = {0167-4048},
	doi = {10.1016/j.cose.2024.104162},
	abstract = {Static taint analysis is a widely used method to identify vulnerabilities in Android applications. However, the existing tools for static analysis often struggle with processing times, particularly when dealing with complex real-world programs. To reduce time consumption, some tools choose to sacrifice analytical precision, e.g., FastDroid sets an upper limit for analysis iterations in Android applications. In this paper, we propose a labeled taint value graph (LTVG) to store taint flows, and implement a fine-grained analysis tool called LabeledDroid. This graph is constructed based on the taint value graph (TVG) of FastDroid, and takes into account both precision and time consumption. That is, we decompile an Android app into Jimple statements, develop finegrained propagation rules to handle List, and construct LTVGs according to these rules. Afterwards, we traverse LTVGs to obtain high-precision taint flows. An analysis of 39 apps from the TaintBench benchmark shows that LabeledDroid is 0.87 s faster than FastDroid on average. Furthermore, if some common accuracy parameters are adapted in both LabeledDroid and FastDroid, the experiment demonstrates that the former is more scalable. Moreover, the maximum analysis time of LabeledDroid is less than 200 s and its average time is 46.25 s, while FastDroid sometimes experiences timeouts with durations longer than 600 s. Additionally, LabeledDroid achieves a precision of 70\% in handling lists, while FastDroid and TaintSA achieve precisions of 38.9\% and 41.2\%, respectively.},
	language = {English},
	journal = {COMPUTERS \& SECURITY},
	publisher = {ELSEVIER ADVANCED TECHNOLOGY},
	author = {Xiang, Dongming and Lin, Shuai and Huang, Ke and Ding, Zuohua and Liu, Guanjun and Li, Xiaofeng},
	month = jan,
	year = {2025},
	note = {Type: Article},
	keywords = {Android security, Vulnerability detection, Fine-grained analysis, Static taint analysis},
}

@article{li_gmadv_2024,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {{GMADV}: {An} android malware variant generation and classification adversarial training framework},
	volume = {84},
	issn = {2214-2126},
	doi = {10.1016/j.jisa.2024.103800},
	abstract = {Android malware uses anti-reverse analysis and APK shelling technology, which leads to the failure of the classification method based on decompiled features and the reduction of the classification accuracy based on single file features. Moreover, the lack of samples in some families of Android malware makes the classification model based on sample learning ineffective. To solve the above problems, this paper proposes a two-layer general framework for Android malware classification and adversarial training named GMADV, which enhances classifier performance through adversarial training. In the sample classification layer, based on the transformation method of the Markov model, it is proposed for the first time to convert the three files in the APK into RGB Markov images, and use VGG13 to automatically extract features and classification; In the variant amplification layer, the idea of “regression for generation” is firstly proposed, and GMM-GAN based on Gaussian process is designed to amplify the diversity of samples within the family. The experimental results show that RGB Markov images have better classification performance than grayscale images. On the three datasets, the classification effect after amplification has been improved to varying degrees, and all F1\_Score reaches 95 \%. Compared with other methods, GMADV has stronger family sample amplification ability and greater adversarial intensity.},
	language = {English},
	journal = {JOURNAL OF INFORMATION SECURITY AND APPLICATIONS},
	publisher = {ELSEVIER},
	author = {Li, Shuangcheng and Tang, Zhangguo and Li, Huanzhou and Zhang, Jian and Wang, Han and Wang, Junfeng},
	month = aug,
	year = {2024},
	note = {Type: Article},
	keywords = {Android malware, GMM-GAN, RGB Markov image, Variant amplification},
}

@article{drabent_how_2024,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {How to {Circumvent} and {Beat} the {Ransomware} in {Android} {Operating} {System}-{A} {Case} {Study} of {Locker}.{CB}!tr},
	volume = {13},
	doi = {10.3390/electronics13112212},
	abstract = {Ransomware is one of the most extended cyberattacks. It consists of encrypting a user's files or locking the smartphone in order to blackmail a victim. The attacking software is ordered on the infected device from the attacker's remote server, known as command and control. In this work, we propose a method to recover from a Locker.CB!tr ransomware attack after it has infected and hit a smartphone. The novelty of our approach lies on exploiting the communication between the ransomware on the infected device and the attacker's command and control server as a point to reverse disruptive actions like screen locking or file encryption. For this purpose, we carried out both a dynamic and a static analysis of decompiled Locker.CB!tr ransomware source code to understand its operation principles and exploited communication patterns from the IP layer to the application layer to fully impersonate the command and control server. This way, we gained full control over the Locker.CB!tr ransomware instance. From that moment, we were able to command the Locker.CB!tr ransomware instance on the infected device to unlock the smartphone or decrypt the files. The contributions of this work are a novel method to recover the mobile phone after ransomware attack based on the analysis of the ransomware communication with the C\&C server; and a mechanism for impersonating the ransomware C\&C server and thus gaining full control over the ransomware instance.},
	language = {English},
	number = {11},
	journal = {ELECTRONICS},
	publisher = {MDPI},
	author = {Drabent, Kornel and Janowski, Robert and Mongay Batalla, Jordi},
	month = jun,
	year = {2024},
	note = {Type: Article},
	keywords = {reverse engineering, Android, ransomware, C\&C server, Locker.CB!tr, malicious symmetric file encryption, ransomware mitigation, smartphone screen locking},
}

@article{wu_comprehensive_2024,
	address = {24-28 OVAL RD, LONDON NW1 7DX, ENGLAND},
	title = {A comprehensive survey of smart contract security: {State} of the art and research directions},
	volume = {226},
	issn = {1084-8045},
	doi = {10.1016/j.jnca.2024.103882},
	abstract = {Future protocols in the digital society will be built on the foundation of smart contracts, which are code and algorithmic contracts. Smart contracts enable all phases of the contracting process without the need for outside parties by using protocols and user interfaces. But as blockchain technology has quickly advanced, many security flaws in smart contracts have also come to light. This article offers a thorough examination and organized summary of the pertinent material of smart contract security analysis. These sections make up the bulk of our survey's contributions. First, a brief history of Ethereum is provided, followed by a proposal of the security difficulties now faced by blockchain smart contracts, with a focus on the analysis and classification of various security flaws. Second, based on a thorough examination of these studies, we present a summary of various smart contract security options, including case studies and detailed descriptions of the state-of-the-art in terms of automatic auditing, subject matter experts, scalable smart contracts, smart contract templates, decompilers, semantic frameworks, and anomaly detection. Finally, we go over each sort of solution's advantages and disadvantages and outline potential future research trajectories.},
	language = {English},
	journal = {JOURNAL OF NETWORK AND COMPUTER APPLICATIONS},
	publisher = {ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD},
	author = {Wu, Guangfu and Wang, Haiping and Lai, Xin and Wang, Mengmeng and He, Daojing and Chan, Sammy},
	month = jun,
	year = {2024},
	note = {Type: Review},
	keywords = {Smart contracts, Anomaly detection, Automated audits, Security vulnerabilities, Semantic frameworks, Smart contract security},
}

@article{avellaneda-tamayo_chemical_2024,
	address = {1155 16TH ST, NW, WASHINGTON, DC 20036 USA},
	title = {Chemical {Multiverse} and {Diversity} of {Food} {Chemicals}},
	volume = {64},
	issn = {1549-9596},
	doi = {10.1021/acs.jcim.3c01617},
	abstract = {Food chemicals have a fundamental role in our lives, with an extended impact on nutrition, disease prevention, and marked economic implications in the food industry. The number of food chemical compounds in public databases has substantially increased in the past few years, which can be characterized using chemoinformatics approaches. We and other groups explored public food chemical libraries containing up to 26,500 compounds. This study aimed to analyze the chemical contents, diversity, and coverage in the chemical space of food chemicals and additives and, from here on, food components. The approach to food components addressed in this study is a public database with more than 70,000 compounds, including those predicted via omics techniques. It was concluded that food components have distinctive physicochemical properties and constitutional descriptors despite sharing many chemical structures with natural products. Food components, on average, have large molecular weights and several apolar structures with saturated hydrocarbons. Compared to reference databases, food component structures have low scaffold and fingerprint-based diversity and high structural complexity, as measured by the fraction of sp3 carbons. These structural features are associated with a large fraction of macronutrients as lipids. Lipids in food components were decompiled by an analysis of the maximum common substructures. The chemical multiverse representation of food chemicals showed a larger coverage of chemical space than natural products and FDA-approved drugs by using different sets of representations.},
	language = {English},
	number = {4},
	journal = {JOURNAL OF CHEMICAL INFORMATION AND MODELING},
	publisher = {AMER CHEMICAL SOC},
	author = {Avellaneda-Tamayo, Juan F. and Chavez-Hernandez, Ana L. and Prado-Romero, Diana L. and Medina-Franco, Jose L.},
	month = feb,
	year = {2024},
	note = {Type: Article},
	pages = {1229--1244},
}

@article{cheremisino_graph_2024,
	address = {PUBLISHING HOUSE SCIENTIFIC \& TECHNICAL LITERATURE, TOMSK, 00000, RUSSIA},
	title = {{GRAPH} {METHODS} {FOR} {RECOGNITION} {OF} {CMOS} {GATES} {IN} {TRANSISTOR}-{LEVEL} {CIRCUITS}},
	issn = {2071-0410},
	doi = {10.17223/20710410/64/4},
	abstract = {The paper focuses on the decompilation of a flat transistor circuit in SPICE formatinto a hierarchical network of logic gates. The problem arises in VLSI layout verifica-tion as well as in reverse engineering transistor circuit to redesign integrated circuitand to detect untrusted attachments. The most general case is considered when theextraction of functional level structure from transistor-level circuit is performed with-out any predetermined cell library. Graph methods for solving some key tasks in thisarea are proposed. The presented graph methods have been implemented in C++ asa part of a decompilation program, which has been tested using practical transistor-level circuits},
	language = {English},
	number = {64},
	journal = {PRIKLADNAYA DISKRETNAYA MATEMATIKA},
	publisher = {PUBLISHING HOUSE SCIENTIFIC \& TECHNICAL LITERATURE},
	author = {Cheremisino, D. I. and Cheremisinova, L. D.},
	year = {2024},
	note = {Type: Article},
	keywords = {CMOS transistor circuit, graph isomorphism, logic gate recognition, SPICE format, subcircuit extraction},
}

@article{rodriguez-bazan_android_2023-1,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Android {Malware} {Classification} {Based} on {Fuzzy} {Hashing} {Visualization}},
	volume = {5},
	doi = {10.3390/make5040088},
	abstract = {The proliferation of Android-based devices has brought about an unprecedented surge in mobile application usage, making the Android ecosystem a prime target for cybercriminals. In this paper, a new method for Android malware classification is proposed. The method implements a convolutional neural network for malware classification using images. The research presents a novel approach to transforming the Android Application Package (APK) into a grayscale image. The image creation utilizes natural language processing techniques for text cleaning, extraction, and fuzzy hashing to represent the decompiled code from the APK in a set of hashes after preprocessing, where the image is composed of n fuzzy hashes that represent an APK. The method was tested on an Android malware dataset with 15,493 samples of five malware types. The proposed method showed an increase in accuracy compared to others in the literature, achieving up to 98.24\% in the classification task.},
	language = {English},
	number = {4},
	journal = {MACHINE LEARNING AND KNOWLEDGE EXTRACTION},
	publisher = {MDPI},
	author = {Rodriguez-Bazan, Horacio and Sidorov, Grigori and Escamilla-Ambrosio, Ponciano Jorge},
	month = feb,
	year = {2023},
	note = {Type: Article},
	keywords = {deep learning, convolutional neural network, fuzzy hashing, malware classification, android malware, natural language processing},
	pages = {1826--1847},
}

@article{lebedev_using_2023,
	address = {PUBLISHING HOUSE SCIENTIFIC \& TECHNICAL LITERATURE, TOMSK, 00000, RUSSIA},
	title = {{USING} {X86} {MODE} {SWITCHING} {FOR} {PROGRAM} {CODE} {PROTECTION}},
	issn = {2071-0410},
	doi = {10.17223/20710410/61/6},
	abstract = {A novel program code obfuscation approach involving the x86 mode switching is proposed in the paper. The details and existing applications of x86 mode switching are reviewed, as well as the possible consequences of using this switching to the reverse engineering tools. Based on this approach, a few specific methods are proposed and evaluated against the most popular reverse engineering tools of various purposes, including disassemblers, decompilers, binary instrumentation and symbolic execution tools. A method of seamless integration of these machine code level obfuscations to the C, C++ and possibly other compilers is also proposed.},
	language = {English},
	number = {61},
	journal = {PRIKLADNAYA DISKRETNAYA MATEMATIKA},
	publisher = {PUBLISHING HOUSE SCIENTIFIC \& TECHNICAL LITERATURE},
	author = {Lebedev, R. K.},
	month = sep,
	year = {2023},
	note = {Type: Article},
	keywords = {decompilation, symbolic execution, reverse engineering, obfuscation, disassembly, code protection, x86 mode switching},
	pages = {104--120},
}

@article{kim_neural_2023,
	address = {HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY},
	title = {A neural machine code and programming framework for the reservoir computer},
	volume = {5},
	doi = {10.1038/s42256-023-00668-8},
	abstract = {From logical reasoning to mental simulation, biological and artificial neural systems possess an incredible capacity for computation. Such neural computers offer a fundamentally novel computing paradigm by representing data continuously and processing information in a natively parallel and distributed manner. To harness this computation, prior work has developed extensive training techniques to understand existing neural networks. However, the lack of a concrete and low-level machine code for neural networks precludes us from taking full advantage of a neural computing framework. Here we provide such a machine code along with a programming framework by using a recurrent neural network-a reservoir computer-to decompile, code and compile analogue computations. By decompiling the reservoir's internal representation and dynamics into an analytic basis of its inputs, we define a low-level neural machine code that we use to program the reservoir to solve complex equations and store chaotic dynamical systems as random-access memory. We further provide a fully distributed neural implementation of software virtualization and logical circuits, and even program a playable game of pong inside of a reservoir computer. Importantly, all of these functions are programmed without requiring any example data or sampling of state space. Finally, we demonstrate that we can accurately decompile the analytic, internal representations of a full-rank reservoir computer that has been conventionally trained using data. Taken together, we define an implementation of neural computation that can both decompile computations from existing neural connectivity and compile distributed programs as new connections.},
	language = {English},
	number = {6},
	journal = {NATURE MACHINE IN℡LIGENCE},
	publisher = {NATURE PORTFOLIO},
	author = {Kim, Jason Z. and Bassett, Dani S.},
	month = jun,
	year = {2023},
	note = {Type: Article},
	pages = {622--630},
}

@article{wu_android_2023,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {An {Android} {Malware} {Detection} {Approach} to {Enhance} {Node} {Feature} {Differences} in a {Function} {Call} {Graph} {Based} on {GCNs}},
	volume = {23},
	doi = {10.3390/s23104729},
	abstract = {The smartphone has become an indispensable tool in our daily lives, and the Android operating system is widely installed on our smartphones. This makes Android smartphones a prime target for malware. In order to address threats posed by malware, many researchers have proposed different malware detection approaches, including using a function call graph (FCG). Although an FCG can capture the complete call-callee semantic relationship of a function, it will be represented as a huge graph structure. The presence of many nonsensical nodes affects the detection efficiency. At the same time, the characteristics of the graph neural networks (GNNs) make the important node features in the FCG tend toward similar nonsensical node features during the propagation process. In our work, we propose an Android malware detection approach to enhance node feature differences in an FCG. Firstly, we propose an API-based node feature by which we can visually analyze the behavioral properties of different functions in the app and determine whether their behavior is benign or malicious. Then, we extract the FCG and the features of each function from the decompiled APK file. Next, we calculate the API coefficient inspired by the idea of the TF-IDF algorithm and extract the sensitive function called subgraph (S-FCSG) based on API coefficient ranking. Finally, before feeding the S-FCSG and node features into the GCN model, we add the self-loop for each node of the S-FCSG. A 1-D convolutional neural network and fully connected layers are used for further feature extraction and classification, respectively. The experimental result shows that our approach enhances the node feature differences in an FCG, and the detection accuracy is greater than that of models using other features, suggesting that malware detection based on a graph structure and GNNs has a lot of space for future study.},
	language = {English},
	number = {10},
	journal = {SENSORS},
	publisher = {MDPI},
	author = {Wu, Haojie and Luktarhan, Nurbol and Tian, Gaoqi and Song, Yangyang},
	month = may,
	year = {2023},
	note = {Type: Article},
	keywords = {Android malware detection, function call graph, graph convolutional network, self-loop, TF-IDF},
}

@article{zhang_chunked_2023,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {A {Chunked} and {Disordered} {Data} {Privacy} {Protection} {Algorithm}: {Application} to {Resource} {Platform} {Systems}},
	volume = {13},
	doi = {10.3390/app13106017},
	abstract = {This paper provides a systematic analysis of existing resource platforms, evaluating their advantages and drawbacks with respect to data privacy protection. To address the privacy and security risks associated with resource platform data, we propose a novel privacy protection algorithm based on chunking disorder. Our algorithm exchanges data within a specific range of chunk size for the position and combines the chunked data with the MD5 value in a differential way, thus ensuring data privacy. To ensure the security of the algorithm, we also discuss the importance of preventing client and server decompilation during its implementation. The findings of our experiments are as follows. Our proposed privacy-preserving algorithm is extremely secure and easy to implement. Our algorithm has a significant avalanche effect, maintaining values of 0.61-0.85, with information entropy being maintained at 4.5-4.9. This indicates that our algorithm is highly efficient without compromising data security. Furthermore, our algorithm has strong encryption and decryption time stability. The key length can be up to 594 bits, rendering it challenging to decrypt. Compared with the traditional DES algorithm, our algorithm has better security under the same conditions and approaches the levels of security offered by the AES and RC4 algorithms.},
	language = {English},
	number = {10},
	journal = {APPLIED SCIENCES-BASEL},
	publisher = {MDPI},
	author = {Zhang, Daike and Chen, Junyang and He, Yihui and Lan, Xiaoqing and Chen, Xian and Dong, Chunlin and Li, Jun},
	month = may,
	year = {2023},
	note = {Type: Article},
	keywords = {information security, data privacy, information entropy, randomness, resource platform, secret key space, side channel attacks, software architecture, symmetric encryption, system development},
}

@article{shi_sfcgdroid_2023,
	address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
	title = {{SFCGDroid}: android malware detection based on sensitive function call graph},
	volume = {22},
	issn = {1615-5262},
	doi = {10.1007/s10207-023-00679-x},
	abstract = {Android is now one of the most popular operating systems in the world because of its open source character, so the threshold for hackers to make malware has also become lower, and more and more malware has started to threaten people's lives. Graphs are used to represent the program's syntactic and semantic structure, and can naturally represent malicious behavior, so we propose a malware detection method named SFCGDroid, which based on sensitive function call graph, so we propose a malware detection method named SFCGDroid, which based on sensitive function call graph. We first decompile the Android application to generate a function call graph (FCG), and extract the sensitive function call graph (SFCG) on the FCG. Secondly, we extract two class features (1) use the Skip-gram model to obtain function embeddings, and (2) treat the SFCG as a social network and extract the triads attribute of the sensitive API. The two types of features are combined as a feature representation of the SFCG and fed into a graph convolutional network (GCN) for malware detection. For experiments on 26,939 Android software datasets, SFCGDroid in this paper can achieve 98.22\% accuracy and 98.20\% F1 score.},
	language = {English},
	number = {5},
	journal = {INTERNATIONAL JOURNAL OF INFORMATION SECURITY},
	publisher = {SPRINGER},
	author = {Shi, Sibo and Tian, Shengwei and Wang, Bo and Zhou, Tiejun and Chen, Guanxin},
	month = oct,
	year = {2023},
	note = {Type: Article},
	keywords = {Malware detection, Graph convolutional network, Sensitive function call graph, Skip-gram, Triads},
	pages = {1115--1124},
}

@article{yuan_optimizing_2023,
	address = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
	title = {Optimizing smart contract vulnerability detection via multi-modality code and entropy embedding},
	volume = {202},
	issn = {0164-1212},
	doi = {10.1016/j.jss.2023.111699},
	abstract = {Smart contracts have been widely used in the blockchain world these years, and simultaneously vulner-ability detection has gained more and more attention due to the staggering economic losses caused by the attacker. Existing tools that analyze vulnerabilities for smart contracts heavily rely on rules predefined by experts, which are labour-intense and require domain knowledge. Moreover, predefined rules tend to be misconceptions and increase the risk of crafty potential back-doors in the future. Recently, researchers mainly used static and dynamic execution analysis to detect the vulnerabilities of smart contracts and have achieved acceptable results. However, the dynamic method cannot cover all the program inputs and execution paths, which leads to some vulnerabilities that are hard to detect. The static analysis method commonly includes symbolic execution and theorem proving, which requires using constraints to detect vulnerability. These shortcomings show that traditional methods are challenging to apply and expand on a large scale. This paper aims to detect vulnerabilities via the Bug Injection framework and transfer learning techniques. First, we train a Transformer encoder using multi-modality code, which contains source code, intermediate representation, and assembly code. The input code consists separately of Solidity source code, intermediate representation, and assembly code. Specifically, we translate source code into the intermediate representation and decompile the byte code into assembly code by the EVM compiler. Then, we propose a novel entropy embedding technique, which combines token embedding, segment embedding, and positional embedding of the Transformer encoder in our approach. After that, we utilize the Bug Injection framework to automatically generate specific types of buggy code for fine-tuning and evaluating the performance of vulnerability detection. The experimental results show that our proposed approach improves the performance in detecting reentrancy vulnerabilities and timestamp dependence. Moreover, our approach is more flexible and scalable than static and dynamic analysis approaches in detecting smart contract vulnerabilities. Our approach improves the baseline approaches by an average of 11.89\% in term of F1 score.(c) 2023 Elsevier Inc. All rights reserved.},
	language = {English},
	journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
	publisher = {ELSEVIER SCIENCE INC},
	author = {Yuan, Dawei and Wang, Xiaohui and Li, Yao and Zhang, Tao},
	month = aug,
	year = {2023},
	note = {Type: Article},
	keywords = {Transfer learning, Smart contract, Vulnerability detection, Bug injection},
}

@article{ortin_analyzing_2023,
	address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
	title = {Analyzing syntactic constructs of {Java} programs with machine learning},
	volume = {215},
	issn = {0957-4174},
	doi = {10.1016/j.eswa.2022.119398},
	abstract = {The massive number of open-source projects in public repositories has notably increased in the last years. Such repositories represent valuable information to be mined for different purposes, such as documenting recurrent syntactic constructs, analyzing the particular constructs used by experts and beginners, using them to teach programming and to detect bad programming practices, and building programming tools such as decompilers, Integrated Development Environments or Intelligent Tutoring Systems. An inherent problem of source code is that its syntactic information is represented with tree structures, while traditional machine learning algorithms use n-dimensional datasets. Therefore, we present a feature engineering process to translate tree structures into homogeneous and heterogeneous n-dimensional datasets to be mined. Then, we run different interpretable (supervised and unsupervised) machine learning algorithms to mine the syntactic information of more than 17 million syntactic constructs in Java code. The results reveal interesting information such as the Java constructs that are barely (and widely) used (e.g., bitwise operators, union types and static blocks), different language features and patterns mostly (and barely) used by beginners (and experts), the discovery of particular types of source code (e.g., helper or utility classes, data transfer objects and too complex abstractions), and how complexity is an inherent characteristic in some clusters of syntactic constructs.},
	language = {English},
	journal = {EXPERT SYSTEMS WITH APPLICATIONS},
	publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
	author = {Ortin, Francisco and Facundo, Guillermo and Garcia, Miguel},
	month = apr,
	year = {2023},
	note = {Type: Article},
	keywords = {Data mining, Abstract syntax tree, Feature engineering, Heterogeneous dataset, Programming idiom, Programming language},
}

@article{chen_predicting_2023,
	address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
	title = {Predicting {Android} malware combining permissions and {API} call sequences},
	volume = {31},
	issn = {0963-9314},
	doi = {10.1007/s11219-022-09602-4},
	abstract = {Malware detection is an important task in software maintenance. It can effectively protect user information from the attack of malicious developers. Existing studies mainly focus on leveraging permission information and API call information to identify malware. However, many studies pay attention to the API call without considering the role of API call sequences. In this study, we propose a new method by combining both the permission information and the API call sequence information to distinguish malicious applications from benign applications. First, we extract features of permission and API call sequence with a decompiling tool. Then, one-hot encoding and Word2Vec are adopted to represent the permission feature and the API call sequence feature for each application, respectively. Based on this, we leverage Random Forest (RF) and Convolutional Neural Networks (CNN) to train a permission-based classifier and an API call sequence-based classifier, respectively. Finally, we design a linear strategy to combine the outputs of these two classifiers to predict the labels of newly arrived applications. By an evaluation with 15,198 malicious applications and 15,129 benign applications, our approach achieves 98.84\% in terms of precision, 98.17\% in terms of recall, 98.50\% in terms of F1-score, and 98.52\% in terms of accuracy on average, and outperforms the state-of-art method Malscan by 2.12\%, 0.27\%, 1.20\%, and 1.24\%, respectively. In addition, we demonstrate that the method combining two features achieves better performance than the methods based on a single feature.},
	language = {English},
	number = {3},
	journal = {SOFTWARE QUALITY JOURNAL},
	publisher = {SPRINGER},
	author = {Chen, Xin and Yu, Haihua and Yu, Dongjin and Chen, Jie and Sun, Xiaoxiao},
	month = sep,
	year = {2023},
	note = {Type: Article},
	keywords = {Malware detection, Android malware, CNN, API call sequence, Permission},
	pages = {655--685},
}

@article{sharma_visualizing_2023,
	address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
	title = {Visualizing {Android} {Malicious} {Applications} {Using} {Texture} {Features}},
	volume = {23},
	issn = {0219-4678},
	doi = {10.1142/S0219467823500523},
	abstract = {Context: Due to the change and advancement in technology, day by day the internet service usages are also increasing. Smartphones have become the necessity for every person these days. It is used to perform all basic daily activities such as calling, SMS, banking, gaming, entertainment, education, etc. Therefore, malware authors are developing new variants of malwares or malicious applications especially for monetary benefits. Objective: Objective of this research paper is to develop a technique that can be used to detect malwares or malicious applications on the android devices that will work for all types of packed or encrypted malicious applications, which usually evade decompiling tools. Method: In the proposed approach, visualization method is used for the detection of malware. In the first phase, application files are converted into images and then in second phase, texture feature of images are extracted using Grey Level Co-occurrence Matrix (GLCM). In the last phase, machine learning classification algorithms are used to classify the malicious and benign applications. Results: The proposed approach is rim on different datasets collected from various repositories. Different efficiency parameters are calculated and the proposed approach is compared with the existing approaches. Conclusion: We have proposed a static technique for efficient detection of malwares. The proposed technique performs better than the existing technique.},
	language = {English},
	number = {06},
	journal = {INTERNATIONAL JOURNAL OF IMAGE AND GRAPHICS},
	publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
	author = {Sharma, Tejpal and Rattan, Dhavleesh},
	month = jan,
	year = {2023},
	note = {Type: Article},
	keywords = {machine learning, mobile apps, Android malware detection, GLCM, malware visualization, texture features},
}

@article{lu_research_2022,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Research on the {Construction} of {Malware} {Variant} {Datasets} and {Their} {Detection} {Method}},
	volume = {12},
	doi = {10.3390/app12157546},
	abstract = {Malware detection is of great significance for maintaining the security of information systems. Malware obfuscation techniques and malware variants are increasingly emerging, but their samples and API (application programming interface) sequences are difficult to obtain. This poses difficulties for the development of malware variant detection models. To address this issue in this paper, we first generated a malware variant dataset using the obfuscation technique based on the disassembly and decompilation of malware. Then, an API call dataset of these malware variants was constructed through sandboxing. Compared to similar work, the malware variants and their obfuscated API call sequences generated in this paper were all runnable. After that, taking a public API call sequence dataset of obfuscation-free malware as input, a BERT (bidirectional encoder representation from transformers) pretrained model for malware detection was constructed. To enhance the ability of this pretrained model to handle obfuscation and variants, in this paper, we used adversarial training to improve the robustness and generalization of the detection model under obfuscation. As the experimental results show, the proposed scheme can improve the classification performance of malware variants under obfuscation. The accuracy of the malware variant classification was close to that of the unobfuscated case.},
	language = {English},
	number = {15},
	journal = {APPLIED SCIENCES-BASEL},
	publisher = {MDPI},
	author = {Lu, Faming and Cai, Zhaoyang and Lin, Zedong and Bao, Yunxia and Tang, Mengfan},
	month = aug,
	year = {2022},
	note = {Type: Article},
	keywords = {malware detection, adversarial training, AI for systems, bidirectional encoder representation from transformers, intelligent data analysis},
}

@article{wang_deep_2022,
	address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
	title = {Deep {Learning}-{Based} {Multi}-classification for {Malware} {Detection} in {IoT}},
	volume = {31},
	issn = {0218-1266},
	doi = {10.1142/S0218126622502978},
	abstract = {Due to the open-source and versatility of the Android operating system, Android malware has exploded, and the malware detection of Android IoT devices has become a research hotspot in recent years. Static analysis technology cannot effectively analyze obfuscated malware. Without decomposing, the existing detection methods are mainly based on grayscale images and single files without analyzing and verifying their anti-obfuscation performance. In addition, the current detection of Android malware using deep learning is concentrated in the field of binary classification. This paper proposes a multi-classification method of the Android malware family based on multi-class feature files and RGB images to solve these problems. The method proposed in this paper does not need to decompile the Android APK installation package. However, it extracts the DEX file and XML file in batch from the APK installation package. Then, it converts the file into an RGB image using the conversion algorithm that converts Android software into images. Finally, the deep neural network automatically obtains the RGB image texture features to realize the multiple classifications of the Android malware family. Experimental data show that the proposed method has high detection performance, and the accuracy of multiple classifications of the Android malware family is as high as 99.84\%. In addition, the method based on RGB image is better than the gray-scale image in detection accuracy, and the effect of RGB image combined with DEX and XML is better than that of separate DEX file image and separate XML file image. Therefore, the method proposed in this paper can effectively detect the obfuscated Android malware, and the detection accuracy of 99.23\% can be achieved for the obfuscated sample data. Furthermore, this method has good anti-obfuscation ability. The proposed method is compared with those based on Multi-Layer Perceptron, Long Short-Term Memory, bidirectional Long Short-Term Memory and Deep Belief Network. The experimental results show the proposed method's effectiveness and high generalization performance.},
	language = {English},
	number = {17},
	journal = {JOURNAL OF CIRCUITS SYSTEMS AND COMPUTERS},
	publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
	author = {Wang, Zhiqiang and Liu, Qian and Wang, Zhuoyue and Chi, Yaping},
	month = nov,
	year = {2022},
	note = {Type: Article},
	keywords = {deep learning, Internet of Things, Android malware family, image feature, multiple classifications},
}

@article{bolanos_rodas_image_2022,
	address = {CARRETERA CENTRAL DEL NORTE, TUNJA, BOY 00000, COLOMBIA},
	title = {In the image and likeness of the nobles: the case of the new neighbours of the city of {Caloto}, {Popayan} province, {Kingdom} of {New} {Granada}, 1784-1800},
	issn = {2027-5137},
	doi = {10.19053/20275137.n25.2022.9872},
	abstract = {The arrival of the Bourbons to the Hispanic Monarchy brought with it new ways of conceiving of the society attached to its body politic. New laws and rights were promoted, which motivated among the free population a recognition of their status and states in the space of the localities of Hispanic America. In this sense, the present work attempts to approach the case of the integration and recognition of the free population of the city of Caloto, And the contiguous seat of Quilichao. The analysis of the census carried out in the year 1784 in Caloto and of the decompiled documentation of the General Archive of the Nation, from the colony section (Cauca populations) allows the observation and identification that in the framework of the interwoven relationships of the free or mestizo population there were some activities that influenced incorporation and the forms of participation in the community.},
	language = {Portuguese},
	number = {25},
	journal = {HISTORIA Y MEMORIA},
	publisher = {UNIV PEDAGOGICA \& TECNOLOGICA COLOMBIA, INST INVESTIG \& FORMACION AVANZADA},
	author = {Bolanos Rodas, Richard Andres and Perea Bonilla, Bissy},
	month = dec,
	year = {2022},
	note = {Type: Article},
	keywords = {Bourbon reforms, Caloto, freemen, Intermarriage, local town halls, neighbours},
	pages = {63--101},
}

@article{shanmugam_electro_2022,
	address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
	title = {Electro search optimization based long short-term memory network for mobile malware detection},
	volume = {34},
	issn = {1532-0626},
	doi = {10.1002/cpe.7044},
	abstract = {Mobile malware is malicious software designed specifically for targeting various mobile gadgets like tablets, smartphones, and so forth, in which any type of malicious code affecting the mobile devices without the knowledge of the user. The increasing number of users encourages the hacker for generating various malware applications. Therefore, in this paper, we utilized three vital phases namely the pre-processing process, Feature extraction process as well as classification process in which the malicious data are detected. In the pre-processing phase, an Androguard tool is used for decompiling and disassembling the android applications. The API call features are extracted in the feature extraction phase and in the classification phase, long short term memory based electro search optimization (LSTM-ESO) is employed to detect the unknown mobile applications as benign or malicious. The malicious mobile detecting accuracy deals in requesting permission and exhibiting malicious code applications. In order to enhance the identification of various malware applications, this paper utilized frequency analysis and permissions of API calls. Finally, the experimental analysis is performed by evaluating the performance measures like accuracy, precision, recall, and F-measure. From the evaluation outcome, it is observed that the classification accuracy obtained is 97.69\%.},
	language = {English},
	number = {19},
	journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
	publisher = {WILEY},
	author = {Shanmugam, Padmapriya and Venkateswarulu, Balajivijayan and Dharmadurai, Rajalakshmi and Ranganathan, Thiagarajan and Indiran, Mohan and Nanjappan, Manikandan},
	month = aug,
	year = {2022},
	note = {Type: Article},
	keywords = {malware, mobile, detection, LSTM, API call, cybersecurity, benign, electro search, malicious, permissions},
}

@article{huang_psps_2022,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {{PSPS}: {A} {Step} toward {Tamper} {Resistance} against {Physical} {Computer} {Intrusion}},
	volume = {22},
	doi = {10.3390/s22051882},
	abstract = {Cyberattacks are increasing in both number and severity for private, corporate, and governmental bodies. To prevent these attacks, many intrusion detection systems and intrusion prevention systems provide computer security by monitoring network packets and auditing system records. However, most of these systems only monitor network packets rather than the computer itself, so physical intrusion is also an important security issue. Furthermore, with the rapid progress of the Internet of Things (IoT) technology, security problems of IoT devices are also increasing. Many IoT devices can be disassembled for decompilation, resulting in the theft of sensitive data. To prevent this, physical intrusion detection systems of the IoT should be considered. We here propose a physical security system that can protect data from unauthorized access when the computer chassis is opened or tampered with. Sensor switches monitor the chassis status at all times and upload event logs to a cloud server for remote monitoring. If the system finds that the computer has an abnormal condition, it takes protective measures and notifies the administrator. This system can be extended to IoT devices to protect their data from theft.},
	language = {English},
	number = {5},
	journal = {SENSORS},
	publisher = {MDPI},
	author = {Huang, Qi-Xian and Lu, Ming-Chang and Chiu, Min-Yi and Tsai, Yuan-Chia and Sun, Hung-Min},
	month = mar,
	year = {2022},
	note = {Type: Article},
	keywords = {computer intrusion, cyber-attack, physical security system},
}

@article{zhu_bigdata_2022,
	address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
	title = {Bigdata {Assisted} {Energy} {Conversion} {Model} for {Innovative} {City} {Application}},
	volume = {22},
	issn = {0219-2659},
	doi = {10.1142/S0219265921410085},
	abstract = {Centrally controlled energy conversion schemes in intelligent residential microgrids are a difficult optimization challenge because of their range of processing and power devices accessible. Typical steps to shrink the weight and seriousness of the issues are decreasing modelling precision, adding several weights, or adjusting the measurement accuracy. Nevertheless, because these interventions modify the specialization issue and thus result in various approaches as expected, this article introduces a Bigdata assisted energy conversion model (BD-ECM) and evaluates a decomposition approach to solve the initial problem recursively. Compared to the initial compact version, the decayed approach is tested to demonstrate that all versions differ less than 18.8\%. Moreover, both methods contribute to the use of roughly similar structures. The results reveal that because of the existing constraints on computational capital and simulation techniques, condensed development of the common law can only be extended to moderate and limited intelligent grids. However, decentralized approaches can be dealing with sizeable dispersed generation structures. To assess the month's environmental and strategic advantages as part of the system, researchers extend the decompiled approach to a massive smart grid. The data reveal that prices can be lowered by 14.0\% in local energy exchanges and pollution by 23.9\% in the situation studied.},
	language = {English},
	number = {SUPP01, 1},
	journal = {JOURNAL OF INTERCONNECTION NETWORKS},
	publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
	author = {Zhu, Jie and Vadivel, Thanjai and Sivaparthipan, C. B.},
	month = mar,
	year = {2022},
	note = {Type: Article},
	keywords = {big data, energy conversation, energy system, Smart city},
}

@article{mladentseva_responding_2022,
	address = {2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND},
	title = {Responding to obsolescence in {Flash}-based net art: a case study on migrating {Sinae} {Kim}'s {Genesis}},
	volume = {45},
	issn = {1945-5224},
	doi = {10.1080/19455224.2021.2007412},
	abstract = {Many internet artworks from the mid-1990s to the early 2000s used Adobe Flash technology for creating animated content. However, in the light of recent web standard developments (HTML5), Adobe has stopped supporting Flash and its related tools. The removal of Flash has made those net artworks non-functional and unviewable, including Sinae Kim's Genesis (2001), the focus of this study. Recently proposed emulation- and virtualisation-based strategies are not always suitable, particularly if there is a desire to keep the artwork on the `live web'. This article outlines an alternative method of migration facilitated by reverse engineering techniques-specifically decompilation-and foregrounds the significance of maintaining online access to the obsolete Adobe Shockwave Flash (SWF) files through the source code. On this premise, the source code is re-imagined as a site for further re-enactment, allowing a departure from its current role as a marker of `authenticity'.},
	language = {English},
	number = {1},
	journal = {JOURNAL OF THE INSTITUTE OF CONSERVATION},
	publisher = {ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD},
	author = {Mladentseva, Anna},
	month = jan,
	year = {2022},
	note = {Type: Article},
	keywords = {source code, reverse engineering, Adobe Flash, internet art, re-enactment, time-based media conservation},
	pages = {52--68},
}

@article{guan_method_2021,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {A {Method} for {Class}-{Imbalance} {Learning} in {Android} {Malware} {Detection}},
	volume = {10},
	issn = {2079-9292},
	doi = {10.3390/electronics10243124},
	abstract = {More and more Android application developers are adopting many different methods against reverse engineering, such as adding a shell, resulting in certain features that cannot be obtained through decompilation, which causes a serious sample imbalance in Android malware detection based on machine learning. Hence, the researchers have focused on how to solve class-imbalance to improve the performance of Android malware detection. However, the disadvantages of the existing class-imbalance learning are mainly the loss of valuable samples and the computational cost. In this paper, we propose a method of Class-Imbalance Learning (CIL), which first selects representative features, uses the clustering K-Means algorithm and under-sampling to retain the important samples of the majority class while reducing the number of samples of the majority class. After that, we use the Synthetic Minority Over-Sampling Technique (SMOTE) algorithm to generate minority class samples for data balance, and finally use the Random Forest (RF) algorithm to build a malware detection model. The result of experiments indicates that CIL effectively improves the performance of Android malware detection based on machine learning, especially for class imbalance. Compared with existing class-imbalance learning methods, CIL is also effective for the Machine Learning Repository from the University of California, Irvine (UCI) and has better performance in some data sets.},
	language = {English},
	number = {24},
	journal = {ELECTRONICS},
	publisher = {MDPI},
	author = {Guan, Jun and Jiang, Xu and Mao, Baolei},
	month = feb,
	year = {2021},
	note = {Type: Article},
	keywords = {android malware, clustering, imbalance data, random forest, SMOTE, under-sampling},
}

@article{liu_learning-based_2021,
	address = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
	title = {Learning-{Based} {Detection} for {Malicious} {Android} {Application} {Using} {Code} {Vectorization}},
	volume = {2021},
	issn = {1939-0114},
	doi = {10.1155/2021/9964224},
	abstract = {The malicious APK (Android Application Package) makers use some techniques such as code obfuscation and code encryption to avoid existing detection methods, which poses new challenges for accurate virus detection and makes it more and more difficult to detect the malicious code. A report indicates that a new malicious app for Android is created every 10 seconds. To combat this serious malware activity, a scalable malware detection approach is needed, which can effectively and efficiently identify the malware apps. Common static detection methods often rely on Hash matching and analysis of viruses, which cannot quickly detect new malicious Android applications and their variants. In this paper, a malicious Android application detection method is proposed, which is implemented by the deep network fusion model. The hybrid model only needs to use the sample training model to achieve high accuracy in the identification of the malicious applications, which is more suitable for the detection of the new malicious Android applications than the existing methods. This method extracts the static features in the core code of the Android application by decompiling APK files, then performs code vectorization processing, and uses the deep learning network for classification and discrimination. Our experiments with a data set containing 10,170 apps show that the decisions from the hybrid model can increase the malware detection rate significantly on a real device, which verifies the superiority of this method in the detection of malicious codes.},
	language = {English},
	journal = {SECURITY AND COMMUNICATION NETWORKS},
	publisher = {WILEY-HINDAWI},
	author = {Liu, Lin and Ren, Wang and Xie, Feng and Yi, Shengwei and Yi, Junkai and Jia, Peng},
	month = aug,
	year = {2021},
	note = {Type: Article},
}

@article{li_malware_2022,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Malware classification based on double byte feature encoding},
	volume = {61},
	issn = {1110-0168},
	doi = {10.1016/j.aej.2021.04.076},
	abstract = {Many researchers analyze malware through static analysis and dynamic analysis technology, and combine it with excellent deep learning algorithm, which has achieved good results in malware classification. However, many researches only use the. ASM file generated by decompiler or. Bytes file represented by hexadecimal for feature extraction. This paper fully integrates the features of these two files, and uses word frequency and two deep learning algorithms to extract 184 opcode features and 16 probability features from ASM file and section file of Kaggle dataset respectively. Then, double byte feature coding method is used to fuse the features of the two files. Finally, convolution neural network is used to classify the fused samples. The experimental results show that the accuracy is 98.68\% and the logarithm loss is 0.022. (C) 2021 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Engineering, Alexandria University.},
	language = {English},
	number = {1},
	journal = {ALEXANDRIA ENGINEERING JOURNAL},
	publisher = {ELSEVIER},
	author = {Li, Lin and Ding, Ying and Li, Bo and Qiao, Mengqing and Ye, Biao},
	month = jan,
	year = {2022},
	note = {Type: Article},
	keywords = {Encoding, Convolutional Neural Network, Double Byte Feature, Feature Selection, Malware Classification},
	pages = {91--99},
}

@article{ortin_cnerator_2021,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Cnerator: {A} {Python} application for the controlled stochastic generation of standard {C} source code},
	volume = {15},
	issn = {2352-7110},
	doi = {10.1016/j.softx.2021.100711},
	abstract = {The Big Code and Mining Software Repositories research lines analyze large amounts of source code to improve software engineering practices. Massive codebases are used to train machine learning models aimed at improving the software development process. One example is decompilation, where C code and its compiled binaries can be used to train machine learning models to improve decompilation. However, obtaining massive codebases of portable C code is not an easy task, since most applications use particular libraries, operating systems, or language extensions. In this paper, we present Cnerator, a Python application that provides the stochastic generation of large amounts of standard C code. It is highly configurable, allowing the user to specify the probability distributions of each language construct, properties of the generated code, and post-processing modifications of the output programs. Cnerator has been successfully used to generate code that, utilized to train machine learning models, has improved the performance of existing decompilers. It has also been used in the implementation of an infrastructure for the automatic extraction of code patterns. (C) 2021 The Author(s). Published by Elsevier B.V.},
	language = {English},
	journal = {SOFTWAREX},
	publisher = {ELSEVIER},
	author = {Ortin, Francisco and Escalada, Javier},
	month = jul,
	year = {2021},
	note = {Type: Article},
	keywords = {Machine learning, Python, Big code, C programming language, Mining software repositories, Stochastic program generation},
}

@article{du_mobile_2021,
	address = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
	title = {A {Mobile} {Malware} {Detection} {Method} {Based} on {Malicious} {Subgraphs} {Mining}},
	volume = {2021},
	issn = {1939-0114},
	doi = {10.1155/2021/5593178},
	abstract = {As mobile phone is widely used in social network communication, it attracts numerous malicious attacks, which seriously threaten users' personal privacy and data security. To improve the resilience to attack technologies, structural information analysis has been widely applied in mobile malware detection. However, the rapid improvement of mobile applications has brought an impressive growth of their internal structure in scale and attack technologies. It makes the timely analysis of structural information and malicious feature generation a heavy burden. In this paper, we propose a new Android malware identification approach based on malicious subgraph mining to improve the detection performance of large-scale graph structure analysis. Firstly, function call graphs (FCGs), sensitive permissions, and application programming interfaces (APIs) are generated from the decompiled files of malware. Secondly, two kinds of malicious subgraphs are generated from malware's decompiled files and put into the feature set. At last, test applications' safety can be automatically identified and classified into malware families by matching their FCGs with malicious structural features. To evaluate our approach, a dataset of 11,520 malware and benign applications is established. Experimental results indicate that our approach has better performance than three previous works and Androguard.},
	language = {English},
	journal = {SECURITY AND COMMUNICATION NETWORKS},
	publisher = {WILEY-HINDAWI},
	author = {Du, Yao and Cui, Mengtian and Cheng, Xiaochun},
	month = apr,
	year = {2021},
	note = {Type: Article},
}

@article{sasidharan_prodroid_2021,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {{ProDroid} - {An} {Android} malware detection framework based on profile hidden {Markov} model},
	volume = {72},
	issn = {1574-1192},
	doi = {10.1016/j.pmcj.2021.101336},
	abstract = {Popularity and openness have made the Android platform a potential target of malware attacks. The hackers continuously evolve and improve attacking strategies to identify vulnerabilities in newer Android versions. Detection and analysis of malware attacks in Android platform pose unique challenges due to the security restrictions and resource limitations present in these devices. This paper proposes a new behavioural method for Android malware detection and classification. In the proposed approach, the Android malware dataset is decompiled to identify the suspicious API classes/methods and generated an encoded list. The multiple sequence alignment for different malware families is created using the encoded patterns and it is further applied to generate profile hidden Markov model. The model classifies an unknown application as benign or malicious based on the log likelihood score generated. The framework provides an accuracy of 94.5\%, which is relatively higher compared to existing similar frameworks for detection of android malware. (C) 2021 Elsevier B.V. All rights reserved.},
	language = {English},
	journal = {PERVASIVE AND MOBILE COMPUTING},
	publisher = {ELSEVIER},
	author = {Sasidharan, Satheesh Kumar and Thomas, Ciza},
	month = apr,
	year = {2021},
	note = {Type: Article},
	keywords = {Android malware detection, Multiple sequence alignment, Profile hidden Markov model},
}

@article{hu_design_2021,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Design of ethnic patterns based on shape grammar and artificial neural network},
	volume = {60},
	issn = {1110-0168},
	doi = {10.1016/j.aej.2020.11.013},
	abstract = {Batik is a traditional handicraft of ethnic minorities in southwestern China's Guizhou province. It carries distinctive features, and inherits profound artistic and cultural values. However, the empirical design model of batik patterns is not fast or diverse enough to meet the highly personalized market demands. Moreover, it is difficult to parametrize the complex ethnic patterns with mathematical models. Hence, the design of batik patterns cannot satisfy the visual cognitive needs of consumers, which are constantly changing, fuzzy, and personalized. In other words, the visual cognitive needs of consumers have not been fully considered in the parametrization and design of batik patterns. To solve the problem, this paper puts forward a generative design method for batik patterns based on shape grammar, and relies on the artificial neural network (ANN) to model the nonlinear mapping between design parameters and visual cognitive image (VCI) values. The three-layer ANN model was optimized with the genetic algorithm (GA) to generate new patterns, and optimize the composition of their VCIs. The workflow and effectiveness of the proposed method were explained and verified through experiments on the generative design of bronze drum patterns, which have rich ethnic and religious connotations. The experimental results show that our method can predict the VCI values of the composition, provide the optimal parameter solution of the VCI values, and decompile the conceptual prototype from the pattern composition that best meets the visual cognitive needs of consumers. (C) 2020 The Authors. Published by Elsevier B.V. on behalf of Faculty of Engineering, Alexandria University.},
	language = {English},
	number = {1},
	journal = {ALEXANDRIA ENGINEERING JOURNAL},
	publisher = {ELSEVIER},
	author = {Hu, Tao and Xie, Qingsheng and Yuan, Qingni and Lv, Jian and Xiong, Qiaoqiao},
	month = feb,
	year = {2021},
	note = {Type: Article},
	keywords = {Artificial Neural Network (ANN), Custom design, Genetic Algorithm (GA), Shape grammar, Visual Cognitive Image (VCI)},
	pages = {1601--1625},
}

@article{ullah_clone_2021,
	address = {TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY},
	title = {Clone detection in {5G}-enabled social {IoT} system using graph semantics and deep learning model},
	volume = {12},
	issn = {1868-8071},
	doi = {10.1007/s13042-020-01246-9},
	abstract = {The protection and privacy of the 5G-IoT framework is a major challenge due to the vast number of mobile devices. Specialized applications running these 5G-IoT systems may be vulnerable to clone attacks. Cloning applications can be achieved by stealing or distributing commercial Android apps to harm the advanced services of the 5G-IoT framework. Meanwhile, most Android app stores run and manage Android apps that developers have submitted separately without any central verification systems. Android scammers sell pirated versions of commercial software to other app stores under different names. Android applications are typically stored on cloud servers, while API access services may be used to detect and prevent cloned applications from being released. In this paper, we proposed a hybrid approach to the Control Flow Graph (CFG) and a deep learning model to secure the smart services of the 5G-IoT framework. First, the newly submitted APK file is extracted and the JDEX decompiler is used to retrieve Java source files from possibly original and cloned applications. Second, the source files are broken down into various android-based components. After generating Control-Flow Graphs (CFGs), the weighted features are stripped from each component. Finally, the Recurrent Neural Network (RNN) is designed to predict potential cloned applications by training features from different components of android applications. Experimental results have shown that the proposed approach can achieve an average accuracy of 96.24\% for cloned applications selected from different android application stores.},
	language = {English},
	number = {11, SI},
	journal = {INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS},
	publisher = {SPRINGER HEIDELBERG},
	author = {Ullah, Farhan and Naeem, Muhammad Rashid and Mostarda, Leonardo and Shah, Syed Aziz},
	month = jan,
	year = {2021},
	note = {Type: Article},
	keywords = {Privacy, Security, Deep learning, Clone detection, 5G IoT, Control flow graph},
	pages = {3115--3127},
}

@article{bodei_fws_2021,
	address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
	title = {{FWS}: {Analyzing}, maintaining and transcompiling firewalls},
	volume = {29},
	issn = {0926-227X},
	doi = {10.3233/JCS-200017},
	abstract = {Firewalls are essential for managing and protecting computer networks. They permit specifying which packets are allowed to enter a network, and also how these packets are modified by IP address translation and port redirection. Configuring a firewall is notoriously hard, and one of the reasons is that it requires using low level, hard to interpret, configuration languages. Equally difficult are policy maintenance and refactoring, as well as porting a configuration from one firewall system to another. To address these issues we introduce a pipeline that assists system administrators in checking if: (i) the intended security policy is actually implemented by a configuration; (ii) two configurations are equivalent; (iii) updates have the desired effect on the firewall behavior; (iv) there are useless or redundant rules; additionally, an administrator can (v) transcompile a configuration into an equivalent one in a different language; and (vi) maintain a configuration using a generic, declarative language that can be compiled into different target languages. The pipeline is based on IFCL, an intermediate firewall language equipped with a formal semantics, and it is implemented in an open source tool called FWS. In particular, the first stage decompiles real firewall configurations for iptables, ipfw, pf and (a subset of) Cisco IOS into IFCL. The second one transforms an IFCL configuration into a logical predicate and uses the Z3 solver to synthesize an abstract specification that succinctly represents the firewall behavior. System administrators can use FWS to analyze the firewall by posing SQL-like queries, and update the configuration to meet the desired security requirements. Finally, the last stage allows for maintaining a configuration by acting directly on its abstract specification and then compiling it to the chosen target language. Tests on real firewall configurations show that FWS can be fruitfully used in real-world scenarios.},
	language = {English},
	number = {1},
	journal = {JOURNAL OF COMPUTER SECURITY},
	publisher = {IOS PRESS},
	author = {Bodei, Chiara and Ceragioli, Lorenzo and Degano, Pierpaolo and Focardi, Riccardo and Galletta, Letterio and Luccio, Flaminia and Tempesta, Mauro and Veronese, Lorenzo},
	year = {2021},
	note = {Type: Article},
	keywords = {Configuration analysis, Firewall configuration languages, maintaining, porting, refactoring, Semantic-based tool},
	pages = {77--134},
}

@article{yang_android_2021,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Android {Malware} {Detection} {Based} on {Structural} {Features} of the {Function} {Call} {Graph}},
	volume = {10},
	doi = {10.3390/electronics10020186},
	abstract = {The openness of Android operating system not only brings convenience to users, but also leads to the attack threat from a large number of malicious applications (apps). Thus malware detection has become the research focus in the field of mobile security. In order to solve the problem of more coarse-grained feature selection and larger feature loss of graph structure existing in the current detection methods, we put forward a method named DGCNDroid for Android malware detection, which is based on the deep graph convolutional network. Our method starts by generating a function call graph for the decompiled Android application. Then the function call subgraph containing the sensitive application programming interface (API) is extracted. Finally, the function call subgraphs with structural features are trained as the input of the deep graph convolutional network. Thus the detection and classification of malicious apps can be realized. Through experimentation on a dataset containing 11,120 Android apps, the method proposed in this paper can achieve detection accuracy of 98.2\%, which is higher than other existing detection methods.},
	language = {English},
	number = {2},
	journal = {ELECTRONICS},
	publisher = {MDPI},
	author = {Yang, Yang and Du, Xuehui and Yang, Zhi and Liu, Xing},
	month = jan,
	year = {2021},
	note = {Type: Article},
	keywords = {Android, malware detection, function call graph, graph convolutional network},
}

@article{connor_forensic_2020,
	address = {TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA},
	title = {A {Forensic} {Disassembly} of the {BIS} {Monitor}},
	volume = {131},
	issn = {0003-2999},
	doi = {10.1213/ANE.0000000000005220},
	abstract = {BACKGROUND: The bispectral index (BIS) monitor has been available for clinical use for {\textgreater}20 years and has had an immense impact on academic activity in Anesthesiology, with {\textgreater}3000 articles referencing the bispectral index. Despite attempts to infer its algorithms by external observation, its operation has nevertheless remained undescribed, in contrast to the algorithms of other less commercially successful monitors of electroencephalogram (EEG) activity under anesthesia. With the expiration of certain key patents, the time is therefore ripe to examine the operation of the monitor on its own terms through careful dismantling, followed by extraction and examination of its internal software. METHODS: An A-2000 BIS Monitor (gunmetal blue case, amber monochrome display) was purchased on the secondary market. After identifying the major data processing and storage components, a set of free or inexpensive tools was used to retrieve and disassemble the monitor's onboard software. The software executes primarily on an ARMv7 microprocessor (Sharp/NXP LH77790B) and a digital signal processor (Texas Instruments TMS320C32). The device software can be retrieved directly from the monitor's hardware by using debugging interfaces that have remained in place from its original development. RESULTS: Critical numerical parameters such as the spectral edge frequency (SEF), total power, and BIS values were retraced from external delivery at the device's serial port back to the point of their calculation in the extracted software. In doing so, the locations of the critical algorithms were determined. To demonstrate the validity of the technique, the algorithms for SEF and total power were disassembled, comprehensively annotated and compared to their theoretically ideal behaviors. A bug was identified in the device's implementation of the SEF algorithm, which can be provoked by a perfectly isoelectric EEG. CONCLUSIONS: This article demonstrates that the electronic design of the A-2000 BIS Monitor does not pose any insuperable obstacles to retrieving its device software in hexadecimal machine code form directly from the motherboard. This software can be reverse engineered through disassembly and decompilation to reveal the methods by which the BIS monitor implements its algorithms, which ultimately must form the definitive statement of its function. Without further revealing any algorithms that might be considered trade secrets, the manufacturer of the BIS monitor should be encouraged to release the device software in its original format to place BIS-related academic literature on a firm theoretical foundation and to promote further academic development of EEG monitoring algorithms.},
	language = {English},
	number = {6},
	journal = {ANESTHESIA AND ANALGESIA},
	publisher = {LIPPINCOTT WILLIAMS \& WILKINS},
	author = {Connor, Christopher W.},
	month = feb,
	year = {2020},
	note = {Type: Article},
	pages = {1923--1933},
}

@article{yang_malware_2020,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Malware {Classification} {Based} on {Shallow} {Neural} {Network}},
	volume = {12},
	doi = {10.3390/fi12120219},
	abstract = {The emergence of a large number of new malicious code poses a serious threat to network security, and most of them are derivative versions of existing malicious code. The classification of malicious code is helpful to analyze the evolutionary trend of malicious code families and trace the source of cybercrime. The existing methods of malware classification emphasize the depth of the neural network, which has the problems of a long training time and large computational cost. In this work, we propose the shallow neural network-based malware classifier (SNNMAC), a malware classification model based on shallow neural networks and static analysis. Our approach bridges the gap between precise but slow methods and fast but less precise methods in existing works. For each sample, we first generate n-grams from their opcode sequences of the binary file with a decompiler. An improved n-gram algorithm based on control transfer instructions is designed to reduce the n-gram dataset. Then, the SNNMAC exploits a shallow neural network, replacing the full connection layer and softmax with the average pooling layer and hierarchical softmax, to learn from the dataset and perform classification. We perform experiments on the Microsoft malware dataset. The evaluation result shows that the SNNMAC outperforms most of the related works with 99.21\% classification precision and reduces the training time by more than half when compared with the methods using DNN (Deep Neural Networks).},
	language = {English},
	number = {12},
	journal = {FUTURE INTERNET},
	publisher = {MDPI},
	author = {Yang, Pin and Zhou, Huiyu and Zhu, Yue and Liu, Liang and Zhang, Lei},
	month = feb,
	year = {2020},
	note = {Type: Article},
	keywords = {classification, malware, static analysis, neural network, n-gram},
}

@article{huang_data_2021,
	address = {Floor 5, Northspring 21-23 Wellington Street, Leeds, W YORKSHIRE, ENGLAND},
	title = {Data access as a big competitive advantage: evidence from {China}'s car-hailing platforms},
	volume = {55},
	issn = {2514-9288},
	doi = {10.1108/DTA-01-2019-0013},
	abstract = {Purpose The online platform is one of the essential components of the platform economy that is constructed by a large scale of the personal data resource. However, accurate empirical test of the competition structure of the data-driven online platform is still less. This research is trying to reveal market allocation structure of the personal data resource of China's car-hailing platforms competition by the empirical data analysis. Design/methodology/approach This research is applying the social network analysis by R packages, which include k-core decomposition and multilevel community detection from the data connectedness via the decompilation and the examination of the application programming interface of terminal applications. Findings This research has found that the car-hailing platforms, which establish more constant personal data connectedness and connectivity with social media platforms, are taking the competitive market advantage within the sample network. Data access discrimination is a complementary method of market power in China's car-hailing industry. Research limitations/implications This research offers a new perspective on the analysis of the multi-sided market from the personal data resource allocation mechanism of the car-hailing platform. However, the measurement of the data connectedness requires more empirical industry data. Practical implications This research reveals the competition structure that relies on personal data resource allocation mechanism. It offers empirical evidence for governance, which is considered as the critical issue of big data research, by reviewing the nature of the data network. Social implications It also reveals the data convergence process of the social system and the technological system. Originality/value This research offers a new research method for the real-time regulation of the car-hailing platform.},
	language = {English},
	number = {2},
	journal = {DATA TECHNOLOGIES AND APPLICATIONS},
	publisher = {EMERALD GROUP PUBLISHING LTD},
	author = {Huang, Lei and Zhao, Yandong and He, Guangxi and Lu, Yangxu and Zhang, Juanjuan and Wu, Peiyi},
	month = apr,
	year = {2021},
	note = {Type: Article},
	keywords = {Car-hailing, Competition, Data access, Multi-sided market, Personal data resource, Platform economy},
	pages = {192--215},
}

@article{ullah_iot-based_2020,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {{IoT}-based green city architecture using secured and sustainable android services},
	volume = {20},
	issn = {2352-1864},
	doi = {10.1016/j.eti.2020.101091},
	abstract = {Green and smart cities deliver services to their residents using mobile applications that make daily life more convenient. The privacy and security of these applications are significant in providing sustainable services in a green city. The software cloning is a severe threat which may breach the security and privacy of android applications. A centrally controlled and automated screening system across multiple app stores is inevitable to prevent the release of copyrighted or cloned copies of these apps. In this paper, we proposed IoT-enabled green city architecture for clone detection in android markets using a deep learning approach. First, the proposed system obtained an original APK file together with potential candidate cloned APKs via the cloud network. For each subject software, the system uses an APK Extractor tool to retrieve Dalvik Executable (DEX) files. The Jdex decompiler is utilized to retrieve Java source files through Dalvik Executables. Second, the AST features are extracted using ANother Tool for Language Recognition (ANTLR) parser. Third, the linear features are mined from these hierarchical structures, and Term Frequency Inverse Document Frequency (TFIDF) is applied to estimate the significance of each feature. Finally, the deep learning model is configured to detect cloned apps. The deep learning model is fine-tuned to get better accuracy. The proposed approach is analyzed on five different cloned applications collected from different android markets. The main objective of this system is to avoid the release of pirated apps with various pirated labels in multiple app markets. (C) 2020 Elsevier B.V. All rights reserved.},
	language = {English},
	journal = {ENVIRONMENTAL TECHNOLOGY \& INNOVATION},
	publisher = {ELSEVIER},
	author = {Ullah, Farhan and Al-Turjman, Fadi and Nayyar, Anand},
	month = jan,
	year = {2020},
	note = {Type: Article},
	keywords = {Deep learning, Mobile applications, Internet of Things, Abstract syntax tree, Green city},
}

@article{li_opcode_2020,
	address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
	title = {Opcode sequence analysis of {Android} malware by a convolutional neural network},
	volume = {32},
	issn = {1532-0626},
	doi = {10.1002/cpe.5308},
	abstract = {The number of malware has exploded due to the openness of the Android platform, and the endless stream of malware poses a threat to the privacy, tariffs, and device of mobile phone users. A novel Android mobile malware detection system is proposed, which employs an optimized deep convolutional neural network to learn from opcode sequences. The optimized convolutional neural network is trained multiple times by the raw opcode sequences extracted from the decompiled Android file, so that the feature information can be effectively learned and the malicious program can be detected more accurately. More critically, thek-max pooling method with better results is adopted in the pooling operation phase, which improves the detection effect of the proposed method. The experimental results show that the detection system achieved the accuracy of 99\%, which is 2\%-11\% higher than the accuracy of the machine learning detection algorithms when using the same data set. It also ensures that the indicators, such as F1-score, recall, and precision, are maintained above 97\%. Based on the detection system, a multi-data set comparison experiment is carried out. The introducedk-max pooling is deeply studied, and the effect ofkofk-max pooling on the overall detection effect is observed.},
	language = {English},
	number = {18, SI},
	journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
	publisher = {WILEY},
	author = {Li, Dan and Zhao, Lichao and Cheng, Qingfeng and Lu, Ning and Shi, Wenbo},
	month = sep,
	year = {2020},
	note = {Type: Article},
	keywords = {classification, k-max pooling, opcode, convolutional neural network, Android malware},
}

@article{xie_accurate_2021,
	address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
	title = {An accurate and efficient two-phase scheme for detecting {Android} cloned applications},
	volume = {33},
	issn = {1532-0626},
	doi = {10.1002/cpe.6009},
	abstract = {The fast-growing Android application market has attracted more and more application developers. However, many plagiarists use decompiled tools to modify original applications to get clones, which has become a serious threat. For detecting cloned applications, most of the existing schemes do not consider the detected accuracy and time consumption at the same time. In this article, we proposea two-phase detection scheme to achieve fast and accurate clone detection in large-scale applications. In the rapid screening phase, a fix-length minhash summary is constructed for each application and the locality-sensitive hashing (LSH) algorithm is used to obtain suspicious cloned applications quickly. In the accurate detection phase, by merging and pruning the layout and interaction information of all user interfaces (UIs) at the application runtime, we obtain the birthmark named merged layout tree (MLT), which can resist nested obfuscation and repacking attack. Finally, cloned apps are detected by calculating the similarity between MLTs from suspicious cloned apps. We evaluate our detection scheme in two app datasets (nearly 170,000 Android applications) and compare it with the state-of-the-art clone detection methods. Extensive experiments show that our method has high accuracy and efficiency for clone detection in large-scale apps.},
	language = {English},
	number = {5},
	journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
	publisher = {WILEY},
	author = {Xie, Jiahao and Yan, Xiai and Lin, Yaping and Wei, Jianhao},
	month = mar,
	year = {2021},
	note = {Type: Article},
	keywords = {android, clone detection, software security, merged layout tree, user interface},
}

@article{javaloy_preliminary_2020,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Preliminary {Results} on {Different} {Text} {Processing} {Tasks} {Using} {Encoder}-{Decoder} {Networks} and the {Causal} {Feature} {Extractor}},
	volume = {10},
	doi = {10.3390/app10175772},
	abstract = {Deep learning methods are gaining popularity in different application domains, and especially in natural language processing. It is commonly believed that using a large enough dataset and an adequate network architecture, almost any processing problem can be solved. A frequent and widely used typology is the encoder-decoder architecture, where the input data is transformed into an intermediate code by means of an encoder, and then a decoder takes this code to produce its output. Different types of networks can be used in the encoder and the decoder, depending on the problem of interest, such as convolutional neural networks (CNN) or long-short term memories (LSTM). This paper uses for the encoder a method recently proposed, called Causal Feature Extractor (CFE). It is based on causal convolutions (i.e., convolutions that depend only on one direction of the input), dilatation (i.e., increasing the aperture size of the convolutions) and bidirectionality (i.e., independent networks in both directions). Some preliminary results are presented on three different tasks and compared with state-of-the-art methods: bilingual translation, LaTeX decompilation and audio transcription. The proposed method achieves promising results, showing its ubiquity to work with text, audio and images. Moreover, it has a shorter training time, requiring less time per iteration, and a good use of the attention mechanisms based on attention matrices.},
	language = {English},
	number = {17},
	journal = {APPLIED SCIENCES-BASEL},
	publisher = {MDPI},
	author = {Javaloy, Adrian and Garcia-Mateos, Gines},
	month = sep,
	year = {2020},
	note = {Type: Article},
	keywords = {natural language processing, bilingual translation, causal encoder, deep neural networks, LaTeX decompilation, speech-to-text},
}

@article{zhao_compile-time_2020,
	address = {OXFORD FULFILLMENT CENTRE THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
	title = {Compile-time code virtualization for android applications},
	volume = {94},
	issn = {0167-4048},
	doi = {10.1016/j.cose.2020.101821},
	abstract = {Infringing intellectual property by reverse analysis is a severe threat to Android applications. By replacing the program instructions with virtual instructions that an adversary is unfamiliar with, code obfuscation based on virtualization is a promising way of protecting Android applications against reverse engineering. However, the current code virtualization approaches for Android only target at the DEX bytecode level. The DEX file with the open file format and more semantic information makes the decode-dispatch pattern easier to expose, which has been identified as a severe vulnerability of security and can be exploited by various attacks. Further, decode-dispatch interpretation frequently uses indirect branches in this structure to introduce extra overhead. This paper presents a novel approach to transfer code virtualization from DEX level to native level, which possesses strong security strength and good stealth, with only modest cost. Our approach contains two components: pre-compilation and compile-time virtualization. Pre-compilation is designed for performance improvement by identifying and decompiling the critical functions which consume a significant fraction of execution time. Compile-time virtualization builds upon the widely used LLVM compiler framework. It automatically translates the DEX bytecode into the common LLVM intermediate representations where a unified code virtualization pass can be applied for DEX code. We have implemented a working prototype Dex2VM of our technique and applied it to eight representative Android applications. Our experimental results show that the proposed approach can effectively protect the target code against a state-of-the-art code reverse engineering tool that is specifically designed for code virtualization, and it achieves good stealth with only modest cost. (C) 2020 Elsevier Ltd. All rights reserved.},
	language = {English},
	journal = {COMPUTERS \& SECURITY},
	publisher = {ELSEVIER ADVANCED TECHNOLOGY},
	author = {Zhao, Yujie and Tang, Zhanyong and Ye, Guixin and Peng, Dongxu and Fang, Dingyi and Chen, Xiaojiang and Wang, Zheng},
	month = jul,
	year = {2020},
	note = {Type: Article},
	keywords = {Compiler, Android packer, Code virtualization, LLVM},
}

@article{ding_android_2020,
	address = {TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY},
	title = {Android malware detection method based on bytecode image},
	issn = {1868-5137},
	doi = {10.1007/s12652-020-02196-4},
	abstract = {Traditional machine learning based malware detection methods often use decompiling techniques or dynamic monitoring techniques to extract the feature representation of malware. This procedure is time consuming and strongly depends on the skills of experts. In addition, malware can be packed or encrypted to evade the analysis of decompiling tools. To solve this issue, we propose a static detection method based on deep learning. We directly extract bytecode file from Android APK file, and convert the bytecode file into a two-dimensional bytecode matrix, then use the deep learning algorithm, convolution neural network (CNN), to train a detection model and apply it to classify malware. CNN can automatically learn features of bytecode file which can be used to recognize malware. The proposed detection model avoids the procedure for analyzing malware features and designing the feature representation of malware. The experimental results show the proposed method is effective to detect malware, especially malware encrypted using polymorphic techniques.},
	language = {English},
	journal = {JOURNAL OF AMBIENT IN℡LIGENCE AND HUMANIZED COMPUTING},
	publisher = {SPRINGER HEIDELBERG},
	author = {Ding, Yuxin and Zhang, Xiao and Hu, Jieke and Xu, Wenting},
	month = jun,
	year = {2020},
	note = {Type: Article; Early Access},
	keywords = {Malware, Android, Binary data, Bytecode, Convolutional neural network},
}

@article{yeom_vulnerability_2019,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Vulnerability {Evaluation} {Method} through {Correlation} {Analysis} of {Android} {Applications}},
	volume = {11},
	doi = {10.3390/su11236637},
	abstract = {Due to people in companies use mobile devices to access corporate data, attackers targeting corporate data use vulnerabilities in mobile devices. Most vulnerabilities in applications are caused by the carelessness of developers, and confused deputy attacks and data leak attacks using inter-application vulnerabilities are possible. These vulnerabilities are difficult to find through the single-application diagnostic tool that is currently being studied. This paper proposes a process to automate the decompilation of all the applications on a user's mobile device and a mechanism to find inter-application vulnerabilities. The mechanism generates a list and matrix, detailing the vulnerabilities in the mobile device. The proposed mechanism is validated through an experiment on an actual mobile device with four installed applications, and the results show that the mechanism can accurately capture all application risks as well as inter-application risks. Through this mechanism, users can expect to find the risks in their mobile devices in advance and prevent damage.},
	language = {English},
	number = {23},
	journal = {SUSTAINABILITY},
	publisher = {MDPI},
	author = {Yeom, Cheolmin and Won, Yoojae},
	month = feb,
	year = {2019},
	note = {Type: Article},
	keywords = {android permission, android security, inter-application vulnerability, vulnerability diagnosis},
}

@article{ullah_detection_2022,
	address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
	title = {Detection of clone scammers in {Android} markets using {IoT}-based edge computing},
	volume = {33},
	issn = {2161-3915},
	doi = {10.1002/ett.3791},
	abstract = {Pirated application developers find an alternate way to publish pirated versions of the same Android mobile applications (apps) on different Android markets. Therefore, a centralized, automated scrutiny system among multiple app stores is inevitable to prevent publishing pirated or cloned version of these Android applications. In this paper, we proposed an Android clone detection system for Internet of things (IoT) (Droid-IoT) devices. First, the proposed system receives an original Android application package (APK) file along with possible candidate cloned APKs over the cloud network. The system uses an apkExtractor tool to extract Dalvik Executable (DEX) files for each subject program. The Jdex decompiler is used to extract Java source files from DEXs. Then, the bag-of-word model is used to extract tokenized features from source files. Further, the weighting filters are used to zoom the importance of each token. Moreover, Synthetic Minority Oversampling is applied to retrieve balanced features for better training of data. Finally, TensorFlow with Keras deep learning model is designed to predict clones in Android applications. The experimental results have shown that Droid-IoT can successfully detect cloned apps with an accuracy of up to 96\%. The primary purpose of this system is to prevent the publishing of pirated apps among different app stores under different pirated names.},
	language = {English},
	number = {6, SI},
	journal = {TRANSACTIONS ON EMERGING ℡ECOMMUNICATIONS TECHNOLOGIES},
	publisher = {WILEY},
	author = {Ullah, Farhan and Naeem, Hamad and Naeem, Muhammad Rashid and Jabbar, Sohail and Khalid, Shehazad and Al-Turjman, Fadi and Abuarqoub, Abdelrahman},
	month = jun,
	year = {2022},
	note = {Type: Article},
}

@article{tang_ssldetecter_2019,
	address = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
	title = {{SSLDetecter}: {Detecting} {SSL} {Security} {Vulnerabilities} of {Android} {Applications} {Based} on a {Novel} {Automatic} {Traversal} {Method}},
	volume = {2019},
	issn = {1939-0114},
	doi = {10.1155/2019/7193684},
	abstract = {Android usually employs the Secure Socket Layer (SSL) protocol to protect the user's privacy in network transmission. However, developers may misuse SSL-related APIs, which would lead attackers to steal user's privacy through man-in-the-middle attacks. Existing methods based on static decompiling technology to detect SSL security vulnerabilities of Android applications cannot cope with the increasingly common packed applications. Meanwhile, dynamic analysis approaches have the disadvantages of excessive resource consumption and time-consuming. In this paper, we propose a dynamic method to solve this issue based on our novel automatic traversal model. At first, we propose several new traversal strategies to optimize the widget tree according to the user interface (UI) types and the interface state similarity. Furthermore, we develop a more granular traversal model by refining the traversal level from the Activity component to the Widget and implement a heuristic depth-first traversal algorithm in combination with our customized traversal strategy. In addition, the man-in-the-middle agent plug-in is extended to implement real-time attack test and return the attack results. Based on the above ideas, we have implemented SSLDetecter, an efficient automated detection system of Android application SSL security vulnerability. We apply it on multiple devices in parallel to detect 2456 popular applications in several mainstream application markets and find that 424 applications are suffering from SSL security vulnerabilities. Compared with the existing system SMV-HUNTER, the time efficiency of our system increases by 38\% and the average detection rate increases by 6.39 percentage points, with many types of SSL vulnerabilities detected.},
	language = {English},
	journal = {SECURITY AND COMMUNICATION NETWORKS},
	publisher = {WILEY-HINDAWI},
	author = {Tang, Junwei and Li, Jingjing and Li, Ruixuan and Han, Hongmu and Gu, Xiwu and Xu, Zhiyong},
	month = oct,
	year = {2019},
	note = {Type: Article},
}

@article{martin_clonespot_2019,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {{CloneSpot}: {Fast} detection of {Android} repackages},
	volume = {94},
	issn = {0167-739X},
	doi = {10.1016/j.future.2018.12.050},
	abstract = {Repackaging of applications is one of the key attack vectors for mobile malware. This is particularly easy and popular in Android Markets, where applications can be downloaded, decompiled, modified and re-uploaded at a very low cost. Detecting clones and victims is often a hard task, especially in markets with several million of applications to analyze, such as Google Play Store. This work proposes CloneSpot, a novel methodology to efficiently detect Repackaged versions of Android apps using Min Hashing techniques applied to applications' meta-data publicly available at Google Play. We validate our approach by analyzing 1.3 Million of applications collected from Google Play in September 2017, from which around 420K are detected as potential repackaged or victim versions of other applications. (C) 2018 Elsevier B.V. All rights reserved.},
	language = {English},
	journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
	publisher = {ELSEVIER},
	author = {Martin, Ignacio and Alberto Hernandez, Jose},
	month = may,
	year = {2019},
	note = {Type: Article},
	keywords = {Android Malware, Meta-information, Min-Hashing, Repackaging},
	pages = {740--748},
}

@article{saito_iterative_2019,
	address = {1155 16TH ST, NW, WASHINGTON, DC 20036 USA},
	title = {Iterative {Assembly} of {Polycyclic} {Saturated} {Heterocycles} from {Monomeric} {Building} {Blocks}},
	volume = {141},
	issn = {0002-7863},
	doi = {10.1021/jacs.9b01537},
	abstract = {Polycyclic saturated heterocycles with predictable shapes and structures are assembled by iterative couplings of bifunctional stannyl amine protocol (SnAP) reagents and a single morpholine-forming assembly reaction. Combinations of just a few monomers enable the programmable construction of rotationally restricted, nonplanar heterocyclic arrays with discrete sizes and molecular shapes. The three-dimensional structures of these constrained scaffolds can be quickly and reliably predicted by DFT calculations and the target structures immediately decompiled into the constituent building blocks and assembly sequences. As a demonstration, in silico combinations of the building blocks predict saturated heptacyclic structures with elementary shapes including helices, S-turns and U-turns, which are synthesized in 5-6 steps from the monomers using just three chemical reactions.},
	language = {English},
	number = {13},
	journal = {JOURNAL OF THE AMERICAN CHEMICAL SOCIETY},
	publisher = {AMER CHEMICAL SOC},
	author = {Saito, Fumito and Trapp, Nils and Bode, Jeffrey W.},
	month = apr,
	year = {2019},
	note = {Type: Article},
	pages = {5544--5554},
}

@article{dong_defect_2018,
	address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
	title = {Defect {Prediction} in {Android} {Binary} {Executables} {Using} {Deep} {Neural} {Network}},
	volume = {102},
	issn = {0929-6212},
	doi = {10.1007/s11277-017-5069-3},
	abstract = {Software defect prediction locates defective code to help developers improve the security of software. However, existing studies on software defect prediction are mostly limited to the source code. Defect prediction for Android binary executables (called apks) has never been explored in previous studies. In this paper, we propose an explorative study of defect prediction in Android apks. We first propose smali2vec, a new approach to generate features that capture the characteristics of smali (decompiled files of apks) files in apks. Smali2vec extracts both token and semantic features of the defective files in apks and such comprehensive features are needed for building accurate prediction models. Then we leverage deep neural network (DNN), which is one of the most common architecture of deep learning networks, to train and build the defect prediction model in order to achieve accuracy. We apply our defect prediction model to more than 90,000 smali files from 50 Android apks and the results show that our model could achieve an AUC (the area under the receiver operating characteristic curve) of 85.98\% and it is capable of predicting defects in apks. Furthermore, the DNN is proved to have a better performance than the traditional shallow machine learning algorithms (e.g., support vector machine and naive bayes) used in previous studies. The model has been used in our practical work and helped locate many defective files in apks.},
	language = {English},
	number = {3, SI},
	journal = {WIRELESS PERSONAL COMMUNICATIONS},
	publisher = {SPRINGER},
	author = {Dong, Feng and Wang, Junfeng and Li, Qi and Xu, Guoai and Zhang, Shaodong},
	month = oct,
	year = {2018},
	note = {Type: Article},
	keywords = {Machine learning, Mobile security, Android binary executables, Deep neural network, Software defect prediction},
	pages = {2261--2285},
}

@article{su_using_2018,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Using {Ad}-{Related} {Network} {Behavior} to {Distinguish} {Ad} {Libraries}},
	volume = {8},
	doi = {10.3390/app8101852},
	abstract = {Mobile app ads pose a far greater security threat to users than adverts on computer browsers. This is because app developers must embed a Software Development Kit (SDK), called an ad library or ad lib for short, provided by ad networks (i.e., ad companies) into their app program, and then merge and compile it into an Android PacKage (APK) execution file. The ad lib thus becomes a part of the entire app, and shares the whole permissions granted to the app. Unfortunately, this also resulted in many security issues, such as ad libs abusing the permissions to collect and leak private data, ad servers redirecting ad requests to download malicious JavaScript from unknown servers to execute it in the background of the mobile operating system without the user's consent. The more well-known an embedded ad lib, the safer the app may be, and vice versa. Importantly, while decompiling an APK to inspect its source code may not identify the ad lib(s), executing the app on a simulator can reveal the network behavior of the embedded ad lib(s). Ad libs exhibit different behavior patterns when communicating with ad servers. This study uses a dynamic analysis method to inspect an executing app, and plots the ad lib behavior patterns related to the advertisement into a graph. It is then determined whether or not the ad lib is from a trusted ad network using comparisons of graph similarities.},
	language = {English},
	number = {10},
	journal = {APPLIED SCIENCES-BASEL},
	publisher = {MDPI},
	author = {Su, Ming-Yang and Wei, Hong-Siou and Chen, Xin-Yu and Lin, Po-Wei and Qiu, Ding-You},
	month = oct,
	year = {2018},
	note = {Type: Article},
	keywords = {ad lib, ad libraries, ad networks, android package (APK), graph, graph similarity, mobile ads, software development kit (SDK)},
}

@article{ragkhitwetsagul_comparison_2018,
	address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
	title = {A comparison of code similarity analysers},
	volume = {23},
	issn = {1382-3256},
	doi = {10.1007/s10664-017-9564-7},
	abstract = {Copying and pasting of source code is a common activity in software engineering. Often, the code is not copied as it is and it may be modified for various purposes; e.g. refactoring, bug fixing, or even software plagiarism. These code modifications could affect the performance of code similarity analysers including code clone and plagiarism detectors to some certain degree. We are interested in two types of code modification in this study: pervasive modifications, i.e. transformations that may have a global effect, and local modifications, i.e. code changes that are contained in a single method or code block. We evaluate 30 code similarity detection techniques and tools using five experimental scenarios for Java source code. These are (1) pervasively modified code, created with tools for source code and bytecode obfuscation, and boiler-plate code, (2) source code normalisation through compilation and decompilation using different decompilers, (3) reuse of optimal configurations over different data sets, (4) tool evaluation using ranked-based measures, and (5) local + global code modifications. Our experimental results show that in the presence of pervasive modifications, some of the general textual similarity measures can offer similar performance to specialised code similarity tools, whilst in the presence of boiler-plate code, highly specialised source code similarity detection techniques and tools outperform textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for three of the tools. Moreover, we demonstrate that optimal configurations are very sensitive to a specific data set. After directly applying optimal configurations derived from one data set to another, the tools perform poorly on the new data set. The code similarity analysers are thoroughly evaluated not only based on several well-known pair-based and query-based error measures but also on each specific type of pervasive code modification. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code.},
	language = {English},
	number = {4},
	journal = {EMPIRICAL SOFTWARE ENGINEERING},
	publisher = {SPRINGER},
	author = {Ragkhitwetsagul, Chaiyong and Krinke, Jens and Clark, David},
	month = aug,
	year = {2018},
	note = {Type: Article},
	keywords = {Clone detection, Empirical study, Code similarity measurement, Parameter optimisation, Plagiarism detection},
	pages = {2464--2519},
}

@article{zhuang_performance_2018,
	address = {OXFORD FULFILLMENT CENTRE THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
	title = {The performance cost of software obfuscation for {Android} applications},
	volume = {73},
	issn = {0167-4048},
	doi = {10.1016/j.cose.2017.10.004},
	abstract = {Software security of Android applications is especially susceptible (vulnerable) to malicious reverse engineer than the native code based software, because its Java bytecode is easier to decompile and to reconstruct the original Java source code. Therefore obfuscation is an essential criterion for the protection of Android applications. Meanwhile, the unpredicted performance loss will be caused by the obfuscation, which might seriously influence the user experience of the software. Therefore the obfuscation degree and the performance cost out of it require being optimized. In this paper, we are working on the problem that: to obfuscate an Android application to a target level of “difficulty” degree, while not substantially “slowing it down”. We measure “difficulty” by utilizing software complexity metrics and the “slow” in CPU cycles. Within the framework designed for obfuscating Android applications, we implement the “Naive Bayesian Classifier” algorithm for the optimized obfuscation of the software complexity and the performance, and show that it outperforms the algorithm whose predictions rely on mean values. We furthermore investigate the performance penalty imposed by obfuscation process when framework targets different complexity values and metrics. We show that some obfuscation methods are more performance costly than others to achieve the same metric value increase. Our result shows that, for any given software complexity, the required performance benefits can be achieved if the right obfuscation techniques are used. (C) 2017 Elsevier Ltd. All rights reserved.},
	language = {English},
	journal = {COMPUTERS \& SECURITY},
	publisher = {ELSEVIER ADVANCED TECHNOLOGY},
	author = {Zhuang, Yan},
	month = mar,
	year = {2018},
	note = {Type: Article},
	keywords = {Software quality, Obfuscation, Android applications, Optimized obfuscation, Performance measurement, Software complexity metric},
	pages = {57--72},
}

@article{chen_tinydroid_2018,
	address = {ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND},
	title = {{TinyDroid}: {A} {Lightweight} and {Efficient} {Model} for {Android} {Malware} {Detection} and {Classification}},
	volume = {2018},
	issn = {1574-017X},
	doi = {10.1155/2018/4157156},
	abstract = {With the popularity of Android applications, Android malware has an exponential growth trend. In order to detect Android malware effectively, this paper proposes a novel lightweight static detection model, TinyDroid, using instruction simplification and machine learning technique. First, a symbol-based simplification method is proposed to abstract the opcode sequence decompiled from Android Dalvik Executable files. Then, N-gram is employed to extract features from the simplified opcode sequence, and a classifier is trained for the malware detection and classification tasks. To improve the efficiency and scalability of the proposed detection model, a compression procedure is also used to reduce features and select exemplars for the malware sample datasct. TinyDroid is compared against the state-of-the-art antivirus tools in real world using Drebin dataset. The experimental results show that TinyDroid can get a higher accuracy rate and lower false alarm rate with satisfied efficiency.},
	language = {English},
	journal = {MOBILE INFORMATION SYSTEMS},
	publisher = {HINDAWI LTD},
	author = {Chen, Tieming and Mao, Qingyu and Yang, Yimin and Lv, Mingqi and Zhu, Jianming},
	year = {2018},
	note = {Type: Article},
}

@article{yan_detecting_2018,
	address = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
	title = {Detecting {Malware} with an {Ensemble} {Method} {Based} on {Deep} {Neural} {Network}},
	issn = {1939-0114},
	doi = {10.1155/2018/7247095},
	abstract = {Malware detection plays a crucial role in computer security. Recent researches mainly use machine learning based methods heavily relying on domain knowledge for manually extracting malicious features. In this paper, we propose MalNet, a novel malware detection method that learns features automatically from the raw data. Concretely, we first generate a grayscale image from malware file, meanwhile extracting its opcode sequences with the decompilation tool IDA. Then MalNet uses CNN and LSTM networks to learn from grayscale image and opcode sequence, respectively, and takes a stacking ensemble for malware classification. We perform experiments on more than 40,000 samples including 20,650 benign files collected from online software providers and 21,736 malwares provided by Microsoft. The evaluation result shows that MalNet achieves 99.88\% validation accuracy for malware detection. In addition, we also take malware family classification experiment on 9 malware families to compare MalNet with other related works, in which MalNet outperforms most of related works with 99.36\% detection accuracy and achieves a considerable speed-up on detecting efficiency comparing with two state-of-the-art results on Microsoft malware dataset.},
	language = {English},
	journal = {SECURITY AND COMMUNICATION NETWORKS},
	publisher = {WILEY-HINDAWI},
	author = {Yan, Jinpei and Qi, Yong and Rao, Qifan},
	year = {2018},
	note = {Type: Article},
}

@article{yan_lstm-based_2018,
	address = {ADAM HOUSE, 3RD FL, 1 FITZROY SQ, LONDON, WIT 5HE, ENGLAND},
	title = {{LSTM}-{Based} {Hierarchical} {Denoising} {Network} for {Android} {Malware} {Detection}},
	issn = {1939-0114},
	doi = {10.1155/2018/5249190},
	abstract = {Mobile security is an important issue on Android platform. Most malware detection methods based on machine learning models heavily rely on expert knowledge for manual feature engineering, which are still difficult to fully describe malwares. In this paper, we present LSTM-based hierarchical denoise network (HDN), a novel static Android malware detection method which uses LSTM to directly learn from the raw opcode sequences extracted from decompiled Android files. However, most opcode sequences are too long for LSTM to train due to the gradient vanishing problem. Hence, HDN uses a hierarchical structure, whose first-level LSTM parallelly computes on opcode subsequences (we called them method blocks) to learn the dense representations; then the second-level LSTM can learn and detect malware through method block sequences. Considering that malicious behavior only appears in partial sequence segments, HDN uses method block denoise module (MBDM) for data denoising by adaptive gradient scaling strategy based on loss cache. We evaluate and compare HDN with the latest mainstream researches on three datasets. The results show that HDN outperforms these Android malware detection methods, and it is able to capture longer sequence features and has better detection efficiency than N-gram-based malware detection which is similar to our method.},
	language = {English},
	journal = {SECURITY AND COMMUNICATION NETWORKS},
	publisher = {WILEY-HINDAWI},
	author = {Yan, Jinpei and Qi, Yong and Rao, Qifan},
	year = {2018},
	note = {Type: Article},
}

@article{al-kaswan_extending_2023-1,
	title = {Extending {Source} {Code} {Pre}-{Trained} {Language} {Models} to {Summarise} {Decompiled} {Binaries}},
	url = {https://arxiv.org/pdf/2301.01701},
	doi = {https://doi.org/10.48550/arXiv.2301.01701},
	abstract = {Reverse engineering binaries is required to understand and analyse programs for which the source code is unavailable. Decompilers can transform the largely unreadable binaries into a more readable source code-like representation. However, reverse engineering is time-consuming, much of which is taken up by labelling the functions with semantic information. While the automated summarisation of decompiled code can help Reverse Engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise decompiled binary functions. Furthermore, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by generating synthetic datasets and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82, and 44.21 for summarising source, decompiled, and synthetically stripped decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
	author = {Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Ashok, Anand and Devanbu, Premkumar and Deursen, van, Arie},
	month = jan,
	year = {2023},
	annote = {SANER 2023 Technical Track Camera Ready},
}

@article{arasteh_trim_2025,
	title = {Trim {My} {View}: {An} {LLM}-{Based} {Code} {Query} {System} for {Module} {Retrieval} in {Robotic} {Firmware}},
	url = {https://arxiv.org/pdf/2503.03969},
	doi = {https://doi.org/10.48550/arXiv.2503.03969},
	abstract = {The software compilation process has a tendency to obscure the original design of the system and makes it difficult both to identify individual components and discern their purpose simply by examining the resulting binary code. Although decompilation techniques attempt to recover higher-level source code from the machine code in question, they are not fully able to restore the semantics of the original functions. Furthermore, binaries are often stripped of metadata, and this makes it challenging to reverse engineer complex binary software. In this paper we show how a combination of binary decomposition techniques, decompilation passes, and LLM-powered function summarization can be used to build an economical engine to identify modules in stripped binaries and associate them with high-level natural language descriptions. We instantiated this technique with three underlying open-source LLMs – CodeQwen, DeepSeek-Coder and CodeStral – and measured its effectiveness in identifying modules in robotics firmware. This experimental evaluation involved 467 modules from four devices from the ArduPilot software suite, and showed that CodeStral, the best-performing backend LLM, achieves an average F1-score of 0.68 with an online running time of just a handful of seconds.},
	author = {Arasteh, Sima and Jandaghi, Pegah and Weideman, Nicolaas and Perepech, Dennis and Raghothaman, Mukund and Hauser, Christophe and Garcia, Luis},
	month = mar,
	year = {2025},
	annote = {11 pages, 5 figures},
}

@article{armengol-estape_learning_2022,
	title = {Learning {C} to x86 {Translation}: {An} {Experiment} in {Neural} {Compilation}},
	url = {https://arxiv.org/pdf/2108.07639},
	doi = {https://doi.org/10.48550/arXiv.2108.07639},
	abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
	author = {Armengol-Estapé, Jordi and O'Boyle, P., F., Michael},
	month = feb,
	year = {2022},
	annote = {Published in AIPLANS 2021},
}

@article{armengol-estape_slade_2024-2,
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	url = {https://arxiv.org/pdf/2305.12520},
	doi = {https://doi.org/10.48550/arXiv.2305.12520},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from ExeBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, P., F., Michael},
	month = feb,
	year = {2024},
}

@article{arranz-olmos_decompiling_2025,
	title = {Decompiling for {Constant}-{Time} {Analysis}},
	url = {https://arxiv.org/pdf/2501.04183},
	doi = {https://doi.org/10.48550/arXiv.2501.04183},
	abstract = {Cryptographic libraries are a main target of timing side-channel attacks. A practical means to protect against these attacks is to adhere to the constant-time (CT) policy. However, it is hard to write constant-time code, and even constant-time code can be turned vulnerable by mainstream compilers. So how can we verify that binary code is constant-time? The obvious answer is to use binary-level CT tools. To do so, a common approach is to use decompilers or lifters as a front-end for CT analysis tools operating on source code or IR. Unfortunately, this approach is problematic with current decompilers. To illustrate this fact, we use the recent Clangover vulnerability and other constructed examples to show that five popular decompilers eliminate CT violations, rendering them not applicable with the approach. In this paper, we develop foundations to asses whether a decompiler is fit for the Decompile-then-Analyze approach. We propose CT transparency, which states that a transformation neither eliminates nor introduces CT violations, and a general method for proving that a program transformation is CT transparent. Then, we build CT-RetDec, a CT analysis tool based on a modified version of the LLVM-based decompiler RetDec. We evaluate CT-RetDec on a benchmark of real-world vulnerabilities in binaries, and show that the modifications had significant impact on CT-RetDec's performance. As a contribution of independent interest, we found that popular tools for binary-level CT analysis rely on decompiler-like transformations before analysis. We show that two such tools employ transformations that are not CT transparent, and, consequently, that they incorrectly accept non-CT programs. While our examples are very specific and do not invalidate the general approach of these tools, we advocate that tool developers counter such potential issues by proving the transparency of such transformations.},
	author = {Arranz-Olmos, Santiago and Barthe, Gilles and Blatter, Lionel and Bouzid, Youcef and Wall, der, van, Sören and Zhang, Zhiyuan},
	month = oct,
	year = {2025},
}

@article{banerjee_variable_2021,
	title = {Variable {Name} {Recovery} in {Decompiled} {Binary} {Code} using {Constrained} {Masked} {Language} {Modeling}},
	url = {https://arxiv.org/pdf/2103.12801},
	doi = {https://doi.org/10.48550/arXiv.2103.12801},
	abstract = {Decompilation is the procedure of transforming binary programs into a high-level representation, such as source code, for human analysts to examine. While modern decompilers can reconstruct and recover much information that is discarded during compilation, inferring variable names is still extremely difficult. Inspired by recent advances in natural language processing, we propose a novel solution to infer variable names in decompiled code based on Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as Transformers and BERT. Our solution takes {\textbackslash}textit\{raw\} decompiler output, the less semantically meaningful code, as input, and enriches it using our proposed {\textbackslash}textit\{finetuning\} technique, Constrained Masked Language Modeling. Using Constrained Masked Language Modeling introduces the challenge of predicting the number of masked tokens for the original variable name. We address this {\textbackslash}textit\{count of token prediction\} challenge with our post-processing algorithm. Compared to the state-of-the-art approaches, our trained VarBERT model is simpler and of much better performance. We evaluated our model on an existing large-scale data set with 164,632 binaries and showed that it can predict variable names identical to the ones present in the original source code up to 84.15\% of the time.},
	author = {Banerjee, Pratyay and Pal, Kumar, Kuntal and Wang, Fish and Baral, Chitta},
	month = mar,
	year = {2021},
	annote = {Work In Progress},
}

@article{bielik_adversarial_2020,
	title = {Adversarial {Robustness} for {Code}},
	url = {https://arxiv.org/pdf/2002.04694},
	doi = {https://doi.org/10.48550/arXiv.2002.04694},
	abstract = {Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code such as finding and fixing bugs, code completion, decompilation, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) combining existing and novel techniques to improve robustness while preserving high accuracy.},
	author = {Bielik, Pavol and Vechev, Martin},
	month = aug,
	year = {2020},
	annote = {Proceedings of the 37th International Conference on Machine Learning, Online, PMLR 119, 2020},
}

@article{bu_smartbugbert_2025,
	title = {{SmartBugBert}: {BERT}-{Enhanced} {Vulnerability} {Detection} for {Smart} {Contract} {Bytecode}},
	url = {https://arxiv.org/pdf/2504.05002},
	doi = {https://doi.org/10.48550/arXiv.2504.05002},
	abstract = {Smart contracts deployed on blockchain platforms are vulnerable to various security vulnerabilities. However, only a small number of Ethereum contracts have released their source code, so vulnerability detection at the bytecode level is crucial. This paper introduces SmartBugBert, a novel approach that combines BERT-based deep learning with control flow graph (CFG) analysis to detect vulnerabilities directly from bytecode. Our method first decompiles smart contract bytecode into optimized opcode sequences, extracts semantic features using TF-IDF, constructs control flow graphs to capture execution logic, and isolates vulnerable CFG fragments for targeted analysis. By integrating both semantic and structural information through a fine-tuned BERT model and LightGBM classifier, our approach effectively identifies four critical vulnerability types: transaction-ordering, access control, self-destruct, and timestamp dependency vulnerabilities. Experimental evaluation on 6,157 Ethereum smart contracts demonstrates that SmartBugBert achieves 90.62\% precision, 91.76\% recall, and 91.19\% F1-score, significantly outperforming existing detection methods. Ablation studies confirm that the combination of semantic features with CFG information substantially enhances detection performance. Furthermore, our approach maintains efficient detection speed (0.14 seconds per contract), making it practical for large-scale vulnerability assessment.},
	author = {Bu, Jiuyang and Li, Wenkai and Li, Zongwei and Zhang, Zeng and Li, Xiaoqi},
	month = apr,
	year = {2025},
}

@article{butz_sum-product_2020,
	title = {Sum-{Product} {Network} {Decompilation}},
	url = {https://arxiv.org/pdf/1912.10092},
	doi = {https://doi.org/10.48550/arXiv.1912.10092},
	abstract = {There exists a dichotomy between classical probabilistic graphical models, such as Bayesian networks (BNs), and modern tractable models, such as sum-product networks (SPNs). The former generally have intractable inference, but provide a high level of interpretability, while the latter admits a wide range of tractable inference routines, but are typically harder to interpret. Due to this dichotomy, tools to convert between BNs and SPNs are desirable. While one direction – compiling BNs into SPNs – is well discussed in Darwiche's seminal work on arithmetic circuit compilation, the converse direction – decompiling SPNs into BNs – has received surprisingly little attention. In this paper, we fill this gap by proposing SPN2BN, an algorithm that decompiles an SPN into a BN. SPN2BN has several salient features when compared to the only other two works decompiling SPNs. Most significantly, the BNs returned by SPN2BN are minimal independence-maps that are more parsimonious with respect to the introduction of latent variables. Secondly, the output BN produced by SPN2BN can be precisely characterized with respect to a compiled BN. More specifically, a certain set of directed edges will be added to the input BN, giving what we will call the moral-closure. Lastly, it is established that our compilation-decompilation process is idempotent. This has practical significance as it limits the size of the decompiled SPN.},
	author = {Butz, J., Cory and Oliveira, S., Jhonatan and Peharz, Robert},
	month = may,
	year = {2020},
}

@article{cao_revisiting_2023-1,
	title = {Revisiting {Deep} {Learning} for {Variable} {Type} {Recovery}},
	url = {https://arxiv.org/pdf/2304.03854},
	doi = {https://doi.org/10.48550/arXiv.2304.03854},
	abstract = {Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.},
	author = {Cao, Kevin and Leach, Kevin},
	month = apr,
	year = {2023},
	annote = {In The 31st International Conference on Program Comprehension(ICPC 2023 RENE)},
}

@article{cao_boosting_2023,
	title = {Boosting {Neural} {Networks} to {Decompile} {Optimized} {Binaries}},
	url = {https://arxiv.org/pdf/2301.00969},
	doi = {https://doi.org/10.48550/arXiv.2301.00969},
	abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21\% higher accuracy than state-of-the-art neural decompilation frameworks.},
	author = {Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei},
	month = jan,
	year = {2023},
}

@article{chakraborty_architecture_2020,
	title = {On {Architecture} to {Architecture} {Mapping} for {Concurrency}},
	url = {https://arxiv.org/pdf/2009.03846},
	doi = {https://doi.org/10.48550/arXiv.2009.03846},
	abstract = {Mapping programs from one architecture to another plays a key role in technologies such as binary translation, decompilation, emulation, virtualization, and application migration. Although multicore architectures are ubiquitous, the state-of-the-art translation tools do not handle concurrency primitives correctly. Doing so is rather challenging because of the subtle differences in the concurrency models between architectures. In response, we address various aspects of the challenge. First, we develop correct and efficient translations between the concurrency models of two mainstream architecture families: x86 and ARM (versions 7 and 8). We develop direct mappings between x86 and ARMv8 and ARMv7, and fence elimination algorithms to eliminate redundant fences after direct mapping. Although our mapping utilizes ARMv8 as an intermediate model for mapping between x86 and ARMv7, we argue that it should not be used as an intermediate model in a decompiler because it disallows common compiler transformations. Second, we propose and implement a technique for inserting memory fences for safely migrating programs between different architectures. Our technique checks robustness against x86 and ARM, and inserts fences upon robustness violations. Our experiments demonstrate that in most of the programs both our techniques introduce significantly fewer fences compared to naive schemes for porting applications across these architectures.},
	author = {Chakraborty, Soham},
	month = sep,
	year = {2020},
}

@article{chawla_decompilation-driven_2026,
	title = {A {Decompilation}-{Driven} {Framework} for {Malware} {Detection} with {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2601.09035},
	doi = {https://doi.org/10.48550/arXiv.2601.09035},
	abstract = {The parallel evolution of Large Language Models (LLMs) with advanced code-understanding capabilities and the increasing sophistication of malware presents a new frontier for cybersecurity research. This paper evaluates the efficacy of state-of-the-art LLMs in classifying executable code as either benign or malicious. We introduce an automated pipeline that first decompiles Windows executable into a C code using Ghidra disassembler and then leverages LLMs to perform the classification. Our evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. We demonstrate that a fine-tuned model, trained on curated malware and benign datasets, significantly outperforms its vanilla counterpart. However, the performance of even this specialized model degrades notably when encountering newer malware. This finding demonstrates the critical need for continuous fine-tuning with emerging threats to maintain model effectiveness against the changing coding patterns and behaviors of malicious software.},
	author = {Chawla, Aniesh and Prasad, Udbhav},
	month = jan,
	year = {2026},
	annote = {6 pages, published in 2025 IEMCON},
}

@article{chen_augmenting_2021,
	title = {Augmenting {Decompiler} {Output} with {Learned} {Variable} {Names} and {Types}},
	url = {https://arxiv.org/pdf/2108.06363},
	doi = {https://doi.org/10.48550/arXiv.2108.06363},
	abstract = {A common tool used by security professionals for reverse-engineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4\% of the time and the original types 75.8\% of the time.},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, J., Edward and Goues, Le, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = aug,
	year = {2021},
	annote = {17 pages to be published in USENIX Security '22},
}

@article{chen_recopilot_2025,
	title = {{ReCopilot}: {Reverse} {Engineering} {Copilot} in {Binary} {Analysis}},
	url = {https://arxiv.org/pdf/2505.16366},
	doi = {https://doi.org/10.48550/arXiv.2505.16366},
	abstract = {Binary analysis plays a pivotal role in security domains such as malware detection and vulnerability discovery, yet it remains labor-intensive and heavily reliant on expert knowledge. General-purpose large language models (LLMs) perform well in programming analysis on source code, while binaryspecific LLMs are underexplored. In this work, we present ReCopilot, an expert LLM designed for binary analysis tasks. ReCopilot integrates binary code knowledge through a meticulously constructed dataset, encompassing continue pretraining (CPT), supervised fine-tuning (SFT), and direct preference optimization (DPO) stages. It leverages variable data flow and call graph to enhance context awareness and employs test-time scaling to improve reasoning capabilities. Evaluations on a comprehensive binary analysis benchmark demonstrate that ReCopilot achieves state-of-the-art performance in tasks such as function name recovery and variable type inference on the decompiled pseudo code, outperforming both existing tools and LLMs by 13\%. Our findings highlight the effectiveness of domain-specific training and context enhancement, while also revealing challenges in building super long chain-of-thought. ReCopilot represents a significant step toward automating binary analysis with interpretable and scalable AI assistance in this domain.},
	author = {Chen, Guoqiang and Sun, Huiqi and Liu, Daguang and Wang, Zhiqi and Wang, Qiang and Yin, Bin and Liu, Lu and Ying, Lingyun},
	month = may,
	year = {2025},
}

@article{chen_suigpt_2025-1,
	title = {{SuiGPT} {MAD}: {Move} {AI} {Decompiler} to {Improve} {Transparency} and {Auditability} on {Non}-{Open}-{Source} {Blockchain} {Smart} {Contract}},
	url = {https://arxiv.org/pdf/2410.15275},
	doi = {https://doi.org/10.48550/arXiv.2410.15275},
	abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33\% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization},
	author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Tingguan, Wu and Wang, Siyun and Chalkias, Kryptos, Kostas},
	month = jan,
	year = {2025},
	annote = {Paper accepted at ACM The Web Conference 2025},
}

@article{chukkol_vulcatch_2024,
	title = {{VulCatch}: {Enhancing} {Binary} {Vulnerability} {Detection} through {CodeT5} {Decompilation} and {KAN} {Advanced} {Feature} {Extraction}},
	url = {https://arxiv.org/pdf/2408.07181},
	doi = {https://doi.org/10.48550/arXiv.2408.07181},
	abstract = {Binary program vulnerability detection is critical for software security, yet existing deep learning approaches often rely on source code analysis, limiting their ability to detect unknown vulnerabilities. To address this, we propose VulCatch, a binary-level vulnerability detection framework. VulCatch introduces a Synergy Decompilation Module (SDM) and Kolmogorov-Arnold Networks (KAN) to transform raw binary code into pseudocode using CodeT5, preserving high-level semantics for deep analysis with tools like Ghidra and IDA. KAN further enhances feature transformation, enabling the detection of complex vulnerabilities. VulCatch employs word2vec, Inception Blocks, BiLSTM Attention, and Residual connections to achieve high detection accuracy (98.88\%) and precision (97.92\%), while minimizing false positives (1.56\%) and false negatives (2.71\%) across seven CVE datasets.},
	author = {Chukkol, Adama, Hamman, Abdulrahman and Luo, Senlin and Sharif, Kashif and Haruna, Yunusa and Abdullahi, Muhammad, Muhammad},
	month = aug,
	year = {2024},
}

@article{cotroneo_can_2025-1,
	title = {Can {Neural} {Decompilation} {Assist} {Vulnerability} {Prediction} on {Binary} {Code}?},
	url = {https://arxiv.org/pdf/2412.07538},
	doi = {https://doi.org/10.48550/arXiv.2412.07538},
	abstract = {Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.},
	author = {Cotroneo, D. and Grasso, C., F. and Natella, R. and Orbinato, V.},
	month = mar,
	year = {2025},
}

@article{cristea_malcve_2025,
	title = {{MalCVE}: {Malware} {Detection} and {CVE} {Association} {Using} {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2510.15567},
	doi = {https://doi.org/10.48550/arXiv.2510.15567},
	abstract = {Malicious software attacks are having an increasingly significant economic impact. Commercial malware detection software can be costly, and tools that attribute malware to the specific software vulnerabilities it exploits are largely lacking. Understanding the connection between malware and the vulnerabilities it targets is crucial for analyzing past threats and proactively defending against current ones. In this study, we propose an approach that leverages large language models (LLMs) to detect binary malware, specifically within JAR files, and utilizes the capabilities of LLMs combined with retrieval-augmented generation (RAG) to identify Common Vulnerabilities and Exposures (CVEs) that malware may exploit. We developed a proof-of-concept tool called MalCVE, which integrates binary code decompilation, deobfuscation, LLM-based code summarization, semantic similarity search, and CVE classification using LLMs. We evaluated MalCVE using a benchmark dataset of 3,839 JAR executables. MalCVE achieved a mean malware detection accuracy of 97\%, at a fraction of the cost of commercial solutions. It is also the first tool to associate CVEs with binary malware, achieving a recall@10 of 65\%, which is comparable to studies that perform similar analyses on source code.},
	author = {Cristea, Andrei, Eduard and Molnes, Petter and Li, Jingyue},
	month = oct,
	year = {2025},
}

@article{david_decompiling_2025,
	title = {Decompiling {Smart} {Contracts} with a {Large} {Language} {Model}},
	url = {https://arxiv.org/pdf/2506.19624},
	doi = {https://doi.org/10.48550/arXiv.2506.19624},
	abstract = {The widespread lack of broad source code verification on blockchain explorers such as Etherscan, where despite 78,047,845 smart contracts deployed on Ethereum (as of May 26, 2025), a mere 767,520 ({\textless} 1\%) are open source, presents a severe impediment to blockchain security. This opacity necessitates the automated semantic analysis of on-chain smart contract bytecode, a fundamental research challenge with direct implications for identifying vulnerabilities and understanding malicious behavior. Prevailing decompilers struggle to reverse bytecode in a readable manner, often yielding convoluted code that critically hampers vulnerability analysis and thwarts efforts to dissect contract functionalities for security auditing. This paper addresses this challenge by introducing a pioneering decompilation pipeline that, for the first time, successfully leverages Large Language Models (LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable and semantically faithful Solidity code. Our novel methodology first employs rigorous static program analysis to convert bytecode into a structured three-address code (TAC) representation. This intermediate representation then guides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset of 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity. This approach uniquely recovers meaningful variable names, intricate control flow, and precise function signatures. Our extensive empirical evaluation demonstrates a significant leap beyond traditional decompilers, achieving an average semantic similarity of 0.82 with original source and markedly superior readability. The practical viability and effectiveness of our research are demonstrated through its implementation in a publicly accessible system, available at https://evmdecompiler.com.},
	author = {David, Isaac and Zhou, Liyi and Song, Dawn and Gervais, Arthur and Qin, Kaihua},
	month = jun,
	year = {2025},
}

@article{dramko_idioms_2025,
	title = {Idioms: {Neural} {Decompilation} {With} {Joint} {Code} and {Type} {Definition} {Prediction}},
	url = {https://arxiv.org/pdf/2502.04536},
	doi = {https://doi.org/10.48550/arXiv.2502.04536},
	abstract = {Decompilers are important tools for reverse engineers that help them analyze software at a higher level of abstraction than assembly code. Unfortunately, because compilation is lossy, deterministic decompilers produce code that is missing many of the details that make source code readable in the first place, like variable names and types. Neural decompilers, on the other hand, offer the ability to statistically fill in these details. Existing work in neural decompilation, however, suffers from substantial limitations that preclude its use on real code, such as the inability to define composite types, which is essential to fully specify function semantics. In this work, we introduce a new dataset, Realtype, that includes substantially more complicated and realistic types than existing neural decompilation benchmarks, and Idioms, a new neural decompilation approach to finetune any LLM into a neural decompiler capable of generating the appropriate user-defined type definitions alongside the decompiled code. We show that our approach yields state-of-the-art results in neural decompilation. On the most challenging existing benchmark, ExeBench, our model achieves 54.4\% accuracy vs. 46.3\% for LLM4Decompile and 37.5\% for Nova; on Realtype, our model performs at least 95\% better.},
	author = {Dramko, Luke and Goues, Le, Claire and Schwartz, J., Edward},
	month = jun,
	year = {2025},
}

@article{enders_dewolf_2022,
	title = {dewolf: {Improving} {Decompilation} by leveraging {User} {Surveys}},
	url = {https://arxiv.org/pdf/2205.06719},
	doi = {https://doi.org/10.48550/arXiv.2205.06719},
	abstract = {Analyzing third-party software such as malware or firmware is a crucial task for security analysts. Although various approaches for automatic analysis exist and are the subject of ongoing research, analysts often have to resort to manual static analysis to get a deep understanding of a given binary sample. Since the source code of encountered samples is rarely available, analysts regularly employ decompilers for easier and faster comprehension than analyzing a binary's disassembly. In this paper, we introduce our decompilation approach dewolf. We developed a variety of improvements over the previous academic state-of-the-art decompiler and some novel algorithms to enhance readability and comprehension, focusing on manual analysis. To evaluate our approach and to obtain a better insight into the analysts' needs, we conducted three user surveys. The results indicate that dewolf is suitable for malware comprehension and that its output quality noticeably exceeds Ghidra and Hex-Rays in certain aspects. Furthermore, our results imply that decompilers aiming at manual analysis should be highly configurable to respect individual user preferences. Additionally, future decompilers should not necessarily follow the unwritten rule to stick to the code-structure dictated by the assembly in order to produce readable output. In fact, the few cases where dewolf already cracks this rule lead to its results considerably exceeding other decompilers. We publish a prototype implementation of dewolf and all survey results on GitHub.},
	author = {Enders, Steffen and Behner, C., Eva-Maria and Bergmann, Niklas and Rybalka, Mariia and Padilla, Elmar and Hui, Xue, Er and Low, Henry and Sim, Nicholas},
	month = may,
	year = {2022},
}

@article{erinfolami_declassifier_2019-1,
	title = {{DeClassifier}: {Class}-{Inheritance} {Inference} {Engine} for {Optimized} {C}++ {Binaries}},
	url = {https://arxiv.org/pdf/1901.10073},
	doi = {https://doi.org/10.48550/arXiv.1901.10073},
	abstract = {Recovering class inheritance from C++ binaries has several security benefits including problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining–or worse exclusion–due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5\% and 71.4\% true positive directed edges in the class hierarchy tree under O0 and O2 optimizations respectively, (2) a combination of ctor+dtor analysis provides much better inference than ctor only analysis.},
	author = {Erinfolami, Ayomide, Rukayat and Prakash, Aravind},
	month = feb,
	year = {2019},
	annote = {13 pages of main paper including references, 1 page of appendix, 2 figures and 10 tables},
}

@article{erinfolami_devil_2020-1,
	title = {Devil is {Virtual}: {Reversing} {Virtual} {Inheritance} in {C}++ {Binaries}},
	url = {https://arxiv.org/pdf/2003.05039},
	doi = {https://doi.org/10.48550/arXiv.2003.05039},
	abstract = {Complexities that arise from implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious to virtual inheritance. Conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10\% on Linux and 12.5\% on Windows) of real-world C++ programs including Mysql and libstdc++. (2) we show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These false positves and negatives either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95\% and 95.5\% (GCC -O0) and a minimum of 77.5\% and 73.8\% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.},
	author = {Erinfolami, Ayomide, Rukayat and Prakash, Aravind},
	month = jun,
	year = {2020},
	annote = {Accepted at CCS20. This is a technical report version},
}

@article{escalada_improving_2021,
	title = {Improving type information inferred by decompilers with supervised machine learning},
	url = {https://arxiv.org/pdf/2101.08116},
	doi = {https://doi.org/10.48550/arXiv.2101.08116},
	abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1\% F1-measure, whereas the best decompiler obtains a 30\% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers.},
	author = {Escalada, Javier and Scully, Ted and Ortin, Francisco},
	month = feb,
	year = {2021},
}

@article{fang_stacksight_2024,
	title = {{StackSight}: {Unveiling} {WebAssembly} through {Large} {Language} {Models} and {Neurosymbolic} {Chain}-of-{Thought} {Decompilation}},
	url = {https://arxiv.org/pdf/2406.04568},
	doi = {https://doi.org/10.48550/arXiv.2406.04568},
	abstract = {WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics.},
	author = {Fang, Weike and Zhou, Zhejian and He, Junzhou and Wang, Weihang},
	month = jun,
	year = {2024},
	annote = {9 pages. In the Proceedings of the 41st International Conference on Machine Learning (ICML' 24)},
}

@article{feng_self-constructed_2024,
	title = {Self-{Constructed} {Context} {Decompilation} with {Fined}-grained {Alignment} {Enhancement}},
	url = {https://arxiv.org/pdf/2406.17233},
	doi = {https://doi.org/10.48550/arXiv.2406.17233},
	abstract = {Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc{\textasciicircum}2dec) method recompiles the LLM's decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90\% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41\%. The code, data, and models are available at https://github.com/AlongWY/sccdec.},
	author = {Feng, Yunlong and Teng, Dechuan and Xu, Yang and Mu, Honglin and Xu, Xiao and Qin, Libo and Zhu, Qingfu and Che, Wanxiang},
	month = oct,
	year = {2024},
	annote = {EMNLP 2024 Findings},
}

@article{feng_ref_2025,
	title = {{ReF} {Decompile}: {Relabeling} and {Function} {Call} {Enhanced} {Decompile}},
	url = {https://arxiv.org/pdf/2502.12221},
	doi = {https://doi.org/10.48550/arXiv.2502.12221},
	abstract = {The goal of decompilation is to convert compiled low-level code (e.g., assembly code) back into high-level programming languages, enabling analysis in scenarios where source code is unavailable. This task supports various reverse engineering applications, such as vulnerability identification, malware analysis, and legacy software migration. The end-to-end decompile method based on large langauge models (LLMs) reduces reliance on additional tools and minimizes manual intervention due to its inherent properties. However, previous end-to-end methods often lose critical information necessary for reconstructing control flow structures and variables when processing binary files, making it challenging to accurately recover the program's logic. To address these issues, we propose the {\textbackslash}textbf\{ReF Decompile\} method, which incorporates the following innovations: (1) The Relabelling strategy replaces jump target addresses with labels, preserving control flow clarity. (2) The Function Call strategy infers variable types and retrieves missing variable information from binary files. Experimental results on the Humaneval-Decompile Benchmark demonstrate that ReF Decompile surpasses comparable baselines and achieves state-of-the-art (SOTA) performance of 61.43\%.},
	author = {Feng, Yunlong and Li, Bohan and Shi, Xiaoming and Zhu, Qingfu and Che, Wanxiang},
	month = feb,
	year = {2025},
}

@article{fu_neural-based_2019,
	title = {A {Neural}-based {Program} {Decompiler}},
	url = {https://arxiv.org/pdf/1906.12029},
	doi = {https://doi.org/10.48550/arXiv.1906.12029},
	abstract = {Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82\% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0\% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70\% program accuracy.},
	author = {Fu, Cheng and Chen, Huili and Liu, Haolan and Chen, Xinyun and Tian, Yuandong and Koushanfar, Farinaz and Zhao, Jishen},
	month = jun,
	year = {2019},
}

@article{gao_decompilebench_2025,
	title = {{DecompileBench}: {A} {Comprehensive} {Benchmark} for {Evaluating} {Decompilers} in {Real}-{World} {Scenarios}},
	url = {https://arxiv.org/pdf/2505.11340},
	doi = {https://doi.org/10.48550/arXiv.2505.11340},
	abstract = {Decompilers are fundamental tools for critical security tasks, from vulnerability discovery to malware analysis, yet their evaluation remains fragmented. Existing approaches primarily focus on syntactic correctness through synthetic micro-benchmarks or subjective human ratings, failing to address real-world requirements for semantic fidelity and analyst usability. We present DecompileBench, the first comprehensive framework that enables effective evaluation of decompilers in reverse engineering workflows through three key components: {\textbackslash}textit\{real-world function extraction\} (comprising 23,400 functions from 130 real-world programs), {\textbackslash}textit\{runtime-aware validation\}, and {\textbackslash}textit\{automated human-centric assessment\} using LLM-as-Judge to quantify the effectiveness of decompilers in reverse engineering workflows. Through a systematic comparison between six industrial-strength decompilers and six recent LLM-powered approaches, we demonstrate that LLM-based methods surpass commercial tools in code understandability despite 52.2\% lower functionality correctness. These findings highlight the potential of LLM-based approaches to transform human-centric reverse engineering. We open source \{https://github.com/Jennieett/DecompileBench\}\{DecompileBench\} to provide a framework to advance research on decompilers and assist security experts in making informed tool selections based on their specific requirements.},
	author = {Gao, Zeyu and Cui, Yuxin and Wang, Hao and Qin, Siliang and Wang, Yuanda and Zhang, Bolun and Zhang, Chao},
	month = may,
	year = {2025},
}

@article{green_stride_2024,
	title = {{STRIDE}: {Simple} {Type} {Recognition} {In} {Decompiled} {Executables}},
	url = {https://arxiv.org/pdf/2407.02733},
	doi = {https://doi.org/10.48550/arXiv.2407.02733},
	abstract = {Decompilers are widely used by security researchers and developers to reverse engineer executable code. While modern decompilers are adept at recovering instructions, control flow, and function boundaries, some useful information from the original source code, such as variable types and names, is lost during the compilation process. Our work aims to predict these variable types and names from the remaining information. We propose STRIDE, a lightweight technique that predicts variable names and types by matching sequences of decompiler tokens to those found in training data. We evaluate it on three benchmark datasets and find that STRIDE achieves comparable performance to state-of-the-art machine learning models for both variable retyping and renaming while being much simpler and faster. We perform a detailed comparison with two recent SOTA transformer-based models in order to understand the specific factors that make our technique effective. We implemented STRIDE in fewer than 1000 lines of Python and have open-sourced it under a permissive license at https://github.com/hgarrereyn/STRIDE.},
	author = {Green, Harrison and Schwartz, J., Edward and Goues, Le, Claire and Vasilescu, Bogdan},
	month = jul,
	year = {2024},
}

@article{harrand_strengths_2019-1,
	title = {The {Strengths} and {Behavioral} {Quirks} of {Java} {Bytecode} {Decompilers}},
	url = {https://arxiv.org/pdf/1908.06895},
	doi = {https://doi.org/10.48550/arXiv.1908.06895},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84\% of classes of our dataset and semantically equivalent code output for 78\% of classes.},
	author = {Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit},
	month = aug,
	year = {2019},
	annote = {11 pages, 6 figures, 9 listings, 3 tables},
}

@article{he_retrofit_2025,
	title = {Retrofit: {Continual} {Learning} with {Bounded} {Forgetting} for {Security} {Applications}},
	url = {https://arxiv.org/pdf/2511.11439},
	doi = {https://doi.org/10.48550/arXiv.2511.11439},
	abstract = {Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference. We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2\% to 38.6\% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.},
	author = {He, Yiling and Lei, Junchi and She, Hongyu and Shao, Shuo and Zheng, Xinran and Liu, Yiping and Qin, Zhan and Cavallaro, Lorenzo},
	month = jan,
	year = {2025},
}

@article{he_benchmarking_2025-1,
	title = {On {Benchmarking} {Code} {LLMs} for {Android} {Malware} {Analysis}},
	url = {https://arxiv.org/pdf/2504.00694},
	doi = {https://doi.org/10.48550/arXiv.2504.00694},
	abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.},
	author = {He, Yiling and She, Hongyu and Qian, Xingzhi and Zheng, Xinran and Chen, Zhuo and Qin, Zhan and Cavallaro, Lorenzo},
	month = apr,
	year = {2025},
	annote = {This paper has been accepted to the 34th ACM SIGSOFT ISSTA Companion (LLMSC Workshop 2025)},
}

@article{hosseini_beyond_2022,
	title = {Beyond the {C}: {Retargetable} {Decompilation} using {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/pdf/2212.08950},
	doi = {https://doi.org/10.48550/arXiv.2212.08950},
	abstract = {The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software. Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation. Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code. In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages. We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation.},
	author = {Hosseini, Iman and Dolan-Gavitt, Brendan},
	month = feb,
	year = {2022},
}

@article{hussain_vulbinllm_2025,
	title = {{VulBinLLM}: {LLM}-powered {Vulnerability} {Detection} for {Stripped} {Binaries}},
	url = {https://arxiv.org/pdf/2505.22010},
	doi = {https://doi.org/10.48550/arXiv.2505.22010},
	abstract = {Recognizing vulnerabilities in stripped binary files presents a significant challenge in software security. Although some progress has been made in generating human-readable information from decompiled binary files with Large Language Models (LLMs), effectively and scalably detecting vulnerabilities within these binary files is still an open problem. This paper explores the novel application of LLMs to detect vulnerabilities within these binary files. We demonstrate the feasibility of identifying vulnerable programs through a combined approach of decompilation optimization to make the vulnerabilities more prominent and long-term memory for a larger context window, achieving state-of-the-art performance in binary vulnerability analysis. Our findings highlight the potential for LLMs to overcome the limitations of traditional analysis methods and advance the field of binary vulnerability detection, paving the way for more secure software systems. In this paper, we present Vul-BinLLM , an LLM-based framework for binary vulnerability detection that mirrors traditional binary analysis workflows with fine-grained optimizations in decompilation and vulnerability reasoning with an extended context. In the decompilation phase, Vul-BinLLM adds vulnerability and weakness comments without altering the code structure or functionality, providing more contextual information for vulnerability reasoning later. Then for vulnerability reasoning, Vul-BinLLM combines in-context learning and chain-of-thought prompting along with a memory management agent to enhance accuracy. Our evaluations encompass the commonly used synthetic dataset Juliet to evaluate the potential feasibility for analysis and vulnerability detection in C/C++ binaries. Our evaluations show that Vul-BinLLM is highly effective in detecting vulnerabilities on the compiled Juliet dataset.},
	author = {Hussain, Nasir and Chen, Haohan and Tran, Chanh and Huang, Philip and Li, Zhuohao and Chugh, Pravir and Chen, William and Kundu, Ashish and Tian, Yuan},
	month = may,
	year = {2025},
}

@article{jiang_can_2025,
	title = {Can {Large} {Language} {Models} {Understand} {Intermediate} {Representations} in {Compilers}?},
	url = {https://arxiv.org/pdf/2502.06854},
	doi = {https://doi.org/10.48550/arXiv.2502.06854},
	abstract = {Intermediate Representations (IRs) play a critical role in compiler design and program analysis, yet their comprehension by Large Language Models (LLMs) remains underexplored. In this paper, we present an explorative empirical study evaluating the capabilities of six state-of-the-art LLMs: GPT-4, GPT-3, DeepSeek, Gemma 2, Llama 3, and Code Llama, in understanding IRs. Specifically, we assess model performance across four core tasks: control flow graph reconstruction, decompilation, code summarization, and execution reasoning. While LLMs exhibit competence in parsing IR syntax and identifying high-level structures, they consistently struggle with instruction-level reasoning, especially in control flow reasoning, loop handling, and dynamic execution. Common failure modes include misinterpreting branching instructions, omitting critical operations, and relying on heuristic reasoning rather than precise instruction-level logic. Our findings highlight the need for IR-specific enhancements in LLM design. We recommend fine-tuning on structured IR datasets and integrating control-flow-sensitive architectures to improve model effectiveness. All experimental data and source code are publicly available at},
	author = {Jiang, Hailong and Zhu, Jianfeng and Wan, Yao and Fang, Bo and Zhang, Hongyu and Jin, Ruoming and Guan, Qiang},
	month = jun,
	year = {2025},
}

@article{jiang_nova_2025,
	title = {Nova: {Generative} {Language} {Models} for {Assembly} {Code} with {Hierarchical} {Attention} and {Contrastive} {Learning}},
	url = {https://arxiv.org/pdf/2311.13721},
	doi = {https://doi.org/10.48550/arXiv.2311.13721},
	abstract = {Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 – 21.58\% (absolute percentage point improvement) higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17\% Recall@1, showing promising abilities on both assembly generation and understanding tasks.},
	author = {Jiang, Nan and Wang, Chengxiao and Liu, Kevin and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu and Babkin, Petr},
	month = jan,
	year = {2025},
	annote = {Published as a conference paper at ICLR 2025},
}

@article{katz_towards_2019,
	title = {Towards {Neural} {Decompilation}},
	url = {https://arxiv.org/pdf/1905.08325},
	doi = {https://doi.org/10.48550/arXiv.1905.08325},
	abstract = {We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code. The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high. We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97\% and 88\% of our benchmarks respectively.},
	author = {Katz, Omer and Olshaker, Yuval and Goldberg, Yoav and Yahav, Eran},
	month = may,
	year = {2019},
}

@article{kc_neural_2023,
	title = {Neural {Machine} {Translation} for {Code} {Generation}},
	url = {https://arxiv.org/pdf/2305.13504},
	doi = {https://doi.org/10.48550/arXiv.2305.13504},
	abstract = {Neural machine translation (NMT) methods developed for natural language processing have been shown to be highly successful in automating translation from one natural language to another. Recently, these NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. In the literature, a variety of different input scenarios have been explored, including generating code based on natural language description, lower-level representations such as binary or assembly (neural decompilation), partial representations of source code (code completion and repair), and source code in another language (code translation). In this paper we survey the NMT for code generation literature, cataloging the variety of methods that have been explored according to input and output representations, model architectures, optimization techniques used, data sets, and evaluation methods. We discuss the limitations of existing methods and future research directions},
	author = {KC, Dharma and Morrison, T., Clayton},
	month = may,
	year = {2023},
	annote = {33 pages, 1 figure},
}

@article{kim_feature_2023,
	title = {Feature {Engineering} {Using} {File} {Layout} for {Malware} {Detection}},
	url = {https://arxiv.org/pdf/2304.02260},
	doi = {https://doi.org/10.48550/arXiv.2304.02260},
	abstract = {Malware detection on binary executables provides a high availability to even binaries which are not disassembled or decompiled. However, a binary-level approach could cause ambiguity problems. In this paper, we propose a new feature engineering technique that use minimal knowledge about the internal layout on a binary. The proposed feature avoids the ambiguity problems by integrating the information about the layout with structural entropy. The experimental results show that our feature improves accuracy and F1-score by 3.3\% and 0.07, respectively, on a CNN based malware detector with realistic benign and malicious samples.},
	author = {Kim, Jeongwoo and Cho, Eun-Sun and Paik, Joon-Young},
	month = apr,
	year = {2023},
	annote = {2pages, no figures, This manuscript was presented in the poster session of The Annual Computer Security Applications Conference (ACSAC) 2020},
}

@article{kumar_systematic_2018,
	title = {A {Systematic} {Study} on {Static} {Control} {Flow} {Obfuscation} {Techniques} in {Java}},
	url = {https://arxiv.org/pdf/1809.11037},
	doi = {https://doi.org/10.48550/arXiv.1809.11037},
	abstract = {Control flow obfuscation (CFO) alters the control flow path of a program without altering its semantics. Existing literature has proposed several techniques; however, a quick survey reveals a lack of clarity in the types of techniques proposed, and how many are unique. What is also unclear is whether there is a disparity in the theory and practice of CFO. In this paper, we systematically study CFO techniques proposed for Java programs, both from papers and commercially available tools. We evaluate 13 obfuscators using a dataset of 16 programs with varying software characteristics, and different obfuscator parameters. Each program is carefully reverse engineered to study the effect of obfuscation. Our study reveals that there are 36 unique techniques proposed in the literature and 7 from tools. Three of the most popular commercial obfuscators implement only 13 of the 36 techniques in the literature. Thus there appears to be a gap between the theory and practice of CFO. We propose a novel classification of the obfuscation techniques based on the underlying component of a program that is transformed. We identify the techniques that are potent against reverse engineering attacks, both from the perspective of a human analyst and an automated program decompiler. Our analysis reveals that majority of the tools do not implement these techniques, thus defeating the protection obfuscation offers. We furnish examples of select techniques and discuss our findings. To the best of our knowledge, we are the first to assemble such a research. This study will be useful to software designers to decide upon the best techniques to use based upon their needs, for researchers to understand the state-of-the-art and for commercial obfuscator developers to develop new techniques.},
	author = {Kumar, Renuka and Kurian, Mariam, Anjana},
	month = sep,
	year = {2018},
	annote = {20 pages, 3 tables},
}

@article{lacomis_dire_2019-1,
	title = {{DIRE}: {A} {Neural} {Approach} to {Decompiled} {Identifier} {Naming}},
	url = {https://arxiv.org/pdf/1909.09029},
	doi = {https://doi.org/10.48550/arXiv.1909.09029},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, J., Edward and Allamanis, Miltiadis and Goues, Le, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = oct,
	year = {2019},
	annote = {2019 International Conference on Automated Software Engineering},
}

@article{li_adabot_2019,
	title = {Adabot: {Fault}-{Tolerant} {Java} {Decompiler}},
	url = {https://arxiv.org/pdf/1908.06748},
	doi = {https://doi.org/10.48550/arXiv.1908.06748},
	abstract = {Reverse Engineering(RE) has been a fundamental task in software engineering. However, most of the traditional Java reverse engineering tools are strictly rule defined, thus are not fault-tolerant, which pose serious problem when noise and interference were introduced into the system. In this paper, we view reverse engineering as a statistical machine translation task instead of rule-based task, and propose a fault-tolerant Java decompiler based on machine translation models. Our model is based on attention-based Neural Machine Translation (NMT) and Transformer architectures. First, we measure the translation quality on both the redundant and purified datasets. Next, we evaluate the fault-tolerance(anti-noise ability) of our framework on test sets with different unit error probability (UEP). In addition, we compare the suitability of different word segmentation algorithms for decompilation task. Experimental results demonstrate that our model is more robust and fault-tolerant compared to traditional Abstract Syntax Tree (AST) based decompilers. Specifically, in terms of BLEU-4 and Word Error Rate (WER), our performance has reached 94.50\% and 2.65\% on the redundant test set; 92.30\% and 3.48\% on the purified test set.},
	author = {Li, Zhiming and Wu, Qing and Qian, Kun},
	month = oct,
	year = {2019},
	annote = {8 pages},
}

@article{li_stan_2020-1,
	title = {{STAN}: {Towards} {Describing} {Bytecodes} of {Smart} {Contract}},
	url = {https://arxiv.org/pdf/2007.09696},
	doi = {https://doi.org/10.48550/arXiv.2007.09696},
	abstract = {More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1\% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users.},
	author = {Li, Xiaoqi and Chen, Ting and Luo, Xiapu and Zhang, Tao and Yu, Le and Xu, Zhou},
	month = jul,
	year = {2020},
	annote = {In Proc. of the 20th IEEE International Conference on Software Quality, Reliability and Security (QRS), 2020},
}

@article{li_neurodex_2025,
	title = {{NeuroDeX}: {Unlocking} {Diverse} {Support} in {Decompiling} {Deep} {Neural} {Network} {Executables}},
	url = {https://arxiv.org/pdf/2509.06402},
	doi = {https://doi.org/10.48550/arXiv.2509.06402},
	abstract = {On-device deep learning models have extensive real world demands. Deep learning compilers efficiently compile models into executables for deployment on edge devices, but these executables may face the threat of reverse engineering. Previous studies have attempted to decompile DNN executables, but they face challenges in handling compilation optimizations and analyzing quantized compiled models. In this paper, we present NeuroDeX to unlock diverse support in decompiling DNN executables. NeuroDeX leverages the semantic understanding capabilities of LLMs along with dynamic analysis to accurately and efficiently perform operator type recognition, operator attribute recovery and model reconstruction. NeuroDeX can recover DNN executables into high-level models towards compilation optimizations, different architectures and quantized compiled models. We conduct experiments on 96 DNN executables across 12 common DNN models. Extensive experimental results demonstrate that NeuroDeX can decompile non-quantized executables into nearly identical high-level models. NeuroDeX can recover functionally similar high-level models for quantized executables, achieving an average top-1 accuracy of 72\%. NeuroDeX offers a more comprehensive and effective solution compared to previous DNN executables decompilers.},
	author = {Li, Yilin and Meng, Guozhu and Sun, Mingyang and Wang, Yanzhong and Sun, Kun and Chang, Hailong and Li, Yuekang},
	month = jan,
	year = {2025},
}

@article{li_empirical_2025,
	title = {Empirical {Study} of {Code} {Large} {Language} {Models} for {Binary} {Security} {Patch} {Detection}},
	url = {https://arxiv.org/pdf/2509.06052},
	doi = {https://doi.org/10.48550/arXiv.2509.06052},
	abstract = {Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing {\textbackslash}textbf\{19,448\} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate {\textbackslash}textbf\{19\} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation.},
	author = {Li, Qingyuan and Li, Binchang and Gao, Cuiyun and Gao, Shuzheng and Li, Zongjie},
	month = sep,
	year = {2025},
}

@article{liang_semantics-recovering_2021,
	title = {Semantics-{Recovering} {Decompilation} through {Neural} {Machine} {Translation}},
	url = {https://arxiv.org/pdf/2112.15491},
	doi = {https://doi.org/10.48550/arXiv.2112.15491},
	abstract = {Decompilation transforms low-level program languages (PL) (e.g., binary code) into high-level PLs (e.g., C/C++). It has been widely used when analysts perform security analysis on software (systems) whose source code is unavailable, such as vulnerability search and malware analysis. However, current decompilation tools usually need lots of experts' efforts, even for years, to generate the rules for decompilation, which also requires long-term maintenance as the syntax of high-level PL or low-level PL changes. Also, an ideal decompiler should concisely generate high-level PL with similar functionality to the source low-level PL and semantic information (e.g., meaningful variable names), just like human-written code. Unfortunately, existing manually-defined rule-based decompilation techniques only functionally restore the low-level PL to a similar high-level PL and are still powerless to recover semantic information. In this paper, we propose a novel neural decompilation approach to translate low-level PL into accurate and user-friendly high-level PL, effectively improving its readability and understandability. Furthermore, we implement the proposed approaches called SEAM. Evaluations on four real-world applications show that SEAM has an average accuracy of 94.41\%, which is much better than prior neural machine translation (NMT) models. Finally, we evaluate the effectiveness of semantic information recovery through a questionnaire survey, and the average accuracy is 92.64\%, which is comparable or superior to the state-of-the-art compilers.},
	author = {Liang, Ruigang and Cao, Ying and Hu, Peiwei and He, Jinwen and Chen, Kai},
	month = feb,
	year = {2021},
}

@article{liu_proving_2021,
	title = {Proving {LTL} {Properties} of {Bitvector} {Programs} and {Decompiled} {Binaries} ({Extended})},
	url = {https://arxiv.org/pdf/2105.05159},
	doi = {https://doi.org/10.48550/arXiv.2105.05159},
	abstract = {There is increasing interest in applying verification tools to programs that have bitvector operations (eg., binaries). SMT solvers, which serve as a foundation for these tools, have thus increased support for bitvector reasoning through bit-blasting and linear arithmetic approximations. In this paper we show that similar linear arithmetic approximation of bitvector operations can be done at the source level through transformations. Specifically, we introduce new paths that over-approximate bitvector operations with linear conditions/constraints, increasing branching but allowing us to better exploit the well-developed integer reasoning and interpolation of verification tools. We show that, for reachability of bitvector programs, increased branching incurs negligible overhead yet, when combined with integer interpolation optimizations, enables more programs to be verified. We further show this exploitation of integer interpolation in the common case also enables competitive termination verification of bitvector programs and leads to the first effective technique for LTL verification of bitvector programs. Finally, we provide an in-depth case study of decompiled ("lifted") binary programs, which emulate X86 execution through frequent use of bitvector operations. We present a new tool DarkSea, the first tool capable of verifying reachability, termination, and LTL of lifted binaries.},
	author = {Liu, Cyrus, Yuandong and Pang, Chengbin and Dietsch, Daniel and Koskinen, Eric and Le, Ton-Chanh and Portokalidis, Georgios and Xu, Jun},
	month = aug,
	year = {2021},
	annote = {39 pages(including Appendix), 10 tables, 4 Postscript figures, accepted to APLAS 2021},
	annote = {39 pages(including Appendix), 10 tables, 4 Postscript figures, accepted to APLAS 2021},
}

@article{liu_decompiling_2022,
	title = {Decompiling x86 {Deep} {Neural} {Network} {Executables}},
	url = {https://arxiv.org/pdf/2210.01075},
	doi = {https://doi.org/10.48550/arXiv.2210.01075},
	abstract = {Due to their widespread use on heterogeneous hardware devices, deep learning (DL) models are compiled into executables by DL compilers to fully leverage low-level hardware primitives. This approach allows DL computations to be undertaken at low cost across a variety of computing platforms, including CPUs, GPUs, and various hardware accelerators. We present BTD (Bin to DNN), a decompiler for deep neural network (DNN) executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. BTD delivers a practical framework to process DNN executables compiled by different DL compilers and with full optimizations enabled on x86 platforms. It employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators. Our evaluation reveals that BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recovered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable. We show that BTD can boost two representative attacks, adversarial example generation and knowledge stealing, against DNN executables. We also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching.},
	author = {Liu, Zhibo and Yuan, Yuanyuan and Wang, Shuai and Xie, Xiaofei and Ma, Lei},
	month = oct,
	year = {2022},
	annote = {The extended version of a paper to appear in the Proceedings of the 32nd USENIX Security Symposium, 2023, (USENIX Security '23), 25 pages},
}

@article{liu_codeinverter_2025,
	title = {The {CodeInverter} {Suite}: {Control}-{Flow} and {Data}-{Mapping} {Augmented} {Binary} {Decompilation} with {LLMs}},
	url = {https://arxiv.org/pdf/2503.07215},
	doi = {https://doi.org/10.48550/arXiv.2503.07215},
	abstract = {Binary decompilation plays a vital role in various cybersecurity and software engineering tasks. Recently, end-to-end decompilation methods powered by large language models (LLMs) have garnered significant attention due to their ability to generate highly readable source code with minimal human intervention. However, existing LLM-based approaches face several critical challenges, including limited capability in reconstructing code structure and logic, low accuracy in data recovery, concerns over data security and privacy, and high computational resource requirements. To address these issues, we develop the CodeInverter Suite, making three contributions: (1) the CodeInverter Workflow (CIW) is a novel prompt engineering workflow that incorporates control flow graphs (CFG) and explicit data mappings to improve LLM-based decompilation. (2) Using CIW on well-known source code datasets, we curate the CodeInverter Dataset (CID), a domain-specific dataset containing 8.69 million samples that contains CFGs and data mapping tables. (3) We train the CoderInverter Models (CIMs) on CID, generating two lightweight LLMs (with 1.3B and 6.7B parameters) intended for efficient inference in privacy-sensitive or resource-constrained environments. Extensive experiments on two benchmarks demonstrate that the CIW substantially enhances the performance of various LLMs across multiple metrics. Our CIM-6.7B can achieve state-of-the-art decompilation performance, outperforming existing LLMs even with over 100x more parameters in decompilation tasks, an average improvement of 11.03\% in re-executability, 6.27\% in edit similarity.},
	author = {Liu, Peipei and Sun, Jian and Sun, Rongkang and Chen, Li and Yan, Zhaoteng and Zhang, Peizheng and Sun, Dapeng and Wang, Dawei and Zhang, Xiaoling and Li, Dan},
	month = may,
	year = {2025},
}

@article{manuel_enhancing_2024,
	title = {Enhancing {Reverse} {Engineering}: {Investigating} and {Benchmarking} {Large} {Language} {Models} for {Vulnerability} {Analysis} in {Decompiled} {Binaries}},
	url = {https://arxiv.org/pdf/2411.04981},
	doi = {https://doi.org/10.48550/arXiv.2411.04981},
	abstract = {Security experts reverse engineer (decompile) binary code to identify critical security vulnerabilities. The limited access to source code in vital systems - such as firmware, drivers, and proprietary software used in Critical Infrastructures (CI) - makes this analysis even more crucial on the binary level. Even with available source code, a semantic gap persists after compilation between the source and the binary code executed by the processor. This gap may hinder the detection of vulnerabilities in source code. That being said, current research on Large Language Models (LLMs) overlooks the significance of decompiled binaries in this area by focusing solely on source code. In this work, we are the first to empirically uncover the substantial semantic limitations of state-of-the-art LLMs when it comes to analyzing vulnerabilities in decompiled binaries, largely due to the absence of relevant datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary code vulnerability dataset. Our dataset is multi-architecture and multi-optimization, focusing on C/C++ due to their wide usage in CI and association with numerous vulnerabilities. Specifically, we curate 150,872 samples of vulnerable and non-vulnerable decompiled binary code for the task of (i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv) recovering function names in the domain of decompiled binaries. Subsequently, we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance increase of 19\%, 24\%, and 21\% in the capabilities of CodeLlama, Llama3, and CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally, using DeBinVul, we report a high performance of 80-90\% on the vulnerability classification task. Furthermore, we report improved performance in function name recovery and vulnerability description tasks.},
	author = {Manuel, Dylan and Islam, Tanveer, Nafis and Khoury, Joseph and Nunez, Ana and Bou-Harb, Elias and Najafirad, Peyman},
	month = jan,
	year = {2024},
}

@article{manuel_codablellm_2025,
	title = {{CodableLLM}: {Automating} {Decompiled} and {Source} {Code} {Mapping} for {LLM} {Dataset} {Generation}},
	url = {https://arxiv.org/pdf/2507.22066},
	doi = {https://doi.org/10.48550/arXiv.2507.22066},
	abstract = {The generation of large, high-quality datasets for code understanding and generation remains a significant challenge, particularly when aligning decompiled binaries with their original source code. To address this, we present CodableLLM, a Python framework designed to automate the creation and curation of datasets by mapping decompiled functions to their corresponding source functions. This process enhances the alignment between decompiled and source code representations, facilitating the development of large language models (LLMs) capable of understanding and generating code across multiple abstraction levels. CodableLLM supports multiple programming languages and integrates with existing decompilers and parsers to streamline dataset generation. This paper presents the design and implementation of CodableLLM, evaluates its performance in dataset creation, and compares it to existing tools in the field. The results demonstrate that CodableLLM offers a robust and efficient solution for generating datasets tailored for code-focused LLMS.},
	author = {Manuel, Dylan and Rad, Paul},
	month = jul,
	year = {2025},
}

@article{mihajlenko_method_2021,
	title = {A method for decompilation of {AMD} {GCN} kernels to {OpenCL}},
	url = {https://arxiv.org/pdf/2107.07809},
	doi = {https://doi.org/10.48550/arXiv.2107.07809},
	abstract = {Introduction: Decompilers are useful tools for software analysis and support in the absence of source code. They are available for many hardware architectures and programming languages. However, none of the existing decompilers support modern AMD GPU architectures such as AMD GCN and RDNA. Purpose: We aim at developing the first assembly decompiler tool for a modern AMD GPU architecture that generates code in the OpenCL language, which is widely used for programming GPGPUs. Results: We developed the algorithms for the following operations: preprocessing assembly code, searching data accesses, extracting system values, decompiling arithmetic operations and recovering data types. We also developed templates for decompilation of branching operations. Practical relevance: We implemented the presented algorithms in Python as a tool called OpenCLDecompiler, which supports a large subset of AMD GCN instructions. This tool automatically converts disassembled GPGPU code into the equivalent OpenCL code, which reduces the effort required to analyze assembly code.},
	author = {Mihajlenko, I., K. and Lukin, A., M. and Stankevich, S., A.},
	month = jul,
	year = {2021},
	annote = {10 pages, 5 figures},
}

@article{nandi_synthesizing_2020-1,
	title = {Synthesizing {Structured} {CAD} {Models} with {Equality} {Saturation} and {Inverse} {Transformations}},
	url = {https://arxiv.org/pdf/1909.12252},
	doi = {https://doi.org/10.48550/arXiv.1909.12252},
	abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
	author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, R., James and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
	month = apr,
	year = {2020},
	annote = {14 pages},
}

@article{palmarini_bayesian_2024,
	title = {Bayesian {Program} {Learning} by {Decompiling} {Amortized} {Knowledge}},
	url = {https://arxiv.org/pdf/2306.07856},
	doi = {https://doi.org/10.48550/arXiv.2306.07856},
	abstract = {DreamCoder is an inductive program synthesis system that, whilst solving problems, learns to simplify search in an iterative wake-sleep procedure. The cost of search is amortized by training a neural search policy, reducing search breadth and effectively "compiling" useful information to compose program solutions across tasks. Additionally, a library of program components is learnt to compress and express discovered solutions in fewer components, reducing search depth. We present a novel approach for library learning that directly leverages the neural search policy, effectively "decompiling" its amortized knowledge to extract relevant program components. This provides stronger amortized inference: the amortized knowledge learnt to reduce search breadth is now also used to reduce search depth. We integrate our approach with DreamCoder and demonstrate faster domain proficiency with improved generalization on a range of domains, particularly when fewer example solutions are available.},
	author = {Palmarini, B., Alessandro and Lucas, G., Christopher and Siddharth, N.},
	month = may,
	year = {2024},
}

@article{pearce_pop_2022,
	title = {Pop {Quiz}! {Can} a {Large} {Language} {Model} {Help} {With} {Reverse} {Engineering}?},
	url = {https://arxiv.org/pdf/2202.01142},
	doi = {https://doi.org/10.48550/arXiv.2202.01142},
	abstract = {Large language models (such as OpenAI's Codex) have demonstrated impressive zero-shot multi-task capabilities in the software domain, including code explanation. In this work, we examine if this ability can be used to help with reverse engineering. Specifically, we investigate prompting Codex to identify the purpose, capabilities, and important variable names or values from code, even when the code is produced through decompilation. Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model. We present an extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks: of the 136,260 questions we posed, it answered 72,754 correctly. A key takeaway is that while promising, LLMs are not yet ready for zero-shot reverse engineering.},
	author = {Pearce, Hammond and Tan, Benjamin and Krishnamurthy, Prashanth and Khorrami, Farshad and Karri, Ramesh and Dolan-Gavitt, Brendan},
	month = feb,
	year = {2022},
	annote = {18 pages, 19 figures. Linked dataset: https://doi.org/10.5281/zenodo.5949075},
}

@article{pochelu_jaxdecompiler_2024,
	title = {{JaxDecompiler}: {Redefining} {Gradient}-{Informed} {Software} {Design}},
	url = {https://arxiv.org/pdf/2403.10571},
	doi = {https://doi.org/10.48550/arXiv.2403.10571},
	abstract = {Among numerical libraries capable of computing gradient descent optimization, JAX stands out by offering more features, accelerated by an intermediate representation known as Jaxpr language. However, editing the Jaxpr code is not directly possible. This article introduces JaxDecompiler, a tool that transforms any JAX function into an editable Python code, especially useful for editing the JAX function generated by the gradient function. JaxDecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by JAX. We highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.},
	author = {Pochelu, Pierrick},
	month = mar,
	year = {2024},
}

@article{pordanesh_exploring_2024,
	title = {Exploring the {Efficacy} of {Large} {Language} {Models} ({GPT}-4) in {Binary} {Reverse} {Engineering}},
	url = {https://arxiv.org/pdf/2406.06637},
	doi = {https://doi.org/10.48550/arXiv.2406.06637},
	abstract = {This study investigates the capabilities of Large Language Models (LLMs), specifically GPT-4, in the context of Binary Reverse Engineering (RE). Employing a structured experimental approach, we analyzed the LLM's performance in interpreting and explaining human-written and decompiled codes. The research encompassed two phases: the first on basic code interpretation and the second on more complex malware analysis. Key findings indicate LLMs' proficiency in general code understanding, with varying effectiveness in detailed technical and security analyses. The study underscores the potential and current limitations of LLMs in reverse engineering, revealing crucial insights for future applications and improvements. Also, we examined our experimental methodologies, such as methods of evaluation and data constraints, which provided us with a technical vision for any future research activity in this field.},
	author = {Pordanesh, Saman and Tan, Benjamin},
	month = jun,
	year = {2024},
}

@article{qin_demystifying_2024,
	title = {Demystifying and {Assessing} {Code} {Understandability} in {Java} {Decompilation}},
	url = {https://arxiv.org/pdf/2409.20343},
	doi = {https://doi.org/10.48550/arXiv.2409.20343},
	abstract = {Decompilation, the process of converting machine-level code into readable source code, plays a critical role in reverse engineering. Given that the main purpose of decompilation is to facilitate code comprehension in scenarios where the source code is unavailable, the understandability of decompiled code is of great importance. In this paper, we propose the first empirical study on the understandability of Java decompiled code and obtained the following findings: (1) Understandability of Java decompilation is considered as important as its correctness, and decompilation understandability issues are even more commonly encountered than decompilation failures. (2) A notable percentage of code snippets decompiled by Java decompilers exhibit significantly lower or higher levels of understandability in comparison to their original source code. (3) Unfortunately, Cognitive Complexity demonstrates relatively acceptable precision while low recall in recognizing these code snippets exhibiting diverse understandability during decompilation. (4) Even worse, perplexity demonstrates lower levels of precision and recall in recognizing such code snippets. Inspired by the four findings, we further proposed six code patterns and the first metric for the assessment of decompiled code understandability. This metric was extended from Cognitive Complexity, with six more rules harvested from an exhaustive manual analysis into 1287 pairs of source code snippets and corresponding decompiled code. This metric was also validated using the original and updated dataset, yielding an impressive macro F1-score of 0.88 on the original dataset, and 0.86 on the test set.},
	author = {Qin, Ruixin and Xiong, Yifan and Lu, Yifei and Pan, Minxue},
	month = sep,
	year = {2024},
	annote = {18 pages, 16 figures},
}

@article{rao_register_2024,
	title = {Register {Aggregation} for {Hardware} {Decompilation}},
	url = {https://arxiv.org/pdf/2409.03119},
	doi = {https://doi.org/10.48550/arXiv.2409.03119},
	abstract = {Hardware decompilation reverses logic synthesis, converting a gate-level digital electronic design, or netlist, back up to hardware description language (HDL) code. Existing techniques decompile data-oriented features in netlists, like loops and modules, but struggle with sequential logic. In particular, they cannot decompile memory elements, which pose difficulty due to their deconstruction into individual bits and the feedback loops they form in the netlist. Recovering multi-bit registers and memory blocks from netlists would expand the applications of hardware decompilation, notably towards retargeting technologies (e.g. FPGAs to ASICs) and decompiling processor memories. We devise a method for register aggregation, to identify relationships between the data flip-flops in a netlist and group them into registers and memory blocks, resulting in HDL code that instantiates these memory elements. We aggregate flip-flops by identifying common enable pins, and derive the bit-order of the resulting registers using functional dependencies. This scales similarly to memory blocks, where we repeat the algorithm in the second dimension with special attention to the read, write, and address ports of each memory block. We evaluate our technique over a dataset of 13 gate-level netlists, comprising circuits from binary multipliers to CPUs, and we compare the quantity and widths of recovered registers and memory blocks with the original source code. The technique successfully recovers memory elements in all of the tested circuits, even aggregating beyond the source code expectation. In 10 / 13 circuits, all source code memory elements are accounted for, and we are able to compact up to 2048 disjoint bits into a single memory block.},
	author = {Rao, Varun and Sisco, D., Zachary},
	month = sep,
	year = {2024},
	annote = {6 pages, 6 figures},
}

@article{reis_tezla_2020,
	title = {Tezla, an {Intermediate} {Representation} for {Static} {Analysis} of {Michelson} {Smart} {Contracts}},
	url = {https://arxiv.org/pdf/2005.11839},
	doi = {https://doi.org/10.48550/arXiv.2005.11839},
	abstract = {This paper introduces Tezla, an intermediate representation of Michelson smart contracts that eases the design of static smart contract analysers. This intermediate representation uses a store and preserves the semantics, ow and resource usage of the original smart contract. This enables properties like gas consumption to be statically verified. We provide an automated decompiler of Michelson smart contracts to Tezla. In order to support our claim about the adequacy of Tezla, we develop a static analyser that takes advantage of the Tezla representation of Michelson smart contracts to prove simple but non-trivial properties.},
	author = {Reis, Santos, João and Crocker, Paul and Sousa, de, Melo, Simão},
	month = may,
	year = {2020},
}

@article{reiter_automatically_2023,
	title = {Automatically {Mitigating} {Vulnerabilities} in {Binary} {Programs} via {Partially} {Recompilable} {Decompilation}},
	url = {https://arxiv.org/pdf/2202.12336},
	doi = {https://doi.org/10.48550/arXiv.2202.12336},
	abstract = {Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD). PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically work on an entire binary, our approach succeeds because it is limited to a few functions, like those identified by our binary fault localization. We evaluate these assumptions and find that, without any grammar or compilation restrictions, 70-89\% of individual functions are successfully decompiled and recompiled with sufficient type recovery. In comparison, only 1.7\% of the full C-binaries succeed. When decompilation succeeds, PRD produces test-equivalent binaries 92.9\% of the time. In addition, we evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, performs as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with these same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, our evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries.},
	author = {Reiter, Pemma and Tay, Jun, Hui and Weimer, Westley and Doupé, Adam and Wang, Ruoyu and Forrest, Stephanie},
	month = jun,
	year = {2023},
}

@article{ringer_proof_2021-1,
	title = {Proof {Repair} across {Type} {Equivalences}},
	url = {https://arxiv.org/pdf/2010.00774},
	doi = {https://doi.org/10.48550/arXiv.2010.00774},
	abstract = {We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in PUMPKIN Pi, an extension to the PUMPKIN PATCH Coq plugin suite for proof repair. We demonstrate PUMPKIN Pi's flexibility on eight case studies, including supporting a benchmark from a user study, easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.},
	author = {Ringer, Talia and Porter, RanDair and Yazdani, Nathaniel and Leo, John and Grossman, Dan},
	month = may,
	year = {2021},
	annote = {Tool repository with code guide: https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/GUIDE.md},
}

@article{saul_is_2024,
	title = {Is {Function} {Similarity} {Over}-{Engineered}? {Building} a {Benchmark}},
	url = {https://arxiv.org/pdf/2410.22677},
	doi = {https://doi.org/10.48550/arXiv.2410.22677},
	abstract = {Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSE-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple basline, one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing, is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.},
	author = {Saul, Rebecca and Liu, Chang and Fleischmann, Noah and Zak, Richard and Micinski, Kristopher and Raff, Edward and Holt, James},
	month = oct,
	year = {2024},
	annote = {To appear in the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks},
}

@article{shang_binmetric_2025,
	title = {{BinMetric}: {A} {Comprehensive} {Binary} {Analysis} {Benchmark} for {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2505.07360},
	doi = {https://doi.org/10.48550/arXiv.2505.07360},
	abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
	author = {Shang, Xiuwei and Chen, Guoqiang and Cheng, Shaoyin and Wu, Benlong and Hu, Li and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
	month = may,
	year = {2025},
	annote = {23 pages, 5 figures, to be published in IJCAI 2025},
	annote = {23 pages, 5 figures, to be published in IJCAI 2025},
}

@article{she_wadec_2024-2,
	title = {{WaDec}: {Decompiling} {WebAssembly} {Using} {Large} {Language} {Model}},
	url = {https://arxiv.org/pdf/2406.11346},
	doi = {https://doi.org/10.48550/arXiv.2406.11346},
	abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art's 116.94\%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%.},
	author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
	month = sep,
	year = {2024},
	annote = {This paper was accepted by ASE 2024},
}

@article{shokri_construct_2023,
	title = {{CONSTRUCT}: {A} {Program} {Synthesis} {Approach} for {Reconstructing} {Control} {Algorithms} from {Embedded} {System} {Binaries} in {Cyber}-{Physical} {Systems}},
	url = {https://arxiv.org/pdf/2308.00250},
	doi = {https://doi.org/10.48550/arXiv.2308.00250},
	abstract = {We introduce a novel approach to automatically synthesize a mathematical representation of the control algorithms implemented in industrial cyber-physical systems (CPS), given the embedded system binary. The output model can be used by subject matter experts to assess the system's compliance with the expected behavior and for a variety of forensic applications. Our approach first performs static analysis on decompiled binary files of the controller to create a sketch of the mathematical representation. Then, we perform an evolutionary-based search to find the correct semantic for the created representation, i.e., the control law. We demonstrate the effectiveness of the introduced approach in practice via three case studies conducted on two real-life industrial CPS.},
	author = {Shokri, Ali and Perez, Alexandre and Chowdhury, Souma and Zeng, Chen and Kaloor, Gerald and Matei, Ion and Schneider, Peter-Patel and Gunasekaran, Akshith and Rane, Shantanu},
	month = jul,
	year = {2023},
}

@article{slawinski_applications_2019-1,
	title = {Applications of {Graph} {Integration} to {Function} {Comparison} and {Malware} {Classification}},
	url = {https://arxiv.org/pdf/1810.04789},
	doi = {https://doi.org/10.48550/arXiv.1810.04789},
	abstract = {We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled .NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms.},
	author = {Slawinski, A., Michael and Wortman, Andy},
	month = jan,
	year = {2019},
}

@article{smith_there_2024,
	title = {There and {Back} {Again}: {A} {Netlist}'s {Tale} with {Much} {Egraphin}'},
	url = {https://arxiv.org/pdf/2404.00786},
	doi = {https://doi.org/10.48550/arXiv.2404.00786},
	abstract = {EDA toolchains are notoriously unpredictable, incomplete, and error-prone; the generally-accepted remedy has been to re-imagine EDA tasks as compilation problems. However, any compiler framework we apply must be prepared to handle the wide range of EDA tasks, including not only compilation tasks like technology mapping and optimization (the "there"\vphantom{\{}\} in our title), but also decompilation tasks like loop rerolling (the "back again"). In this paper, we advocate for equality saturation – a term rewriting framework – as the framework of choice when building hardware toolchains. Through a series of case studies, we show how the needs of EDA tasks line up conspicuously well with the features equality saturation provides.},
	author = {Smith, Henry, Gus and Sisco, D., Zachary and Techaumnuaiwit, Thanawat and Xia, Jingtao and Canumalla, Vishal and Cheung, Andrew and Tatlock, Zachary and Nandi, Chandrakana and Balkind, Jonathan},
	month = mar,
	year = {2024},
}

@article{stuglik_j-parallelio_2023,
	title = {J-{Parallelio} – automatic parallelization framework for {Java} virtual machine code},
	url = {https://arxiv.org/pdf/2303.08746},
	doi = {https://doi.org/10.48550/arXiv.2303.08746},
	abstract = {Manual translation of the algorithms from sequential version to its parallel counterpart is time consuming and can be done only with the specific knowledge of hardware accelerator architecture, parallel programming or programming environment. The automation of this process makes porting the code much easier and faster. The key aspect in this case is how efficient the generated parallel code will be. The paper describes J-Parallelio, the framework for automatic analysis of the bytecode source codes and its parallelisation on multicore processors. The process consists of a few steps. First step is a process of decompilation of JVM and its translation to internal abstract syntax tree, the dependency extraction and memory analysis is performed. Finally, the mapping process is performed which consists of a set of rules responsible for translating the input virtual machine source code to its parallel version. The main novelty is that it can deal with pure Java virtual machine and can generate parallel code for multicore processors. This makes the system portable and it can work with different languages based on JVM after some small modifications. The efficiency of automatically translated source codes were compared with their manually written counterparts on chosen benchmarks.},
	author = {Stuglik, Krzysztof and Listkiewicz, Piotr and Kulczyk, Mateusz and Pietron, Marcin},
	month = feb,
	year = {2023},
}

@article{szafraniec_code_2023,
	title = {Code {Translation} with {Compiler} {Representations}},
	url = {https://arxiv.org/pdf/2207.03578},
	doi = {https://doi.org/10.48550/arXiv.2207.03578},
	abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11\% on average, and up to 79\% for the Java -{\textgreater} Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.},
	author = {Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh and Charton, Francois and Labatut, Patrick and Synnaeve, Gabriel},
	month = apr,
	year = {2023},
	annote = {9 pages},
}

@article{tan_llm4decompile_2024,
	title = {{LLM4Decompile}: {Decompiling} {Binary} {Code} with {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2403.05286},
	doi = {https://doi.org/10.48550/arXiv.2403.05286},
	abstract = {Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100\% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2\% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results. Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile},
	author = {Tan, Hanzhuo and Luo, Qi and Li, Jing and Zhang, Yuqun},
	month = oct,
	year = {2024},
}

@article{tan_sk2decompile_2025,
	title = {{SK2Decompile}: {LLM}-based {Two}-{Phase} {Binary} {Decompilation} from {Skeleton} to {Skin}},
	url = {https://arxiv.org/pdf/2509.22114},
	doi = {https://doi.org/10.48550/arXiv.2509.22114},
	abstract = {Large Language Models (LLMs) have emerged as a promising approach for binary decompilation. However, the existing LLM-based decompilers still are somewhat limited in effectively presenting a program's source-level structure with its original identifiers. To mitigate this, we introduce SK2Decompile, a novel two-phase approach to decompile from the skeleton (semantic structure) to the skin (identifier) of programs. Specifically, we first apply a Structure Recovery model to translate a program's binary code to an Intermediate Representation (IR) as deriving the program's "skeleton", i.e., preserving control flow and data structures while obfuscating all identifiers with generic placeholders. We also apply reinforcement learning to reward the model for producing program structures that adhere to the syntactic and semantic rules expected by compilers. Second, we apply an Identifier Naming model to produce meaningful identifiers which reflect actual program semantics as deriving the program's "skin". We train the Identifier Naming model with a separate reinforcement learning objective that rewards the semantic similarity between its predictions and the reference code. Such a two-phase decompilation process facilitates advancing the correctness and readability of decompilation independently. Our evaluations indicate that SK2Decompile, significantly outperforms the SOTA baselines, achieving 21.6\% average re-executability rate gain over GPT-5-mini on the HumanEval dataset and 29.4\% average R2I improvement over Idioms on the GitHub2025 benchmark.},
	author = {Tan, Hanzhuo and Li, Weihao and Tian, Xiaolong and Wang, Siyi and Liu, Jiaming and Li, Jing and Zhang, Yuqun},
	month = sep,
	year = {2025},
}

@article{tan_decompile-bench_2025,
	title = {Decompile-{Bench}: {Million}-{Scale} {Binary}-{Source} {Function} {Pairs} for {Real}-{World} {Binary} {Decompilation}},
	url = {https://arxiv.org/pdf/2505.12668},
	doi = {https://doi.org/10.48550/arXiv.2505.12668},
	abstract = {Recent advances in LLM-based decompilers have been shown effective to convert low-level binaries into human-readable source code. However, there still lacks a comprehensive benchmark that provides large-scale binary-source function pairs, which is critical for advancing the LLM decompilation technology. Creating accurate binary-source mappings incurs severe issues caused by complex compilation settings and widespread function inlining that obscure the correspondence between binaries and their original source code. Previous efforts have either relied on used contest-style benchmarks, synthetic binary-source mappings that diverge significantly from the mappings in real world, or partially matched binaries with only code lines or variable names, compromising the effectiveness of analyzing the binary functionality. To alleviate these issues, we introduce Decompile-Bench, the first open-source dataset comprising two million binary-source function pairs condensed from 100 million collected function pairs, i.e., 450GB of binaries compiled from permissively licensed GitHub projects. For the evaluation purposes, we also developed a benchmark Decompile-Bench-Eval including manually crafted binaries from the well-established HumanEval and MBPP, alongside the compiled GitHub repositories released after 2025 to mitigate data leakage issues. We further explore commonly-used evaluation metrics to provide a thorough assessment of the studied LLM decompilers and find that fine-tuning with Decompile-Bench causes a 20\% improvement over previous benchmarks in terms of the re-executability rate. Our code and data has been released in HuggingFace and Github. https://github.com/albertan017/LLM4Decompile},
	author = {Tan, Hanzhuo and Tian, Xiaolong and Qi, Hanrui and Liu, Jiaming and Gao, Zuchen and Wang, Siyi and Luo, Qi and Li, Jing and Zhang, Yuqun},
	month = oct,
	year = {2025},
}

@article{thurnherr_neural_2024,
	title = {Neural {Decompiling} of {Tracr} {Transformers}},
	url = {https://arxiv.org/pdf/2410.00061},
	doi = {https://doi.org/10.48550/arXiv.2410.00061},
	abstract = {Recently, the transformer architecture has enabled substantial progress in many areas of pattern recognition and machine learning. However, as with other neural network models, there is currently no general method available to explain their inner workings. The present paper represents a first step towards this direction. We utilize {\textbackslash}textit\{Transformer Compiler for RASP\} (Tracr) to generate a large dataset of pairs of transformer weights and corresponding RASP programs. Based on this dataset, we then build and train a model, with the aim of recovering the RASP code from the compiled model. We demonstrate that the simple form of Tracr compiled transformer weights is interpretable for such a decompiler model. In an empirical evaluation, our model achieves exact reproductions on more than 30\% of the test objects, while the remaining 70\% can generally be reproduced with only few errors. Additionally, more than 70\% of the programs, produced by our model, are functionally equivalent to the ground truth, and therefore a valid decompilation of the Tracr compiled transformer weights.},
	author = {Thurnherr, Hannes and Riesen, Kaspar},
	month = sep,
	year = {2024},
}

@article{verbeek_formally_2025-2,
	title = {Formally {Verified} {Binary}-level {Pointer} {Analysis}},
	url = {https://arxiv.org/pdf/2501.17766},
	doi = {https://doi.org/10.48550/arXiv.2501.17766},
	abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows "meaningful" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
	author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
	month = jan,
	year = {2025},
}

@article{wan_bridging_2025,
	title = {Bridging {Vision}, {Language}, and {Mathematics}: {Pictographic} {Character} {Reconstruction} with {Bézier} {Curves}},
	url = {https://arxiv.org/pdf/2511.00076},
	doi = {https://doi.org/10.48550/arXiv.2511.00076},
	abstract = {While Vision-language Models (VLMs) have demonstrated strong semantic capabilities, their ability to interpret the underlying geometric structure of visual information is less explored. Pictographic characters, which combine visual form with symbolic structure, provide an ideal test case for this capability. We formulate this visual recognition challenge in the mathematical domain, where each character is represented by an executable program of geometric primitives. This is framed as a program synthesis task, training a VLM to decompile raster images into programs composed of Bézier curves. Our model, acting as a "visual decompiler", demonstrates performance superior to strong zero-shot baselines, including GPT-4o. The most significant finding is that when trained solely on modern Chinese characters, the model is able to reconstruct ancient Oracle Bone Script in a zero-shot context. This generalization provides strong evidence that the model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to a more structured form of visual understanding.},
	author = {Wan, Zihao and Xu, Lin, Tong, Pau and Luo, Fuwen and Wang, Ziyue and Li, Peng and Liu, Yang},
	month = oct,
	year = {2025},
}

@article{wang_graph_2023,
	title = {Graph {Neural} {Networks} {Enhanced} {Smart} {Contract} {Vulnerability} {Detection} of {Educational} {Blockchain}},
	url = {https://arxiv.org/pdf/2303.04477},
	doi = {https://doi.org/10.48550/arXiv.2303.04477},
	abstract = {With the development of blockchain technology, more and more attention has been paid to the intersection of blockchain and education, and various educational evaluation systems and E-learning systems are developed based on blockchain technology. Among them, Ethereum smart contract is favored by developers for its “event-triggered" mechanism for building education intelligent trading systems and intelligent learning platforms. However, due to the immutability of blockchain, published smart contracts cannot be modified, so problematic contracts cannot be fixed by modifying the code in the educational blockchain. In recent years, security incidents due to smart contract vulnerabilities have caused huge property losses, so the detection of smart contract vulnerabilities in educational blockchain has become a great challenge. To solve this problem, this paper proposes a graph neural network (GNN) based vulnerability detection for smart contracts in educational blockchains. Firstly, the bytecodes are decompiled to get the opcode. Secondly, the basic blocks are divided, and the edges between the basic blocks according to the opcode execution logic are added. Then, the control flow graphs (CFG) are built. Finally, we designed a GNN-based model for vulnerability detection. The experimental results show that the proposed method is effective for the vulnerability detection of smart contracts. Compared with the traditional approaches, it can get good results with fewer layers of the GCN model, which shows that the contract bytecode and GCN model are efficient in vulnerability detection.},
	author = {Wang, Zhifeng and Wu, Wanxuan and Zeng, Chunyan and Yao, Jialong and Yang, Yang and Xu, Hongmin},
	month = mar,
	year = {2023},
	annote = {8 pages, 8 figures},
}

@article{wang_context-guided_2025,
	title = {Context-{Guided} {Decompilation}: {A} {Step} {Towards} {Re}-executability},
	url = {https://arxiv.org/pdf/2511.01763},
	doi = {https://doi.org/10.48550/arXiv.2511.01763},
	abstract = {Binary decompilation plays an important role in software security analysis, reverse engineering, and malware understanding when source code is unavailable. However, existing decompilation techniques often fail to produce source code that can be successfully recompiled and re-executed, particularly for optimized binaries. Recent advances in large language models (LLMs) have enabled neural approaches to decompilation, but the generated code is typically only semantically plausible rather than truly executable, limiting their practical reliability. These shortcomings arise from compiler optimizations and the loss of semantic cues in compiled code, which LLMs struggle to recover without contextual guidance. To address this challenge, we propose ICL4Decomp, a hybrid decompilation framework that leverages in-context learning (ICL) to guide LLMs toward generating re-executable source code. We evaluate our method across multiple datasets, optimization levels, and compilers, demonstrating around 40\% improvement in re-executability over state-of-the-art decompilation methods while maintaining robustness.},
	author = {Wang, Xiaohan and Hu, Yuxin and Leach, Kevin},
	month = jan,
	year = {2025},
}

@article{wang_salt4decompile_2025,
	title = {{SALT4Decompile}: {Inferring} {Source}-level {Abstract} {Logic} {Tree} for {LLM}-{Based} {Binary} {Decompilation}},
	url = {https://arxiv.org/pdf/2509.14646},
	doi = {https://doi.org/10.48550/arXiv.2509.14646},
	abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose {\textbackslash}saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of {\textbackslash}saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, {\textbackslash}saltm constructs a Source-level Abstract Logic Tree ({\textbackslash}salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed {\textbackslash}salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare {\textbackslash}saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that {\textbackslash}saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
	author = {Wang, Yongpan and Xu, Xin and Zhu, Xiaojie and Gu, Xiaodong and Shen, Beijun},
	month = sep,
	year = {2025},
	annote = {13 pages, 7 figures},
	annote = {13 pages, 7 figures},
}

@article{wang_tracellm_2025,
	title = {{TraceLLM}: {Security} {Diagnosis} {Through} {Traces} and {Smart} {Contracts} in {Ethereum}},
	url = {https://arxiv.org/pdf/2509.03037},
	doi = {https://doi.org/10.48550/arXiv.2509.03037},
	abstract = {Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet comprehensive security analysis remains difficult due to unverified code, proxy-based architectures, and the reliance on manual inspection of complex execution traces. Existing approaches fall into two main categories: anomaly transaction detection, which flags suspicious transactions but offers limited insight into specific attack strategies hidden in execution traces inside transactions, and code vulnerability detection, which cannot analyze unverified contracts and struggles to show how identified flaws are exploited in real incidents. As a result, analysts must still manually align transaction traces with contract code to reconstruct attack scenarios and conduct forensics. To address this gap, TraceLLM is proposed as a framework that leverages LLMs to integrate execution trace-level detection with decompiled contract code. We introduce a new anomaly execution path identification algorithm and an LLM-refined decompile tool to identify vulnerable functions and provide explicit attack paths to LLM. TraceLLM establishes the first benchmark for joint trace and contract code-driven security analysis. For comparison, proxy baselines are created by jointly transmitting the results of three representative code analysis along with raw traces to LLM. TraceLLM identifies attacker and victim addresses with 85.19\% precision and produces automated reports with 70.37\% factual precision across 27 cases with ground truth expert reports, achieving 25.93\% higher accuracy than the best baseline. Moreover, across 148 real-world Ethereum incidents, TraceLLM automatically generates reports with 66.22\% expert-verified accuracy, demonstrating strong generalizability.},
	author = {Wang, Shuzheng and Huang, Yue and Xu, Zhuoer and Huang, Yuming and Tang, Jing},
	month = sep,
	year = {2025},
}

@article{wong_refining_2023,
	title = {Refining {Decompiled} {C} {Code} with {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2310.06530},
	doi = {https://doi.org/10.48550/arXiv.2310.06530},
	abstract = {A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly. This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable. We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler – IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75\% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions.},
	author = {Wong, Kin, Wai and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
	month = jan,
	year = {2023},
}

@article{wu_exploring_2023,
	title = {Exploring the {Limits} of {ChatGPT} in {Software} {Security} {Applications}},
	url = {https://arxiv.org/pdf/2312.05275},
	doi = {https://doi.org/10.48550/arXiv.2312.05275},
	abstract = {Large language models (LLMs) have undergone rapid evolution and achieved remarkable results in recent times. OpenAI's ChatGPT, backed by GPT-3.5 or GPT-4, has gained instant popularity due to its strong capability across a wide range of tasks, including natural language tasks, coding, mathematics, and engaging conversations. However, the impacts and limits of such LLMs in system security domain are less explored. In this paper, we delve into the limits of LLMs (i.e., ChatGPT) in seven software security applications including vulnerability detection/repair, debugging, debloating, decompilation, patching, root cause analysis, symbolic execution, and fuzzing. Our exploration reveals that ChatGPT not only excels at generating code, which is the conventional application of language models, but also demonstrates strong capability in understanding user-provided commands in natural languages, reasoning about control and data flows within programs, generating complex data structures, and even decompiling assembly code. Notably, GPT-4 showcases significant improvements over GPT-3.5 in most security tasks. Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.},
	author = {Wu, Fangzhou and Zhang, Qingzhao and Bajaj, Priya, Ati and Bao, Tiffany and Zhang, Ning and Wang, "Fish", Ruoyu and Xiao, Chaowei},
	month = feb,
	year = {2023},
}

@article{wu_is_2024,
	title = {Is {This} the {Same} {Code}? {A} {Comprehensive} {Study} of {Decompilation} {Techniques} for {WebAssembly} {Binaries}},
	url = {https://arxiv.org/pdf/2411.02278},
	doi = {https://doi.org/10.48550/arXiv.2411.02278},
	abstract = {WebAssembly is a low-level bytecode language designed for client-side execution in web browsers. The need for decompilation techniques that recover high-level source code from WASM binaries has grown as WASM continues to gain widespread adoption and its security concerns. However little research has been done to assess the quality of decompiled code from WASM. This paper aims to fill this gap by conducting a comprehensive comparative analysis between decompiled C code from WASM binaries and state-of-the-art native binary decompilers. We presented a novel framework for empirically evaluating C-based decompilers from various aspects including correctness/ readability/ and structural similarity. The proposed metrics are validated practicality in decompiler assessment and provided insightful observations regarding the characteristics and constraints of existing decompiled code. This in turn contributes to bolstering the security and reliability of software systems that rely on WASM and native binaries.},
	author = {Wu, Wei-Cheng and Yan, Yutian and Egilsson, David, Hallgrimur and Park, David and Chan, Steven and Hauser, Christophe and Wang, Weihang},
	month = jan,
	year = {2024},
	annote = {SecureComm'24: Proceedings of the 20th EAI International Conference on Security and Privacy in Communication Networks},
}

@article{xie_deqompile_2025,
	title = {{DeQompile}: quantum circuit decompilation using genetic programming for explainable quantum architecture search},
	url = {https://arxiv.org/pdf/2504.08310},
	doi = {https://doi.org/10.48550/arXiv.2504.08310},
	abstract = {Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains. This work addresses the challenge of explainability in quantum architecture search (QAS) by introducing a novel genetic programming-based decompiler framework for reverse-engineering high-level quantum algorithms from low-level circuit representations. The proposed approach, implemented in the open-source tool DeQompile, employs program synthesis techniques, including symbolic regression and abstract syntax tree manipulation, to distill interpretable Qiskit algorithms from quantum assembly language. Validation of benchmark algorithms demonstrates the efficacy of our tool. By integrating the decompiler with online learning frameworks, this research potentiates explainable QAS by fostering the development of generalizable and provable quantum algorithms.},
	author = {Xie, Shubing and Sarkar, Aritra and Feld, Sebastian},
	month = apr,
	year = {2025},
}

@article{xu_symbol_2024,
	title = {Symbol {Preference} {Aware} {Generative} {Models} for {Recovering} {Variable} {Names} from {Stripped} {Binary}},
	url = {https://arxiv.org/pdf/2306.02546},
	doi = {https://doi.org/10.48550/arXiv.2306.02546},
	abstract = {Decompilation aims to recover the source code form of a binary executable. It has many security applications, such as malware analysis, vulnerability detection, and code hardening. A prominent challenge in decompilation is to recover variable names. We propose a novel technique that leverages the strengths of generative models while mitigating model biases. We build a prototype, GenNm, from pre-trained generative models CodeGemma-2B, CodeLlama-7B, and CodeLlama-34B. We finetune GenNm on decompiled functions and teach models to leverage contextual information. GenNm includes names from callers and callees while querying a function, providing rich contextual information within the model's input token limitation. We mitigate model biases by aligning the output distribution of models with symbol preferences of developers. Our results show that GenNm improves the state-of-the-art name recovery precision by 5.6-11.4 percentage points on two commonly used datasets and improves the state-of-the-art by 32\% (from 17.3\% to 22.8\%) in the most challenging setup where ground-truth variable names are not seen in the training dataset.},
	author = {Xu, Xiangzhe and Zhang, Zhuo and Su, Zian and Huang, Ziyang and Feng, Shiwei and Ye, Yapeng and Jiang, Nan and Xie, Danning and Cheng, Siyuan and Tan, Lin and Zhang, Xiangyu},
	month = feb,
	year = {2024},
}

@article{you_depyf_2024,
	title = {depyf: {Open} the {Opaque} {Box} of {PyTorch} {Compiler} for {Machine} {Learning} {Researchers}},
	url = {https://arxiv.org/pdf/2403.13839},
	doi = {https://doi.org/10.48550/arXiv.2403.13839},
	abstract = {PyTorch {\textbackslash}texttt\{2.x\} introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, adapting to the PyTorch compiler to full potential can be challenging. The compiler operates at the Python bytecode level, making it appear as an opaque box. To address this, we introduce {\textbackslash}texttt\{depyf\}, a tool designed to demystify the inner workings of the PyTorch compiler. {\textbackslash}texttt\{depyf\} decompiles bytecode generated by PyTorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, {\textbackslash}texttt\{depyf\} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is \{https://github.com/thuml/depyf\}\{ openly available\} and is recognized as a \{https://pytorch.org/ecosystem/\}\{PyTorch ecosystem project\}.},
	author = {You, Kaichao and Bai, Runsheng and Cao, Meng and Wang, Jianmin and Stoica, Ion and Long, Mingsheng},
	month = mar,
	year = {2024},
	annote = {16 pages, 2 figures},
}

@article{zhang_novel_2024-1,
	title = {A {Novel} {Approach} to {Malicious} {Code} {Detection} {Using} {CNN}-{BiLSTM} and {Feature} {Fusion}},
	url = {https://arxiv.org/pdf/2410.09401},
	doi = {https://doi.org/10.48550/arXiv.2410.09401},
	abstract = {With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness. This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware. In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability. The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.},
	author = {Zhang, Lixia and Liu, Tianxu and Shen, Kaihui and Chen, Cheng},
	month = oct,
	year = {2024},
}

@article{zhou_fidelitygpt_2025,
	title = {{FidelityGPT}: {Correcting} {Decompilation} {Distortions} with {Retrieval} {Augmented} {Generation}},
	url = {https://arxiv.org/pdf/2510.19615},
	doi = {https://doi.org/10.48550/arXiv.2510.19615},
	abstract = {Decompilation converts machine code into human-readable form, enabling analysis and debugging without source code. However, fidelity issues often degrade the readability and semantic accuracy of decompiled output. Existing methods, such as variable renaming or structural simplification, provide partial improvements but lack robust detection and correction, particularly for complex closed-source binaries. We present FidelityGPT, a framework that enhances decompiled code accuracy and readability by systematically detecting and correcting semantic distortions. FidelityGPT introduces distortion-aware prompt templates tailored to closed-source settings and integrates Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity algorithm to locate distorted lines and retrieve semantically similar code from a database. A variable dependency algorithm further mitigates long-context limitations by analyzing redundant variables and integrating their dependencies into the prompt context. Evaluated on 620 function pairs from a binary similarity benchmark, FidelityGPT achieved an average detection accuracy of 89\% and a precision of 83\%. Compared to the state-of-the-art DeGPT (Fix Rate 83\%, Corrected Fix Rate 37\%), FidelityGPT attained 94\% FR and 64\% CFR, demonstrating significant gains in accuracy and readability. These results highlight its potential to advance LLM-based decompilation and reverse engineering.},
	author = {Zhou, Zhiping and Li, Xiaohong and Feng, Ruitao and Zhang, Yao and Li, Yuekang and Feng, Wenbu and Wang, Yunqian and Li, Yuqing},
	month = oct,
	year = {2025},
}

@article{zhou_decompiling_2025,
	title = {Decompiling {Rust}: {An} {Empirical} {Study} of {Compiler} {Optimizations} and {Reverse} {Engineering} {Challenges}},
	url = {https://arxiv.org/pdf/2507.18792},
	doi = {https://doi.org/10.48550/arXiv.2507.18792},
	abstract = {Decompiling Rust binaries is challenging due to the language's rich type system, aggressive compiler optimizations, and widespread use of high-level abstractions. In this work, we conduct a benchmark-driven evaluation of decompilation quality across core Rust features and compiler build modes. Our automated scoring framework shows that generic types, trait methods, and error handling constructs significantly reduce decompilation quality, especially in release builds. Through representative case studies, we analyze how specific language constructs affect control flow, variable naming, and type information recovery. Our findings provide actionable insights for tool developers and highlight the need for Rust-aware decompilation strategies.},
	author = {Zhou, Zixu},
	month = jul,
	year = {2025},
}

@article{zou_d-lift_2025,
	title = {D-{LiFT}: {Improving} {LLM}-based {Decompiler} {Backend} via {Code} {Quality}-driven {Fine}-tuning},
	url = {https://arxiv.org/pdf/2506.10125},
	doi = {https://doi.org/10.48550/arXiv.2506.10125},
	abstract = {As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3\% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2\% of all the functions produced by the native decompiler.},
	author = {Zou, Muqi and Cai, Hongyu and Wu, Hongwei and Basque, Leonahenahe, Zion and Khan, Arslan and Celik, Berkay and {Dave} and {Tian} and Bianchi, Antonio and {Ruoyu} and {Wang} and Xu, Dongyan},
	month = aug,
	year = {2025},
}

@article{wang_proton_2022,
	title = {Proton {Stability}: {From} the {Standard} {Model} to {Beyond} {Grand} {Unification}},
	url = {https://arxiv.org/abs/2204.08393v2},
	doi = {10.1103/PhysRevD.106.025016},
	abstract = {A proton is known for its longevity, but what is its lifetime? While many Grand Unified Theories predict the proton decay with a finite lifetime, we show that the Standard Model (SM) and some versions of Ultra Unification (which replace sterile neutrinos with new exotic gapped/gapless sectors, e.g., topological or conformal field theory under global anomaly cancellation constraints) with a discrete baryon plus lepton symmetry permit a stable proton. For the 4d SM with {\textbackslash}N\_f{\textbackslash} families of 15 or 16 Weyl fermions, in addition to the continuous baryon minus lepton U(1)\_\{{\textbackslash}bf B - L\}{\textbackslash} symmetry, there is also a compatible discrete baryon plus lepton {\textbackslash}mathbb\{Z\}\_\{2N\_f, {\textbackslash}bf B + L\}{\textbackslash} symmetry. The {\textbackslash}mathbb\{Z\}\_\{2N\_f, {\textbackslash}bf B + L\}{\textbackslash} is discrete due to the ABJ anomaly under the BPST SU(2) instanton. Although both U(1)\_\{{\textbackslash}bf B - L\}{\textbackslash} and {\textbackslash}mathbb\{Z\}\_\{2N\_f, {\textbackslash}bf B + L\}{\textbackslash} symmetries are anomaly-free under the dynamical SM gauge field, it is important to check whether they have mixed anomalies with the gravitational background field and higher symmetries (whose charged objects are Wilson electric or 't Hooft magnetic line operators) of SM. We can also replace the U(1)\_\{{\textbackslash}bf B - L\}{\textbackslash} with a discrete variant {\textbackslash}mathbb\{Z\}\_\{4,X\}{\textbackslash} for {\textbackslash}X {\textbackslash}equiv 5(\{{\textbackslash}bf B - L\})-{\textbackslash}frac\{2\}\{3\} \{{\textbackslash}tilde Y\}{\textbackslash} of electroweak hypercharge \{{\textbackslash}tilde Y\}{\textbackslash}. We explore a systematic classification of candidate perturbative local and nonperturbative global anomalies of the 4d SM, including all these gauge and gravitational backgrounds, via a cobordism theory, which controls the SM's deformation class. We discuss the proton stability of the SM and Ultra Unification in the presence of discrete \{{\textbackslash}bf B + L\}{\textbackslash} symmetry protection, in particular (U(1)\_\{{\textbackslash}bf B - L\} {\textbackslash}times {\textbackslash}mathbb\{Z\}\_\{2N\_f,{\textbackslash}bf B + L\})/\{{\textbackslash}mathbb\{Z\}\_2ˆ\{{\textbackslash}rm F\}\}{\textbackslash} or {\textbackslash}({\textbackslash}mathbb\{Z\}\_\{4,X\} {\textbackslash}times {\textbackslash}mathbb\{Z\}\_\{2N\_f, {\textbackslash}bf B + L\})/\{{\textbackslash}mathbb\{Z\}\_2ˆ\{{\textbackslash}rm F\}\}{\textbackslash} symmetry with the fermion parity {\textbackslash}mathbb\{Z\}\_2ˆ\{{\textbackslash}rm F\}{\textbackslash}.},
	journal = {arXiv preprint arXiv:2204.08393},
	author = {Wang, Juven and Wan, Zheyan and You, Yi-Zhuang},
	month = apr,
	year = {2022},
	note = {\_eprint: 2204.08393},
	annote = {6 pages. Sequel to arXiv:2112.14765. v2: refinement. Related talks: https://www.youtube.com/results?search\_query=cobordism+deformation+standard+model+crticiality. The original title "Proton Stability: From the Standard Model to Ultra Unification" is adjusted in PRD},
	annote = {Journal reference: Phys. Rev. D 106, 025016 (2022)},
}

@article{wang_cobordism_2021,
	title = {Cobordism and {Deformation} {Class} of the {Standard} {Model}},
	url = {https://arxiv.org/abs/2112.14765v4},
	doi = {10.1103/PhysRevD.106.L041701},
	abstract = {'t Hooft anomalies of quantum field theories (QFTs) with an invertible global symmetry G (including spacetime and internal symmetries) in a {\textbackslash}d{\textbackslash}d spacetime are known to be classified by a {\textbackslash}d+1{\textbackslash}d cobordism group TP\_\{d+1\}{\textbackslash}(G), whose group generator is a {\textbackslash}d+1{\textbackslash}d cobordism invariant written as an invertible topological field theory (iTFT) Z\_\{d+1\}{\textbackslash}. The deformation class of QFT is recently proposed to be specified by its symmetry G and an iTFT Z\_\{d+1\}{\textbackslash}. Seemingly different QFTs of the same deformation class can be deformed to each other via quantum phase transitions. In this work, we ask which deformation class controls the 4d ungauged or gauged (SU(3){\textbackslash}times{\textbackslash}SU(2){\textbackslash}times{\textbackslash}U(1))/{\textbackslash}mathbb\{Z\}\_q{\textbackslash} Standard Model (SM) for {\textbackslash}q=1,2,3,6{\textbackslash} with a continuous or discrete {\textbackslash}({\textbackslash}bf\{B\}-{\textbackslash}bf\{L\}){\textbackslash} symmetry. We show that the answer contains some combination of 5d iTFTs: two {\textbackslash}mathbb\{Z\}{\textbackslash} classes associated with {\textbackslash}({\textbackslash}bf\{B\}-{\textbackslash}bf\{L\})ˆ3{\textbackslash} and {\textbackslash}({\textbackslash}bf\{B\}-{\textbackslash}bf\{L\}){\textbackslash}-(gravity){\textbackslash}ˆ2{\textbackslash} 4d perturbative local anomalies, a mod 16 class Atiyah-Patodi-Singer {\textbackslash}η{\textbackslash} invariant and a mod 2 class Stiefel-Whitney {\textbackslash}w\_2w\_3{\textbackslash} invariant associated with 4d nonperturbative global anomalies, and additional {\textbackslash}mathbb\{Z\}\_3{\textbackslash}times{\textbackslash}mathbb\{Z\}\_2{\textbackslash} classes involving higher symmetries whose charged objects are Wilson electric or 't Hooft magnetic line operators. Out of {\textbackslash}mathbb\{Z\}{\textbackslash} classes of local anomalies and 24576 classes of global anomalies, we pin down a deformation class of SM labeled by {\textbackslash}(N\_f,n\_\{ν\_\{R\}\},{\textbackslash} p{\textbackslash}',q){\textbackslash}, the family and "right-handed sterile" neutrino numbers, magnetic monopole datum, and mod {\textbackslash}q{\textbackslash} relation. Grand Unifications and Ultra Unification that replaces sterile neutrinos with new exotic gapped/gapless sectors (e.g., topological or conformal field theory) or gravitational sectors with topological or cobordism constraints, all reside in an SM deformation class. Neighbor phases/transitions/critical regions near SM exhibit beyond SM phenomena.},
	journal = {arXiv preprint arXiv:2112.14765},
	author = {Wang, Juven and Wan, Zheyan and You, Yi-Zhuang},
	month = dec,
	year = {2021},
	note = {\_eprint: 2112.14765},
	annote = {6 pages. Sequel to arXiv:1910.14668, arXiv:2006.16996, arXiv:2008.06499, arXiv:2012.15860, arXiv:2106.16248, arXiv:2111.10369, arXiv:2202.13498, arXiv:2204.08393. Related prior talks: https://www.youtube.com/results?search\_query=cobordism+deformation+standard+model+ultra+unification+crticiality. v4: Phys. Rev. D (Letter)},
	annote = {Journal reference: Phys. Rev. D 106, L041701 (2022)},
}

@article{wang_ultra_2020,
	title = {Ultra {Unification}},
	url = {https://arxiv.org/abs/2012.15860v3},
	doi = {10.1103/PhysRevD.103.105024},
	abstract = {Strong, electromagnetic, and weak forces were unified in the Standard Model (SM) with spontaneous gauge symmetry breaking. These forces were further conjectured to be unified in a simple Lie group gauge interaction in the Grand Unification (GUT). In this work, we propose a theory beyond the SM and GUT by adding new gapped Topological Phase Sectors consistent with the nonperturbative global anomaly cancellation and cobordism constraints (especially from the baryon minus lepton number \{{\textbackslash}bf B\}-\{{\textbackslash}bf L\}{\textbackslash}, the electroweak hypercharge {\textbackslash}Y{\textbackslash}, and the mixed gauge-gravitational anomaly). Gapped Topological Phase Sectors are constructed via symmetry extension, whose low energy contains unitary Lorentz invariant topological quantum field theories (TQFTs): either 3+1d non-invertible TQFT, or 4+1d invertible or non-invertible TQFT (short-range or long-range entangled gapped phase). Alternatively, there could also be right-handed "sterile" neutrinos, gapless unparticle physics, more general interacting conformal field theories, or gravity with topological cobordism constraints, or their combinations to altogether cancel the mixed gauge-gravitational anomaly. We propose that a new high-energy physics frontier beyond the conventional 0d particle physics relies on the new Topological Force and Topological Matter including gapped extended objects (gapped 1d line and 2d surface operators or defects, etc., whose open ends carry deconfined fractionalized particle or anyonic string excitations) or gapless conformal matter. Physical characterizations of these gapped extended objects require the mathematical theories of cohomology, cobordism, or category. Although weaker than the weak force, Topological Force is infinite-range or long-range which does not decay in the distance, and mediates between the linked worldvolume trajectories via fractional or categorical statistical interactions.},
	journal = {arXiv preprint arXiv:2012.15860},
	author = {Wang, Juven},
	month = dec,
	year = {2020},
	note = {\_eprint: 2012.15860},
	annote = {38 pages. Sequel to: arXiv:1910.14668, arXiv:2006.16996, arXiv:2008.06499. Supplementary: arXiv:1809.11171, arXiv:1705.06728. Physical Review journal version accepted subtitle: Unified model beyond grand unification. v3: refinement, and more clarifications on math/physics and neutrino masses. Related talks: https://www.youtube.com/results?search\_query=Ultra+Unification+Quantum+Criticality+Wang},
	annote = {Journal reference: Phys. Rev. D 103, 105024 (2021)},
}

@article{wang_gauge_2021,
	title = {Gauge {Enhanced} {Quantum} {Criticality} {Beyond} the {Standard} {Model}},
	url = {https://arxiv.org/abs/2106.16248v3},
	doi = {10.1103/PhysRevD.106.025013},
	abstract = {Standard lore views our 4d quantum vacuum governed by one of the candidate Standard Models (SMs), while lifting towards some Grand Unification-like structure (GUT) at higher energy scales. In contrast, in our work, we introduce an alternative view that the SM arises from various neighbor vacua competition in a quantum phase diagram. In general, we regard the SM arising near the gapless quantum criticality (either critical points or critical regions) between the competing neighbor vacua. In particular, we demonstrate how the {\textbackslash}su(3){\textbackslash}times su(2){\textbackslash}times u(1){\textbackslash} SM with 16n Weyl fermions arises near the quantum criticality between the GUT competition of Georgi-Glashow (GG) {\textbackslash}su(5){\textbackslash} and Pati-Salam (PS) {\textbackslash}su(4){\textbackslash}times su(2){\textbackslash}times su(2){\textbackslash}. We propose two enveloping toy models. Model I is a conventional {\textbackslash}so(10){\textbackslash} GUT with a Spin(10) gauge group plus GUT-Higgs potential inducing various Higgs transitions. Model II modifies Model I plus a 4d discrete torsion Wess-Zumino-Witten-like term built from GUT-Higgs field (that matches a nonperturbative global mixed gauge-gravity anomaly captured by a 5d invertible topological field theory {\textbackslash}w\_2w\_3{\textbackslash}), which manifests a Beyond-Landau-Ginzburg criticality between GG and PS models, with extra Beyond-the-Standard-Model (BSM) excitations emerging near a quantum critical region. If the internal symmetries were treated as global symmetries, we show a gapless 4d deconfined quantum criticality with new BSM fractionalized fragmentary excitations of Color-Flavor separation, and gauge enhancement including a Dark Gauge force sector, altogether requiring a double fermionic Spin structure named DSpin. If the internal symmetries are dynamically gauged, we show a 4d boundary criticality such that only appropriately gauge enhanced dynamical GUT gauge fields propagate into an extra-dimensional 5d bulk. The phenomena may be regarded as SM deformation or morphogenesis.},
	journal = {arXiv preprint arXiv:2106.16248},
	author = {Wang, Juven and You, Yi-Zhuang},
	month = jun,
	year = {2021},
	note = {\_eprint: 2106.16248},
	annote = {65 pages. Primers: https://www.youtube.com/results?search\_query=ultra+unification+quantum+crticiality+deformation+standard+model. Dedicate to Subir Sachdev (60), Xiao-Gang Wen (60), Edward Witten (70), Shing-Tung Yau (72). v3: Add more figures, motivations, comments on proton decay, non-perturbative effects on perturbatively irrelevant deformations. Sequel: arXiv:2111.10369, arXiv:2112.14765},
	annote = {Journal reference: Phys. Rev. D 106, 025013 (2022)},
}

@article{latune_quantum_2025,
	title = {Quantum {Thermodynamics} and {Quantum} {Perspectives}},
	url = {https://arxiv.org/abs/2512.24296v1},
	abstract = {After a brief historical perspective, we introduce the key notions of work and heat for quantum systems, to then apply them to quantum engines operating on quantum Otto and Carnot cycles. The irreversible and dissipative character of the quantum Otto cycle is briefly analyzed, contrasting with the energetic optimality of the quantum Carnot cycle. The central question of quantum effects is also addressed and illustrated with several examples. Finally, the last part strives to explain the role that quantum thermodynamics plays for quantum applications and quantum technologies, particularly in relation to energy optimization and the trade-off between performances and energy costs.},
	journal = {arXiv preprint arXiv:2512.24296},
	author = {Latune, Camille L.},
	month = dec,
	year = {2025},
	note = {\_eprint: 2512.24296},
	annote = {in French language, Contribution to the celebration of the 200 years of Sadi Carnot's book "Reflections on the Motive Power of Fire", in French and part of the book "Chaleur, énergie, thermodynamique: le message de Carnot aujourd'hui... 200 ans après", direction G. Bertrand, Ed. Univ. de Dijon (2025). https://eud.ube.fr/sciences/891-chaleur-energie-thermodynamique-9782364415829.html?search\_query=carnot\&results=3},
	annote = {Journal reference: "Chaleur, Energie, Thermodynamique: le message de Carnot aujourd'hui... 200 ans apres", sous la direction de Gilles Bertrand - Editions universitaires de Dijon (2025)},
}

@article{wang_lift_2025,
	title = {{LIFT}: {Automating} {Symbolic} {Execution} {Optimization} with {Large} {Language} {Models} for {AI} {Networks}},
	url = {https://arxiv.org/abs/2507.04931v1},
	abstract = {Dynamic Symbolic Execution (DSE) is a key technique in program analysis, widely used in software testing, vulnerability discovery, and formal verification. In distributed AI systems, DSE plays a crucial role in identifying hard-to-detect bugs, especially those arising from complex network communication patterns. However, traditional approaches to symbolic execution are often hindered by scalability issues and inefficiencies, particularly in large-scale systems. This paper introduces LIFT (Large-language-model Integrated Functional-equivalent-IR Transformation), a novel framework that leverages Large Language Models (LLMs) to automate the optimization of Intermediate Representations (IRs) in symbolic execution. LIFT addresses the challenges of symbolic execution by providing a scalable, context-sensitive solution for IR transformation. The framework consists of two phases: IR Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and Symbolic Execution and Validation, which includes benchmarking and semantic verification to ensure correctness and generalizability. Experiments on real-world binaries demonstrated significant performance improvements, including a 53.5{\textbackslash}\% reduction in execution time for bigtest and a 10.24{\textbackslash}\% reduction for random, along with reductions in IR statements, PUT instructions, and temporary variables. These results demonstrate that LLMs simplify IRs while maintaining functional correctness, enhancing symbolic execution in distributed AI systems.},
	journal = {arXiv preprint arXiv:2507.04931},
	author = {Wang, Ruoxi and Li, Kun and Xu, Minghui and Zhang, Yue and Xu, Kaidi and Liu, Chunchi and Xiao, Yinhao and Cheng, Xiuzhen},
	month = jul,
	year = {2025},
	note = {\_eprint: 2507.04931},
	annote = {Accepted by ACM SIGCOMM 2025 - 2nd Workshop on Networks for AI Computing (NAIC). 7 pages, 2 figures, 2 tables},
}

@article{tehranijamsaz_graphbinmatch_2023,
	title = {{GraphBinMatch}: {Graph}-based {Similarity} {Learning} for {Cross}-{Language} {Binary} and {Source} {Code} {Matching}},
	url = {https://arxiv.org/abs/2304.04658v1},
	abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate our approach performance on single-language binary-to-source code matching. Experimental results show that GraphBinMatch outperforms state-of-the-art significantly, with improvements as high as 15\% over the F1 score.},
	journal = {arXiv preprint arXiv:2304.04658},
	author = {TehraniJamsaz, Ali and Chen, Hanze and Jannesari, Ali},
	month = apr,
	year = {2023},
	note = {\_eprint: 2304.04658},
}

@article{wong_faster_2018,
	title = {Faster {Variational} {Execution} with {Transparent} {Bytecode} {Transformation}},
	url = {https://arxiv.org/abs/1809.04193v1},
	abstract = {Variational execution is a novel dynamic analysis technique for exploring highly configurable systems and accurately tracking information flow. It is able to efficiently analyze many configurations by aggressively sharing redundancies of program executions. The idea of variational execution has been demonstrated to be effective in exploring variations in the program, especially when the configuration space grows out of control. Existing implementations of variational execution often require heavy lifting of the runtime interpreter, which is painstaking and error-prone. Furthermore, the performance of this approach is suboptimal. For example, the state-of-the-art variational execution interpreter for Java, VarexJ, slows down executions by 100 to 800 times over a single execution for small to medium size Java programs. Instead of modifying existing JVMs, we propose to transform existing bytecode to make it variational, so it can be executed on an unmodified commodity JVM. Our evaluation shows a dramatic improvement on performance over the state-of-the-art, with a speedup of 2 to 46 times, and high efficiency in sharing computations.},
	journal = {arXiv preprint arXiv:1809.04193},
	author = {Wong, Chu-Pan and Meinicke, Jens and Lazarek, Lukas and Kästner, Christian},
	month = sep,
	year = {2018},
	note = {\_eprint: 1809.04193},
}

@article{lu_chameleon_2023,
	title = {Chameleon: {Plug}-and-{Play} {Compositional} {Reasoning} with {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2304.09842v3},
	abstract = {Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54\% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37\%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0\%, lifting the state of the art to 98.78\%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.},
	journal = {arXiv preprint arXiv:2304.09842},
	author = {Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
	month = apr,
	year = {2023},
	note = {\_eprint: 2304.09842},
	annote = {32 pages, 10 figures, 24 tables. Accepted to NeurIPS 2023},
}

@article{zhang_towards_2025,
	title = {Towards {Semantics} {Lifting} for {Scientific} {Computing}: {A} {Case} {Study} on {FFT}},
	url = {https://arxiv.org/abs/2501.09201v1},
	abstract = {The rise of automated code generation tools, such as large language models (LLMs), has introduced new challenges in ensuring the correctness and efficiency of scientific software, particularly in complex kernels, where numerical stability, domain-specific optimizations, and precise floating-point arithmetic are critical. We propose a stepwise semantics lifting approach using an extended SPIRAL framework with symbolic execution and theorem proving to statically derive high-level code semantics from LLM-generated kernels. This method establishes a structured path for verifying the source code's correctness via a step-by-step lifting procedure to high-level specification. We conducted preliminary tests on the feasibility of this approach by successfully lifting GPT-generated fast Fourier transform code to high-level specifications.},
	journal = {arXiv preprint arXiv:2501.09201},
	author = {Zhang, Naifeng and Rao, Sanil and Franusich, Mike and Franchetti, Franz},
	month = jan,
	year = {2025},
	note = {\_eprint: 2501.09201},
	annote = {Accepted at the Theory and Practice of Static Analysis Workshop (TPSA), in conjunction with the ACM SIGPLAN Symposium on Principles of Programming Languages (POPL), 2025},
}

@article{mccully_bi-directional_2024,
	title = {Bi-{Directional} {Transformers} vs. word2vec: {Discovering} {Vulnerabilities} in {Lifted} {Compiled} {Code}},
	url = {https://arxiv.org/abs/2405.20611v3},
	doi = {10.1109/CARS61786.2024.10778724},
	abstract = {Detecting vulnerabilities within compiled binaries is challenging due to lost high-level code structures and other factors such as architectural dependencies, compilers, and optimization options. To address these obstacles, this research explores vulnerability detection using natural language processing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn semantics from intermediate representation (LLVM IR) code. Long short-term memory (LSTM) neural networks were trained on embeddings from encoders created using approximately 48k LLVM functions from the Juliet dataset. This study is pioneering in its comparison of word2vec models with multiple bidirectional transformers (BERT, RoBERTa) embeddings built using LLVM code to train neural networks to detect vulnerabilities in compiled binaries. Word2vec Skip-Gram models achieved 92\% validation accuracy in detecting vulnerabilities, outperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This suggests that complex contextual embeddings may not provide advantages over simpler word2vec models for this task when a limited number (e.g. 48K) of data samples are used to train the bidirectional transformer-based models. The comparative results provide novel insights into selecting optimal embeddings for learning compiler-independent semantic code representations to advance machine learning detection of vulnerabilities in compiled binaries.},
	journal = {arXiv preprint arXiv:2405.20611},
	author = {McCully, Gary A. and Hastings, John D. and Xu, Shengjie and Fortier, Adam},
	month = may,
	year = {2024},
	note = {\_eprint: 2405.20611},
	annote = {Journal reference: 2024 IEEE Cyber Awareness and Research Symposium (CARS), Grand Forks, ND, USA, 2024, pp. 1-8},
	annote = {Updated with improvements},
}

@article{joshi_neuro-lift_2025,
	title = {Neuro-{LIFT}: {A} {Neuromorphic}, {LLM}-based {Interactive} {Framework} for {Autonomous} {Drone} {FlighT} at the {Edge}},
	url = {https://arxiv.org/abs/2501.19259v2},
	abstract = {The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.},
	journal = {arXiv preprint arXiv:2501.19259},
	author = {Joshi, Amogh and Sanyal, Sourav and Roy, Kaushik},
	month = jan,
	year = {2025},
	note = {\_eprint: 2501.19259},
	annote = {Accepted for publication at the International Joint Conference on Neural Networks (IJCNN) 2025},
}

@article{li_quantum-classical_2025,
	title = {Quantum-{Classical} {Hybrid} {Quantized} {Neural} {Network}},
	url = {https://arxiv.org/abs/2506.18240v4},
	abstract = {In this work, we introduce a novel Quadratic Binary Optimization (QBO) framework for training a quantized neural network. The framework enables the use of arbitrary activation and loss functions through spline interpolation, while Forward Interval Propagation addresses the nonlinearities and the multi-layered, composite structure of neural networks via discretizing activation functions into linear subintervals. This preserves the universal approximation properties of neural networks while allowing complex nonlinear functions accessible to quantum solvers, broadening their applicability in artificial intelligence. Theoretically, we derive an upper bound on the approximation error and the number of Ising spins required by deriving the sample complexity of the empirical risk minimization problem from an optimization perspective. A key challenge in solving the associated large-scale Quadratic Constrained Binary Optimization (QCBO) model is the presence of numerous constraints. To overcome this, we adopt the Quantum Conditional Gradient Descent (QCGD) algorithm, which solves QCBO directly on quantum hardware. We establish the convergence of QCGD under a quantum oracle subject to randomness, bounded variance, and limited coefficient precision, and further provide an upper bound on the Time-To-Solution. To enhance scalability, we further incorporate a decomposed copositive optimization scheme that replaces the monolithic lifted model with sample-wise subproblems. This decomposition substantially reduces the quantum resource requirements and enables efficient low-bit neural network training. We further propose the usage of QCGD and Quantum Progressive Hedging (QPH) algorithm to efficiently solve the decomposed problem.},
	journal = {arXiv preprint arXiv:2506.18240},
	author = {Li, Wenxin and Wang, Chuan and Zhu, Hongdong and Gao, Qi and Ma, Yin and Wei, Hai and Wen, Kai},
	month = jun,
	year = {2025},
	note = {\_eprint: 2506.18240},
}

@article{li_binary_2023,
	title = {Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy},
	url = {https://arxiv.org/abs/2304.12866v1},
	doi = {10.1002/aisy.202300399},
	abstract = {Deep learning needs high-precision handling of forwarding signals, backpropagating errors, and updating weights. This is inherently required by the learning algorithm since the gradient descent learning rule relies on the chain product of partial derivatives. However, it is challenging to implement deep learning in hardware systems that use noisy analog memristors as artificial synapses, as well as not being biologically plausible. Memristor-based implementations generally result in an excessive cost of neuronal circuits and stringent demands for idealized synaptic devices. Here, we demonstrate that the requirement for high precision is not necessary and that more efficient deep learning can be achieved when this requirement is lifted. We propose a binary stochastic learning algorithm that modifies all elementary neural network operations, by introducing (i) stochastic binarization of both the forwarding signals and the activation function derivatives, (ii) signed binarization of the backpropagating errors, and (iii) step-wised weight updates. Through an extensive hybrid approach of software simulation and hardware experiments, we find that binary stochastic deep learning systems can provide better performance than the software-based benchmarks using the high-precision learning algorithm. Also, the binary stochastic algorithm strongly simplifies the neural network operations in hardware, resulting in an improvement of the energy efficiency for the multiply-and-accumulate operations by more than three orders of magnitudes.},
	journal = {arXiv preprint arXiv:2304.12866},
	author = {Li, Yang and Wang, Wei and Wang, Ming and Dou, Chunmeng and Ma, Zhengyu and Zhou, Huihui and Zhang, Peng and Lepri, Nicola and Zhang, Xumeng and Luo, Qing and Xu, Xiaoxin and Yang, Guanhua and Zhang, Feng and Li, Ling and Ielmini, Daniele and Liu, Ming},
	month = apr,
	year = {2023},
	note = {\_eprint: 2304.12866},
	annote = {Journal reference: Adv. Intel. Sys., 6(1), 2300399, 2024},
}

@article{gui_cross-language_2022,
	title = {Cross-{Language} {Binary}-{Source} {Code} {Matching} with {Intermediate} {Representations}},
	url = {https://arxiv.org/abs/2201.07420v1},
	abstract = {Binary-source code matching plays an important role in many security and software engineering related tasks such as malware detection, reverse engineering and vulnerability assessment. Currently, several approaches have been proposed for binary-source code matching by jointly learning the embeddings of binary code and source code in a common vector space. Despite much effort, existing approaches target on matching the binary code and source code written in a single programming language. However, in practice, software applications are often written in different programming languages to cater for different requirements and computing platforms. Matching binary and source code across programming languages introduces additional challenges when maintaining multi-language and multi-platform applications. To this end, this paper formulates the problem of cross-language binary-source code matching, and develops a new dataset for this new problem. We present a novel approach XLIR, which is a Transformer-based neural network by learning the intermediate representations for both binary and source code. To validate the effectiveness of XLIR, comprehensive experiments are conducted on two tasks of cross-language binary-source code matching, and cross-language source-source code matching, on top of our curated dataset. Experimental results and analysis show that our proposed XLIR with intermediate representations significantly outperforms other state-of-the-art models in both of the two tasks.},
	journal = {arXiv preprint arXiv:2201.07420},
	author = {Gui, Yi and Wan, Yao and Zhang, Hongyu and Huang, Huifang and Sui, Yulei and Xu, Guandong and Shao, Zhiyuan and Jin, Hai},
	month = jan,
	year = {2022},
	note = {\_eprint: 2201.07420},
	annote = {SANER2022},
}

@article{jiang_binaryai_2024,
	title = {{BinaryAI}: {Binary} {Software} {Composition} {Analysis} via {Intelligent} {Binary} {Source} {Code} {Matching}},
	url = {https://arxiv.org/abs/2401.11161v3},
	abstract = {While third-party libraries are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis, proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@1 and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product.},
	journal = {arXiv preprint arXiv:2401.11161},
	author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
	month = jan,
	year = {2024},
	note = {\_eprint: 2401.11161},
	annote = {In Proceedings of the 46th International Conference on Software Engineering (ICSE'24)},
}

@article{zhan_verified_2024,
	title = {Verified {Lifting} of {Deep} learning {Operators}},
	url = {https://arxiv.org/abs/2412.20992v1},
	abstract = {Deep learning operators are fundamental components of modern deep learning frameworks. With the growing demand for customized operators, it has become increasingly common for developers to create their own. However, designing and implementing operators is complex and error-prone, due to hardware-specific optimizations and the need for numerical stability. There is a pressing need for tools that can summarize the functionality of both existing and user-defined operators. To address this gap, this work introduces a novel framework for the verified lifting of deep learning operators, which synthesizes high-level mathematical formulas from low-level implementations. Our approach combines symbolic execution, syntax-guided synthesis, and SMT-based verification to produce readable and formally verified mathematical formulas. In synthesis, we employ a combination of top-down and bottom-up strategies to explore the vast search space efficiently; In verification, we design invariant synthesis patterns and leverage SMT solvers to validate the correctness of the derived summaries; In simplification, we use egraph-based techniques with custom rules to restore complex formulas to their natural, intuitive forms. Evaluated on a dataset of deep learning operators implemented in Triton from the real world, our method demonstrates the effectiveness of synthesis and verification compared to existing techniques. This framework bridges the gap between low-level implementations and high-level abstractions, improving understanding and reliability in deep learning operator development.},
	journal = {arXiv preprint arXiv:2412.20992},
	author = {Zhan, Qi and Hu, Xing and Xia, Xin and Li, Shanping},
	month = dec,
	year = {2024},
	note = {\_eprint: 2412.20992},
}

@article{graf_selective_2019,
	title = {Selective {Lambda} {Lifting}},
	url = {https://arxiv.org/abs/1910.11717v2},
	abstract = {Lambda lifting is a well-known transformation, traditionally employed for compiling functional programs to supercombinators. However, more recent abstract machines for functional languages like OCaml and Haskell tend to do closure conversion instead for direct access to the environment, so lambda lifting is no longer necessary to generate machine code. We propose to revisit selective lambda lifting in this context as an optimising code generation strategy and conceive heuristics to identify beneficial lifting opportunities. We give a static analysis for estimating impact on heap allocations of a lifting decision. Performance measurements of our implementation within the Glasgow Haskell Compiler on a large corpus of Haskell benchmarks suggest modest speedups.},
	journal = {arXiv preprint arXiv:1910.11717},
	author = {Graf, Sebastian and Jones, Simon Peyton},
	month = oct,
	year = {2019},
	note = {\_eprint: 1910.11717},
	annote = {Rejected from ICFP 2019},
}

@article{prenner_runbugrun_2023,
	title = {{RunBugRun} – {An} {Executable} {Dataset} for {Automated} {Program} {Repair}},
	url = {https://arxiv.org/abs/2304.01102v1},
	abstract = {Recently, we can notice a transition to data-driven techniques in Automated Program Repair (APR), in particular towards deep neural networks. This entails training on hundreds of thousands or even millions of non-executable code fragments. We would like to bring more attention to an aspect of code often neglected in Neural Program Repair (NPR), namely its execution. Code execution has several significant advantages. It allows for test-based evaluation of candidate fixes and can provide valuable information to aid repair. In this work we present a fully executable dataset of 450,000 small buggy/fixed program pairs originally submitted to programming competition websites written in eight different programming languages. Along with the dataset we provide infrastructure to compile, safely execute and test programs as well as fine-grained bug-type labels. To give a point of reference, we provide basic evaluation results for two baselines, one based on a generate-and-validate approach and one on deep learning. With this dataset we follow several goals: we want to lift Neural Program Repair beyond fully static code representations, foster the use of execution-based features and, by including several different languages, counterbalance the predominance of Java in the current landscape of APR datasets and benchmarks.},
	journal = {arXiv preprint arXiv:2304.01102},
	author = {Prenner, Julian Aron and Robbes, Romain},
	month = apr,
	year = {2023},
	note = {\_eprint: 2304.01102},
}

@article{kulshrestha_vlad-grasp_2025,
	title = {{VLAD}-{Grasp}: {Zero}-shot {Grasp} {Detection} via {Vision}-{Language} {Models}},
	url = {https://arxiv.org/abs/2511.05791v1},
	abstract = {Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.},
	journal = {arXiv preprint arXiv:2511.05791},
	author = {Kulshrestha, Manav and Bukhari, S. Talha and Conover, Damon and Bera, Aniket},
	month = nov,
	year = {2025},
	note = {\_eprint: 2511.05791},
	annote = {8 pages, 4 figures, under review},
}

@article{cheng_lps-gnn_2025,
	title = {{LPS}-{GNN} : {Deploying} {Graph} {Neural} {Networks} on {Graphs} with 100-{Billion} {Edges}},
	url = {https://arxiv.org/abs/2507.14570v1},
	abstract = {Graph Neural Networks (GNNs) have emerged as powerful tools for various graph mining tasks, yet existing scalable solutions often struggle to balance execution efficiency with prediction accuracy. These difficulties stem from iterative message-passing techniques, which place significant computational demands and require extensive GPU memory, particularly when dealing with the neighbor explosion issue inherent in large-scale graphs. This paper introduces a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN, which can perform representation learning on 100 billion graphs with a single GPU in 10 hours and shows a 13.8\% improvement in User Acquisition scenarios. We examine existing graph partitioning methods and design a superior graph partition algorithm named LPMetis. In particular, LPMetis outperforms current state-of-the-art (SOTA) approaches on various evaluation metrics. In addition, our paper proposes a subgraph augmentation strategy to enhance the model's predictive performance. It exhibits excellent compatibility, allowing the entire framework to accommodate various GNN algorithms. Successfully deployed on the Tencent platform, LPS-GNN has been tested on public and real-world datasets, achieving performance lifts of 8. 24\% to 13. 89\% over SOTA models in online applications.},
	journal = {arXiv preprint arXiv:2507.14570},
	author = {Cheng, Xu and Yao, Liang and He, Feng and Cen, Yukuo and He, Yufei and Zhang, Chenhui and Feng, Wenzheng and Cai, Hongyun and Tang, Jie},
	month = jul,
	year = {2025},
	note = {\_eprint: 2507.14570},
}

@article{li_striderspd_2026,
	title = {{StriderSPD}: {Structure}-{Guided} {Joint} {Representation} {Learning} for {Binary} {Security} {Patch} {Detection}},
	url = {https://arxiv.org/abs/2601.05772v1},
	abstract = {Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose {\textbackslash}textbf\{{\textbackslash}textit\{StriderSPD\}\}, a {\textbackslash}underline\{Str\}ucture-gu{\textbackslash}underline\{ide\}d joint {\textbackslash}underline\{r\}epresentation {\textbackslash}underline\{SPD\} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.},
	journal = {arXiv preprint arXiv:2601.05772},
	author = {Li, Qingyuan and Yu, Chenchen and Li, Chuanyi and Wen, Xin-Cheng and Lee, Cheryl and Gao, Cuiyun and Luo, Bin},
	month = jan,
	year = {2026},
	note = {\_eprint: 2601.05772},
}

@article{zhang_comprehension_2025,
	title = {Comprehension {Without} {Competence}: {Architectural} {Limits} of {LLMs} in {Symbolic} {Computation} and {Reasoning}},
	url = {https://arxiv.org/abs/2507.10624v3},
	abstract = {Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between {\textbackslash}textit\{comprehension\} and {\textbackslash}textit\{competence\}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying them–a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational {\textbackslash}textit\{split-brain syndrome\}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.},
	journal = {arXiv preprint arXiv:2507.10624},
	author = {Zhang, Zheng},
	month = jul,
	year = {2025},
	note = {\_eprint: 2507.10624},
	annote = {v2: Two TMLR revision rounds addressing reviewer feedback. Added real-world validation (3.4), interpretability analysis (7), computational hallucination framework, strengthened theory. v3: Sec 3.2 - added transformer architecture diagram, clarified UAT capacity vs computational limits, improved role specialization theorem presentation},
}

@article{su_source_2024,
	title = {Source {Code} {Foundation} {Models} are {Transferable} {Binary} {Analysis} {Knowledge} {Bases}},
	url = {https://arxiv.org/abs/2405.19581v2},
	abstract = {Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3\% relative gain in CHRF and a 16.7\% relative gain in a GPT4-based metric for summarization, as well as a 6.7\% and 7.4\% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.},
	journal = {arXiv preprint arXiv:2405.19581},
	author = {Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Kaiyuan and Zhang, Xiangyu},
	month = may,
	year = {2024},
	note = {\_eprint: 2405.19581},
}

@article{armengol-estape_learning_2021,
	title = {Learning {C} to x86 {Translation}: {An} {Experiment} in {Neural} {Compilation}},
	url = {https://arxiv.org/abs/2108.07639v2},
	abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
	journal = {arXiv preprint arXiv:2108.07639},
	author = {Armengol-Estapé, Jordi and O'Boyle, Michael F. P.},
	month = aug,
	year = {2021},
	note = {\_eprint: 2108.07639},
	annote = {Journal reference: Armengol-Estapé, J. and O'Boyle, M. Learning C to x86 translation: An experiment in neural compilation. In Advances in Programming Languages and Neurosymbolic Systems Workshop, 2021. URL \{https://openreview.net/forum?id=444ug\_EYXet\}},
	annote = {Published in AIPLANS 2021},
}

@article{qiu_tenspiler_2024,
	title = {Tenspiler: {A} {Verified} {Lifting}-{Based} {Compiler} for {Tensor} {Operations} ({Extended} {Version})},
	url = {https://arxiv.org/abs/2404.18249v3},
	abstract = {Tensor processing infrastructures such as deep learning frameworks and specialized hardware accelerators have revolutionized how computationally intensive code from domains such as deep learning and image processing is executed and optimized. These infrastructures provide powerful and expressive abstractions while ensuring high performance. However, to utilize them, code must be written specifically using the APIs / ISAs of such software frameworks or hardware accelerators. Importantly, given the fast pace of innovation in these domains, code written today quickly becomes legacy as new frameworks and accelerators are developed, and migrating such legacy code manually is a considerable effort. To enable developers in leveraging such DSLs while preserving their current programming paradigm, we introduce Tenspiler, a verified lifting-based compiler that uses program synthesis to translate sequential programs written in general-purpose programming languages (e.g., C++ or Python code) into tensor operations. Central to Tenspiler is our carefully crafted yet simple intermediate language, named TensIR, that expresses tensor operations. TensIR enables efficient lifting, verification, and code generation. Currently, Tenspiler already supports {\textbackslash}textbf\{six\}{\textbackslash} DSLs, spanning a broad spectrum of software and hardware environments. Furthermore, we show that new backends can be easily supported by Tenspiler by adding simple pattern-matching rules for TensIR. Using 10 real-world code benchmark suites, our experimental evaluation shows that by translating code to be executed on {\textbackslash}textbf\{6\}{\textbackslash} different software frameworks and hardware devices, Tenspiler offers on average 105{\textbackslash}times{\textbackslash} kernel and 9.65{\textbackslash}times{\textbackslash} end-to-end execution time improvement over the fully-optimized sequential implementation of the same benchmarks.},
	journal = {arXiv preprint arXiv:2404.18249},
	author = {Qiu, Jie and Cai, Colin and Bhatia, Sahil and Hasabnis, Niranjan and Seshia, Sanjit A. and Cheung, Alvin},
	month = apr,
	year = {2024},
	note = {\_eprint: 2404.18249},
}

@article{kang_3d_2023,
	title = {{3D} {Human} {Pose} {Lifting} with {Grid} {Convolution}},
	url = {https://arxiv.org/abs/2302.08760v1},
	abstract = {Existing lifting networks for regressing 3D human poses from 2D single-view poses are typically constructed with linear layers based on graph-structured representation learning. In sharp contrast to them, this paper presents Grid Convolution (GridConv), mimicking the wisdom of regular convolution operations in image space. GridConv is based on a novel Semantic Grid Transformation (SGT) which leverages a binary assignment matrix to map the irregular graph-structured human pose onto a regular weave-like grid pose representation joint by joint, enabling layer-wise feature learning with GridConv operations. We provide two ways to implement SGT, including handcrafted and learnable designs. Surprisingly, both designs turn out to achieve promising results and the learnable one is better, demonstrating the great potential of this new lifting representation learning formulation. To improve the ability of GridConv to encode contextual cues, we introduce an attention module over the convolutional kernel, making grid convolution operations input-dependent, spatial-aware and grid-specific. We show that our fully convolutional grid lifting network outperforms state-of-the-art methods with noticeable margins under (1) conventional evaluation on Human3.6M and (2) cross-evaluation on MPI-INF-3DHP. Code is available at https://github.com/OSVAI/GridConv},
	journal = {arXiv preprint arXiv:2302.08760},
	author = {Kang, Yangyuxuan and Liu, Yuyang and Yao, Anbang and Wang, Shandong and Wu, Enhua},
	month = feb,
	year = {2023},
	note = {\_eprint: 2302.08760},
	annote = {Oral paper at AAAI 2023. Project website: https://github.com/OSVAI/GridConv},
}

@article{angermann_random_2019,
	title = {Random 2.{5D} {U}-net for {Fully} {3D} {Segmentation}},
	url = {https://arxiv.org/abs/1910.10398v1},
	doi = {10.1007/978-3-030-33327-0_19},
	abstract = {Convolutional neural networks are state-of-the-art for various segmentation tasks. While for 2D images these networks are also computationally efficient, 3D convolutions have huge storage requirements and therefore, end-to-end training is limited by GPU memory and data size. To overcome this issue, we introduce a network structure for volumetric data without 3D convolution layers. The main idea is to include projections from different directions to transform the volumetric data to a sequence of images, where each image contains information of the full data. We then apply 2D convolutions to these projection images and lift them again to volumetric data using a trainable reconstruction algorithm. The proposed architecture can be applied end-to-end to very large data volumes without cropping or sliding-window techniques. For a tested sparse binary segmentation task, it outperforms already known standard approaches and is more resistant to generation of artefacts.},
	journal = {arXiv preprint arXiv:1910.10398},
	author = {Angermann, Christoph and Haltmeier, Markus},
	month = oct,
	year = {2019},
	note = {\_eprint: 1910.10398},
	annote = {Submission for joint MICCAI-Workshops on Computing and Visualization for Intravascular Imaging and Computer Assisted Stenting (CVII-STENT) 2019},
}

@article{gao_pr2_2025,
	title = {{PR2}: {Peephole} {Raw} {Pointer} {Rewriting} with {LLMs} for {Translating} {C} to {Safer} {Rust}},
	url = {https://arxiv.org/abs/2505.04852v2},
	abstract = {There has been a growing interest in translating C code to Rust due to Rust's robust memory and thread safety guarantees. Tools such as C2RUST enable syntax-guided transpilation from C to semantically equivalent Rust code. However, the resulting Rust programs often rely heavily on unsafe constructs–particularly raw pointers–which undermines Rust's safety guarantees. This paper aims to improve the memory safety of Rust programs generated by C2RUST by eliminating raw pointers. Specifically, we propose a peephole raw pointer rewriting technique that lifts raw pointers in individual functions to appropriate Rust data structures. Technically, PR2 employs decision-tree-based prompting to guide the pointer lifting process. Additionally, it leverages code change analysis to guide the repair of errors introduced during rewriting, effectively addressing errors encountered during compilation and test case execution. We implement PR2 as a prototype and evaluate it using gpt-4o-mini on 28 real-world C projects. The results show that PR2 successfully eliminates 13.22\% of local raw pointers across these projects, significantly enhancing the safety of the translated Rust code. On average, PR2 completes the transformation of a project in 5.44 hours, at an average cost of \$1.46.},
	journal = {arXiv preprint arXiv:2505.04852},
	author = {Gao, Yifei and Wang, Chengpeng and Huang, Pengxiang and Liu, Xuwei and Zheng, Mingwei and Zhang, Xiangyu},
	month = may,
	year = {2025},
	note = {\_eprint: 2505.04852},
}

@article{wang_semguard_2025,
	title = {{SemGuard}: {Real}-{Time} {Semantic} {Evaluator} for {Correcting} {LLM}-{Generated} {Code}},
	url = {https://arxiv.org/abs/2509.24507v1},
	abstract = {Large Language Models (LLMs) can translate natural language requirements into code, yet empirical analyses of representative models reveal that semantic errors-programs that compile but behave incorrectly-constitute the majority of observed faults (e.g., {\textgreater}60\% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc repair pipelines detect such faults only after execution, incurring latency, relying on incomplete test suites, and often mis-localizing the defect. Since semantic drift originates in the autoregressive decoding process, intervening while the code is being generated is a direct way to stop error propagation. Constrained-decoding approaches such as ROCODE attempt this, but still wait until the entire program runs to obtain feedback and use entropy heuristics that do not truly capture semantics. A more effective solution must inject semantic signals-early and precisely-into the decoding process.We present SemGuard, a semantic-evaluator-driven framework that performs real-time, line-level semantic supervision. To train the evaluator, we build SemDiff, the first dataset with fine-grained annotations that mark the exact line where a correct and an incorrect implementation diverge. The evaluator, once embedded in the LLM's decoder, flags deviations on partial code, rolls back to the faulty line, and guides regeneration-without executing the program or requiring test cases. Across four benchmarks, SemGuard consistently outperforms state-of-the-art baselines. It lowers the semantic error rate by 19.86\% on SemDiff relative to ROCODE, and lifts Pass@1 by 48.92\% on the real-world LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating model- and language-agnostic effectiveness.},
	journal = {arXiv preprint arXiv:2509.24507},
	author = {Wang, Qinglin and Sun, Zhihong and Wang, Ruyun and Huang, Tao and Jin, Zhi and Li, Ge and Lyu, Chen},
	month = sep,
	year = {2025},
	note = {\_eprint: 2509.24507},
	annote = {Accepted by the 40th IEEE/ACM Automated Software Engineering Conference (ASE 2025)},
}

@article{haghighi_trlf_2018,
	title = {{TRLF}: {An} {Effective} {Semi}-fragile {Watermarking} {Method} for {Tamper} {Detection} and {Recovery} based on {LWT} and {FNN}},
	url = {https://arxiv.org/abs/1802.07119v1},
	abstract = {This paper proposes a novel method for tamper detection and recovery using semi-fragile data hiding, based on Lifting Wavelet Transform (LWT) and Feed-Forward Neural Network (FNN). In TRLF, first, the host image is decomposed up to one level using LWT, and the Discrete Cosine Transform (DCT) is applied to each 2*2 blocks of diagonal details. Next, a random binary sequence is embedded in each block as the watermark by correlating {\textbackslash}DC{\textbackslash} coefficients. In authentication stage, first, the watermarked image geometry is reconstructed by using Speeded Up Robust Features (SURF) algorithm and extract watermark bits by using FNN. Afterward, logical exclusive-or operation between original and extracted watermark is applied to detect tampered region. Eventually, in the recovery stage, tampered regions are recovered by image digest which is generated by inverse halftoning technique. The performance and efficiency of TRLF and its robustness against various geometric, non-geometric and hybrid attacks are reported. From the experimental results, it can be seen that TRLF is superior in terms of robustness and quality of the digest and watermarked image respectively, compared to the-state-of-the-art fragile and semi-fragile watermarking methods. In addition, imperceptibility has been improved by using different correlation steps as the gain factor for flat (smooth) and texture (rough) blocks.},
	journal = {arXiv preprint arXiv:1802.07119},
	author = {Haghighi, Behrouz Bolourian and Taherinia, Amir Hossein and Monsefi, Reza},
	month = feb,
	year = {2018},
	note = {\_eprint: 1802.07119},
}

@article{shirani_non-interactive_2022,
	title = {On {Non}-{Interactive} {Source} {Simulation} via {Fourier} {Transform}},
	url = {https://arxiv.org/abs/2212.09239v1},
	abstract = {The non-interactive source simulation (NISS) scenario is considered. In this scenario, a pair of distributed agents, Alice and Bob, observe a distributed binary memoryless source {\textbackslash}(Xˆd,Yˆd){\textbackslash} generated based on joint distribution {\textbackslash}P\_\{X,Y\}{\textbackslash}. The agents wish to produce a pair of discrete random variables {\textbackslash}(U\_d,V\_d){\textbackslash} with joint distribution {\textbackslash}P\_\{U\_d,V\_d\}{\textbackslash}, such that {\textbackslash}P\_\{U\_d,V\_d\}{\textbackslash} converges in total variation distance to a target distribution {\textbackslash}Q\_\{U,V\}{\textbackslash} as the input blocklength {\textbackslash}d{\textbackslash} is taken to be asymptotically large. Inner and outer bounds are obtained on the set of distributions {\textbackslash}Q\_\{U,V\}{\textbackslash} which can be produced given an input distribution {\textbackslash}P\_\{X,Y\}{\textbackslash}. To this end, a bijective mapping from the set of distributions {\textbackslash}Q\_\{U,V\}{\textbackslash} to a union of star-convex sets is provided. By leveraging proof techniques from discrete Fourier analysis along with a novel randomized rounding technique, inner and outer bounds are derived for each of these star-convex sets, and by inverting the aforementioned bijective mapping, necessary and sufficient conditions on {\textbackslash}Q\_\{U,V\}{\textbackslash} and {\textbackslash}P\_\{X,Y\}{\textbackslash} are provided under which {\textbackslash}Q\_\{U,V\}{\textbackslash} can be produced from {\textbackslash}P\_\{X,Y\}{\textbackslash}. The bounds are applicable in NISS scenarios where the output alphabets {\textbackslash}mathcal\{U\}{\textbackslash} and {\textbackslash}mathcal\{V\}{\textbackslash} have arbitrary finite size. In case of binary output alphabets, the outer-bound recovers the previously best-known outer-bound.},
	journal = {arXiv preprint arXiv:2212.09239},
	author = {Shirani, Farhad and Heidari, Mohsen},
	month = dec,
	year = {2022},
	note = {\_eprint: 2212.09239},
}

@article{liu_no_2023,
	title = {No {Need} to {Lift} a {Finger} {Anymore}? {Assessing} the {Quality} of {Code} {Generation} by {ChatGPT}},
	url = {https://arxiv.org/abs/2308.04838v2},
	abstract = {Large language models (LLMs) have demonstrated impressive capabilities across various NLP tasks. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using ChatGPT. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by ChatGPT, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of ChatGPT in tackling code generation tasks over the three critical aspects. Overall, our findings uncover potential issues and limitations that arise in the ChatGPT-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.},
	journal = {arXiv preprint arXiv:2308.04838},
	author = {Liu, Zhijie and Tang, Yutian and Luo, Xiapu and Zhou, Yuming and Zhang, Liang Feng},
	month = aug,
	year = {2023},
	note = {\_eprint: 2308.04838},
}

@article{huang_themis_2025,
	title = {{THEMIS}: {Towards} {Practical} {Intellectual} {Property} {Protection} for {Post}-{Deployment} {On}-{Device} {Deep} {Learning} {Models}},
	url = {https://arxiv.org/abs/2503.23748v1},
	abstract = {On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14\%, showing the practicality of THEMIS.},
	journal = {arXiv preprint arXiv:2503.23748},
	author = {Huang, Yujin and Zhang, Zhi and Zhao, Qingchuan and Yuan, Xingliang and Chen, Chunyang},
	month = mar,
	year = {2025},
	note = {\_eprint: 2503.23748},
	annote = {To Appear in the 34th USENIX Security Symposium, August 13-15, 2025},
}

@article{armengol-estape_forklift_2024,
	title = {Forklift: {An} {Extensible} {Neural} {Lifter}},
	url = {https://arxiv.org/abs/2404.16041v1},
	abstract = {The escalating demand to migrate legacy software across different Instruction Set Architectures (ISAs) has driven the development of assembly-to-assembly translators to map between their respective assembly languages. However, the development of these tools requires substantial engineering effort. State-of-the-art approaches use lifting, a technique where source assembly code is translated to an architecture-independent intermediate representation (IR) (for example, the LLVM IR) and use a pre-existing compiler to recompile the IR to the target ISA. However, the hand-written rules these lifters employ are sensitive to the particular compiler and optimization level used to generate the code and require significant engineering effort to support each new ISA. We propose Forklift, the first neural lifter that learns how to translate assembly to LLVM IR using a token-level encoder-decoder Transformer. We show how to incrementally add support to new ISAs by fine tuning the assembly encoder and freezing the IR decoder, improving the overall accuracy and efficiency. We collect millions of parallel LLVM IR, x86, ARM, and RISC-V programs across compilers and optimization levels to train Forklift and set up an input/output-based accuracy harness. We evaluate Forklift on two challenging benchmark suites and translate 2.5x more x86 programs than a state-of-the-art hand-written lifter and 4.4x more x86 programs than GPT-4 as well as enabling translation from new ISAs.},
	journal = {arXiv preprint arXiv:2404.16041},
	author = {Armengol-Estapé, Jordi and Rocha, Rodrigo C. O. and Woodruff, Jackson and Minervini, Pasquale and O'Boyle, Michael F. P.},
	month = apr,
	year = {2024},
	note = {\_eprint: 2404.16041},
}

@article{zhou_programming_2024,
	title = {Programming {Every} {Example}: {Lifting} {Pre}-training {Data} {Quality} {Like} {Experts} at {Scale}},
	url = {https://arxiv.org/abs/2409.17115v2},
	abstract = {Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2\% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb, FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6\% over Mistral-7B, with 14.6\% for Llama-2-7B and 20.3\% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training. We are open-sourcing ProX with {\textgreater}500B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX},
	journal = {arXiv preprint arXiv:2409.17115},
	author = {Zhou, Fan and Wang, Zengzhi and Liu, Qian and Li, Junlong and Liu, Pengfei},
	month = sep,
	year = {2024},
	note = {\_eprint: 2409.17115},
	annote = {47 pages, 13 figures, 34 tables},
}

@article{bressan_theory_2024,
	title = {A {Theory} of {Interpretable} {Approximations}},
	url = {https://arxiv.org/abs/2406.10529v1},
	abstract = {Can a deep neural network be approximated by a small decision tree based on simple features? This question and its variants are behind the growing demand for machine learning models that are *interpretable* by humans. In this work we study such questions by introducing *interpretable approximations*, a notion that captures the idea of approximating a target concept {\textbackslash}c{\textbackslash} by a small aggregation of concepts from some base class {\textbackslash}mathcal\{H\}{\textbackslash}. In particular, we consider the approximation of a binary concept {\textbackslash}c{\textbackslash} by decision trees based on a simple class {\textbackslash}mathcal\{H\}{\textbackslash} (e.g., of bounded VC dimension), and use the tree depth as a measure of complexity. Our primary contribution is the following remarkable trichotomy. For any given pair of {\textbackslash}mathcal\{H\}{\textbackslash} and {\textbackslash}c{\textbackslash}, exactly one of these cases holds: (i) {\textbackslash}c{\textbackslash} cannot be approximated by {\textbackslash}mathcal\{H\}{\textbackslash} with arbitrary accuracy; (ii) {\textbackslash}c{\textbackslash} can be approximated by {\textbackslash}mathcal\{H\}{\textbackslash} with arbitrary accuracy, but there exists no universal rate that bounds the complexity of the approximations as a function of the accuracy; or (iii) there exists a constant {\textbackslash}κ{\textbackslash} that depends only on {\textbackslash}mathcal\{H\}{\textbackslash} and {\textbackslash}c{\textbackslash} such that, for *any* data distribution and *any* desired accuracy level, {\textbackslash}c{\textbackslash} can be approximated by {\textbackslash}mathcal\{H\}{\textbackslash} with a complexity not exceeding {\textbackslash}κ{\textbackslash}. This taxonomy stands in stark contrast to the landscape of supervised classification, which offers a complex array of distribution-free and universally learnable scenarios. We show that, in the case of interpretable approximations, even a slightly nontrivial a-priori guarantee on the complexity of approximations implies approximations with constant (distribution-free and accuracy-free) complexity. We extend our trichotomy to classes {\textbackslash}mathcal\{H\}{\textbackslash} of unbounded VC dimension and give characterizations of interpretability based on the algebra generated by {\textbackslash}mathcal\{H\}{\textbackslash}.},
	journal = {arXiv preprint arXiv:2406.10529},
	author = {Bressan, Marco and Cesa-Bianchi, Nicolò and Esposito, Emmanuel and Mansour, Yishay and Moran, Shay and Thiessen, Maximilian},
	month = jun,
	year = {2024},
	note = {\_eprint: 2406.10529},
	annote = {To appear at COLT 2024},
}

@article{wang_repomaster_2025,
	title = {{RepoMaster}: {Autonomous} {Exploration} and {Understanding} of {GitHub} {Repositories} for {Complex} {Task} {Solving}},
	url = {https://arxiv.org/abs/2505.21577v3},
	abstract = {The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110\% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7\% to 62.9\% while reducing token usage by 95\%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster.},
	journal = {arXiv preprint arXiv:2505.21577},
	author = {Wang, Huacan and Ni, Ziyi and Zhang, Shuo and Lu, Shuo and Hu, Sen and He, Ziyang and Hu, Chen and Lin, Jiaye and Guo, Yifu and Chen, Ronghao and Li, Xin and Jiang, Daxin and Du, Yuntao and Lyu, Pin},
	month = may,
	year = {2025},
	note = {\_eprint: 2505.21577},
	annote = {A novel approach; Very practical},
}

@article{aguirre_pre-expectation_2019,
	title = {A {Pre}-{Expectation} {Calculus} for {Probabilistic} {Sensitivity}},
	url = {https://arxiv.org/abs/1901.06540v2},
	abstract = {Sensitivity properties describe how changes to the input of a program affect the output, typically by upper bounding the distance between the outputs of two runs by a monotone function of the distance between the corresponding inputs. When programs are probabilistic, the distance between outputs is a distance between distributions. The Kantorovich lifting provides a general way of defining a distance between distributions by lifting the distance of the underlying sample space; by choosing an appropriate distance on the base space, one can recover other usual probabilistic distances, such as the Total Variation distance. We develop a relational pre-expectation calculus to upper bound the Kantorovich distance between two executions of a probabilistic program. We illustrate our methods by proving algorithmic stability of a machine learning algorithm, convergence of a reinforcement learning algorithm, and fast mixing for card shuffling algorithms. We also consider some extensions: proving lower bounds on the Total Variation distance and convergence to the uniform distribution. Finally, we describe an asynchronous extension of our calculus to reason about pairs of program executions with different control flow.},
	journal = {arXiv preprint arXiv:1901.06540},
	author = {Aguirre, Alejandro and Barthe, Gilles and Hsu, Justin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph},
	month = jan,
	year = {2019},
	note = {\_eprint: 1901.06540},
	annote = {Major revision},
}

@article{yang_energy-efficient_2019,
	title = {Energy-{Efficient} {Processing} and {Robust} {Wireless} {Cooperative} {Transmission} for {Edge} {Inference}},
	url = {https://arxiv.org/abs/1907.12475v2},
	abstract = {Edge machine learning can deliver low-latency and private artificial intelligent (AI) services for mobile devices by leveraging computation and storage resources at the network edge. This paper presents an energy-efficient edge processing framework to execute deep learning inference tasks at the edge computing nodes whose wireless connections to mobile devices are prone to channel uncertainties. Aimed at minimizing the sum of computation and transmission power consumption with probabilistic quality-of-service (QoS) constraints, we formulate a joint inference tasking and downlink beamforming problem that is characterized by a group sparse objective function. We provide a statistical learning based robust optimization approach to approximate the highly intractable probabilistic-QoS constraints by nonconvex quadratic constraints, which are further reformulated as matrix inequalities with a rank-one constraint via matrix lifting. We design a reweighted power minimization approach by iteratively reweighted {\textbackslash}ell\_1{\textbackslash} minimization with difference-of-convex-functions (DC) regularization and updating weights, where the reweighted approach is adopted for enhancing group sparsity whereas the DC regularization is designed for inducing rank-one solutions. Numerical results demonstrate that the proposed approach outperforms other state-of-the-art approaches.},
	journal = {arXiv preprint arXiv:1907.12475},
	author = {Yang, Kai and Shi, Yuanming and Yu, Wei and Ding, Zhi},
	month = jul,
	year = {2019},
	note = {\_eprint: 1907.12475},
	annote = {This paper has been accepted by IEEE Internet of Things Journal},
}

@article{hu_optimizing_2022,
	title = {Optimizing {Binary} {Decision} {Diagrams} with {MaxSAT} for classification},
	url = {https://arxiv.org/abs/2203.11386v1},
	abstract = {The growing interest in explainable artificial intelligence (XAI) for critical decision making motivates the need for interpretable machine learning (ML) models. In fact, due to their structure (especially with small sizes), these models are inherently understandable by humans. Recently, several exact methods for computing such models are proposed to overcome weaknesses of traditional heuristic methods by providing more compact models or better prediction quality. Despite their compressed representation of Boolean functions, Binary decision diagrams (BDDs) did not gain enough interest as other interpretable ML models. In this paper, we first propose SAT-based models for learning optimal BDDs (in terms of the number of features) that classify all input examples. Then, we lift the encoding to a MaxSAT model to learn optimal BDDs in limited depths, that maximize the number of examples correctly classified. Finally, we tackle the fragmentation problem by introducing a method to merge compatible subtrees for the BDDs found via the MaxSAT model. Our empirical study shows clear benefits of the proposed approach in terms of prediction quality and intrepretability (i.e., lighter size) compared to the state-of-the-art approaches.},
	journal = {arXiv preprint arXiv:2203.11386},
	author = {Hu, Hao and Huguet, Marie-José and Siala, Mohamed},
	month = mar,
	year = {2022},
	note = {\_eprint: 2203.11386},
	annote = {This is the preprint version of the paper accepted in AAAI'22},
}

@article{stojnic_capacity_2024,
	title = {Capacity of the {Hebbian}-{Hopfield} network associative memory},
	url = {https://arxiv.org/abs/2403.01907v1},
	abstract = {In {\textbackslash}cite\{Hop82\}, Hopfield introduced a {\textbackslash}emph\{Hebbian\} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, {\textbackslash}m{\textbackslash}) scales linearly with each pattern's size, {\textbackslash}n{\textbackslash}. Moreover, he famously predicted {\textbackslash}α\_c={\textbackslash}łim\_\{n{\textbackslash}rightarrow{\textbackslash}ınfty\}{\textbackslash}frac\{m\}\{n\}{\textbackslash}approx 0.14{\textbackslash}. We study this very same scenario with two famous pattern's basins of attraction: {\textbackslash}textbf\{{\textbackslash}emph\{(i)\}\} The AGS one from {\textbackslash}cite\{AmiGutSom85\}; and {\textbackslash}textbf\{{\textbackslash}emph\{(ii)\}\} The NLT one from {\textbackslash}cite\{Newman88,Louk94,Louk94a,Louk97,Tal98\}. Relying on the {\textbackslash}emph\{fully lifted random duality theory\} (fl RDT) from {\textbackslash}cite\{Stojnicflrdt23\}, we obtain the following explicit capacity characterizations on the first level of lifting: {\textbackslash}begin\{equation\} α\_cˆ\{(AGS,1)\} = {\textbackslash}łeft ( {\textbackslash}max\_\{δ{\textbackslash}ın {\textbackslash}łeft ( 0,{\textbackslash}frac\{1\}\{2\}{\textbackslash}right ) \}{\textbackslash}frac\{1-2δ\}\{{\textbackslash}sqrt\{2\} {\textbackslash}mbox\{erfinv\} {\textbackslash}łeft ( 1-2δ{\textbackslash}right )\} - {\textbackslash}frac\{2\}\{{\textbackslash}sqrt\{2π\}\} eˆ\{-{\textbackslash}łeft ( {\textbackslash}mbox\{erfinv\}{\textbackslash}łeft ( 1-2δ{\textbackslash}right ){\textbackslash}right )ˆ2\}{\textbackslash}right )ˆ2 {\textbackslash}approx {\textbackslash}mathbf\{0.137906\} {\textbackslash}end\{equation\} {\textbackslash}begin\{equation\} α\_cˆ\{(NLT,1)\} = {\textbackslash}frac\{{\textbackslash}mbox\{erf\}(x)ˆ2\}\{2xˆ2\}-1+{\textbackslash}mbox\{erf\}(x)ˆ2 {\textbackslash}approx {\textbackslash}mathbf\{0.129490\}, {\textbackslash}quad 1-{\textbackslash}mbox\{erf\}(x)ˆ2- {\textbackslash}frac\{2{\textbackslash}mbox\{erf\}(x)eˆ\{-xˆ2\}\}\{{\textbackslash}sqrtπx\}+{\textbackslash}frac\{2eˆ\{-2xˆ2\}\}π=0. {\textbackslash}end\{equation\} A substantial numerical work gives on the second level of lifting {\textbackslash}α\_cˆ\{(AGS,2)\} {\textbackslash}approx {\textbackslash}mathbf\{0.138186\}{\textbackslash} and {\textbackslash}α\_cˆ\{(NLT,2)\} {\textbackslash}approx {\textbackslash}mathbf\{0.12979\}{\textbackslash}, effectively uncovering a remarkably fast lifting convergence. Moreover, the obtained AGS characterizations exactly match the replica symmetry based ones of {\textbackslash}cite\{AmiGutSom85\} and the corresponding symmetry breaking ones of {\textbackslash}cite\{SteKuh94\}.},
	journal = {arXiv preprint arXiv:2403.01907},
	author = {Stojnic, Mihailo},
	month = mar,
	year = {2024},
	note = {\_eprint: 2403.01907},
}

@article{tepper_procrustean_2020,
	title = {Procrustean {Orthogonal} {Sparse} {Hashing}},
	url = {https://arxiv.org/abs/2006.04847v1},
	abstract = {Hashing is one of the most popular methods for similarity search because of its speed and efficiency. Dense binary hashing is prevalent in the literature. Recently, insect olfaction was shown to be structurally and functionally analogous to sparse hashing [6]. Here, we prove that this biological mechanism is the solution to a well-posed optimization problem. Furthermore, we show that orthogonality increases the accuracy of sparse hashing. Next, we present a novel method, Procrustean Orthogonal Sparse Hashing (POSH), that unifies these findings, learning an orthogonal transform from training data compatible with the sparse hashing mechanism. We provide theoretical evidence of the shortcomings of Optimal Sparse Lifting (OSL) [22] and BioHash [30], two related olfaction-inspired methods, and propose two new methods, Binary OSL and SphericalHash, to address these deficiencies. We compare POSH, Binary OSL, and SphericalHash to several state-of-the-art hashing methods and provide empirical results for the superiority of the proposed methods across a wide range of standard benchmarks and parameter settings.},
	journal = {arXiv preprint arXiv:2006.04847},
	author = {Tepper, Mariano and Sengupta, Dipanjan and Willke, Ted},
	month = jun,
	year = {2020},
	note = {\_eprint: 2006.04847},
}

@article{zhang_parameterized_2024,
	title = {Parameterized {Dynamic} {Logic} – {Towards} {A} {Cyclic} {Logical} {Framework} for {General} {Program} {Specification} and {Verification}},
	url = {https://arxiv.org/abs/2404.18098v4},
	abstract = {We present a theory of parameterized dynamic logic, namely DLp, for specifying and reasoning about a rich set of program models based on their transitional behaviours. Different from most dynamic logics that deal with regular expressions or a particular type of formalisms, DLp introduces a type of labels called "program configurations" as explicit program status for symbolic executions, allowing programs and formulas to be of arbitrary forms according to interested domains. This characteristic empowers dynamic logical formulas with a direct support of symbolic-execution-based reasoning, while still maintaining reasoning based on syntactic structures in traditional dynamic logics through a rule-lifting process. We propose a proof system and build a cyclic preproof structure special for DLp, which guarantees the soundness of infinite proof trees induced by symbolically executing programs with explicit/implicit loop structures. The soundness of DLp is formally analyzed and proved. DLp provides a flexible verification framework based on the theories of dynamic logics. It helps reduce the burden of developing different dynamic-logic theories for different programs, and save the additional transformations in the derivations of non-compositional programs. We give some examples of instantiations of DLp in particular domains, showing the potential and advantages of using DLp in practical usage.},
	journal = {arXiv preprint arXiv:2404.18098},
	author = {Zhang, Yuanrui},
	month = apr,
	year = {2024},
	note = {\_eprint: 2404.18098},
	annote = {Major revisions from last comments: 1. fix the whole proof system of DLp and its related proofs; 2. add additional two examples for illustrations of lifting processes and an implication of a more complex configuration; 3. further revise the introduction part to adapt these changes; 4. add a formal definition of while programs in the logic},
}

@article{delabeye_unsupervised_2023,
	title = {Unsupervised {Complex} {Semi}-{Binary} {Matrix} {Factorization} for {Activation} {Sequence} {Recovery} of {Quasi}-{Stationary} {Sources}},
	url = {https://arxiv.org/abs/2310.02295v1},
	abstract = {Advocating for a sustainable, resilient and human-centric industry, the three pillars of Industry 5.0 call for an increased understanding of industrial processes and manufacturing systems, as well as their energy sustainability. One of the most fundamental elements of comprehension is knowing when the systems are operated, as this is key to locating energy intensive subsystems and operations. Such knowledge is often lacking in practice. Activation statuses can be recovered from sensor data though. Some non-intrusive sensors (accelerometers, current sensors, etc.) acquire mixed signals containing information about multiple actuators at once. Despite their low cost as regards the fleet of systems they monitor, additional signal processing is required to extract the individual activation sequences. To that end, sparse regression techniques can extract leading dynamics in sequential data. Notorious dictionary learning algorithms have proven effective in this regard. This paper considers different industrial settings in which the identification of binary subsystem activation sequences is sought. In this context, it is assumed that each sensor measures an extensive physical property, source signals are periodic, quasi-stationary and independent, albeit these signals may be correlated and their noise distribution is arbitrary. Existing methods either restrict these assumptions, e.g., by imposing orthogonality or noise characteristics, or lift them using additional assumptions, typically using nonlinear transforms.},
	journal = {arXiv preprint arXiv:2310.02295},
	author = {Delabeye, Romain and Ghienne, Martin and Penas, Olivia and Dion, Jean-Luc},
	month = oct,
	year = {2023},
	note = {\_eprint: 2310.02295},
}

@article{heunen_quantum_2025,
	title = {Quantum {Circuits} {Are} {Just} a {Phase}},
	url = {https://arxiv.org/abs/2507.11676v2},
	doi = {10.1145/3776731},
	abstract = {Quantum programs today are written at a low level of abstraction - quantum circuits akin to assembly languages - and the unitary parts of even advanced quantum programming languages essentially function as circuit description languages. This state of affairs impedes scalability, clarity, and support for higher-level reasoning. More abstract and expressive quantum programming constructs are needed. To this end, we introduce a simple syntax for generating unitaries from "just a phase"; we combine a (global) phase operation that captures phase shifts with a quantum analogue of the "if let" construct that captures subspace selection via pattern matching. This minimal language lifts the focus from gates to eigendecomposition, conjugation, and controlled unitaries; common building blocks in quantum algorithm design. We demonstrate several aspects of the expressive power of our language in several ways. Firstly, we establish that our representation is universal by deriving a universal quantum gate set. Secondly, we show that important quantum algorithms can be expressed naturally and concisely, including Grover's search algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give clean denotational semantics grounded in categorical quantum mechanics. Finally, we implement a prototype compiler that efficiently translates terms of our language to quantum circuits, and prove that it is sound with respect to these semantics. Collectively, these contributions show that this construct offers a principled and practical step toward more abstract and structured quantum programming.},
	journal = {arXiv preprint arXiv:2507.11676},
	author = {Heunen, Chris and Lemonnier, Louis and McNally, Christopher and Rice, Alex},
	month = jul,
	year = {2025},
	note = {\_eprint: 2507.11676},
	annote = {42 pages, 5 figures},
}

@article{mayoral-vilches_cybersecurity_2026,
	title = {Cybersecurity {AI}: {A} {Game}-{Theoretic} {AI} for {Guiding} {Attack} and {Defense}},
	url = {https://arxiv.org/abs/2601.05887v1},
	abstract = {AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence –Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop {\textbackslash}emph\{guiding\} the agent's actions. Across five real-world exercises, G-CTR matches 70–90\% of expert graph structure while running 60–245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0\% to 42.9\%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.},
	journal = {arXiv preprint arXiv:2601.05887},
	author = {Mayoral-Vilches, Víctor and Sanz-Gómez, María and Balassone, Francesco and Rass, Stefan and Salas-Espejo, Lidia and Jablonski, Benjamin and Navarrete-Lozano, Luis Javier and Torres, Maite del Mundo de and Chavez, Cristóbal R. J. Veas},
	month = jan,
	year = {2026},
	note = {\_eprint: 2601.05887},
}

@article{gross_sift_2025,
	title = {Sift or {Get} {Off} the {PoC}: {Applying} {Information} {Retrieval} to {Vulnerability} {Research} with {SiftRank}},
	url = {https://arxiv.org/abs/2512.06155v1},
	abstract = {Security research is fundamentally a problem of resource constraint and consequent prioritization. There is simply too much attack surface and too little time and energy to spend analyzing it all. The most effective security researchers are often those who are most skilled at intuitively deciding which part of an expansive attack surface to investigate. We demonstrate that this problem of selecting the most promising option from among many possibilities can be reframed as an information retrieval problem, and solved using document ranking techniques with LLMs performing the heavy lifting as general-purpose rankers. We present SiftRank, a ranking algorithm achieving O(n) complexity through three key mechanisms: listwise ranking using an LLM to order documents in small batches of approximately 10 items at a time; inflection-based convergence detection that adaptively terminates ranking when score distributions have stabilized; and iterative refinement that progressively focuses ranking effort on the most relevant documents. Unlike existing reranking approaches that require a separate first-stage retrieval step to narrow datasets to approximately 100 candidates, SiftRank operates directly on thousands of items, with each document evaluated across multiple randomized batches to mitigate inconsistent judgments by an LLM. We demonstrate practical effectiveness on N-day vulnerability analysis, successfully identifying a vulnerability-fixing function among 2,197 changed functions in a stripped binary firmware patch within 99 seconds at an inference cost of \$0.82. Our approach enables scalable security prioritization for problems that are generally constrained by manual analysis, requiring only standard LLM API access without specialized infrastructure, embedding, or domain-specific fine-tuning. An open-source implementation of SiftRank may be found at https://github.com/noperator/siftrank.},
	journal = {arXiv preprint arXiv:2512.06155},
	author = {Gross, Caleb},
	month = dec,
	year = {2025},
	note = {\_eprint: 2512.06155},
}

@article{paul_masked_2023,
	title = {Masked {Autoencoders} are {Efficient} {Continual} {Federated} {Learners}},
	url = {https://arxiv.org/abs/2306.03542v2},
	abstract = {Machine learning is typically framed from a perspective of i.i.d., and more importantly, isolated data. In parts, federated learning lifts this assumption, as it sets out to solve the real-world challenge of collaboratively learning a shared model from data distributed across clients. However, motivated primarily by privacy and computational constraints, the fact that data may change, distributions drift, or even tasks advance individually on clients, is seldom taken into account. The field of continual learning addresses this separate challenge and first steps have recently been taken to leverage synergies in distributed supervised settings, in which several clients learn to solve changing classification tasks over time without forgetting previously seen ones. Motivated by these prior works, we posit that such federated continual learning should be grounded in unsupervised learning of representations that are shared across clients; in the loose spirit of how humans can indirectly leverage others' experience without exposure to a specific task. For this purpose, we demonstrate that masked autoencoders for distribution estimation are particularly amenable to this setup. Specifically, their masking strategy can be seamlessly integrated with task attention mechanisms to enable selective knowledge transfer between clients. We empirically corroborate the latter statement through several continual federated scenarios on both image and binary datasets.},
	journal = {arXiv preprint arXiv:2306.03542},
	author = {Paul, Subarnaduti and Frey, Lars-Joel and Kamath, Roshni and Kersting, Kristian and Mundt, Martin},
	month = jun,
	year = {2023},
	note = {\_eprint: 2306.03542},
}

@inproceedings{alrabaee_bindeep_2021,
	title = {{BinDeep}: {Binary} to {Source} {Code} {Matching} {Using} {Deep} {Learning}},
	issn = {2324-9013},
	doi = {10.1109/TrustCom53373.2021.00150},
	abstract = {Mapping a binary function taken from a compiled binary to the same function in the original source code has many security applications, such as discovering reused free open source code in malware binaries. To facilitate malware analysis, we present BINDEEP, a framework that learns the semantic relationships among binary functions based on assembly code. It also learns semantic information about the source functions in order to carry out function matching. We demonstrate how BINDEEP can be applied to fingerprint the origin of functions in malware binaries, and then benchmark its performance against that of five competing systems (i.e., RESOURCE, the Binary Analysis Tool (BAT), BinPro, Statistical Machine Translation (SMT), and FOSSIL). The findings show that BINDEEP is more robust and achieves significant improvement over these existing systems when confronted with changes introduced by code transformation methods or the use of different compilers and optimization levels. Furthermore, BINDEEP is able to discover source packages in malware binaries, such as Zeus and Citadel, that match those listed in existing security reports.},
	booktitle = {2021 {IEEE} 20th {International} {Conference} on {Trust}, {Security} and {Privacy} in {Computing} and {Communications} ({TrustCom})},
	author = {Alrabaee, Saed and Choo, Kim-Kwang Raymond and Qbea'h, Mohammad and Khasawneh, Mahmoud},
	month = oct,
	year = {2021},
	keywords = {Codes, Privacy, Program processors, Reverse engineering, Semantics, Malware, machine learning, Manuals, malicious code, binary code},
	pages = {1100--1107},
}

@inproceedings{gao_grayscale_2010,
	title = {Grayscale watermarking resistant to geometric attacks based on lifting wavelet transform and neural network},
	doi = {10.1109/WCICA.2010.5554891},
	abstract = {A blind robust grayscale watermarking scheme that resists geometric attacks is proposed. Firstly, 1-level lifting wavelet transform (LWT) is performed on the cover image and the scrambled grayscale watermarking image, and then the integral wavelet coefficients of the watermarking image are translated into binary bits, which are subsequently embedded into the corresponding frequency domains of the cover image according to perceptual importance. Secondly, to predict the geometric transformation parameters of the attacked image, low-order Tchebichef moments as eigenvectors and an improved back propagation (BP) neural network are utilized to construct the forecasting model, by which the attacked image can be geometrically corrected. Finally the watermarking is extracted from the corrected image. Simulation results show that the proposed scheme is robust to both conventional signal processing and general geometric attacks.},
	booktitle = {2010 8th {World} {Congress} on {Intelligent} {Control} and {Automation}},
	author = {Gao, Guangyong and Jiang, Guoping},
	month = jul,
	year = {2010},
	keywords = {Training, Robustness, Watermarking, neural network, Chaos, geometric attacks, Grayscale watermarking, LWT, Pixel, Tchebichef moments, Wavelet transforms},
	pages = {1305--1310},
}

@inproceedings{jiang_binaryai_2024-1,
	title = {{BinaryAI}: {Binary} {Software} {Composition} {Analysis} via {Intelligent} {Binary} {Source} {Code} {Matching}},
	issn = {1558-1225},
	doi = {10.1145/3597503.3639100},
	abstract = {While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@l and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product Black Duck. E-https://www.binaryai.net},
	booktitle = {2024 {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
	month = apr,
	year = {2024},
	keywords = {Codes, Source coding, Software, Transformers, Semantics, Syntactics, Redundancy, Software Composition Analysis, Static Binary Analysis},
	pages = {2771--2783},
}

@inproceedings{tehranijamsaz_graphbinmatch_2024,
	title = {{GraphBinMatch}: {Graph}-{Based} {Similarity} {Learning} for {Cross}-{Language} {Binary} and {Source} {Code} {Matching}},
	doi = {10.1109/IPDPSW63119.2024.00103},
	abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate the performance of our approach on single-language binary-to-source code matching. Experimental results show that GraphBinMatch significantly outperforms state-of-the-art, with improvements as high as 15\% over the F1 score.},
	booktitle = {2024 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {TehraniJamsaz, Ali and Chen, Hanze and Jannesari, Ali},
	month = may,
	year = {2024},
	keywords = {Binary codes, Source coding, Reverse engineering, Graph neural networks, Computer languages, binary-source matching, code similarity, cross-language, Distributed processing, Programming},
	pages = {506--515},
}

@inproceedings{donisi_machine_2022,
	title = {Machine {Learning} and {Biosignals} are able to discriminate biomechanical risk classes according to the {Revised} {NIOSH} {Lifting} {Equation}},
	doi = {10.1109/MetroXRAINE54828.2022.9967528},
	abstract = {Many work activities can imply a biomechanical overload. Among these activities, lifting loads may determine work-related musculoskeletal disorders. To limit injuries, the National Institute for Occupational Safety and Health (NIOSH) proposed a methodology to assess biomechanical risk in lifting tasks through an equation based on intensity, duration, frequency and other geometrical characteristics of lifting tasks. In this work, we explored the feasibility of tree-based machine learning algorithms to classify biomechanical risk according to the Revised NIOSH lifting equation). Electromyography signals acquired from the biceps and sternum acceleration signals collected during lifting loads were registered using a wearable sensor (BITalino (r)evolution) worn by 5 healthy young subjects. Electromyography and acceleration signals were segmented as to extract the region of interest related to the lifting actions and, for each region of interest, several time and frequency domain features were extracted. Interesting results were obtained in terms of evaluation metrics for a binary risk/no-risk classification. In conclusion, this work indicates the proposed combination of features and machine learning algorithms represents a valid approach to automatically classify risk activities according to the Revised NIOSH lifting equation. Future investigation on enriched study populations could confirm the capabilities of this methodology to automatically classify potential risky activities.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Metrology} for {Extended} {Reality}, {Artificial} {Intelligence} and {Neural} {Engineering} ({MetroXRAINE})},
	author = {Donisi, Leandro and Cesarelli, Giuseppe and Capodaglio, Edda and Panigazzi, Monica and Cesarelli, Mario and D’Addio, Giovanni},
	month = oct,
	year = {2022},
	keywords = {Feature extraction, machine learning, Machine learning algorithms, bicep muscle, Biomechanical risk assessment, Biomechanics, biosignal segmentation, Electromyography, ergonomics, lifting, Musculoskeletal system, Revised NIOSH lifting equation, Sociology, Sternum, sternum acceleration, work-related musculoskeletal disorders},
	pages = {346--351},
}

@inproceedings{prisco_feasibility_2023,
	title = {Feasibility of {Tree}-{Based} {Machine} {Learning} {Models} to {Discriminate} {Safe} and {Unsafe} {Posture} {During} {Weight} {Lifting}},
	doi = {10.1109/MetroXRAINE58569.2023.10405830},
	abstract = {The weight lifting is defined as any activity requiring the use of human force to lift or move a load which can be potentially harmful of onsetting work-related musculoskeletal disorders. The purpose of this study was to explore the feasibility of four tree-based Machine Learning (ML) models - fed with time-domain features extracted from signals acquired by means of one inertial measurement unit (IMU) - to classify safe and unsafe postures during weight lifting. Inertial signals -linear acceleration and angular velocity - acquired from sternum of 4 healthy subjects were registered using the Mobility Lab System. The signals were manually segmented in order to extract for each region of interest, corresponding to the lifting, several time-domain features. Four tree-based predictive models - namely Decision Tree, Random Forest, Rotation Forest and AdaBoost Tree - were implemented and their performances were tested. Interesting results in terms of evaluation metrics for a binary safe/unsafe posture classification were obtained with accuracy values greater than 93\%. In conclusion the present study indicated that tree-based ML models fed with specific features were able to discriminate safe and unsafe posture during weight lifting using only one IMU placed on the sternum. Future investigation on larger cohort could confirm the potential of the proposed methodology.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Metrology} for {eXtended} {Reality}, {Artificial} {Intelligence} and {Neural} {Engineering} ({MetroXRAINE})},
	author = {Prisco, Giuseppe and Romano, Maria and Esposito, Fabrizio and Cesarelli, Mario and Santone, Antonella and Donisi, Leandro},
	month = oct,
	year = {2023},
	keywords = {Feature extraction, machine learning, feature extraction, Random forests, lifting, Musculoskeletal system, Sternum, work-related musculoskeletal disorders, health monitoring, inertial measurement unit, Measurement units, occupational ergonomics, safe/unsafe posture, Time-domain analysis, wearable sensors, weight lifting, Weight measurement},
	pages = {870--875},
}

@inproceedings{boufama_deep-learning_2024,
	title = {A {Deep}-{Learning} {Approach} for {Task} {Recognition} of {Industrial} {Workers} and {RULA} {Score} {Calculation}},
	issn = {2832-8337},
	doi = {10.1109/ISIVC61350.2024.10577819},
	abstract = {Human activity has been closely related to the development of musculoskeletal disorders. Workers in industries such as manufacturing, assembly, and construction often engage in repetitive and various motions, such as lifting heavy objects or performing the same task for a long period. Because such activities can increase the risk of various muscular disorders, it is important to help workers choose the best postures when performing their activities. In this study, we propose a deep learning approach for task recognition and RULA score calculation. Our approach uses a revised version of the Long Term Recurrent Convolutional Neural network-based model to classify work activities based on video input and then applies a separate neural network to estimate the RULA score for each input activity. We trained and evaluated our approach using a dataset of annotated work activities. Our results show that our approach achieves competitive accuracy for activity recognition and RULA score estimation, demonstrating the potential of deep learning for improving ergonomic assessments in the workplace.},
	booktitle = {2024 {IEEE} 12th {International} {Symposium} on {Signal}, {Image}, {Video} and {Communications} ({ISIVC})},
	author = {Boufama, Boubakeur and Hussein, Sonaila and Kim, Eunsik and Ahmad, Imran},
	month = may,
	year = {2024},
	keywords = {Deep learning, Neural networks, Industries, Musculoskeletal system, Employment, Ergonomics, Human activity recognition, Image recognition, LRCN, musculoskeletal disorders, RULA},
	pages = {1--6},
}

@inproceedings{xu_biomechtronic_2019,
	title = {Biomechtronic {Design} of a {Supernumerary} {Robotic} {Limbs} for {Industrial} {Assembly}},
	doi = {10.1109/ICARM.2019.8833774},
	abstract = {We present a new wearable robot that can help users by hand-held objects, weight lifting, and simplified tasks. The composition of the supernumerary robotic limbs includes two additional robot arms and an installation backplane that is easy for the wearer to wear. If SRL works closely with the user and acts more human-like, they can be seen as a part of human's body. Finally, humans will be able to expand existing operations and upgrade their skills, carry out tasks more effectively. This paper summarizes a basic design concept of SRL, establishes the kinematics model and dynamic model of the SRL. The adaptive neural network control method for SRL are described, aimed to track the trajectory.},
	booktitle = {2019 {IEEE} 4th {International} {Conference} on {Advanced} {Robotics} and {Mechatronics} ({ICARM})},
	author = {Xu, Cuichao and Liu, Yueyue and Li, Zhijun},
	month = jul,
	year = {2019},
	keywords = {Task analysis, Actuators, Kinematics, Service robots, Stress, Torque},
	pages = {553--558},
}

@inproceedings{delpreto_sharing_2019,
	title = {Sharing the {Load}: {Human}-{Robot} {Team} {Lifting} {Using} {Muscle} {Activity}},
	issn = {2577-087X},
	doi = {10.1109/ICRA.2019.8794414},
	abstract = {Seamless communication of desired motions and goals is essential for enabling effective physical human-robot collaboration. In such cases, muscle activity measured via surface electromyography (EMG) can provide insight into a person's intentions while minimally distracting from the task. The presented system uses two muscle signals to create a control framework for team lifting tasks in which a human and robot lift an object together. A continuous setpoint algorithm uses biceps activity to estimate changes in the user's hand height, and also allows the user to explicitly adjust the robot by stiffening or relaxing their arm. In addition to this pipeline, a neural network trained only on previous users classifies biceps and triceps activity to detect up or down gestures on a rolling basis; this enables finer control over the robot and expands the feasible workspace. The resulting system is evaluated by 10 untrained subjects performing a variety of team lifting and assembly tasks with rigid and flexible objects.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {DelPreto, Joseph and Rus, Daniela},
	month = may,
	year = {2019},
	keywords = {Task analysis, Pipelines, Signal processing algorithms, Electromyography, Muscles, Robot kinematics},
	pages = {7906--7912},
}

@article{postma_electromagnetic_1998,
	title = {Electromagnetic flat-faced robot gripper for handling multiple industrial transformer core lamination plates},
	volume = {34},
	issn = {1941-0069},
	doi = {10.1109/20.668070},
	abstract = {In the industrial transformer core assembly process, significant productivity gains can be achieved by utilizing a robotic gripper that is able to handle, and accurately position, multiple transformer laminations during each pick-and-place cycle. This paper reports on the development of a novel electromagnetic lifter for such applications. The lifter has the unique capability to selectively pick a given number of laminations at a time (the usual requirement is three) from a stack. By considering an analytical model of the magnetic circuit of the electromagnet-lamination stack combination, closed form solutions are obtained for the flux flow pattern and the lifting force on each lamination in the stack which closely agrees with the numerical results obtained using two-dimensional finite element analysis software. Results of experiments conducted on a prototype electromagnet are also presented that validate the analytical model findings.},
	number = {3},
	journal = {IEEE Transactions on Magnetics},
	author = {Postma, B. and Vinay, T. and Kangsanant, T. and Harvey, A.},
	month = may,
	year = {1998},
	keywords = {Analytical models, Service robots, Closed-form solution, Electrical equipment industry, Grippers, Lamination, Magnetic circuits, Productivity, Robotic assembly, Transformer cores},
	pages = {700--707},
}

@inproceedings{feng_novel_2023,
	title = {A {Novel} {Binary} {Classification} {Algorithm} for {Carpal} {Tunnel} {Syndrome} {Detection} {Using} {LSTM}},
	doi = {10.1109/SEAI59139.2023.10217512},
	abstract = {Carpal tunnel syndrome (CTS) is one of the common neurological disorders caused by prolonged compression of the median nerve. Thus, CTS patients’ daily tasks are significantly affected. Traditional diagnostic methods are invasive or subjective, causing pain or inaccuracy. Therefore, a more accurate machine/deep learning classifier is needed to provide an accessible assessment approach that can help screen out early-stage patients to prevent further deterioration. Behavioral biomechanics has shown great potential to be used for CTS and its severity classification. The biomechanical parameters are collected when identified patients and healthy individuals perform daily life activities, such as grasping and lifting in a controlled manner. Facing the challenges of time series biomechanical data with small sample sizes and high dimensions, we propose a novel classification algorithm to create an ensemble model for CTS detection using Long Short-Term Memory (LSTM). The proposed algorithm achieves 93\% accuracy on average for CTS detection using biomechanical data of daily life activities.},
	booktitle = {2023 {IEEE} 3rd {International} {Conference} on {Software} {Engineering} and {Artificial} {Intelligence} ({SEAI})},
	author = {Feng, Weicong and Zhang, Wei and Meng, Meng and Gong, Yifei and Gu, Feng},
	month = jun,
	year = {2023},
	keywords = {Software algorithms, deep learning, classification, machine learning, Classification algorithms, ensemble learning, Machine learning algorithms, LSTM, Biomechanics, CTS, Neurological diseases, Pain, RNN, time series, Time series analysis},
	pages = {143--147},
}

@inproceedings{hosseini_assessing_2024,
	title = {Assessing the risk of musculoskeletal injuries in a car seat assembly line using a novel pose estimation method},
	doi = {10.1109/ICBME64381.2024.10895149},
	abstract = {Musculoskeletal disorders are highly common among workers performing manual material handling tasks thus resulting in significant costs. This study focused on the risk assessments of occupational activities related to installing seats in a car assembly line. Pose estimation was conducted using a neural network-based method and motion analysis techniques. To evaluate the risk of musculoskeletal injuries during sequential tasks of lifting the seat from the trolley, pulling, lifting the seat beside the car, and installing the seat, 4 biomechanical tools (e.g., OpenSim) and 17 ergonomic tools (e.g., NIOSH and WISHA) were utilized. It was found that the lifting activity beside the car created the highest risk of injury. The study proposed three interventions that effectively managed the injury risk.},
	booktitle = {2024 31st {National} and 9th {International} {Iranian} {Conference} on {Biomedical} {Engineering} ({ICBME})},
	author = {Hosseini, Milad and Fereydani, Koosha Abdolah and Khezri, Matin and Hallaji, Asma and Arjmand, Navid},
	month = jan,
	year = {2024},
	keywords = {Manuals, Assembly, Musculoskeletal system, Ergonomics, Automobiles, Biomechanical Tools, Ergonomic Tools, Injuries, Intervention, Materials handling, Motion analysis, Neural Network, Pose estimation, Pose Estimation, Risk Assessment, Risk management},
	pages = {365--371},
}

@inproceedings{donisi_feasibility_2022,
	title = {Feasibility of {Tree}-based {Machine} {Learning} algorithms fed with surface electromyographic features to discriminate risk classes according to {NIOSH}},
	doi = {10.1109/MeMeA54994.2022.9856521},
	abstract = {Many work activities can imply a biomechanical overload. Among these activities, lifting loads may determine work-related musculoskeletal disorders. In order to limit injuries, the National Institute for Occupational Safety and Health (NIOSH) proposed a methodology for assessing biomechanical risk in lifting tasks by means of a math formula based on intensity, duration, frequency and other geometrical characteristic of the lifting. In this work, we explored the feasibility of tree-based machine learning algorithms to classify biomechanical risk according to the Revised NIOSH lifting equation. Electromyography signals acquired from the biceps during lifting loads were collected using a wearable sensors for surface electromyography on a study population composed of 5 healthy young subjects. The EMG signals were segmented in order to extract the region of interest related to the lifting actions and for each region of interest several features in time and frequency domains were extracted. High results - greater than 95\% - were obtained in terms of evaluation metrics for a binary risk/no-risk classification. In conclusion, this work indicates the proposed combination of features and machine learning algorithms represents a valid approach to automatically classify risk activities according to the Revised NISOH lifting equation. Future investigation on enriched study population could confirm the potentiality of this methodology to automatically classify potential risky activities.},
	booktitle = {2022 {IEEE} {International} {Symposium} on {Medical} {Measurements} and {Applications} ({MeMeA})},
	author = {Donisi, Leandro and Capodaglio, Edda and Pagano, Gaetano and Amitrano, Federica and Cesarelli, Mario and Panigazzi, Monica and D'Addio, Giovanni},
	month = jun,
	year = {2022},
	keywords = {Feature extraction, machine learning, Machine learning algorithms, bicep muscle, Biomechanical risk assessment, Biomechanics, Electromyography, ergonomics, lifting, Musculoskeletal system, Revised NIOSH lifting equation, Sociology, work-related musculoskeletal disorders, Occupational safety},
	pages = {1--6},
}

@inproceedings{majewski_building_2016,
	title = {Building {Innovative} {Speech} {Interfaces} {Using} {Patterns} and {Antipatterns} of {Commands} for {Controlling} {Loader} {Cranes}},
	doi = {10.1109/CSCI.2016.0105},
	abstract = {The paper describes research and development of innovative intelligent speech interaction systems between mobile lifting devices and their human operators, which use patterns and antipatterns of commands. A general processing scheme, using several functional modules, for the human-machine interactive communication has been presented, covering also the integration with vision and sensorial systems. The aim of the experimental research is to design a prototype of an innovative interaction system, equipped with a speech interface in a natural language, augmented reality and interactive manipulators with force feedback. The system is equipped with several adaptive intelligent layers for human biometric identification, speech recognition, word recognition, analysis and recognition of commands and messages, sentence meaning analysis, command effect analysis and safety assessment, process supervision and human reaction assessment. The paper also proposes a concept of synergistic control systems using patterns and antipatterns of commands. The concept consists of a methodology based on hybrid binary neural networks and deep learning convolutional networks.},
	booktitle = {2016 {International} {Conference} on {Computational} {Science} and {Computational} {Intelligence} ({CSCI})},
	author = {Majewski, Maciej and Kacalak, Wojciech},
	month = feb,
	year = {2016},
	keywords = {neural networks, Control systems, natural language processing, antipatterns, Cranes, Force feedback, intelligent communication, intelligent control, interactive system, Machine vision, pattern recognition, Speech, speech interface, Speech recognition, Text recognition},
	pages = {525--530},
}

@inproceedings{atef_simulation-based_2010,
	title = {A simulation-based planning system for wind turbine construction},
	issn = {1558-4305},
	doi = {10.1109/WSC.2010.5679020},
	abstract = {Wind turbine construction is a challenging undertaking due to the need to lift heavy loads to high locations in conditions of high and variable wind speeds. These conditions create great risks to contractors during the turbine assembly process. This paper presents a simulation-based system to aid in the construction planning of wind turbines. The system is composed of three main components; 1) A wind speed forecasting module based on artificial neural networks, 2) A series of discrete event simulation models that act as a test bed for different turbine construction methods and resource utilizations, and 3) A rule-based system that relates prevalent wind speed to the impact on lifting activity durations. Actual wind speed data from the Zafarana wind farm in Egypt is used and turbine construction productivity and resource utilization is compared for two common turbine construction methods.},
	booktitle = {Proceedings of the 2010 {Winter} {Simulation} {Conference}},
	author = {Atef, Dina and Osman, Hesham and Ibrahim, Moheeb and Nassar, Khaled},
	month = feb,
	year = {2010},
	keywords = {Assembly, Cranes, Planning, Resource management, Wind farms, Wind speed, Wind turbines},
	pages = {3283--3294},
}

@inproceedings{sable_customized_2021,
	title = {Customized {Adaptive} {Gradient} and {Orientation} histogram for faces altered by {Face} surgery},
	doi = {10.1109/ICEES51510.2021.9383643},
	abstract = {Recently, the ability of different algorithms to create complexity of face recognition is exchanged with. plastic surgery due to skin differences. Although plastic surgery is a serious problem in facial expression, a major concern to be studied in terms of hypothetical and diagnostic ideas. In this article, it is anticipated that the removal of the histogram-based markers of the site of formation and adaptive gradient adaptation (AGLOH) will allow obtaining an effective facial recognition of plastic surgery. The elements are extracted from a small gap of the granules of the surface area. In the textbook, a variety of local binary models will outline moving AGLOH characteristics. Therefore, magnitude of characteristics is abbreviated as primary object analysis as it provide training to ANN. In essence, neural network is made up of trainees who use particle swarm optimization (PSO) rather than standard learning algorithms. These tests are performed on 150 face-to-face plastic surgeries, including blepharoplasty, eyebrow lifting, lip shaving, malar enlargement, mentoplasty, otoplasty, rhinoplasty, rhytidectomy, skin peeling. Finally, it is proved that the algorithm proposed in the article shows its unique performance.},
	booktitle = {2021 7th {International} {Conference} on {Electrical} {Energy} {Systems} ({ICEES})},
	author = {Sable, Archana H.},
	month = feb,
	year = {2021},
	keywords = {Training, Standards, Neural network, Adaptive GLOH, face recognition, Face recognition, plastic surgery, Plastics, Principal component analysis, Principal Component Analysis (PCA), PSO, Skin, Surgery},
	pages = {593--599},
}

@article{park_object-aware_2025,
	title = {Object-{Aware} {Impedance} {Control} for {Human}–{Robot} {Collaborative} {Task} {With} {Online} {Object} {Parameter} {Estimation}},
	volume = {22},
	issn = {1558-3783},
	doi = {10.1109/TASE.2024.3477471},
	abstract = {Physical human-robot interactions (pHRIs) can improve robot autonomy and reduce physical demands on humans. In this paper, we consider a collaborative task with a considerably long object and no prior knowledge of the object’s parameters. An integrated control framework with an online object parameter estimator and a Cartesian object-aware impedance controller is proposed to realize complicated scenarios. During the transportation task, the object parameters are estimated online while a robot and human keep lifting an object. The perturbation motion is incorporated into the null space of the desired trajectory to enhance the estimator precision. An object-aware impedance controller is designed by incorporating the real-time estimation results to effectively transmit the intended human motion to the robot through the object. Experimental demonstrations of collaborative tasks, including object transportation and assembly, are implemented to show the effectiveness of our proposed method. The proposed controller was also compared to a conventional impedance controller through subjective testing and found to be more sensitive, requiring less human effort. Note to Practitioners—This research was motivated by the need to facilitate collaboration between humans and robots in handling heavy or considerable long objects, which can be challenging for a single operator. This paper proposes a physical Human-Robot Interaction (pHRI) approach that enables physical interaction between the human and the robot through the object without additional sensors such as camera or human-machine interfaces. To achieve collaborative task, the separation between the intended human motion and the object dynamics is essential. Most of real-world situations involve uncertain or unknown objects, making it challenging to assume prior knowledge of the target object’s properties. Therefore, this research introduces a real-time approach for estimating the dynamic parameters of unknown objects during collaboration, without requiring additional operational time. Consequently, the design of object-aware impedance controller can be achieved by real-time incorporation of object dynamics. Collaborative transportation and assembly task is demonstrated with whole-body controlled mobile manipulator and 1.5m long object. In future research, we will focus on enhancing the estimation precision through the use of physical informed neural network methods.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Park, Jinseong and Shin, Yong-Sik and Kim, Sanghyun},
	year = {2025},
	keywords = {Robot kinematics, Cartesian impedance control, Collaboration, Dynamics, Estimation, Impedance, mobile manipulation, object-aware control, online object parameter estimation, Parameter estimation, Physical human-robot interaction, Robot sensing systems, Robots, Sensors, Transportation},
	pages = {8081--8094},
}

@inproceedings{wang_dga_2020,
	title = {{DGA} fuzzy logic diagnostic method based on subordinating function},
	doi = {10.1109/ITOEC49072.2020.9141578},
	abstract = {Due to the differences in the diagnostic characteristics and criteria of various methods, the conclusions are often inconsistent when using these methods to diagnose faults. when transformer faults occur, it is often a combination of multiple fault types, and the correspondence between fault types and fault symptoms is fuzzy. In order to describe the fuzzy correspondence in the complex fault diagnosis, the fuzzy mathematical theory is further introduced on the basis of the information fusion diagnosis method, and a comprehensive diagnosis method based on the information fusion and fuzzy logic is proposed. Among them, semi-cauchy lifting function was used as the subordinating function to replace the original binary function for boundary fuzzification of the criterion coding boundary. Furthermore, the coding criteria are expressed as subordinating function to form the subordinating correlation matrix. For diagnosis logic reasoning, “intersection” and “union” in fuzzy logic reasoning are respectively used to replace “and” and “or logic in binary logic to form multi-value output of fuzzy decision. The diagnosis results represent the probability of occurrence of different fault types.},
	booktitle = {2020 {IEEE} 5th {Information} {Technology} and {Mechatronics} {Engineering} {Conference} ({ITOEC})},
	author = {Wang, Xuelei and Guo, Fangfang and Xu, Wei},
	month = jun,
	year = {2020},
	keywords = {Encoding, Fault diagnosis, Uncertainty, Cognition, fuzzy logic, Fuzzy logic, fuzzy multivalued decision, IEC, incidence matrix, subordinating function},
	pages = {1381--1384},
}

@inproceedings{adapa_fatigue_2023,
	title = {Fatigue {Classification} and {Onset} estimation using {Surface} {EMG} {Signals} during {Strength} {Training}},
	issn = {2640-0103},
	doi = {10.1109/APSIPAASC58517.2023.10317229},
	abstract = {Muscle fatigue onset detection and estimation have applications in supporting athletes during strength training. In this paper, the problem of surface electromyography (sEMG) based binary classification of fatigue and non-fatigue of human muscle activity during strength training is addressed. Further, the learned model is used to determine the fatigue onset using sEMG signals corresponding to a training activity. A novelty of this work is the analysis of the proposed machine learning method under different measurement conditions, such as varying postures, experience levels and lifting loads. A novel sEMG dataset titled ElectroMyography Analysis of Human Activities - DataBase 6 (EMAHA-DB6) is developed for strength training activities. EMAHA-DB6 includes sEMG signals collected from the seven prominent muscle sites of the right arm of 11 subjects engaged in strength training. The participants performed five distinct training exercises using three load weights in two body postures. A set of wavelet and time domain features are extracted and the support vector machines are used to classify them. Based on the binary testing results from a given time series, the fatigue onset is estimated using the majority voting. The SVM model is compared with other classifiers in terms of testing accuracy in the mentioned measurement scenarios. The SVM has outperformed the other models with a test accuracy of 86.5\% under all conditions scenario. In terms of the impact of measurement conditions, the SVM has 94\% in the low load condition, 88.5\% in the stand posture and nearly 88\% in the case of subjects with intermediate training status. Finally, in the proposed approach, the estimation of the fatigue onset has an average relative error of 12\%.},
	booktitle = {2023 {Asia} {Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Adapa, Eswar and Turlapaty, Anish C and Naidu, Surya},
	month = oct,
	year = {2023},
	keywords = {Training, Feature extraction, Support vector machines, Muscles, Estimation, Fatigue, Wavelet domain},
	pages = {304--310},
}

@article{chubbuck_metal_1930,
	title = {Metal clad gum filled switchgear},
	volume = {49},
	issn = {2376-5976},
	doi = {10.1109/JAIEE.1930.6537009},
	abstract = {Metal clad gum filled switchgear has recently been developed using well-known standard American types of round tank, oil tight oil circuit breakers. This metal clad gear has important advantages over previous types of open or cell gear in the features of safety, compactness, service, maintenance and installation. Busses, etc., are mounted on micarta and are insulated and sealed in a gum of high dielectric value. The oil circuit breakers are raised to their bus contacts either by means of a common truck or by individual lifting mechanism per compartment. Very compact accessories, such as oil disconnecting switches, potential transformer assemblies, etc., are used. With the high factor of safety provided for the bus, etc., the old standard double-bus scheme is not necessary as a spare bus, though a transfer bus is valuable to permit inspection of any breaker without interruption of service. A description is given of various installations, including the large isolated-phase, outdoor switchgear under construction at Leaside Station, Toronto.},
	number = {11},
	journal = {Journal of the A.I.E.E.},
	author = {Chubbuck, L. B.},
	month = jan,
	year = {1930},
	keywords = {Switches, Circuit breakers, Insulators, Metals, Oils, Switchgear, Voltage transformers},
	pages = {925--929},
}

@article{jose_robust_2020,
	title = {Robust {Classification} of {Intramuscular} {EMG} {Signals} to {Aid} the {Diagnosis} of {Neuromuscular} {Disorders}},
	volume = {1},
	issn = {2644-1276},
	doi = {10.1109/OJEMB.2020.3017130},
	abstract = {Goal: This article presents the design and validation of an accurate automatic diagnostic system to classify intramuscular EMG (iEMG) signals into healthy, myopathy, or neuropathy categories to aid the diagnosis of neuromuscular diseases. Methods: First, an iEMG signal is decimated to produce a set of “disjoint” downsampled signals, which are decomposed by the lifting wavelet transform (LWT). The Higuchi's fractal dimensions (FDs) of LWT coefficients in the subbands are computed. The FDs of LWT subband coefficients are fused with one-dimensional local binary pattern derived from each downsampled signal. Next, a multilayer perceptron neural network (MLPNN) determines the class labels of downsampled signals. Finally, the sequence of class labels is fed to the Boyer-Moore majority vote (BMMV) algorithm, which assigns a class to every iEMG signal. Results: The MLPNN-BMMV classifier was experimented with 250 iEMG signals belonging to three categories. The performance of the classifier was validated in comparison with state-of-the-art approaches. The MLPNN-BMMV has resulted in impressive performance measures (\%) using a 10-fold cross-validation—accuracy = 99.87{\textbackslash}pm 0.25, sensitivity (normal) = 99.97{\textbackslash}pm 0.13, sensitivity (myopathy) = 99.68{\textbackslash}pm 0.95, sensitivity (neuropathy) = 99.76{\textbackslash}pm 0.66, specificity (normal) = 99.72{\textbackslash}pm 0.61, specificity (myopathy) = 99.98{\textbackslash}pm 0.10, and specificity (neuropathy) = 99.96{\textbackslash}pm 0.14—surpassing the existing approaches. Conclusions: A future research direction is to validate the classifier performance with diverse iEMG datasets, which would lead to the design of an affordable real-time expert system for neuromuscular disorder diagnosis.},
	journal = {IEEE Open Journal of Engineering in Medicine and Biology},
	author = {Jose, Shobha and George, S. Thomas and Subathra, M. S. P. and Handiru, Vikram Shenoy and Jeevanandam, Poornaselvan Kittu and Amato, Umberto and Suviseshamuthu, Easter Selvan},
	year = {2020},
	keywords = {Feature extraction, Classification algorithms, Wavelet transforms, Electromyography, Fractal dimension, intramuscular electromyography, lifting wavelet transform, local binary pattern, majority vote, multilayer perceptron neural network, Neuromuscular, neuromuscular disorders, Sensitivity},
	pages = {235--242},
}

@inproceedings{lutska_monitoring_2024,
	title = {Monitoring and {Forecasting} {Quality} {Indicators} of {Predough} {Production} in the {Bakery} {Industry}},
	issn = {3064-9579},
	doi = {10.1109/KhPIWeek61434.2024.10878070},
	abstract = {The work demonstrates the process of developing a machine learning model for a system for monitoring and controlling the quality of predough at a bakery. This system will provide continuous analysis of the predough parameters, such as temperature, humidity, lifting force and titranic acidity, which will ensure timely detection of deviations in the quality of the predough and avoid production problems. In addition, it will help automate the quality control process and ensure the stability of production processes. Analysis and forecasting of dough parameters will improve the quality of produced bread or other bakery products. Management based also on the predicted values of technological indicators of the predough will allow the enterprise to avoid unnecessary costs of materials and energy and increase economic efficiency. As part of the study, seven machine learning models were created and reviewed, including three linear and four varieties of binary decision trees. An analysis of the best model - a regression decision tree - was carried out, confirming its effectiveness and the validity of the resulting hierarchical structure, in particular, the prediction of important indicators of predough with a high degree of reliability, which indicates its importance in practical applications in production. The created model can be used to monitor and control product quality in real time, allowing operators to quickly identify and correct any deviations in process performance. Also, by analyzing changes in the variables of the predough production process, it is possible to identify precursors of possible production accidents or deviations in product quality, which will allow timely measures to be taken to avoid them.},
	booktitle = {2024 {IEEE} 5th {KhPI} {Week} on {Advanced} {Technology} ({KhPIWeek})},
	author = {Lutska, Nataliia and Zaiets, Nataliia and Vlasenko, Lidiia and Zhyltsov, Andrii},
	month = oct,
	year = {2024},
	keywords = {Process control, Production, machine learning, Machine learning, Force, Predictive models, Monitoring, baking production, decision tree, forecasting, Humidity, monitoring, predough, Quality control, Stability analysis, Thermal stability},
	pages = {1--6},
}

@article{lee_overflow-detectable_2024,
	title = {Overflow-{Detectable} {Floating}-{Point} {Fully} {Homomorphic} {Encryption}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3351738},
	abstract = {A floating-point fully homomorphic encryption (FPFHE) is proposed, which is based on torus fully homomorphic encryption equipped with programmable bootstrapping. Specifically, FPFHE for 32-bit and 64-bit floating-point messages are implemented, the latter showing the state-of-the-art precision among FHEs. Also, a ciphertext is constructed for checking if an overflow has occurred or not while evaluating arithmetic circuits with the proposed FPFHE, which is useful when the message space or arithmetic circuit is too complex to estimate a bound of outputs such as some deep learning applications. Also, homomorphic algorithms, which are crucial components of overflow detectable (OD)-FPFHE, are constructed. First, a state-of-the-art bootstrapping method of TFHE is extended to bootstrap larger messages by using NTT-friendly integer modulus. Second, a subgaussian analysis method is proposed without assuming independent heuristic on AP/GINX-bootstrapping even if the deterministic gadget decomposition is used. Third, the blind rotation algorithm of TFHE is modified such that any secret key having finite non-zero values can be used while keeping the number of NTT operations the same as when the binary key is used. Fourth, various homomorphic algorithms are proposed such as evaluating min and max, lifting a constant message to the monomial exponent, counting the number of consecutive zeros from the most significant in the fraction, and performing carryover after homomorphic operation of floating-point numbers. Finally, 32-bit and 64-bit OD-FPFHEs are implemented and simulation results are provided to confirm that they work well even for extreme cases. Also, it is verified that homomorphic overflow detection is well-operated.},
	journal = {IEEE Access},
	author = {Lee, Seunghwan and Shin, Dong-Joon},
	year = {2024},
	keywords = {Deep learning, Encoding, Transforms, Random variables, Error analysis, Floating-point arithmetic, Fully homomorphic encryption, Gaussian processes, Homomorphic encryption, homomorphic floating-point arithmetic, homomorphic overflow detection, subgaussian error analysis},
	pages = {6160--6180},
}

@inproceedings{noauthor_table_2014,
	title = {Table of contents},
	doi = {10.1109/ICHPCA.2014.7045288},
	abstract = {The following topics are dealt with: greedy load balancing algorithms; heterogeneous distributed computing system; exploiting fault tolerance; cache memory structures; reconfigurable cache architecture; multiphase applications scheduling; identical parallel multiprocessor; MATLAB distributed computing server; AIM; resource provisioning strategy; dynamic cloud environment; cloud computing memory allocation; timestamped signature scheme; message recovery; CMMSPEED; reliable real-time protocol; industrial mesh network; MANET; TCP variants protocols; wireless network; selective cooperation method; dynamic traffic pattern; NS2; data hiding; halftone images; mathematical morphology; conjugate ordered dithering; secure group key agreement protocol design; elliptic curve cryptography; fingerprint based symmetric cryptography; BER performance comparison; MIMO System; STBC; MRC; different fading channels; fault node detection algorithm; wireless sensor networks; secured packet inspection; hierarchical pattern matching; incremental clustering algorithm; GPU; STBC-OFDM WiMAX system performance evaluation; graphics processing unit; string sorting; many-threaded architectures; multithreaded architectures; differential evolution; cost reduction cellular network; game theory; mobile wireless sensor network; GNU radio; error rate performance enhancement; hybrid equalization technique; MIMO-OFDM systems; BER; Brewster's angle; polarization diversity technique; indoor visible light communication system; route stability; AODV; parallel Hadamard transform; multimesh network; dense matrix multiplication; 2D mesh; scalable parallel clustering approach; parallel K means; firefly algorithms; apriori algorithm; HDFS; parameter optimization; nonlinear fitting; epidemic disease propagation detection algorithm; MapReduce; realistic social contact networks; leather defects identification; auto adaptive edge detection image processing algorithm; list scheduling algorithms analysis; parallel system; evolutionary Jordan Pi-sigma neural network; gradient descent learning; HDFS performance evaluation; big data management; securing financial network system; multilevel security; cyber physical system; data mining concepts; Web crawling; convex points; convex hull; image searching algorithm development; social network; graph mining techniques; clustering algorithm; binary data sets; human facial expression recognition; image color analysis; FPGA realization; particle swarm optimisation algorithm; floating point arithmetic; gradient local auto correlation; handwritten devanagari character recognition; feature extraction; feature matching; speech recognition; intelligent steganography detection; fuzzy logic; wind energy conversion system; solide oxide fuel; biobjective portfolio optimisation; muliobjective simulated annealing; test case prioritization technique; free-text user authentication technique; keystroke dynamics; weighted bag hybrid multiple classifier machine; boosting prediction accuracy; automated document indexing; intelligent hierarchical clustering; training feedforward neural network; novel gravitational search optimization; real time robotic arm control; hand gestures; hybrid differential evolution-flower pollination algorithm; function minimization; bijective mapping; arbitrary finite state machine; high performance computing application; lattice physics code VISWAM; stencil computation; CPU-GPU; threshold voltage roll-off; triple gate FinFET analysis; Ultra-Thin Si directly; cardiac pulse measurement; multicore architecture; optimum gene selection; gene expression dataset; cancer dataset; PKI; timestamped secure signing tool; e-documents; trigger action reaction model; artificial energy drinks; stability enhancement; differential evolution algorithm; molecular dynamics application optimization; Intel Xeon Phi Coprocessor; pair programming team;UTB-SG; DG MOSFET; security robot-SECBOT;AODV routing protocol modification; VANET; lifting biorthogonal wavelet design; edge detection; intrusion detection systems; fuzzy based median filter; generation reliability evaluation; wind energy penetrated power system; decoder segment optimization; ROM design; quantum dot cellular automata; reconfigurable viterbi decoder; SDR; mobile communications; texture analysis; medical images; adaptive background subtraction; impulse noise detection and neighborhood switching median filter.},
	booktitle = {2014 {International} {Conference} on {High} {Performance} {Computing} and {Applications} ({ICHPCA})},
	month = feb,
	year = {2014},
	pages = {iii--x},
}

@inproceedings{zhao_gigl_2025,
	address = {New York, NY, USA},
	series = {{KDD} '25},
	title = {{GiGL}: {Large}-{Scale} {Graph} {Neural} {Networks} at {Snapchat}},
	isbn = {979-8-4007-1454-2},
	url = {https://doi.org/10.1145/3711896.3737229},
	doi = {10.1145/3711896.3737229},
	abstract = {Recent advances in graph machine learning (ML) with the introduction of Graph Neural Networks (GNNs) have led to a widespread interest in applying these approaches to business applications at scale. GNNs enable differentiable end-to-end (E2E) learning of model parameters given graph structure which enables optimization towards popular node, edge (link) and graph-level tasks. While the research innovation in new GNN layers and training strategies has been rapid, industrial adoption and utility of GNNs has lagged considerably due to the unique scale challenges that large-scale graph ML problems create. In this work, we share our approach to training, inference, and utilization of GNNs at Snapchat. To this end, we present GiGL (Gigantic Graph Learning), an open-source library to enable large-scale distributed graph ML to the benefit of researchers, ML engineers, and practitioners. We use GiGL internally at Snapchat to manage the heavy lifting of GNN workflows, including graph data preprocessing from relational DBs, subgraph sampling, distributed training, inference, and orchestration. GiGL is designed to interface cleanly with open-source GNN modeling libraries prominent in academia like PyTorch Geometric (PyG), while handling scaling and productionization challenges that make it easier for internal practitioners to focus on modeling. GiGL is used in multiple production settings, and has powered over 35 launches across multiple business domains in the last 2 years in the contexts of friend recommendation, content recommendation and advertising. This work details high-level design and tools the library provides, scaling properties, case studies in diverse business settings with large-scale graphs up to hundreds of millions of nodes, tens of billions of edges, and hundreds of node and edge features, and several key lessons learned in employing graph ML at scale on large social data. GiGL is open-sourced at https://github.com/Snapchat/GiGL.},
	booktitle = {Proceedings of the 31st {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} {V}.2},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Tong and Liu, Yozen and Kolodner, Matthew and Montemayor, Kyle and Ghazizadeh, Elham and Batra, Ankit and Fan, Zihao and Gao, Xiaobin and Guo, Xuan and Ren, Jiwen and Park, Serim and Yu, Peicheng and Yu, Jun and Vij, Shubham and Shah, Neil},
	year = {2025},
	keywords = {distributed machine learning, graph machine learning, graph neural networks, large scale machine learning},
	pages = {5225--5236},
}

@article{han_neural_2024,
	address = {New York, NY, USA},
	title = {A {Neural} {Network} {Model} for {Efficient} {Musculoskeletal}-{Driven} {Skin} {Deformation}},
	volume = {43},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3658135},
	doi = {10.1145/3658135},
	abstract = {We present a comprehensive neural network to model the deformation of human soft tissues including muscle, tendon, fat and skin. Our approach provides kinematic and active correctives to linear blend skinning [Magnenat-Thalmann et al. 1989] that enhance the realism of soft tissue deformation at modest computational cost. Our network accounts for deformations induced by changes in the underlying skeletal joint state as well as the active contractile state of relevant muscles. Training is done to approximate quasistatic equilibria produced from physics-based simulation of hyperelastic soft tissues in close contact. We use a layered approach to equilibrium data generation where deformation of muscle is computed first, followed by an inner skin/fascia layer, and lastly a fat layer between the fascia and outer skin. We show that a simple network model which decouples the dependence on skeletal kinematics and muscle activation state can produce compelling behaviors with modest training data burden. Active contraction of muscles is estimated using inverse dynamics where muscle moment arms are accurately predicted using the neural network to model kinematic musculotendon geometry. Results demonstrate the ability to accurately replicate compelling musculoskeletal and skin deformation behaviors over a representative range of motions, including the effects of added weights in body building motions.},
	number = {4},
	journal = {ACM Trans. Graph.},
	publisher = {Association for Computing Machinery},
	author = {Han, Yushan and Chen, Yizhou and Ong, Carmichael and Chen, Jingyu and Hicks, Jennifer and Teran, Joseph},
	month = jul,
	year = {2024},
	keywords = {biomechanical model, human body simulation, muscle simulation},
}

@inproceedings{hu_zipzap_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {{ZipZap}: {Efficient} {Training} of {Language} {Models} for {Large}-{Scale} {Fraud} {Detection} on {Blockchain}},
	isbn = {979-8-4007-0171-9},
	url = {https://doi.org/10.1145/3589334.3645352},
	doi = {10.1145/3589334.3645352},
	abstract = {Language models (LMs) have demonstrated superior performance in detecting fraudulent activities on Blockchains. Nonetheless, the sheer volume of Blockchain data results in excessive memory and computational costs when training LMs from scratch, limiting their capabilities to large-scale applications. In this paper, we present ZipZap, a framework tailored to achieve both parameter and computational efficiency when training LMs on large-scale transaction data. First, with the frequency-aware compression, an LM can be compressed down to a mere 7.5\% of its initial size with an imperceptible performance dip. This technique correlates the embedding dimension of an address with its occurrence frequency in the dataset, motivated by the observation that embeddings of low-frequency addresses are insufficiently trained and thus negating the need for a uniformly large dimension for knowledge representation. Second, ZipZap accelerates the speed through the asymmetric training paradigm: It performs transaction dropping and cross-layer parameter-sharing to expedite the pre-training process, while revert to the standard training paradigm for fine-tuning to strike a balance between efficiency and efficacy, motivated by the observation that the optimization goals of pre-training and fine-tuning are inconsistent. Evaluations on real-world, large-scale datasets demonstrate that ZipZap delivers notable parameter and computational efficiency improvements for training LMs. Our implementation is available at: https://github.com/git-disl/ZipZap.},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Hu, Sihao and Huang, Tiansheng and Chow, Ka-Ho and Wei, Wenqi and Wu, Yanzhao and Liu, Ling},
	year = {2024},
	keywords = {language models, blockchain, ethereum, computational efficient, parameter-efficient},
	pages = {2807--2816},
}

@inproceedings{li_control_2025,
	address = {New York, NY, USA},
	series = {{ISCCN} '25},
	title = {Control {Method} of {Cam} {Jacking} {Transfer} {Machine} based on {BP} {Neural} {Network}},
	isbn = {979-8-4007-1520-4},
	url = {https://doi.org/10.1145/3732945.3732958},
	doi = {10.1145/3732945.3732958},
	abstract = {The cam jacking transfer machine achieves stable transmission and vertical diversion of items simultaneously through the rotation of the cam. In this operation process, rapidly determining the object's position and controlling the cam's rotation are critical steps. In this paper, a control method based on a BP neural network is designed for the cam jacking transfer machine, and simulations are conducted. The simulation results indicate that the BP neural network model can accurately determine the positional state of the items and effectively capture the complex nonlinear relationships between the input features and the items states during the training process, enabling efficient and accurate transmission and vertical diversion of the items.},
	booktitle = {Proceedings of the 2025 4th {International} {Conference} on {Intelligent} {Systems}, {Communications} and {Computer} {Networks}},
	publisher = {Association for Computing Machinery},
	author = {Li, Fei and Zhang, Xu and Peng, Ruping and Zhou, Yihan and Wei, Yulan and Zhang, Qingzhu},
	year = {2025},
	keywords = {BP Neural Network, Cam Jacking, Transfer Machine, Vertical Shunts},
	pages = {91--96},
}

@inproceedings{wang_research_2025,
	address = {New York, NY, USA},
	series = {{DEBAI} '25},
	title = {Research on the {Construction} of {Supply} {Chain} {Financial} {Credit} {Risk} {Management} {Model} {Based} on {Machine} {Learning}},
	isbn = {979-8-4007-1349-1},
	url = {https://doi.org/10.1145/3762249.3762267},
	doi = {10.1145/3762249.3762267},
	abstract = {With the diversification of global trade and the complexity of supply chain network, supply chain finance, as a new financial service model, is facing a high degree of credit risk while promoting the coordinated development of industrial chain. However, the traditional credit risk management methods mostly rely on limited data information and a single evaluation mechanism, which is difficult to adapt to the rapidly changing market environment. In view of this situation, this paper will comprehensively analyze the application feasibility of machine learning technology in this field, and propose a brand-new supply chain financial credit risk management model to improve the level of supply chain financial credit risk management. Practice has proved that the whole model is based on support vector machine (SVM), and the LightGBM machine learning algorithm is integrated into it through ensemble learning, so as to achieve the purpose of improving the complexity of the model, thus improving the accuracy and execution efficiency of the risk prediction model and providing strong technical support for the steady development of supply chain finance.},
	booktitle = {Proceedings of the 2025 2nd {International} {Conference} on {Digital} {Economy}, {Blockchain} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Jia and Shafie, Nur Aima and Kasim, Eley Suzana},
	year = {2025},
	keywords = {machine learning, ensemble learning, credit risk management model, supply chain finance},
	pages = {101--105},
}

@article{zheng_deep_2023,
	address = {New York, NY, USA},
	title = {Deep {Learning}-based {Human} {Pose} {Estimation}: {A} {Survey}},
	volume = {56},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3603618},
	doi = {10.1145/3603618},
	abstract = {Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey article is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 260 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided: .},
	number = {1},
	journal = {ACM Comput. Surv.},
	publisher = {Association for Computing Machinery},
	author = {Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
	month = aug,
	year = {2023},
	keywords = {2D and 3D pose estimation, deep learning-based pose estimation, pose estimation datasets, pose estimation metrics, Survey of human pose estimation},
}

@inproceedings{wang_research_2024-1,
	address = {New York, NY, USA},
	series = {{ISAIMS} '23},
	title = {Research on {Acupuncture} {Technique} {Recognition} {Technology} {Based} on {Deep} {Learning} {Algorithms}},
	isbn = {979-8-4007-0813-8},
	url = {https://doi.org/10.1145/3644116.3644195},
	doi = {10.1145/3644116.3644195},
	abstract = {Combining deep learning-based methods and key posture sensor data can improve the accuracy of automatically identifying basic acupuncture techniques. This study aims to propose a novel deep learning-based method to identify the time characteristics of the physical parameters of acupuncture techniques, including twisting-supplementing (TS), twisting-draining (TD), level-supplementing level-draining (LSLD), lifting-inserting-supplementing (LIS), and lifting-inserting-draining (LID) methods. During the acupuncture process, six-axis posture sensors collect parameters such as lifting and inserting speed, twisting frequency, and angular rotation rate, and analyze the mapping relationship between physical parameters and different techniques. By using multi-dimensional signal control decision algorithms, the performance of our method is enhanced, addressing the issue of poor recognition based on single-dimensional signals. Experimental results demonstrate that our method achieves high accuracy in identifying five techniques. Our proposed method can effectively identify acupuncture techniques, making it useful for the evaluation and teaching of acupuncture techniques, facilitating the inheritance of traditional Chinese acupuncture techniques.},
	booktitle = {Proceedings of the 2023 4th {International} {Symposium} on {Artificial} {Intelligence} for {Medicine} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Qihang and Yang, Hao and Zhang, Jun and Jiang, Tianyun and Tian, Peng and Yu, Kang and Dai, Zhiwei and Wu, Shuang and Gao, Junhong and Yu, Xiaochun and Zhang, Haiying and Wang, Yunfeng},
	year = {2024},
	pages = {481--487},
}

@incollection{li_construction_2025,
	address = {New York, NY, USA},
	title = {Construction of {Carbon} {Emission} {Prediction} {Model} in {Materialization} {Stage} of {Construction} {Engineering} {Based} on {BIM} and {Machine} {Learning}},
	isbn = {979-8-4007-1874-8},
	url = {https://doi.org/10.1145/3773365.3773576},
	abstract = {Under the background of “dual carbon goals”, the materialization stage of construction engineering, as the key link of carbon emission management in the whole life cycle, is directly related to the low-carbon transformation of construction industry. However, in the practical application stage, the traditional carbon emission prediction method mainly relies on the bill of quantities multiplied by static carbon emission factors, which has some problems such as difficult data acquisition, low prediction accuracy and poor project adaptability, and can not meet the actual needs of the construction industry. In this regard, this paper will deeply study the application feasibility of machine learning technology in this field, and build a brand-new carbon emission prediction model in the materialization stage of building engineering by combining the building information model (BIM) technology, aiming at improving the accuracy of prediction results and strengthening the adaptability to multiple complex scenarios. Experiments show that on the one hand, the overall model establishes the relationship between BIM model and carbon emission factors to solve the problem of data acquisition; on the other hand, the model uses extreme gradient lifting algorithm (XGBoost) to analyze the relationship between the materialization stage of mining and carbon emission, so as to realize the accurate prediction of carbon emission in the materialization stage of construction projects, and then provide decision support for carbon emission management of construction projects.},
	booktitle = {Proceedings of the 2025 8th {International} {Conference} on {Computer} {Information} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Li, Xiang},
	year = {2025},
	pages = {1337--1342},
}

@inproceedings{sun_investigation_2025,
	address = {New York, NY, USA},
	series = {{ICIIS} '25},
	title = {An investigation and prediction of work-related {Musculoskeletal} disorders based on machine learning among coal miners in {Western} {China}},
	isbn = {979-8-4007-1515-0},
	url = {https://doi.org/10.1145/3745676.3745703},
	doi = {10.1145/3745676.3745703},
	abstract = {Objective: Studies form industrialized show that work-related musculoskeletal disorders (WMSDs) occur commonly in workers. This study aimed to investigate the prevalence of WMSDs and associated risk factors among coal miners in Western China. Moreover, it constructs a model based on the analysis to predict the possible level of musculoskeletal damage in miners.Methods: A cross-sectional survey was conducted with 2297 coal miners from 6 selected coal mines in western China. Work-related musculoskeletal disorders (using the Nordic musculature questionnaire) and research factors were recorded. Analysis of variance is used to identify the difference between work type groups. Correlation analysis is used to quantify associations between research factors and WMSDs. XGboost is applied for predicting the possible level of musculoskeletal damage in miners based on survey data. By comparing it to any other methods, it shows better performance on forecast.Results: There was 80.4\% participation rate (1846 valid samples). Variance analysis shows that there were differences among the different work type groups on lower back, knees and shoulders. The correlation analysis shows that neck, shoulders, lower back, thighs, knees and ankles pain are positively correlated with ages and working age. Only upper back and ankles pain are negatively correlated with income level. Every body sites are positively correlated with injury time. Shoulders, lower back, hands, knees and ankles pain are negatively correlated with education. The XGboost method is excellent at learning patterns from the information in questionnaire data to enable prediction of the degree of musculoskeletal damage in miners (with 80.8\% accuracy).Conclusions: It finds that the relationship between WMSDs and occupational factors in research. This study provides more information related to WMSDs in coal miners. The model provides a reference for predicting and controlling the prevalence of WMSDs among coal miners.},
	booktitle = {Proceedings of the 2025 2nd {International} {Conference} on {Innovation} {Management} and {Information} {System}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Yuming and Yuan, Xiaofang},
	year = {2025},
	keywords = {Machine learning, Coal miners, Western China, Work-related Musculoskeletal disorders (WMSDs)},
	pages = {179--186},
}

@inproceedings{qiu_path_2022,
	address = {New York, NY, USA},
	series = {{IPEC} '22},
	title = {Path {Planning} {Algorithm} of {English} {Retrieval} based on {Associative} {Memory} {Neural} {Network}},
	isbn = {978-1-4503-9578-6},
	url = {https://doi.org/10.1145/3544109.3544996},
	doi = {10.1145/3544109.3544996},
	abstract = {English collocation retrieval is to extract the phrases or idioms combined with various grammatical relations from the corpus, which is used to systematically analyze and study the collocations of words. In the absence of such corpus retrieval tools, English teachers mainly use the traditional method of definition and description to identify and analyze synonyms. In order to further enrich the English vocabulary of English learners, combined with the related algorithms of similarity, this paper proposes an English retrieval path planning algorithm based on associative memory neural network. The algorithm implements a search mechanism that uses the adjacency list data structure and restricts the search area to reasonably limit the search area of the algorithm. The BP algorithm based on the associative memory neural network model greatly reduces the network training times of the sample variable system. The system effectively reduces the network training time of the sample variable system, and provides the algorithm basis for the BP algorithm to be applied to occasions with high real-time requirements. Combined with the practical application of the path planning algorithm in the English retrieval system, through the hierarchical scheduling of tasks of different importance, a satisfactory decision result is obtained. The algorithm has the advantages of small search space and fast solution speed. Simulation results verify the effectiveness of the algorithm.},
	booktitle = {Proceedings of the 3rd {Asia}-{Pacific} {Conference} on {Image} {Processing}, {Electronics} and {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Qiu, Hehua},
	year = {2022},
	pages = {1049--1054},
}

@article{liu_recent_2022,
	address = {New York, NY, USA},
	title = {Recent {Advances} of {Monocular} {2D} and {3D} {Human} {Pose} {Estimation}: {A} {Deep} {Learning} {Perspective}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3524497},
	doi = {10.1145/3524497},
	abstract = {Estimation of the human pose from a monocular camera has been an emerging research topic in the computer vision community with many applications. Recently, benefiting from the deep learning technologies, a significant amount of research efforts have advanced the monocular human pose estimation both in 2D and 3D areas. Although there have been some works to summarize different approaches, it still remains challenging for researchers to have an in-depth view of how these approaches work from 2D to 3D. In this article, we provide a comprehensive and holistic 2D-to-3D perspective to tackle this problem. First, we comprehensively summarize the 2D and 3D representations of human body. Then, we summarize the mainstream and milestone approaches for these human body presentations since the year 2014 under unified frameworks. Especially, we provide insightful analyses for the intrinsic connections and methods evolution from 2D to 3D pose estimation. Furthermore, we analyze the solutions for challenging cases, such as the lack of data, the inherent ambiguity between 2D and 3D, and the complex multi-person scenarios. Next, we summarize the benchmarks, evaluation metrics, and the quantitative performance of popular approaches. Finally, we discuss the challenges and give deep thinking of promising directions for future research. We believe this survey will provide the readers (researchers, engineers, developers, etc.) with a deep and insightful understanding of monocular human pose estimation.},
	number = {4},
	journal = {ACM Comput. Surv.},
	publisher = {Association for Computing Machinery},
	author = {Liu, Wu and Bao, Qian and Sun, Yu and Mei, Tao},
	month = jan,
	year = {2022},
	keywords = {deep learning, 2D and 3D pose, Human pose estimation, monocular images},
}

@inproceedings{wei_towards_2021,
	address = {New York, NY, USA},
	series = {{AICCC} '20},
	title = {Towards {Diagnosis} of {Carpal} {Tunnel} {Syndrome} {Using} {Machine} {Learning}},
	isbn = {978-1-4503-8883-2},
	url = {https://doi.org/10.1145/3442536.3442549},
	doi = {10.1145/3442536.3442549},
	abstract = {Carpal Tunnel Syndrome (CTS) is the most common peripheral neuropathy affecting the hand function. Although the most complains from patients with CTS are fine motor control failures in daily manual activities, parameters of hand functional control have not been considered neither in current diagnostic nor evaluation process. In addition, CTS has been identified as an occupational disease. Over 50\% of reported CTS cases are work related. However, early screening protocols of CTS at a preliminary stage are absent, and thus unable to prevent further complications, especially for high-risk populations who can advance their CTS stage on daily work basis. In the current protocol, we aim to identify important parameters of hand functional control that are indicative of CTS clinical occurrence and severity stages. Based on designed experiments during hand grasping, we performed machine learning classifiers to detect, filter and subtract important biomarkers or groups of biomarkers that dominantly classify the CTS hand and its severity. The identified biomarkers not only provide a high potential of a paradigm shift in CTS management, but also are able to shed light on hand functional evaluations associated with this neuropathy. In this paper, we adopt one of machine learning approaches, random forests, to the raw experimental hand function gripping data to identify the most important biomarkers for CTS. The experimental results show the effectiveness of the proposed work.},
	booktitle = {Proceedings of the 2020 3rd {Artificial} {Intelligence} and {Cloud} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Wei, Yuan and Zhang, Wei and Gu, Feng},
	year = {2021},
	keywords = {Random forests, Carpal tunnel syndrome, Classification, Grasping control, Small-sized data sample},
	pages = {76--82},
}

@inproceedings{jiang_binaryai_2024-2,
	address = {New York, NY, USA},
	series = {{ICSE} '24},
	title = {{BinaryAI}: {Binary} {Software} {Composition} {Analysis} via {Intelligent} {Binary} {Source} {Code} {Matching}},
	isbn = {979-8-4007-0217-4},
	url = {https://doi.org/10.1145/3597503.3639100},
	doi = {10.1145/3597503.3639100},
	abstract = {While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@1 and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product Black Duck.https://www.binaryai.net},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
	year = {2024},
	keywords = {software composition analysis, static binary analysis},
}

@inproceedings{asvadishirehjini_ginn_2022,
	address = {New York, NY, USA},
	series = {{CODASPY} '22},
	title = {{GINN}: {Fast} {GPU}-{TEE} {Based} {Integrity} for {Neural} {Network} {Training}},
	isbn = {978-1-4503-9220-4},
	url = {https://doi.org/10.1145/3508398.3511503},
	doi = {10.1145/3508398.3511503},
	abstract = {Machine learning models based on Deep Neural Networks (DNNs) are increasingly deployed in a wide variety of applications, ranging from self-driving cars to COVID-19 diagnosis. To support the computational power necessary to train a DNN, cloud environments with dedicated Graphical Processing Unit (GPU) hardware support have emerged as critical infrastructure. However, there are many integrity challenges associated with outsourcing the computation to use GPU power, due to its inherent lack of safeguards to ensure computational integrity. Various approaches have been developed to address these challenges, building on trusted execution environments (TEE). Yet, no existing approach scales up to support realistic integrity-preserving DNN model training for heavy workloads (e.g., deep architectures and millions of training examples) without sustaining a significant performance hit. To mitigate the running time difference between pure TEE (i.e., full integrity) and pure GPU (i.e., no integrity) , we combine random verification of selected computation steps with systematic adjustments of DNN hyperparameters (e.g., a narrow gradient clipping range), which limits the attacker's ability to shift the model parameters arbitrarily. Experimental analysis shows that the new approach can achieve a 2X to 20X performance improvement over a pure TEE-based solution while guaranteeing an extremely high probability of integrity (e.g., 0.999) with respect to state-of-the-art DNN backdoor attacks.},
	booktitle = {Proceedings of the {Twelfth} {ACM} {Conference} on {Data} and {Application} {Security} and {Privacy}},
	publisher = {Association for Computing Machinery},
	author = {Asvadishirehjini, Aref and Kantarcioglu, Murat and Malin, Bradley},
	year = {2022},
	keywords = {deep learning, integrity preserving deep learning training, intel sgx, trusted exexution environments},
	pages = {4--15},
}

@article{li_guided_2025,
	address = {New York, NY, USA},
	title = {Guided {Tensor} {Lifting}},
	volume = {9},
	url = {https://doi.org/10.1145/3729330},
	doi = {10.1145/3729330},
	abstract = {Domain-specific languages (DSLs) for machine learning are revolutionizing the speed and efficiency of machine learning workloads as they enable users easy access to high-performance compiler optimizations and accelerators. However, to take advantage of these capabilities, a user must first translate their legacy code from the language it is currently written in, into the new DSL. The process of automatically lifting code into these DSLs has been identified by several recent works, which propose program synthesis as a solution. However, synthesis is expensive and struggles to scale without carefully designed and hard-wired heuristics. In this paper, we present an approach for lifting that combines an enumerative synthesis approach with a Large Language Model used to automatically learn the domain-specific heuristics for program lifting, in the form of a probabilistic grammar. Our approach outperforms the state-of-the-art tools in this area, despite only using learned heuristics.},
	number = {PLDI},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Li, Yixuan and Magalhães, José Wesley de Souza and Brauckmann, Alexander and O'Boyle, Michael F. P. and Polgreen, Elizabeth},
	month = jun,
	year = {2025},
	keywords = {Large Language Model, Program Synthesis, Code Generation, Code Optimization, Lifting, Tensor Algebra},
}

@inproceedings{lamprou_guarding_2025,
	address = {New York, NY, USA},
	series = {{PACMI} '25},
	title = {Guarding {LLM}-aided {Software} {Transformation} {Tasks} via {Component} {Exoskeletons}},
	isbn = {979-8-4007-2205-9},
	url = {https://doi.org/10.1145/3766882.3767171},
	doi = {10.1145/3766882.3767171},
	abstract = {Large language models (LLMs) are achieving state-of-the-art results across a wide variety of software transformation tasks—including translating across languages and lifting opaque software components to high-level languages. Unfortunately, their results are often subtly incorrect, insecure, or underperformant—affecting the widespread deployment of these LLM-driven techniques in settings that go beyond the narrow scope of academic papers. This paper posits that such widespread deployment crucially depends on developing appropriate model guardrails for safeguarding the results of the transformation process. Such guardrails can be supported by component exoskeletons, tunable partial specifications extracted mostly automatically from the original, pre-transformed component. Exoskeletons serve as component projections that supplement, and often go through, the entire transformation process, confirming that the new, transformed component meets the original specifications. They show promise on several real-world scenarios and unearth exciting research directions.},
	booktitle = {Proceedings of the 4th {Workshop} on {Practical} {Adoption} {Challenges} of {ML} for {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Lamprou, Evangelos and Kalhauge, Christian Gram and Rinard, Martin C. and Vasilakis, Nikos},
	year = {2025},
	keywords = {large language models, component exoskeletons, software transformation},
	pages = {13--18},
}

@inproceedings{deshpande_stackbert_2021,
	address = {New York, NY, USA},
	series = {{AISec} '21},
	title = {{StackBERT}: {Machine} {Learning} {Assisted} {Static} {Stack} {Frame} {Size} {Recovery} on {Stripped} and {Optimized} {Binaries}},
	isbn = {978-1-4503-8657-9},
	url = {https://doi.org/10.1145/3474369.3486865},
	doi = {10.1145/3474369.3486865},
	abstract = {The call stack represents one of the core abstractions that compiler-generated programs leverage to organize binary execution at runtime. For many use cases reasoning about stack accesses of binary functions is crucial: security-sensitive applications may require patching even after deployment, and binary instrumentation, rewriting, and lifting all necessitate detailed knowledge about the function frame layout of the affected program. As no comprehensive solution to the stack symbolization problem exists to date, existing approaches have to resort to workarounds like emulated stack environments, resulting in increased runtime overheads.In this paper we present StackBERT, a framework to statically reason about and reliably recover stack frame information of binary functions in stripped and highly optimized programs. The core idea behind our approach is to formulate binary analysis as a self-supervised learning problem by automatically generating ground truth data from a large corpus of open-source programs. We train a state-of-the-art Transformer model with self-attention and finetune for stack frame size prediction. We show that our finetuned model yields highly accurate estimates of a binary function's stack size from its function body alone across different instruction-set architectures, compiler toolchains, and optimization levels. We successfully verify the static estimates against runtime data through dynamic executions of standard benchmarks and additional studies, demonstrating that StackBERT's predictions generalize to 93.44\% of stripped and highly optimized test binaries not seen during training. We envision these results to be useful for improving binary rewriting and lifting approaches in the future.},
	booktitle = {Proceedings of the 14th {ACM} {Workshop} on {Artificial} {Intelligence} and {Security}},
	publisher = {Association for Computing Machinery},
	author = {Deshpande, Chinmay and Gens, David and Franz, Michael},
	year = {2021},
	keywords = {machine learning, binary lifting, recompilation, stack symbolization},
	pages = {85--95},
}

@inproceedings{zeng_obfusx_2021,
	address = {New York, NY, USA},
	series = {{ASPDAC} '21},
	title = {{ObfusX}: {Routing} {Obfuscation} with {Explanatory} {Analysis} of a {Machine} {Learning} {Attack}},
	isbn = {978-1-4503-7999-1},
	url = {https://doi.org/10.1145/3394885.3431600},
	doi = {10.1145/3394885.3431600},
	abstract = {This is the first work that incorporates recent advancements in "explainability" of machine learning (ML) to build a routing obfuscator called ObfusX. We adopt a recent metric—the SHAP value—which explains to what extent each layout feature can reveal each unknown connection for a recent ML-based split manufacturing attack model. The unique benefits of SHAP-based analysis include the ability to identify the best candidates for obfuscation, together with the dominant layout features which make them vulnerable. As a result, ObfusX can achieve better hit rate (97\% lower) while perturbing significantly fewer nets when obfuscating using a via perturbation scheme, compared to prior work. When imposing the same wirelength limit using a wire lifting scheme, ObfusX performs significantly better in performance metrics (e.g., 2.4 times more reduction on average in percentage of netlist recovery).},
	booktitle = {Proceedings of the 26th {Asia} and {South} {Pacific} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Zeng, Wei and Davoodi, Azadeh and Topaloglu, Rasit Onur},
	year = {2021},
	keywords = {machine learning, explainable artificial intelligence, routing obfuscation, split manufacturing},
	pages = {548--554},
}

@inproceedings{shoji_poster_2025,
	address = {New York, NY, USA},
	series = {{BuildSys} '25},
	title = {Poster {Abstract}: {Data} {Assimilation} for {HVAC} {Simulations} in {Koopman}-{Invariant} {Subspace} using {Kalman} {Filter}},
	isbn = {979-8-4007-1945-5},
	url = {https://doi.org/10.1145/3736425.3772109},
	doi = {10.1145/3736425.3772109},
	abstract = {Real-time state estimation in nonlinear HVAC dynamics requires computationally expensive nonlinear filtering methods. We present a data assimilation framework enabling standard linear Kalman filtering for nonlinear systems through Koopman operator theory. We transform nonlinear CFD simulations into linear dynamics using extended dynamic mode decomposition with neural network lifting functions. Our experiments demonstrate successful reconstruction of temperature and velocity fields from only 18 temperature sensors, achieving temperature RMSE of 0.16 °C and velocity RMSEs of 0.035 m/s and 0.031 m/s for u- and v-components respectively, notably inferring unobserved velocity fields solely from temperature measurements.},
	booktitle = {Proceedings of the 12th {ACM} {International} {Conference} on {Systems} for {Energy}-{Efficient} {Buildings}, {Cities}, and {Transportation}},
	publisher = {Association for Computing Machinery},
	author = {Shoji, Yutaka and Arisaka, Sohei and Ono, Eikichi and Mihara, Kuniaki},
	year = {2025},
	pages = {306--307},
}

@inproceedings{malhotra_decoding_2022,
	address = {New York, NY, USA},
	series = {{APIT} '22},
	title = {Decoding the {Brain} {Waves} using {EEG} signals for classifying {Body} {Gestures} by applying suitable {ML} \&amp; {DL} {Techniques}},
	isbn = {978-1-4503-9557-1},
	url = {https://doi.org/10.1145/3512353.3512370},
	doi = {10.1145/3512353.3512370},
	abstract = {This work aims to use EEG brain signals captured from the scalp of subjects at 500 Hz for classification of grasping tasks which is useful for BCI (Brain-Computer Interface) and comparative analysis of appropriate Machine Learning and Deep Learning Algorithms for the same. The dataset consists of 32 features and 6 output classes. Multinomial Logistic Regression, LSTM, CNN, RNN and ANN Algorithms were used for this purpose.},
	booktitle = {Proceedings of the 2022 4th {Asia} {Pacific} {Information} {Technology} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Malhotra, Ruchika and Deswal, Dhananjay and Chaudhry, Jai},
	year = {2022},
	keywords = {CNN, LSTM, RNN, ANN, Comparative Analysis, EEG, LR},
	pages = {116--122},
}

@inproceedings{buet-golfouse_lifting_2023,
	address = {New York, NY, USA},
	series = {{ICAIF} '23},
	title = {Lifting {Volterra} {Diffusions} via {Kernel} {Decomposition}},
	isbn = {979-8-4007-0240-2},
	url = {https://doi.org/10.1145/3604237.3626914},
	doi = {10.1145/3604237.3626914},
	abstract = {Rough volatility models have garnered considerable attention among practitioners due to their remarkable empirical fit. However, their non-Markovian nature arises from the presence of a kernel (leading to so-called Voltera diffusions), which complicates pricing and calibration tasks. In this paper, we present our novel contribution of employing machine learning techniques to either approximate or learn the kernel function in a manner that renders it Markovian, effectively “lifting” the non-Markovian Volterra diffusion. Through empirical investigations encompassing a diverse set of kernels, we demonstrate the efficacy of our approach, opening new avenues for improved modelling and analysis in rough volatility models.},
	booktitle = {Proceedings of the {Fourth} {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {Association for Computing Machinery},
	author = {Buet-Golfouse, Francois and Martin, Nicholas WD},
	year = {2023},
	keywords = {Fractional Kernel, Kernel Decomposition, Random Fourier Features, Rough Heston Model, Volterra Diffusion},
	pages = {481--489},
}

@inproceedings{devarakonda_design_2024,
	address = {New York, NY, USA},
	series = {{ICRAI} '23},
	title = {Design and development of medical cobot to assist surgeon},
	isbn = {979-8-4007-0828-2},
	url = {https://doi.org/10.1145/3637843.3637853},
	doi = {10.1145/3637843.3637853},
	abstract = {This paper presents an automated collaborative robot that works with a surgeon during a surgical operation incorporated with Machine learning algorithms and Deep learning algorithms to assist the surgeon by performing auxiliary actions. Our main objective is to create a framework for a collaborative nurse robot that upon voice instruction, will recognize and classify the surgical instruments present in the surgical tray and execute manual tasks such as picking up and delivering that instrument to its operator. There have been discussions on the types of co-bots and the types that can embrace the prescribed idea. After visualizing a co-bot model, designing it in CAD software is a crucial initiation to judge the practicality of the robot. The robot was designed to test its navigation before prototyping it. The main objectives to accomplish the working of an assistant medical co-bot is discussed. For object detection Aruco marker that uses Convolution Neural Network is implicated, for voice detection Google's speech recognition system having Deep neural network algorithm is embraced and for training the Co-bot using Arduino libraries is being discussed. Python has been used as a programming language and pyserial to communicate with Arduino from other two systems i.e., voice recognition system and object detection system. This medical Co-bot reflects our effort to lessen stress on surgeon and human error during operation.},
	booktitle = {Proceedings of the 2023 9th {International} {Conference} on {Robotics} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Devarakonda, Sachidananda Bharadwaj and Sharma, Soumyajit Sen and Rudra Pal, Abhishek},
	year = {2024},
	keywords = {deep learning, machine learning, python, assistant nurse, automation, collaborative robots, Component, image classification, modeling, surgical instruments, voice recognition},
	pages = {38--44},
}

@inproceedings{chang_lifting_2025,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} {Conference} {Papers} '25},
	title = {Lifting the {Winding} {Number}: {Precise} {Discontinuities} in {Neural} {Fields} for {Physics} {Simulation}},
	isbn = {979-8-4007-1540-2},
	url = {https://doi.org/10.1145/3721238.3730597},
	doi = {10.1145/3721238.3730597},
	abstract = {Cutting thin-walled deformable structures is common in daily life, but poses significant challenges for simulation due to the introduced spatial discontinuities. Traditional methods rely on mesh-based domain representations, which require frequent remeshing and refinement to accurately capture evolving discontinuities. These challenges are further compounded in reduced-space simulations, where the basis functions are inherently geometry- and mesh-dependent, making it difficult or even impossible for the basis to represent the diverse family of discontinuities introduced by cuts.Recent advances in representing basis functions with neural fields offer a promising alternative, leveraging their discretization-agnostic nature to represent deformations across varying geometries. However, the inherent continuity of neural fields is an obstruction to generalization, particularly if discontinuities are encoded in neural network weights.We present Wind Lifter, a novel neural representation designed to accurately model complex cuts in thin-walled deformable structures. Our approach constructs neural fields that reproduce discontinuities precisely at specified locations, without “baking in” the position of the cut line. To achieve this, we augment the input coordinates of the neural field with the generalized winding number of any given cut line, effectively lifting the input from two to three dimensions. Lifting allows the network to focus on the easier problem of learning a 3D everywhere-continuous volumetric field, while a corresponding restriction operator enables the final output field to precisely resolve strict discontinuities. Crucially, our approach does not embed the discontinuity in the neural network’s weights, opening avenues to generalization of cut placement.Our method achieves real-time simulation speeds and supports dynamic updates to cut line geometry during the simulation. Moreover, the explicit representation of discontinuities makes our neural field intuitive to control and edit, offering a significant advantage over traditional neural fields, where discontinuities are embedded within the network’s weights, and enabling new applications that rely on general cut placement.},
	booktitle = {Proceedings of the {Special} {Interest} {Group} on {Computer} {Graphics} and {Interactive} {Techniques} {Conference} {Conference} {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Chang, Yue and Liu, Mengfei and Wang, Zhecheng and Chen, Peter Yichen and Grinspun, Eitan},
	year = {2025},
	keywords = {Computational design, Cutting, Discontinuity, Implicit neural representation, Reduced-order modeling},
}

@inproceedings{zebaze_bearing_2025,
	address = {New York, NY, USA},
	series = {{WSDM} '25},
	title = {Bearing {Power} {Loss} {Predictions} in {Wind} {Turbine} {Gearbox}: {An} {Approach} {Based} on {LLMs}},
	isbn = {979-8-4007-1329-3},
	url = {https://doi.org/10.1145/3701551.3707419},
	doi = {10.1145/3701551.3707419},
	abstract = {A constant and consistent supply in electrical energy in a location is a reflection of a good economy. Developing countries nevertheless don't have access to this quality of energy, which slows down their economy and consequently development. Wind is a clean, sustainable and renewable resource which can be used to meet the energy needs in such countries. However, the intermittent nature of wind yields fluctuations on the amount of energy produced by a wind turbine. Coupled with frictional power losses in the wind turbine gearbox bearings, one can't be sure on the exact amount of energy that will be produced. This leads to the distribution and management issues. To tackle this issue, we propose here the use of Large Language models. These are tools which have been proving their potential in various domains till date and whose potential are still to be seen in the field to our knowledge. Taking advantage of their flexibility and adaptability to any model and dataset, we intend to explore its abilities in the fields of wind energy and tribology. Making use of available data, predictions on the wind energy potential and power losses will be carried out using Large Language models such as BERT. The results of this work intends to promote the use of wind energy by lifting barriers in thee management and knowledge of the resource.},
	booktitle = {Proceedings of the {Eighteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zebaze, Janice Anta and Jiomekong, Azanzi and Souopgui, Innocent and Djuidje Kenmoe, Germaine},
	year = {2025},
	keywords = {large language models, artificial intelligence, data, physical sciences and engineering, rolling element bearings, tribology},
	pages = {1080--1083},
}

@inproceedings{bagga_real-time_2024,
	address = {New York, NY, USA},
	series = {{MCHM}'24},
	title = {Real-{Time} {Posture} {Monitoring} and {Risk} {Assessment} for {Manual} {Lifting} {Tasks} {Using} {MediaPipe} and {LSTM}},
	isbn = {979-8-4007-1195-4},
	url = {https://doi.org/10.1145/3688868.3689199},
	doi = {10.1145/3688868.3689199},
	abstract = {This research focuses on developing a real-time posture monitoring and risk assessment system for manual lifting tasks using advanced AI and computer vision technologies. Musculoskeletal disorders (MSDs) are a significant concern for workers involved in manual lifting, and traditional methods for posture correction are often inadequate due to delayed feedback and lack of personalized assessment. Our proposed solution integrates AI-driven posture detection, detailed keypoint analysis, risk level determination, and real-time feedback delivered through a user-friendly web interface. The system aims to improve posture, reduce the risk of MSDs, and enhance user engagement. The research involves comprehensive data collection, model training, and iterative development to ensure high accuracy and user satisfaction. The solution's effectiveness is evaluated against existing methodologies, demonstrating significant improvements in real-time feedback and risk assessment. This study contributes to the field by offering a novel approach to posture correction that addresses existing gaps and provides practical, immediate benefits to users.},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Multimedia} {Computing} for {Health} and {Medicine}},
	publisher = {Association for Computing Machinery},
	author = {Bagga, Ereena and Yang, Ang},
	year = {2024},
	keywords = {machine learning., artificial intelligence, computer vision, musculoskeletal disorders (msds), real-time posture monitoring},
	pages = {79--85},
}

@inproceedings{zhang_prediction_2025,
	address = {New York, NY, USA},
	series = {{ICDLT} '25},
	title = {Prediction of {Air} {Marshals}' {Physical} {Performance} {Based} on {Least} {Squares} {Support} {Vector} {Machine} and {Prediction} {Error} {Correction}},
	isbn = {979-8-4007-1852-6},
	url = {https://doi.org/10.1145/3760658.3760666},
	doi = {10.1145/3760658.3760666},
	abstract = {In order to obtain better prediction results of air marshals' physical performance, the air marshals' physical performance prediction model with least squares support vector machine and prediction error correction is proposed. Firstly, the physical performance of air marshals is modeled and predicted by lifting wavelets and least squares support vector machine, then the prediction results of physical performance of air marshals are corrected by prediction error correction, and finally, tested through a case study on air marshals' physical performance prediction, and comparison experiments are carried out with other prediction models of the physical performance of air marshals to validate its superiority. The results show that the model proposed in this paper reduces the prediction error of air marshals' physical performance, and improves the stability of the prediction results of air marshals' physical performance through prediction error correction, and the prediction accuracy is better than other air marshals' physical performance prediction models.},
	booktitle = {Proceedings of the 2025 9th {International} {Conference} on {Deep} {Learning} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Fan and Zhang, Lixia and Li, Xiang and Li, Mingjia and Ren, Yuxi},
	year = {2025},
	keywords = {air marshals' physical performance, least squares support vector machine, prediction accuracy, prediction error correction, prediction model},
	pages = {52--56},
}

@inproceedings{xiao_intelligent_2025,
	address = {New York, NY, USA},
	series = {{CISAI} '25},
	title = {Intelligent {Control} and {Monitoring} of {AI}-{Driven} {Landing} {Double} {Rocker} {Arm} {Holding} {Systems} in {Mountainous} {Power} {Grid} {Construction}},
	isbn = {979-8-4007-1874-8},
	url = {https://doi.org/10.1145/3773365.3773502},
	doi = {10.1145/3773365.3773502},
	abstract = {Artificial Intelligence (AI) has emerged as a transformative force in power grid construction, offering novel solutions to challenges such as labor shortages, safety risks, and inefficiencies—particularly in complex terrains like mountainous regions. This paper presents both a comprehensive review of AI-driven technologies in tower assembly and a detailed account of an innovative engineering solution: the development of an intelligent, lightweight, self-erecting landing double rocker arm holding system. The proposed system integrates advanced structural design principles with AI-based real-time monitoring and control, enabling rapid deployment and enhanced safety in narrow and high-altitude tower windows. Finite element simulations and field applications on 110 kV and 220 kV projects demonstrate significant reductions in labor demands, material usage, and setup time. The results highlight the synergy of intelligent equipment and AI in achieving safe, efficient, and scalable tower assembly, setting a replicable benchmark for smart construction practices in the power grid sector.},
	booktitle = {Proceedings of the 2025 8th {International} {Conference} on {Computer} {Information} {Science} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Xiao, Qunan and Wang, Hao and Liu, Zhixi and Zhang, Xiao and Ma, Bo and Lin, Chen and Liu, Chang and Nie, Jieliang and Li, Xiqin and Huang, Yue},
	year = {2025},
	keywords = {Machine Learning, Artificial Intelligence, Automated Control, Computer Vision, Intelligent control, Lightweight design, Mast system, Power Grid Construction, Safety Monitoring, Tower Assembly},
	pages = {875--881},
}

@inproceedings{toledo_deeper_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {Deeper {Notions} of {Correctness} in {Image}-{Based} {DNNs}: {Lifting} {Properties} from {Pixel} to {Entities}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3613079},
	doi = {10.1145/3611643.3613079},
	abstract = {Deep Neural Networks (DNNs) that process images are being widely used for many safety-critical tasks, from autonomous vehicles to medical diagnosis. Currently, DNN correctness properties are defined at the pixel level over the entire input. Such properties are useful to expose system failures related to sensor noise or adversarial attacks, but they cannot capture features that are relevant to domain-specific entities and reflect richer types of behaviors. To overcome this limitation, we envision the specification of properties based on the entities that may be present in image input, capturing their semantics and how they change. Creating such properties today is difficult as it requires determining where the entities appear in images, defining how each entity can change, and writing a specification that is compatible with each particular V\&amp;V client. We introduce an initial framework structured around those challenges to assist in the generation of Domain-specific Entity-based properties automatically by leveraging object detection models to identify entities in images and creating properties based on entity features. Our feasibility study provides initial evidence that the new properties can uncover interesting system failures, such as changes in skin color can modify the output of a gender classification network. We conclude by analyzing the framework potential to implement the vision and by outlining directions for future work.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Toledo, Felipe and Shriver, David and Elbaum, Sebastian and Dwyer, Matthew B.},
	year = {2023},
	keywords = {Neural networks, fairness, properties, validation, verification},
	pages = {2122--2126},
}

@inproceedings{gao_research_2024,
	address = {New York, NY, USA},
	series = {{ICCSIE} '24},
	title = {Research on the {Optimization} {Algorithm} to {Avoid} {Malicious} {Code} {Attacks}},
	isbn = {979-8-4007-1813-7},
	url = {https://doi.org/10.1145/3689236.3696046},
	doi = {10.1145/3689236.3696046},
	abstract = {In order to solve the accuracy, time consuming and anti-detection ability of traditional LAN malicious attack code classification method, this paper proposes a LAN malicious code classification method based on improved optimization algorithm. This method adopts 3 D coordinate technology to expand the spatial location of malicious code to a wider range, so as to improve the characteristics of malicious code, so as to effectively prevent malicious attacks. To solve the time-consuming problem, a new transfer learning technology is adopted to improve the anti-confounding ability. In addition, in order to solve the problem of large network computing volume and slow code recovery speed, the MobileNetV3 convolutional neural network model is adopted, combined with the Ranger optimization algorithm technology, to speed up the time-consuming network convergence process. Both the CIFAR-10 and VisualQA datasets achieved 93.5\% and 91.2\%, with a 2.3\% improvement in accuracy and a 25\% improvement in detection efficiency compared to Sentiment140. The improved algorithm has significantly improved the accuracy in the classification than the traditional method, reaching the level of 1.05\%. These improved algorithms not only improve the accuracy of the model, but also enhance the resistance to confounding, and improve the efficiency of classification detection.},
	booktitle = {Proceedings of the 2024 9th {International} {Conference} on {Cyber} {Security} and {Information} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Gao, Ruimei and Zhang, Lina},
	year = {2024},
	pages = {242--251},
}

@article{li_support_2025,
	address = {New York, NY, USA},
	title = {Support {Triangle} {Machine}},
	volume = {9},
	url = {https://doi.org/10.1145/3729255},
	doi = {10.1145/3729255},
	abstract = {Approximations play a pivotal role in verifying deep neural networks (DNNs). Existing approaches typically rely on either single-neuron approximations (simpler to design but less precise) or multi-neuron approximations (higher precision but significantly more complex to construct). Between them, a notable gap exists. This work bridges the gap. The idea is to lift single-neuron approximations into multi-neuron approximations with precision gain. To this end, we formulate the approximation transition as a novel problem, named Convex Approximation Lifting (CAL), and propose a constructive approach, Support Triangle Machine (STM), to solving it. STM is grounded in two core insights: (i) there exists a simple geometric structure, called the support triangle, along with an efficient triangle lifting technique that connects single-neuron approximations and multi-neuron approximations; and (ii) typical single-neuron approximations can be easily decomposed into multiple atomically liftable components. Specifically, given a CAL instance, STM constructs a multi-neuron approximation by iteratively processing each output coordinate. For each coordinate, it decomposes the single-neuron approximation into several linear parts, lifts each of them using the triangle lifting technique, and then synthesize an intermediate approximation, which later servers as input for the next iteration. We theoretically prove the correctness of STM and empirically evaluate its performance on a variety of CAL problems and DNN verification tasks. Experimental results demonstrate STM's broad applicability, improved precision, and sustained efficiency. Beyond DNN verification, STM has the potential to facilitate approximation construction process in more general tasks, and we expect it to catalyze further research in related fields.},
	number = {PLDI},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Li, Jiaying and Hao, Chunxue},
	month = jun,
	year = {2025},
	keywords = {Approximation Lifting, Convex Approximation, Support Triangle Machine},
}

@inproceedings{collie_program_2024,
	series = {{PACT} '21},
	title = {Program {Lifting} using {Gray}-{Box} {Behavior}},
	isbn = {978-1-6654-4278-7},
	url = {https://doi.org/10.1109/PACT52795.2021.00012},
	doi = {10.1109/PACT52795.2021.00012},
	abstract = {Porting specialized application components to new platforms is difficult. This is particularly true if the components depend on proprietary libraries, or specific hardware. To tackle this, existing work has sought to recover high-level descriptions of application components to ease their retargeting. However, existing schemes are either too limited, targeting just one application domain, or too weak, making them ill-suited to recovering real-world programs. Additionally, many rely on help in the form of problem-specific user annotations or complex specifications.This paper develops a new approach using gray-box program synthesis, which recovers code by automatically constructing a program to match the behavior of an unknown component. However, unlike other synthesis approaches, it exploits the dynamic or gray-box behavior of a component to guide recovery. For example, the execution time, memory access patterns or observed instruction traces can all be used to direct synthesis.We evaluate our technique (Haze) extensively against existing program synthesizers and a domain-specific lifter. Our scheme is able to generalize effectively across domains, synthesizing and lifting more programs than prior techniques, without any external assistance. We validate our methodology using bounded model checking, demonstrating that our synthesized programs are correct. Finally, we apply our approach to machine learning workloads, obtaining significant speedups automatically.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {IEEE Press},
	author = {Collie, Bruce and O'Boyle, Michael F.P.},
	year = {2024},
	keywords = {program synthesis, compilers, lifting},
	pages = {60--74},
}

@inproceedings{huang_functional_2025,
	address = {New York, NY, USA},
	series = {{AIPD} '25},
	title = {Functional analysis of patient-assisted transfer equipment based on the {Analytic} {Hierarchy} {Process} combined with {Artificial} {In}-telligence},
	isbn = {979-8-4007-1954-7},
	url = {https://doi.org/10.1145/3765416.3765434},
	doi = {10.1145/3765416.3765434},
	abstract = {With the worsening of population ageing and the continuous increase in the number of chronic disease patients, traditional patient transportation methods are not only inefficient, but also difficult to fully guarantee the comfort and safety of patients. Therefore, improving patient handling efficiency and reducing the labour intensity of nursing staff have become important directions for optimising current medical services. The article elaborates on the complementary roles of the Analytic Hierarchy Process and Artificial Intelligence in decision-making. Subsequently, based on the Analytic Hierarchy Process, the functionality of patient-assisted transfer equipment was analysed, and a hierarchical structure model was constructed based on survey data to determine the priority of core functions and provide key guidance for device development.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Artificial} {Intelligence} and {Product} {Design}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Shipei and Liang, Jian and Zhang, Yang and Yao, Wanyin and Liang, Yaoxin},
	year = {2025},
	keywords = {artificial intelligence, aging, assistive equipment, behavior},
	pages = {104--108},
}

@inproceedings{jain_contextual_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {Contextual {Predictive} {Mutation} {Testing}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3616289},
	doi = {10.1145/3611643.3616289},
	abstract = {Mutation testing is a powerful technique for assessing and improving test suite quality that artificially introduces bugs and checks whether the test suites catch them. However, it is also computationally expensive and thus does not scale to large systems and projects. One promising recent approach to tackling this scalability problem uses machine learning to predict whether the tests will detect the synthetic bugs, without actually running those tests. However, existing predictive mutation testing approaches still misclassify 33\% of detection outcomes on a randomly sampled set of mutant-test suite pairs. We introduce MutationBERT, an approach for predictive mutation testing that simultaneously encodes the source method mutation and test method, capturing key context in the input representation. Thanks to its higher precision, MutationBERT saves 33\% of the time spent by a prior approach on checking/verifying live mutants. MutationBERT, also outperforms the state-of-the-art in both same project and cross project settings, with meaningful improvements in precision, recall, and F1 score. We validate our input representation, and aggregation approaches for lifting predictions from the test matrix level to the test suite level, finding similar improvements in performance. MutationBERT not only enhances the state-of-the-art in predictive mutation testing, but also presents practical benefits for real-world applications, both in saving developer time and finding hard to detect mutants that prior approaches do not.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jain, Kush and Alon, Uri and Groce, Alex and Le Goues, Claire},
	year = {2023},
	keywords = {code coverage, mutation analysis, test oracles},
	pages = {250--261},
}

@inproceedings{ibrahim_space-time_2025,
	address = {New York, NY, USA},
	series = {{PASC} '25},
	title = {Space-{Time} {Parallel} {Scaling} of {Parareal} with a {Physics}-{Informed} {Fourier} {Neural} {Operator} {Coarse} {Propagator} {Applied} to the {Black}-{Scholes} {Equation}},
	isbn = {979-8-4007-1886-1},
	url = {https://doi.org/10.1145/3732775.3733574},
	doi = {10.1145/3732775.3733574},
	abstract = {Iterative parallel-in-time algorithms like Parareal can extend scaling beyond the saturation of purely spatial parallelization when solving initial value problems. However, they require the user to build coarse models to handle the unavoidable serial transport of information in time. This is a time-consuming and difficult process since there is still limited theoretical insight into what constitutes a good and efficient coarse model. Novel approaches from machine learning to solve differential equations could provide a more generic way to find coarse-level models for multi-level parallel-in-time algorithms. This paper demonstrates that a physics-informed Fourier Neural Operator (PINO) is an effective coarse model for the parallelization in time of the two-asset Black-Scholes equation using Parareal. We demonstrate that PINO-Parareal converges as fast as a bespoke numerical coarse model and that, in combination with spatial parallelization by domain decomposition, it provides better overall speedup than both purely spatial parallelization and space-time parallelization with a numerical coarse propagator.},
	booktitle = {Proceedings of the {Platform} for {Advanced} {Scientific} {Computing} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Ibrahim, Abdul Qadir and Götschel, Sebastian and Ruprecht, Daniel},
	year = {2025},
	keywords = {machine learning, black-scholes equation, parallel-in-time integration, parareal, physics-informed neural operator, space-time parallelization},
	pages = {1--11},
}

@inproceedings{lyu_non-collocated_2024,
	address = {New York, NY, USA},
	series = {{UbiComp} '24},
	title = {A {Non}-collocated {Wearable} {Framework} for {Back} {Support} {Exoskeleton} {Payload} {Estimation}},
	isbn = {979-8-4007-1058-2},
	url = {https://doi.org/10.1145/3675094.3678999},
	doi = {10.1145/3675094.3678999},
	abstract = {Back-support exoskeletons(BSEs) show promise in preventing occupational low back pain (OLBP). To address a counter-intuitive interpretation of users' demand with the conventional BSEs, which are controlled based on the lifting speed, and offer users to interpret and anticipate the system's output performance. This paper develops a non-collocated, easy-to-equip wearable system enhancing BSEs' capability to perceive operational loads. We designed a Convolution-inspired Trunk Event Recognition (CTER) algorithm for trunk lifting and bending motion detection. Moreover, conceptualizing human motion signals as audio sequences, the study proposed two deep learning classification models leveraging speech processing techniques. This system is capable of detecting body trunk lifting and bending motions, and features onboard estimation of payload categorized into 0, 5, 10, and 15 kg groups.},
	booktitle = {Companion of the 2024 on {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Lyu, Tian and Yu, Yiang and Narayan, Ashwin and Yu, Haoyong},
	year = {2024},
	keywords = {back support exoskeleton, occupational low back pain, payload estimation, trunk motion detection},
	pages = {877--880},
}

@inproceedings{takahira_insitutale_2025,
	address = {New York, NY, USA},
	series = {{UIST} '25},
	title = {{InSituTale}: {Enhancing} {Augmented} {Data} {Storytelling} with {Physical} {Objects}},
	isbn = {979-8-4007-2037-6},
	url = {https://doi.org/10.1145/3746059.3747678},
	doi = {10.1145/3746059.3747678},
	abstract = {Augmented data storytelling enhances narrative delivery by integrating visualizations with physical environments and presenter actions. Existing systems predominantly rely on body gestures or speech to control visualizations, leaving interactions with physical objects largely underexplored. We introduce augmented physical data storytelling, an approach enabling presenters to manipulate visualizations through physical object interactions. To inform this approach, we first conducted a survey of data-driven presentations to identify common visualization commands. We then conducted workshops with nine HCI/VIS researchers to collect mappings between physical manipulations and these commands. Guided by these insights, we developed InSituTale, a prototype that combines object tracking via a depth camera with Vision-LLM for detecting real-world events. Through physical manipulations, presenters can dynamically execute various visualization commands, delivering cohesive data storytelling experiences that blend physical and digital elements. A user study with 12 participants demonstrated that InSituTale enables intuitive interactions, offers high utility, and facilitates an engaging presentation experience.},
	booktitle = {Proceedings of the 38th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Takahira, Kentaro and Yu, Yue and Fujiwara, Takanori and Suzuki, Ryo and Qu, Huamin},
	year = {2025},
	keywords = {Visualization, Augmented Presentation, Augmented Reality, Data-Driven Storytelling, Tangible Interaction, Video},
}

@inproceedings{yongfeng_method_2020,
	address = {New York, NY, USA},
	series = {{RCAE} 2019},
	title = {Method and {Experiment} {Research} for {Moving} of {Main} {UHV} {Substations} {Equipment} in {Full} {Assembly} {State}},
	isbn = {978-1-4503-7622-8},
	url = {https://doi.org/10.1145/3372047.3372056},
	doi = {10.1145/3372047.3372056},
	abstract = {In UHV substation, moving main transformer and HV shunt reactor in full assembly can reduce the interruption duration in equipment replacement, which is important for the operation of the grid. The UHV transformer and HV shunt reactor are large in volume and heavy in weight. The moving of them in full assembly is a sophisticated process, considering the vibration in the moving will have negative effect on the stable operation of the equipment. This paper carried out method and experimental research on the moving of transformer and HV shunt reactor in full assembly. The research results indicated that the 1,000kV transformer can be moved in full assembly with specially designed track system and vehicle. The maximum acceleration in the moving test was 0.4g and the maximum pull force was 76.7 kN. Traction machinery with output force larger than 200kN is recommended for practice. The 1,000kV HV shunt reactor could be moved in full assembly by tractor vehicle, and the maximum acceleration in test was 0.32g. The results showed that the system designed in this paper is effective in vibration control of the moving process. To further control the vibration amplitude, it is recommended to improve the jacking process, such as applying elastic cushion in the contacting surface of jack or lifting the equipment synchronously.},
	booktitle = {Proceedings of the 2019 {The} 2nd {International} {Conference} on {Robotics}, {Control} and {Automation} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Yongfeng, Cheng and Sheng, Li and Zhicheng, Lu and Cheng, Wang and Xiaoning, Wang and Shujun, Zhang},
	year = {2020},
	keywords = {Transformer, HV shunt reactor, Movement, Ultra-high voltage, Vibration},
	pages = {243--249},
}

@inproceedings{hula_understanding_2024,
	address = {New York, NY, USA},
	series = {{CIKM} '24},
	title = {Understanding {GNNs} for {Boolean} {Satisfiability} through {Approximation} {Algorithms}},
	isbn = {979-8-4007-0436-9},
	url = {https://doi.org/10.1145/3627673.3679813},
	doi = {10.1145/3627673.3679813},
	abstract = {This paper delves into the interpretability of Graph Neural Networks in the context of Boolean Satisfiability. The goal is to demystify the internal workings of these models and provide insightful perspectives into their decision-making processes. This is done by uncovering connections to two approximation algorithms studied in the domain of Boolean Satisfiability: Belief Propagation and Semidefinite Programming Relaxations. Revealing these connections has empowered us to introduce a suite of impactful enhancements. The first significant enhancement is a curriculum training procedure, which incrementally increases the problem complexity in the training set, together with increasing the number of message passing iterations of the Graph Neural Network. We show that the curriculum, together with several other optimizations, reduces the training time by more than an order of magnitude compared to the baseline without the curriculum. Furthermore, we apply decimation and sampling of initial embeddings, which significantly increase the percentage of solved problems.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Hůla, Jan and Mojíek, David and Janota, Mikolá},
	year = {2024},
	keywords = {graph neural networks, approximation algorithms, belief propagation, boolean satisfiability, neuro-symbolic AI, semidefinite programming},
	pages = {953--961},
}

@inproceedings{egan_dog_2024,
	address = {New York, NY, USA},
	series = {{MIG} '24},
	title = {Dog {Code}: {Human} to {Quadruped} {Embodiment} using {Shared} {Codebooks}},
	isbn = {979-8-4007-1090-2},
	url = {https://doi.org/10.1145/3677388.3696339},
	doi = {10.1145/3677388.3696339},
	abstract = {Many VR animal embodiment sytsems suffer from poor animation fidelity, typically animating the animal avatars using inverse kinematics. We address this issue, presenting a novel deep-learning method, centred around a shared codebook, for mapping human motion to quadruped motion. Rather than trying to directly bridge the gap from human motion to quadruped motion, a task which has proven difficult, we first use a rule-based retargeter, relying on inverse and forward kinematics, to retarget human motions to an intermediate motion domain in which the motions share the same skeleton as the quadruped. We then use finite scalar quantization to construct a shared latent space, or codebook, between this intermediate domain and the quadruped motion domain. We do this by first pre-defining a finite number of discrete latent codes and then teaching these codes, using unsupervised deep-learning, to represent semantically similar motions in the two domains. We incorporate our real-time human-to-quadruped motion mapping into a VR quadruped embodiment system. The output quadruped animations are natural and realistic, while also preserving the semantics of users’ actions. Moreover, there is a strong synchrony between the input human motions and retargeted quadruped motions, an important factor for inducing a strong sense of VR embodiment.},
	booktitle = {Proceedings of the 17th {ACM} {SIGGRAPH} {Conference} on {Motion}, {Interaction}, and {Games}},
	publisher = {Association for Computing Machinery},
	author = {Egan, Donal and Jovane, Alberto and Szkaradek, Jan and Fletcher, George and Cosker, Darren and McDonnell, Rachel},
	year = {2024},
	keywords = {Deep-learning, Motion retargeting, Quadruped embodiment, VR embodiment},
}

@inproceedings{yang_dgpl_2025,
	address = {New York, NY, USA},
	series = {{AITC} '25},
	title = {{DGPL}: {A} {Cross}-{View} {Semantic} {Segmentation} {Perception} {Model} in {Bird}'s-{Eye} {View}},
	isbn = {979-8-4007-1862-5},
	url = {https://doi.org/10.1145/3762329.3762341},
	doi = {10.1145/3762329.3762341},
	abstract = {To address the challenges of low feature fusion efficiency, weak long-range object recognition, and high computational overhead in cross-view transformation for autonomous driving perception, we propose DGPL—an end-to-end Bird's Eye View (BEV) semantic segmentation perception model. The model simultaneously extracts multi-camera image semantic features and pixel-level depth information through a shared convolutional neural network. Innovatively, it incorporates a non-uniform learnable depth estimation module to achieve high-precision depth prediction for near-field targets while ensuring efficiency for far-field targets. A reverse grid pooling optimization module is designed to significantly accelerate the projection of 3D features into BEV space. Concurrently, a distance-aware loss function is optimized to enhance recognition capability for distant small objects. Experiments on the nuScenes dataset demonstrate that DGPL achieves state-of-the-art performance in 3D object detection with 42.5 mAP and 45.8 NDS, while outperforming mainstream methods in BEV semantic segmentation tasks. Moreover, the model requires only 9.8 million parameters and achieves an inference speed of 38.5 fps, substantially enhancing feasibility for engineering deployment.},
	booktitle = {Proceedings of the 2nd {International} {Conference} on {Artificial} {Intelligence} of {Things} and {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Xiao and Li, Chuanchang and Zhang, Weiwei},
	year = {2025},
	keywords = {Bird's eye view, Cross-view perception, Semantic segmentation},
	pages = {59--64},
}

@inproceedings{li_queryable_2025,
	address = {New York, NY, USA},
	series = {{MM} '25},
	title = {Queryable {3D} {Scene} {Representation}: {A} {Multi}-{Modal} {Framework} for {Semantic} {Reasoning} and {Robotic} {Task} {Planning}},
	isbn = {979-8-4007-2035-2},
	url = {https://doi.org/10.1145/3746027.3758177},
	doi = {10.1145/3746027.3758177},
	abstract = {To enable robots to comprehend high-level human instructions and perform complex tasks, a key challenge lies in achieving comprehensive scene understanding: interpreting and interacting with the 3D environment in a meaningful way. This requires a smart map that fuses accurate geometric structure with rich, human-understandable semantics. To address this, we introduce the 3D Queryable Scene Representation (3D QSR), a novel framework built on multimedia data that unifies three complementary 3D representations: (1) 3D-consistent novel view rendering and segmentation from panoptic reconstruction, (2) precise geometry from 3D point clouds, and (3) structured, scalable organization via 3D scene graphs. Built on an object-centric design, the framework integrates with large vision-language models to enable semantic queryability by linking multimodal object embeddings, and supporting object-level retrieval of geometric, visual, and semantic information. The retrieved data are then loaded into a robotic task planner for downstream execution.We evaluate our approach through simulated robotic task planning scenarios in Unity, guided by abstract language instructions and using the indoor public dataset Replica. Furthermore, we apply it in a digital duplicate of a real wet lab environment to test QSR-supported robotic task planning for emergency response. The results demonstrate the framework's ability to facilitate scene understanding and integrate spatial and semantic reasoning, effectively translating high-level human instructions into precise robotic task planning in complex 3D environments.},
	booktitle = {Proceedings of the 33rd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Li, Xun and Santa Cruz, Rodrigo and Xi, Mingze and Zhang, Hu and Perera, Madhawa and Wang, Ziwei and Ravendran, Ahalya and Matthews, Brandon J. and Xu, Feng and Adcock, Matt and Wang, Dadong and Liu, Jiajun},
	year = {2025},
	keywords = {3d scene representation, 3d scene understanding, multi-modal, multi-view captioning, robotic perception, robotic task planning, unity, vision-language models, visual language grounding},
	pages = {12492--12500},
}

@inproceedings{zhang_hebridge_2024,
	address = {New York, NY, USA},
	series = {{WAHC} '24},
	title = {{HEBridge}: {Connecting} {Arithmetic} and {Logic} {Operations} in {FV}-style {HE} {Schemes}},
	isbn = {979-8-4007-1241-8},
	url = {https://doi.org/10.1145/3689945.3694807},
	doi = {10.1145/3689945.3694807},
	abstract = {Fully homomorphic encryption (FHE) allows computation over encrypted data without decryption and is considered one of the most essential primitives for privacy-preserving applications. However, there are still no universal FHE schemes that can support efficient and precise evaluation of both arithmetic and logic operations. Many endeavors have been made to enhance the capability of FHE for general computation, such as scheme switching and polynomial approximation. However, the overhead of scheme switching remains prohibitive for real applications with large bit-width inputs. On the other hand, the approximation methods have large errors around the pivotal point, limiting their application in precision-sensitive tasks. Recent studies show that FV-style HE schemes can support efficient and precise logic operations via polynomial interpolation. However, these methods cannot be seamlessly incorporated with arithmetic operations. In this work, we introduce HEBridge to connect the arithmetic and logic operations in the FV-style HE schemes. We first demonstrate that the arithmetic and logic operations operate over different underlying plaintext spaces. To enable continuous arithmetic and logic operations, we propose a reduction function and a lifting function to switch between these plaintext spaces. With HEBridge, we can exploit fast arithmetic and precise logic operations simultaneously in FV-style HE schemes. Experimental results show that the proposed HEBridge is 32.9× faster than direct interpolation methods and 1 to 3 orders of magnitude more efficient than scheme switching on large bit-width inputs.},
	booktitle = {Proceedings of the 12th {Workshop} on {Encrypted} {Computing} \&amp; {Applied} {Homomorphic} {Cryptography}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Yancheng and Chen, Xun and Lou, Qian},
	year = {2024},
	keywords = {digit decomposition, polynomial interpolation, universal fully homomorphic encryption},
	pages = {23--35},
}

@article{fan_reducing_2022,
	address = {New York, NY, USA},
	title = {Reducing the {Latency} of {Touch} {Tracking} on {Ad}-hoc {Surfaces}},
	volume = {6},
	url = {https://doi.org/10.1145/3567730},
	doi = {10.1145/3567730},
	abstract = {Touch sensing on ad-hoc surfaces has the potential to transform everyday surfaces in the environment - desks, tables and walls - into tactile, touch-interactive surfaces, creating large, comfortable interactive spaces without the cost of large touch sensors. Depth sensors are a promising way to provide touch sensing on arbitrary surfaces, but past systems have suffered from high latency and poor touch detection accuracy. We apply a novel state machine-based approach to analyzing touch events, combined with a machine-learning approach to predictively classify touch events from depth data with lower latency and higher touch accuracy than previous approaches. Our system can reduce end-to-end touch latency to under 70ms, comparable to conventional capacitive touchscreens. Additionally, we open-source our dataset of over 30,000 touch events recorded in depth, infrared and RGB for the benefit of future researchers.},
	number = {ISS},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	publisher = {Association for Computing Machinery},
	author = {Fan, Neil Xu and Xiao, Robert},
	month = jan,
	year = {2022},
	keywords = {ad-hoc surfaces, latency reduction, touch detection},
}

@article{dai_mseva_2022,
	address = {New York, NY, USA},
	title = {{MSEva}: {A} {Musculoskeletal} {Rehabilitation} {Evaluation} {System} {Based} on {EMG} {Signals}},
	volume = {19},
	issn = {1550-4859},
	url = {https://doi.org/10.1145/3522739},
	doi = {10.1145/3522739},
	abstract = {In order to better assist the rehabilitation treatment of patients with musculoskeletal injury, standard rehabilitation actions are needed to guide the musculoskeletal rehabilitation process. With more and more urgent demands, the musculoskeletal rehabilitation evaluation systems have attracted a high degree of attention. Experts have proposed a series of systems based on laser, ultrasound, and image, which can give reasonable recognition and judgment. However, these systems either require specialized and expensive equipment or can be affected by ionizing radiation. How to construct a musculoskeletal rehabilitation evaluation system with low cost, good effect, and little injury is still a great challenge. In this article, we propose MSEva, a musculoskeletal rehabilitation evaluation system based on EMG signals. Specifically, the system uses EMG sensors to collect a large amount of data for five rehabilitation actions. Secondly, MSEva uses Wavelet Transform (WT) to extract the signal features and then puts the processed data into the Long Short-Term Memory (LSTM) network for model training. Finally, the system uses the LSTM model to evaluate the normality of the EMG response of rehabilitation actions. The results show that the average accuracy of MSEva reaches 94.37\%, which has important evaluation value in guiding the rehabilitation of musculoskeletal patients.},
	number = {1},
	journal = {ACM Trans. Sen. Netw.},
	publisher = {Association for Computing Machinery},
	author = {Dai, Yuanchao and Wu, Jing and Fan, Yuanzhao and Wang, Jin and Niu, Jianwei and Gu, Fei and Shen, Shigen},
	month = feb,
	year = {2022},
	keywords = {EMG signals, LSTM network, rehabilitation evaluation},
}

@inproceedings{amirtha_varshini_real-time_2021,
	address = {New York, NY, USA},
	series = {{ISEEIE} 2021},
	title = {Real-time {Hand} {Gesture} {Recognition} for {Robotic} {Arm} and {Home} {Automation}},
	isbn = {978-1-4503-8983-9},
	url = {https://doi.org/10.1145/3459104.3459142},
	doi = {10.1145/3459104.3459142},
	abstract = {Hand gestures are a symbolic and non-vocal language and are used by an individual to communicate. With computer vision, hand gestures can be detected and be used to talk with a capable computer, leading to the field of Human-Computer interconnection. The field of computer vision has been achieving cutting edge results with the advent of deep learning models. The work implements the Inception v3 architecture [1], which is a convolutional neural network. The model is retrained on our data set using Transfer learning, with which we reduce the requirements on computational resources, data and time. In this project, a hand gesture is performed in front of a web camera of a system. The gestures are predicted as one among six gestures with a corresponding probability. This project deals with the applications of the detected hand gestures in home automation and control of a robotic arm. Hand gestures are simple to perform, and it makes managing home effortless compared to manually intervening and providing instructions to a machine. In the home automation model, the gesture classification results from the system are transmitted to the microcontroller which switches on or off a home device. The robotic arm is a mechanical system which is used in manipulating the movement of lifting, moving, and placing the workpiece to lighten the work of man. It is equipped with servo motors and is controlled by our hand gestures to perform lifting and dropping of objects and rotation of the robotic arm.},
	booktitle = {2021 {International} {Symposium} on {Electrical}, {Electronics} and {Information} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Amirtha Varshini, A.S. and Bhavani, Guguloth and {Vithya} and Thilagavathy, R.},
	year = {2021},
	pages = {218--223},
}

@inproceedings{guichoux_2d_2024,
	address = {New York, NY, USA},
	series = {{IVA} '24},
	title = {{2D} or not {2D}: {How} {Does} the {Dimensionality} of {Gesture} {Representation} {Affect} {3D} {Co}-{Speech} {Gesture} {Generation}?},
	isbn = {979-8-4007-0625-7},
	url = {https://doi.org/10.1145/3652988.3673934},
	doi = {10.1145/3652988.3673934},
	abstract = {Co-speech gestures are fundamental for communication. The advent of recent deep learning techniques has facilitated the creation of lifelike, synchronous co-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets, aggregating video content from platforms like YouTube via human pose detection technologies, provide a feasible solution by offering 2D skeletal sequences aligned with speech. Concurrent developments in lifting models enable the conversion of these 2D sequences into 3D gesture databases. However, it is important to note that the 3D poses estimated from the 2D extracted poses are, in essence, approximations of the ground-truth, which remains in the 2D domain. This distinction raises questions about the impact of gesture representation dimensionality on the quality of generated motions. Our study examines the effect of using either 2D or 3D joint coordinates as training data on the performance of speech-to-gesture deep generative models.},
	booktitle = {Proceedings of the 24th {ACM} {International} {Conference} on {Intelligent} {Virtual} {Agents}},
	publisher = {Association for Computing Machinery},
	author = {Guichoux, Téo and Soulier, Laure and Obin, Nicolas and Pelachaud, Catherine},
	year = {2024},
	keywords = {Co-speech gesture generation, Diffusion Models, Pose Representation, Sequence modeling},
}

@inproceedings{amara_nearest_2022,
	address = {New York, NY, USA},
	series = {{ICMR} '22},
	title = {Nearest {Neighbor} {Search} with {Compact} {Codes}: {A} {Decoder} {Perspective}},
	isbn = {978-1-4503-9238-9},
	url = {https://doi.org/10.1145/3512527.3531408},
	doi = {10.1145/3512527.3531408},
	abstract = {Modern approaches for fast retrieval of similar vectors on billion-scaled datasets rely on compressed-domain approaches such as binary sketches or product quantization. These methods minimize a certain loss, typically the Mean Squared Error or other objective functions tailored to the retrieval problem. In this paper, we re-interpret popular methods such as binary hashing or product quantizers as auto-encoders, and point out that they implicitly make suboptimal assumptions on the form of the decoder. We design backward-compatible decoders that improve the reconstruction of the vectors from the same codes, which translates to a better performance in nearest neighbor search. Our method significantly improves over binary hashing methods and product quantization on popular benchmarks.},
	booktitle = {Proceedings of the 2022 {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Amara, Kenza and Douze, Matthijs and Sablayrolles, Alexandre and Jégou, Hervé},
	year = {2022},
	keywords = {indexing, nearest-neighbors, quantization},
	pages = {167--175},
}

@inproceedings{ying_retouchingffhq_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {{RetouchingFFHQ}: {A} {Large}-scale {Dataset} for {Fine}-grained {Face} {Retouching} {Detection}},
	isbn = {979-8-4007-0108-5},
	url = {https://doi.org/10.1145/3581783.3611843},
	doi = {10.1145/3581783.3611843},
	abstract = {The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of digital appearances and the impact of deceptive advertising. To address these issues, there is a pressing need to develop advanced face retouching techniques. However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this field. In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over half a million conditionally-retouched images. RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization. By including four typical types of face retouching operations and different retouching levels, we extend the binary face retouching detection into a fine-grained, multi-retouching type, and multi-retouching level estimation problem. Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-scale representation learning. Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance on face retouching detection.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Ying, Qichao and Liu, Jiaxin and Li, Sheng and Xu, Haisheng and Qian, Zhenxing and Zhang, Xinpeng},
	year = {2023},
	keywords = {deep learning, forensics, datasets, face retouching detection},
	pages = {737--746},
}

@incollection{zhao_design_2025,
	address = {New York, NY, USA},
	title = {The {Design} of {Small}-{Scale} {Intelligent} {Hovercraft}},
	isbn = {979-8-4007-1435-1},
	url = {https://doi.org/10.1145/3756423.3756545},
	abstract = {To overcome the limitations of traditional hovercraft in complex sea conditions and multifunctional collaborative applications, our team has designed a small intelligent unmanned hovercraft featuring an efficient cushion-lifting system. Based on the core principles of the air-spring model and simulation studies of cushion flow characteristics, we utilized Abaqus software to deeply explore the hovercraft's water-entry dynamics and attitude variations under varying pressures, achieving precise capture of airbag centroid velocity and displacement. A physical model was constructed for CFD simulations, laying a solid foundation for aerodynamic design optimization. The hovercraft integrates multiple sensors, including LIDAR and sonar, to form a comprehensive environmental perception matrix for real-time of obstacle, current and terrain. Driven by data, the intelligent control system incorporates machine learning and expert systems for smart decision-making, enabling precise control of parameters such as speed and steering to achieve adaptive navigation in complex waters or terrains. This innovation opens up new avenues for multi-domain applications and leads the trend in hovercraft technology transformation.},
	booktitle = {Proceedings of the 2025 {International} {Conference} on {Artificial} {Intelligence} and {Smart} {Manufacturing}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Zhiyao and Fu, Tingyu and Yan, Yifei and Han, Bingchen and Xiong, Yueling and Wu, Peng},
	year = {2025},
	pages = {742--745},
}

@inproceedings{liu_exploiting_2025,
	address = {New York, NY, USA},
	series = {{ASPDAC} '25},
	title = {Exploiting {Differential}-{Based} {Data} {Encoding} for {Enhanced} {Query} {Efficiency}},
	isbn = {979-8-4007-0635-6},
	url = {https://doi.org/10.1145/3658617.3697565},
	doi = {10.1145/3658617.3697565},
	abstract = {Storing large-scale high-dimensional data, which is rapidly generated by both industry and academia, poses substantial challenges, primarily in terms of storage and maintenance costs. While data compression techniques offer a potential solution to these challenges, they must overcome two critical hurdles: 1) preserving data integrity within lossless bounds and 2) maintaining query performance on compressed data.To address the above challenges, we propose a novel approach based on differential techniques that combine high-dimensional data compression with an efficient query engine. Specifically, Our methodology begins by employing vector quantization to transform high-dimensional vectors into compact integer sequences, known as quantization codes—a classical and versatile compression method. Subsequently, we introduce DCQ, which builds upon the differential concept to perform lossless compression on quantization codes. DCQ organizes quantization codes into a tree structure and constructs a hierarchy of trees by analyzing the dissimilarity between two quantization codes. To minimize the impact on query performance, we have developed an adaptive reconstruction algorithm that partitions and reconstructs the original tree structure. This algorithm effectively balances the workload for searching each sub-tree, optimizing the compression-performance trade-off. Finally, we adopt a storage format based on multiple subtrees for rapid searching, thereby enabling differential storage to facilitate efficient queries. Extensive experiments on various large-scale real-world datasets show that DCQ achieves a compression ratio of up to 2.5 on the quantization codes and achieves \&gt; 50× performance improvement compared to the state-of-the-art general-purpose lossless compression techniques.},
	booktitle = {Proceedings of the 30th {Asia} and {South} {Pacific} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Fangxin and Wang, Zongwu and Xu, Peng and Huang, Shiyuan and Jiang, Li},
	year = {2025},
	pages = {594--600},
}

@inproceedings{thangamani_lifting_2023,
	address = {New York, NY, USA},
	series = {{CGO} '23},
	title = {Lifting {Code} {Generation} of {Cardiac} {Physiology} {Simulation} to {Novel} {Compiler} {Technology}},
	isbn = {979-8-4007-0101-6},
	url = {https://doi.org/10.1145/3579990.3580008},
	doi = {10.1145/3579990.3580008},
	abstract = {The study of numerical models for the human body has become a major focus of the research community in biology and medicine. For instance, numerical ionic models of a complex organ, such as the heart, must be able to represent individual cells and their interconnections through ionic channels, forming a system with billions of cells, and requiring efficient code to handle such a large system. The modeling of the electrical system of the heart combines a compute-intensive kernel that calculates the intensity of current flowing through cell membranes, and feeds a linear solver for computing the electrical potential of each cell. Considering this context, we propose limpetMLIR, a code generator and compiler transformer to accelerate the kernel phase of ionic models and bridge the gap between compiler technology and electrophysiology simulation. LimpetMLIR makes use of the MLIR infrastructure, its dialects, and transformations to drive forward the study of ionic models, and accelerate the execution of multi-cell systems. Experiments conducted in 43 ionic models show that our limpetMLIR based code generation greatly outperforms current state-of-the-art simulation systems by an average of 2.9x, reaching peak speedups of more than 15x in some cases. To our knowledge, this is the first work that deeply connects an optimizing compiler infrastructure to electrophysiology models of the human body, showing the potential benefits of using compiler technology in the simulation of human cell interactions.},
	booktitle = {Proceedings of the 21st {ACM}/{IEEE} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {Association for Computing Machinery},
	author = {Thangamani, Arun and Jost, Tiago Trevisan and Loechner, Vincent and Genaud, Stéphane and Bramas, Bérenger},
	year = {2023},
	keywords = {Code generation and optimization, code transformation, domain-specific languages, vectorization},
	pages = {68--80},
}

@inproceedings{mehmood_safe_2021,
	address = {New York, NY, USA},
	series = {{CAADCPS} '21},
	title = {Safe {CPS} from unsafe controllers},
	isbn = {978-1-4503-8399-8},
	url = {https://doi.org/10.1145/3457335.3461712},
	doi = {10.1145/3457335.3461712},
	abstract = {Modern cyber-physical systems (CPS) interact with the physical world, hence their correctness is important. In this work, we build upon the Simplex Architecture, where control authority may switch from an unverified and potentially unsafe advanced controller to a verified-safe baseline controller in order to maintain system safety. We take the approach further by lifting the requirement that the baseline controller must be verified or even correct, instead also treating it as a black-box component. This change is important; there are many types of powerful control techniques—model predictive control and neural network controllers—that often work well in practice, but are unlikely to be formally proven correct due to complexity. We prove such an architecture maintains safety, and present case studies where model-predictive control provides safety for multi-robot coordination, and unverified neural networks provably prevent collisions for groups of F-16 aircraft.},
	booktitle = {Proceedings of the {Workshop} on {Computation}-{Aware} {Algorithmic} {Design} for {Cyber}-{Physical} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Mehmood, Usama and Bak, Stanley and Smolka, Scott A. and Stoller, Scott D.},
	year = {2021},
	keywords = {formal verification, cyber-physical systems, simplex architecture},
	pages = {26--28},
}

@article{mack_tutorial_2024,
	address = {New York, NY, USA},
	title = {Tutorial: {A} {Novel} {Runtime} {Environment} for {Accelerator}-{Rich} {Heterogeneous} {Architectures}},
	volume = {24},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3687463},
	doi = {10.1145/3687463},
	abstract = {As the landscape of computing advances, system designers are increasingly exploring methodologies that leverage higher levels of heterogeneity to enhance performance within constrained size, weight, power, and cost parameters. CEDR (Compiler-integrated Extensible DSSoC Runtime) stands as an ecosystem facilitating productive and efficient application development and deployment across heterogeneous computing systems. It fosters the co-design of applications, scheduling heuristics, and accelerators within a unified framework. Our goal is to present CEDR as a promising environment for lifting the barriers to research on heterogeneous systems and addressing the broader challenges within domain-specific architectures. We introduce CEDR and discuss the evolutionary design decisions underlying its programming model. Subsequently, we explore its utility for a broad range of users through design sweeps on off-the-shelf heterogeneous platforms across scheduling heuristics, hardware compositions, and workload scenarios.},
	number = {1},
	journal = {ACM Trans. Embed. Comput. Syst.},
	publisher = {Association for Computing Machinery},
	author = {Mack, Joshua and Krishnakumar, Anish and Ogras, Umit and Akoglu, Ali},
	month = feb,
	year = {2024},
	keywords = {Domain-specific SoCs, heterogeneous application runtimes},
}

@inproceedings{shafiei_position_2022,
	address = {New York, NY, USA},
	series = {{WoRMA} '22},
	title = {Position {Paper}: {On} {Advancing} {Adversarial} {Malware} {Generation} {Using} {Dynamic} {Features}},
	isbn = {978-1-4503-9179-5},
	url = {https://doi.org/10.1145/3494110.3528244},
	doi = {10.1145/3494110.3528244},
	abstract = {Along the evolution of malware detection systems, adversaries develop sophisticated evasion techniques that render malicious samples undetectable. Especially for ML-based detection systems, an effective approach is to craft adversarial malware to evade detection. In this position paper, we conduct a critical review of existing adversarial attacks against malware detection, and conclude that current research focuses mainly on evasion techniques against static analysis; generating adversarial Windows samples to evade dynamic analysis remains largely unexplored. In the context of black-box attack scenarios, we investigate an adversary's potential to carry out practical transformations in order to influence behavioral features observed by ML systems and security products. Moreover, we investigate the range of dynamic behavior transformations and identify critical properties and associated challenges that relate to feasibility, automation, technical costs and detection risks. Through this discussion, we propose solutions to important challenges and present promising paths for future research on evasive malware under dynamic analysis.},
	booktitle = {Proceedings of the 1st {Workshop} on {Robust} {Malware} {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Shafiei, Ali and Rimmer, Vera and Tsingenopoulos, Ilias and Desmet, Lieven and Joosen, Wouter},
	year = {2022},
	keywords = {malware detection, adversarial attack, dynamic analysis, evasion},
	pages = {15--20},
}

@inproceedings{srinivasan_pluto_2025,
	address = {New York, NY, USA},
	series = {{IUI} '25},
	title = {Pluto: {Authoring} {Semantically} {Aligned} {Text} and {Charts} for {Data}-{Driven} {Communication}},
	isbn = {979-8-4007-1306-4},
	url = {https://doi.org/10.1145/3708359.3712122},
	doi = {10.1145/3708359.3712122},
	abstract = {Textual content (including titles, annotations, and captions) plays a central role in helping readers understand a visualization by emphasizing, contextualizing, or summarizing the depicted data. Yet, existing visualization tools provide limited support for jointly authoring the two modalities of text and visuals such that both convey semantically-rich information and are cohesively integrated. In response, we introduce Pluto, a mixed-initiative authoring system that uses features of a chart’s construction (e.g., visual encodings) as well as any textual descriptions a user may have drafted to make suggestions about the content and presentation of the two modalities. For instance, a user can begin to type out a description and interactively brush a region of interest in the chart, and Pluto\&nbsp;will generate a relevant auto-completion of the sentence. Similarly, based on a written description, Pluto\&nbsp;may suggest lifting a sentence out as an annotation or the visualization’s title, or may suggest applying a data transformation (e.g., sort) to better align the two modalities. A preliminary user study revealed that Pluto’s recommendations were particularly useful for bootstrapping the authoring process and helped identify different strategies participants adopt when jointly authoring text and charts. Based on study feedback, we discuss design implications for integrating interactive verification features between charts and text, offering control over text verbosity and tone, and enhancing the bidirectional flow in unified text and chart authoring tools.},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Intelligent} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Srinivasan, Arjun and Setlur, Vidya and Satyanarayan, Arvind},
	year = {2025},
	keywords = {Visualization, caption, description, mixed-initiative, recommendation.},
	pages = {1123--1140},
}

@inproceedings{saber_latibari_iret_2024,
	address = {New York, NY, USA},
	series = {{GLSVLSI} '24},
	title = {{IRET}: {Incremental} {Resolution} {Enhancing} {Transformer}},
	isbn = {979-8-4007-0605-9},
	url = {https://doi.org/10.1145/3649476.3660380},
	doi = {10.1145/3649476.3660380},
	abstract = {In our research paper, we introduce a revolutionary approach to designing energy-aware dynamically prunable Vision Transformers for use in edge applications. Our solution denoted as Incremental Resolution Enhancing Transformer (IRET), works by the sequential sampling of the input image. However, in our case, the embedding size of input tokens is considerably smaller than prior-art solutions. This embedding is used in the first few layers of the IRET vision transformer until a reliable attention matrix is formed. Then the attention matrix is used to sample additional information using a learnable 2D lifting scheme only for important tokens and IRET drops the tokens receiving low attention scores. Hence, as the model pays more attention to a subset of tokens for its task, its focus and resolution also increase. This incremental attention-guided sampling of input and dropping of unattended tokens allow IRET to significantly prune its computation tree on demand. By controlling the threshold for dropping unattended tokens and increasing the focus of attended ones, we can train a model that dynamically trades off complexity for accuracy. This is especially useful for edge devices, where accuracy and complexity could be dynamically traded based on factors such as battery life, reliability, etc.},
	booktitle = {Proceedings of the {Great} {Lakes} {Symposium} on {VLSI} 2024},
	publisher = {Association for Computing Machinery},
	author = {Saber Latibari, Banafsheh and Salehi, Soheil and Homayoun, Houman and Sasan, Avesta},
	year = {2024},
	keywords = {Vision Transformer, Attention, Focus, Token Dropping},
	pages = {620--625},
}

@incollection{wicaksono_idiffpose_2025,
	address = {New York, NY, USA},
	title = {{IDiffPose}: {Equilibrium} {Substitution} for {Lightweight} {Diffusion}-{Graph}-{Based} {Pose} {Estimation}},
	isbn = {979-8-4007-2005-5},
	url = {https://doi.org/10.1145/3743093.3771064},
	abstract = {Recent diffusion–graph frameworks such as DiffPose have advanced monocular 2D-to-3D pose lifting by coupling graph convolutional networks (GCNs) with denoising diffusion models. However, their depth is still bounded by the number of explicit attention–GCN blocks, limiting the receptive field under a fixed parameter budget. We propose IDiffPose, short for Implicit Diffusion Pose, an equilibrium substitution that replaces a selected deep block with a weight-tied operator whose representation is computed via a small number of unrolled fixed-point iterations. The training objective remains standard DDPM-style denoising, inference uses a deterministic DDIM sampler, and the forward process injects heteroscedastic Gaussian noise scaled by 2D heat-map uncertainty. Because the same weights are re-used across iterations, the model attains large effective depth with a compact parameter footprint; when non-equilibrium blocks are frozen, the number of trainable parameters drops substantially. On Human3.6M, substituting only the middle block improves MPJPE from 31.55 to 31.10\&nbsp;mm and P-MPJPE from 24.72 to 24.47\&nbsp;mm with 167,521 trainable parameters (vs. 1,025,674 explicit), while making all blocks implicit reaches 30.90/24.51\&nbsp;mm. These results show equilibrium substitution is a practical drop-in upgrade for diffusion-based pose lifting, yielding consistent accuracy with favorable efficiency controls.},
	booktitle = {Proceedings of the 7th {ACM} {International} {Conference} on {Multimedia} in {Asia}},
	publisher = {Association for Computing Machinery},
	author = {Wicaksono, Nugroho and Muflikhah, Lailil and Yudistira, Novanto},
	year = {2025},
}

@article{wang_toothfairy_2024,
	address = {New York, NY, USA},
	title = {{ToothFairy}: {Real}-time {Tooth}-by-tooth {Brushing} {Monitor} {Using} {Earphone} {Reversed} {Signals}},
	volume = {7},
	url = {https://doi.org/10.1145/3631412},
	doi = {10.1145/3631412},
	abstract = {Tooth brushing monitors have the potential to enhance oral hygiene and encourage the development of healthy brushing habits. However, previous studies fall short of recognizing each tooth due to limitations in external sensors and variations among users. To address these challenges, we present ToothFairy, a real-time tooth-by-tooth brushing monitor that uses earphone reverse signals captured within the oral cavity to identify each tooth during brushing. The key component of ToothFairy is a novel bone-conducted acoustic attenuation model, which quantifies sound propagation within the oral cavity. This model eliminates the need for machine learning and can be calibrated with just one second of brushing data for each tooth by a new user. ToothFairy also addresses practical issues such as brushing detection and tooth region determination. Results from extensive experiments, involving 10 volunteers and 25 combinations of five commercial off-the-shelf toothbrush and earphone models each, show that ToothFairy achieves tooth recognition with an average accuracy of 90.5\%.},
	number = {4},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	publisher = {Association for Computing Machinery},
	author = {Wang, Yang and Hong, Feng and Jiang, Yufei and Bao, Chenyu and Liu, Chao and Guo, Zhongwen},
	month = jan,
	year = {2024},
	keywords = {Acoustic Attenuation, Earphone Reversed Signals, Toothbrushing},
}

@inproceedings{dinella_inferring_2024,
	address = {New York, NY, USA},
	series = {{FSE} 2024},
	title = {Inferring {Natural} {Preconditions} via {Program} {Transformation}},
	isbn = {979-8-4007-0658-5},
	url = {https://doi.org/10.1145/3663529.3663865},
	doi = {10.1145/3663529.3663865},
	abstract = {We introduce an approach for inferring natural preconditions from code. Prior works generate preconditions from scratch through combinations of boolean predicates, but fall short in readability and ease of comprehension. In contrast, our technique leverages the structure of the target method as a seed to infer a precondition through program transformations. Our technique is a multi-phase approach involving iterative test generation and program transformation. Our evaluation shows that humans can more easily reason over preconditions inferred using our approach. Consumers of our preconditions completed reasoning tasks more accurately (92.86\%) with an average total duration of 109 seconds. In contrast, consumers of the preconditions inferred by prior work took twice as long (217 seconds) to finish the study and answered with lower accuracy (85.61\%).},
	booktitle = {Companion {Proceedings} of the 32nd {ACM} {International} {Conference} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Dinella, Elizabeth and Lahiri, Shuvendu K. and Naik, Mayur},
	year = {2024},
	keywords = {Automated Testing, Natural Language Processing, Preconditions},
	pages = {657--658},
}

@article{vasilenko_safe_2022,
	address = {New York, NY, USA},
	title = {Safe couplings: coupled refinement types},
	volume = {6},
	url = {https://doi.org/10.1145/3547643},
	doi = {10.1145/3547643},
	abstract = {We enhance refinement types with mechanisms to reason about relational properties of probabilistic computations. Our mechanisms, which are inspired from probabilistic couplings, are applicable to a rich set of probabilistic properties, including expected sensitivity, which ensures that the distance between outputs of two probabilistic computations can be controlled from the distance between their inputs. We implement our mechanisms in the type system of Liquid Haskell and we use them to formally verify Haskell implementations of two classic machine learning algorithms: Temporal Difference (TD) reinforcement learning and stochastic gradient descent (SGD). We formalize a fragment of our system for discrete distributions and we prove soundness with respect to a set-theoretical semantics.},
	number = {ICFP},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Vasilenko, Elizaveta and Vazou, Niki and Barthe, Gilles},
	month = aug,
	year = {2022},
	keywords = {Haskell, program verification, refinement types, relational types},
}

@inproceedings{watson_financial_2021,
	address = {New York, NY, USA},
	series = {{ICAIF} '20},
	title = {Financial table extraction in image documents},
	isbn = {978-1-4503-7584-9},
	url = {https://doi.org/10.1145/3383455.3422520},
	doi = {10.1145/3383455.3422520},
	abstract = {Table extraction has long been a pervasive problem in financial services. This is more challenging in the image domain, where content is locked behind cumbersome pixel format. Luckily, advances in deep learning for image segmentation, OCR, and sequence modeling provides the necessary heavy lifting to achieve impressive results. This paper presents an end-to-end pipeline for identifying, extracting and transcribing tabular content in image documents, while retaining the original spatial relations with high fidelity.},
	booktitle = {Proceedings of the {First} {ACM} {International} {Conference} on {AI} in {Finance}},
	publisher = {Association for Computing Machinery},
	author = {Watson, William and Liu, Bo},
	year = {2021},
	keywords = {financial tables, image segmentation, optical character recognition, sequence modeling, table extraction},
}

@inproceedings{fothi_skel3d_2025,
	address = {New York, NY, USA},
	series = {{IntRob} '25},
	title = {{Skel3D}: {Skeleton} {Guided} {Novel} {View} {Synthesis}},
	isbn = {979-8-4007-1589-1},
	url = {https://doi.org/10.1145/3759355.3759627},
	doi = {10.1145/3759355.3759627},
	abstract = {In this paper, we present an approach for monocular open-set novel view synthesis (NVS) that leverages object skeletons to guide the underlying diffusion model. Building upon a baseline that utilizes a pre-trained 2D image generator, our method takes advantage of the Objaverse dataset, which includes animated objects with bone structures. By introducing a skeleton guide layer following the existing ray conditioning normalization (RCN) layer, our approach enhances pose accuracy and multiview consistency. The skeleton guide layer provides detailed structural information for the generative model, improving the quality of the synthesized views. Experimental results demonstrate that our skeleton-guided method significantly enhances consistency and accuracy across diverse object categories within the Objaverse dataset. Our method outperforms existing state-of-the-art NVS techniques both quantitatively and qualitatively, without relying on explicit 3D representations. The code is available at https://github.com/skel3d/skel3d},
	booktitle = {Proceedings of the {Intelligent} {Robotics} {FAIR} 2025},
	publisher = {Association for Computing Machinery},
	author = {Fóthi, Áron and Fazekas, Bence and Gyöngyössy, Natabara Máté and Fenech, Kristian},
	year = {2025},
	pages = {92--99},
}

@article{wei_large-area_2025,
	address = {New York, NY, USA},
	title = {Large-{Area} {Fabrication}-aware {Computational} {Diffractive} {Optics}},
	volume = {44},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3763358},
	doi = {10.1145/3763358},
	abstract = {Differentiable optics, as an emerging paradigm that jointly optimizes optics and (optional) image processing algorithms, has made many innovative optical designs possible across a broad range of imaging and display applications. Many of these systems utilize diffractive optical components for holography, PSF engineering, or wavefront shaping. Existing approaches have, however, mostly remained limited to laboratory prototypes, owing to a large quality gap between simulation and manufactured devices.We aim at lifting the fundamental technical barriers to the practical use of learned diffractive optical systems. To this end, we propose a fabrication-aware design pipeline for diffractive optics fabricated by direct-write grayscale lithography followed by replication with nano-imprinting, which is directly suited for inexpensive mass-production of large area designs. We propose a super-resolved neural lithography model that can accurately predict the 3D geometry generated by the fabrication process. This model can be seamlessly integrated into existing differentiable optics frameworks, enabling fabrication-aware, end-to-end optimization of computational optical systems. To tackle the computational challenges, we also devise tensor-parallel compute framework centered on distributing large-scale FFT computation across many GPUs.As such, we demonstrate large scale diffractive optics designs up to 32.16 mm × 21.44 mm, simulated on grids of up to 128,640 by 85,760 feature points. We find adequate agreement between simulation and fabricated prototypes for applications such as holography and PSF engineering. We also achieve high image quality from an imaging system comprised only of a single diffractive optical element, with images processed only by a one-step inverse filter utilizing the simulation PSF. We believe our findings lift the fabrication limitations for real-world applications of diffractive optics and differentiable optical design.},
	number = {6},
	journal = {ACM Trans. Graph.},
	publisher = {Association for Computing Machinery},
	author = {Wei, Kaixuan and Romero, Hector and Amata, Hadi and Sun, Jipeng and Fu, Qiang and Heide, Felix and Heidrich, Wolfgang},
	month = feb,
	year = {2025},
	keywords = {computational fabrication, computational imaging, computational optics},
}

@inproceedings{ahmad_vector_2022,
	address = {New York, NY, USA},
	series = {{ASPLOS} '22},
	title = {Vector instruction selection for digital signal processors using program synthesis},
	isbn = {978-1-4503-9205-1},
	url = {https://doi.org/10.1145/3503222.3507714},
	doi = {10.1145/3503222.3507714},
	abstract = {Instruction selection, whereby input code represented in an intermediate representation is translated into executable instructions from the target platform, is often the most target-dependent component in optimizing compilers. Current approaches include pattern matching, which is brittle and tedious to design, or search-based methods, which are limited by scalability of the search algorithm. In this paper, we propose a new algorithm that first abstracts the target platform instructions into high-level uber-instructions, with each uber-instruction unifying multiple concrete instructions from the target platform. Program synthesis is used to lift input code sequences into semantically equivalent sequences of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared to code generated by a professionally-developed optimizing compiler for the same platform.},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Ahmad, Maaz Bin Safeer and Root, Alexander J. and Adams, Andrew and Kamil, Shoaib and Cheung, Alvin},
	year = {2022},
	keywords = {program synthesis, compiler optimizations, Instruction selection},
	pages = {1004--1016},
}

@article{aguirre_pre-expectation_2021,
	address = {New York, NY, USA},
	title = {A pre-expectation calculus for probabilistic sensitivity},
	volume = {5},
	url = {https://doi.org/10.1145/3434333},
	doi = {10.1145/3434333},
	abstract = {Sensitivity properties describe how changes to the input of a program affect the output, typically by upper bounding the distance between the outputs of two runs by a monotone function of the distance between the corresponding inputs. When programs are probabilistic, the distance between outputs is a distance between distributions. The Kantorovich lifting provides a general way of defining a distance between distributions by lifting the distance of the underlying sample space; by choosing an appropriate distance on the base space, one can recover other usual probabilistic distances, such as the Total Variation distance. We develop a relational pre-expectation calculus to upper bound the Kantorovich distance between two executions of a probabilistic program. We illustrate our methods by proving algorithmic stability of a machine learning algorithm, convergence of a reinforcement learning algorithm, and fast mixing for card shuffling algorithms. We also consider some extensions: using our calculus to show convergence of Markov chains to the uniform distribution over states and an asynchronous extension to reason about pairs of program executions with different control flow.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Aguirre, Alejandro and Barthe, Gilles and Hsu, Justin and Kaminski, Benjamin Lucien and Katoen, Joost-Pieter and Matheja, Christoph},
	month = jan,
	year = {2021},
	keywords = {verification, probabilistic programming},
}

@inproceedings{liu_unite_2023,
	address = {New York, NY, USA},
	series = {{CIKM} '23},
	title = {{UniTE}: {A} {Unified} {Treatment} {Effect} {Estimation} {Method} for {One}-sided and {Two}-sided {Marketing}},
	isbn = {979-8-4007-0124-5},
	url = {https://doi.org/10.1145/3583780.3615100},
	doi = {10.1145/3583780.3615100},
	abstract = {Many internet platforms are two-sided markets that involve two types of participants. Examples include e-commerce platforms like Taobao (retailers and consumers) and ride-hailing platforms like Uber (drivers and passengers). Participants of different types in the two-sided market have relationships (i.e., supply and demand) that provide externalities and network benefits. On two-sided platforms, marketing campaigns are designed by subsidizing supply or demand. Uplift models built in this scenario usually consider the treatment assignment for only one of the two sides. However, ignoring the interaction of treatments between two sides or treating them as noises may result in incomplete models and inaccurate predictions. As far as we know, there is not much work related to modeling the combinational treatment effects in the two-sided market. In this paper, we first introduce the two-sided treatment effects estimation problem and then propose a Unified Treatment effect Estimation (UniTE) method for one-sided and two-sided marketing. We extend the Robinson Decomposition to two-sided, in which the relationship of the three involved tasks, namely the outcome, the propensity, and the treatment effect, is theoretically derived. Based on the decomposition result, a multi-task-based neural network model is proposed to integrate the three tasks and learn the inter-task-related common information, which prompts the model to estimate the treatment effects better. We also propose a unified synthetic data generation method that adapts to one/two-sided situations to verify the treatment effects estimation performance. Extensive and comprehensive experimental results show that our method outperforms the other methods.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Runshi and Hou, Zhipeng},
	year = {2023},
	keywords = {neural networks, multi-task learning, optimization, treatment effects, two-sided market, uplift},
	pages = {1472--1481},
}

@article{lemercier_storm_2023,
	title = {{StoRM}: {A} {Diffusion}-{Based} {Stochastic} {Regeneration} {Model} for {Speech} {Enhancement} and {Dereverberation}},
	volume = {31},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2023.3294692},
	doi = {10.1109/TASLP.2023.3294692},
	abstract = {Diffusion models have shown a great ability at bridging the performance gap between predictive and generative approaches for speech enhancement. We have shown that they may even outperform their predictive counterparts for non-additive corruption types or when they are evaluated on mismatched conditions. However, diffusion models suffer from a high computational burden, mainly as they require to run a neural network for each reverse diffusion step, whereas predictive approaches only require one pass. As diffusion models are generative approaches they may also produce vocalizing and breathing artifacts in adverse conditions. In comparison, in such difficult scenarios, predictive models typically do not produce such artifacts but tend to distort the target speech instead, thereby degrading the speech quality. In this work, we present a stochastic regeneration approach where an estimate given by a predictive model is provided as a guide for further diffusion. We show that the proposed approach uses the predictive model to remove the vocalizing and breathing artifacts while producing very high quality samples thanks to the diffusion model, even in adverse conditions. We further show that this approach enables to use lighter sampling schemes with fewer diffusion steps without sacrificing quality, thus lifting the computational burden by an order of magnitude. Source code and audio examples are available online.},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	publisher = {IEEE Press},
	author = {Lemercier, Jean-Marie and Richter, Julius and Welker, Simon and Gerkmann, Timo},
	month = jul,
	year = {2023},
	pages = {2724--2737},
}

@inproceedings{guo_real-time_2021,
	address = {New York, NY, USA},
	series = {{ICCMS} '21},
	title = {Real-time {Obstacles} {Avoidance} for {Crawler} {Crane} based on {DQN}},
	isbn = {978-1-4503-8979-2},
	url = {https://doi.org/10.1145/3474963.3474993},
	doi = {10.1145/3474963.3474993},
	abstract = {Safety always is a crucial consideration for crawler cranes running with dozens of tons of load in its work site. However, around its work site, besides static obstacles, there are many dynamic obstacles, such as workers and engineering vehicles. These obstacles are potential risk for crane in congested work site. Therefore, risk probability and loss can be reduced in project of lifting engineering by dynamic lift-path planning system. However, presently, most proposed methods pay attention to search path in totally known environment. In this paper, we present a dynamic obstacles avoidance planner for crawler cranes in no-walk scenarios in partially known environment. This planner considers the lifted module as a 3DOF convex robot with discrete rotational and translational motions. First, we improved the structure of neural network of Deep Q-network by applying Resnet block for achieving real-time path planning for crawler crane. Thereafter, we also improved Artificial Potential Field Method (APFM) and apply it to search path for lifted module. And for shortening training time, we also apply the data generated by APFM to pre-train our neural network. Finally, we verified and compared the performance of APFM and DQN by simulation. The result of simulation can demonstrate the effectiveness of the presented planner.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Computer} {Modeling} and {Simulation}},
	publisher = {Association for Computing Machinery},
	author = {Guo, Wanda and Wang, Xin and Jiao, Bo},
	year = {2021},
	keywords = {APFM, Crane, DQN, Path planning},
	pages = {210--217},
}

@inproceedings{qiu_ivt_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {{IVT}: {An} {End}-to-{End} {Instance}-guided {Video} {Transformer} for {3D} {Pose} {Estimation}},
	isbn = {978-1-4503-9203-7},
	url = {https://doi.org/10.1145/3503161.3547871},
	doi = {10.1145/3503161.3547871},
	abstract = {Video 3D human pose estimation aims to localize the 3D coordinates of human joints from videos. Recent transformer-based approaches focus on capturing the spatiotemporal information from sequential 2D poses, which cannot model the contextual depth feature effectively since the visual depth features are lost in the step of 2D pose estimation. In this paper, we simplify the paradigm into an end-to-end framework, Instance-guided Video Transformer (IVT), which enables learning spatiotemporal contextual depth information from visual features effectively and predicts 3D poses directly from video frames. In particular, we firstly formulate video frames as a series of instance-guided tokens and each token is in charge of predicting the 3D pose of a human instance. These tokens contain body structure information since they are extracted by the guidance of joint offsets from the human center to the corresponding body joints. Then, these tokens are sent into IVT for learning spatiotemporal contextual depth. In addition, we propose a cross-scale instance-guided attention mechanism to handle the variational scales among multiple persons. Finally, the 3D poses of each person are decoded from instance-guided tokens by coordinate regression. Experiments on three widely-used 3D pose estimation benchmarks show that the proposed IVT achieves state-of-the-art performances.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Qiu, Zhongwei and Yang, Qiansheng and Wang, Jian and Fu, Dongmei},
	year = {2022},
	keywords = {human pose estimation, video transformer},
	pages = {6174--6182},
}

@incollection{xu_mechanical_2025,
	address = {New York, NY, USA},
	title = {Mechanical structure design of lower limb rehabilitation training robot},
	isbn = {979-8-4007-1264-7},
	url = {https://doi.org/10.1145/3727648.3727699},
	abstract = {In this paper, a rehabilitation training robot with passive and active rehabilitation training modes is designed for patients who are unable to stand and walk, such as those paralysed by traumatic brain injury or stroke, and the selection and design of the robot's mechanical structure and control system are completed according to the design parameters and requirements. The three-dimensional virtual prototype of the lower limb rehabilitation training robot is established by Solidworks and simplified into a two-dimensional planar model for easy analysis to derive the dynamics model of the system in the joint space, which is capable of describing the mechanical characteristics and dynamic behaviour of the robot in the process of movement, and it has an important practical application value for the in-depth study of lower limb rehabilitation robots.},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Computer}, {Artificial} {Intelligence} and {Control} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Wei and Wang, Jirong},
	year = {2025},
	pages = {303--309},
}

@inproceedings{jang_toonify3d_2024,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '24},
	title = {{Toonify3D}: {StyleGAN}-based {3D} {Stylized} {Face} {Generator}},
	isbn = {979-8-4007-0525-0},
	url = {https://doi.org/10.1145/3641519.3657480},
	doi = {10.1145/3641519.3657480},
	abstract = {Recent advances in generative models enable high-quality facial image stylization. Toonify is a popular StyleGAN-based framework that has been widely used for facial image stylization. Our goal is to create expressive 3D faces by turning Toonify into a 3D stylized face generator. Toonify is fine-tuned with a few gradient descent steps from StyleGAN trained for standard faces, and its features would carry semantic and visual information aligned with the features of the original StyleGAN model. Based on this observation, we design a versatile 3D-lifting method for StyleGAN, StyleNormal, that regresses a surface normal map of a StyleGAN-generated face using StyleGAN features. Due to the feature alignment between Toonify and StyleGAN, although StyleNormal is trained for regular faces, it can be applied for various stylized faces without additional fine-tuning. To learn local geometry of faces under various illuminations, we introduce a novel regularization term, the normal consistency loss, based on lighting manipulation in the GAN latent space. Finally, we present Toonify3D, a fully automated framework based on StyleNormal, that can generate full-head 3D stylized avatars and support GAN-based 3D facial expression editing.},
	booktitle = {{ACM} {SIGGRAPH} 2024 {Conference} {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Jang, Wonjong and Jung, Yucheol and Kim, Hyomin and Ju, Gwangjin and Son, Chaewon and Son, Jooeun and Lee, Seungyong},
	year = {2024},
	keywords = {3D face stylization, StyleGAN, Toonify},
}

@inproceedings{liu_precise_2025,
	address = {New York, NY, USA},
	series = {{SA} {Conference} {Papers} '25},
	title = {Precise {Gradient} {Discontinuities} in {Neural} {Fields} for {Subspace} {Physics}},
	isbn = {979-8-4007-2137-3},
	url = {https://doi.org/10.1145/3757377.3763810},
	doi = {10.1145/3757377.3763810},
	abstract = {Discontinuities in spatial derivatives appear in a wide range of physical systems, from creased thin sheets to materials with sharp stiffness transitions. Accurately modeling these features is essential for simulation but remains challenging for traditional mesh-based methods, which require discontinuity-aligned remeshing—entangling geometry with simulation and hindering generalization across shape families. Neural fields offer an appealing alternative by encoding basis functions as smooth, continuous functions over space, enabling simulation across varying shapes. However, their smoothness makes them poorly suited for representing gradient discontinuities. Prior work addresses discontinuities in function values, but capturing sharp changes in spatial derivatives while maintaining function continuity has received little attention. We introduce a neural field construction that captures gradient discontinuities without baking their location into the network weights. By augmenting input coordinates with a smoothly clamped distance function in a lifting framework, we enable encoding of gradient jumps at evolving interfaces. This design supports discretization-agnostic simulation of parametrized shape families with heterogeneous materials and evolving creases, enabling new reduced-order capabilities such as shape morphing, interactive crease editing, and simulation of soft-rigid hybrid structures. We further demonstrate that our method can be combined with previous lifting techniques to jointly capture both gradient and value discontinuities, supporting simultaneous cuts and creases within a unified model.},
	booktitle = {Proceedings of the {SIGGRAPH} {Asia} 2025 {Conference} {Papers}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Mengfei and Chang, Yue and Wang, Zhecheng and Chen, Peter Yichen and Grinspun, Eitan},
	year = {2025},
	keywords = {Discontinuity, Implicit neural representation, Reduced-order modeling, Crease, Heterogeneous Elastodynamics},
}

@inproceedings{wang_distracted_2021,
	address = {New York, NY, USA},
	series = {{MobiCom} '21},
	title = {Distracted driving detection by sensing the hand gripping of the phone},
	isbn = {978-1-4503-8342-4},
	url = {https://doi.org/10.1145/3447993.3482861},
	doi = {10.1145/3447993.3482861},
	abstract = {Phone usage while driving is unanimously considered a really dangerous habit due to a strong correlation with road accidents. This paper proposes a phone-use monitoring system that detects the driver's handheld phone use and eliminates the distraction at once. Specifically, the proposed system emits periodic ultrasonic pulses to sense if the phone is being held in hand or placed on support surfaces (e.g., seat and cup holder) by capturing the unique signal interference resulted from the contact object's damping, reflection and refraction. We derive the short-time Fourier transform from the microphone data to describe such impacts and develop a CNN-based binary classifier to discriminate the phone use between the handheld and the handsfree status. Additionally, we design a classification error correction filter to correct the classification errors during the monitoring. The experiments with six people, one phone and one car model show that our system achieves 99\% accuracy in recognizing handheld phone-use activities.},
	booktitle = {Proceedings of the 27th {Annual} {International} {Conference} on {Mobile} {Computing} and {Networking}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Ruxin and Huang, Long and Wang, Chen},
	year = {2021},
	keywords = {acoustic sensing, driver safety, phone-use monitoring},
	pages = {828--830},
}

@inproceedings{wang_design_2025,
	address = {New York, NY, USA},
	series = {{ICTCE} '24},
	title = {Design of {Micro} {Digital} {Power} {Analysis} and {Detection} {Device}},
	isbn = {979-8-4007-0963-0},
	url = {https://doi.org/10.1145/3705391.3705419},
	doi = {10.1145/3705391.3705419},
	abstract = {A power analyzer is an instrument used to test the performance of electrical appliances and troubleshoot electrical faults. At present, the power analyzer on the market generally has the disadvantages of difficult to use, high energy consumption, expensive and so on. Therefore, a micro digital power analysis and detection device is proposed in this paper, which can solve the above problems well. This device uses STM32F407 as the core processor, combined with the OPA227,TLC2254A and other precision operational amplifiers to form the core part of the device. The transformer circuit in the device can collect the power signal of the electrical equipment, and convert the power signal into a low-level signal that can be collected by ADC through the conversion circuit and digital phase discriminator, and then transmit it to the MCU for FFT calculation and filtering processing. Finally, the parameters such as active power and total harmonic distortion (THD) are displayed on the screen, so as to analyze the performance and state of the electrical equipment. The experimental results show that the device has the advantages of low power consumption, less cost and simple use when the measurement parameters are accurate.},
	booktitle = {Proceedings of the 2024 6th {International} {Conference} on {Telecommunications} and {Communication} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Wenbo and Lai, Wugang and Lin, Fanqiang and Zhao, Hongshun},
	year = {2025},
	keywords = {Fourier Transform Algorithm, Operational Amplifier, Power Analyzer, STM32},
	pages = {175--179},
}

@inproceedings{hartfill_emg-based_2025,
	address = {New York, NY, USA},
	series = {{MuC} '25},
	title = {{EMG}-based {Confirmation} of {Gaze} {Selection} in {Extended} {Reality}},
	isbn = {979-8-4007-1582-2},
	url = {https://doi.org/10.1145/3743049.3743062},
	doi = {10.1145/3743049.3743062},
	abstract = {Gaze input is an emerging technology for head-mounted displays (HMDs). While selecting a target object with gaze can be quick, confirming this selection solely through gaze is difficult. Thus, alternative approaches (like vision-based pinching) are needed, but may not be accessible (e.g., because of missing limbs), feasible (e.g., in low light), or acceptable (e.g., in public spaces). This paper explores electrode placement for electromyography (EMG) on legs, arms, and face and compares four methods for confirming gaze selection: (A) Palm lifting, (B) Finger tapping, (C) Biting, and (D) Foot tapping. In a preliminary study, we evaluated these techniques based on task completion time and user preference and compared them to a camera-based hand gesture selection method (pinching) as a baseline. Users performed specific gestures recognized via electrodes. Results indicated similar task completion times across all placements, with Foot tapping and Finger tapping preferred by users. We then conducted a user study assessing EMG-based confirmations with Foot tapping and Finger tapping versus a hand gesture method (pinching), using built-in hand tracking. Results showed that while most participants preferred pinching, EMG-based gestures performed comparably and demonstrated potential as an alternative to camera-based confirmation techniques.},
	booktitle = {Proceedings of the {Mensch} {Und} {Computer} 2025},
	publisher = {Association for Computing Machinery},
	author = {Hartfill, Judith and Arz, Michael and Steinicke, Frank},
	year = {2025},
	keywords = {confirmation, EMG, Eye gaze, gesture, VR, XR},
	pages = {375--384},
}

@inproceedings{xu_phrase-gesture_2022,
	address = {New York, NY, USA},
	series = {{UIST} '22},
	title = {Phrase-{Gesture} {Typing} on {Smartphones}},
	isbn = {978-1-4503-9320-1},
	url = {https://doi.org/10.1145/3526113.3545683},
	doi = {10.1145/3526113.3545683},
	abstract = {We study phrase-gesture typing, a gesture typing method that allows users to type short phrases by swiping through all the letters of the words in a phrase using a single, continuous gesture. Unlike word-gesture typing, where text needs to be entered word by word, phrase-gesture typing enters text phrase by phrase. To demonstrate the usability of phrase-gesture typing, we implemented a prototype called PhraseSwipe. Our system is composed of a frontend interface designed specifically for typing through phrases and a backend phrase-level gesture decoder developed based on a transformer-based neural language model. Our decoder was trained using five million phrases of varying lengths of up to five words, chosen randomly from the Yelp Review Dataset. Through a user study with 12 participants, we demonstrate that participants could type using PhraseSwipe at an average speed of 34.5 WPM with a Word Error Rate of 1.1\%.},
	booktitle = {Proceedings of the 35th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Zheer and Meng, Yankang and Bi, Xiaojun and Yang, Xing-Dong},
	year = {2022},
	keywords = {machine learning, gesture input, language model, text entry},
}

@inproceedings{mao_research_2025,
	address = {New York, NY, USA},
	series = {{IoTCCT} '25},
	title = {Research on the {Integration} of {Automated} {Loading} {System} and {High}-bay {Warehouse} {Sorting} {System}},
	isbn = {979-8-4007-2114-4},
	url = {https://doi.org/10.1145/3776865.3776893},
	doi = {10.1145/3776865.3776893},
	abstract = {This paper explores the deep integration design and application effects of a newly added automated loading system with the existing high-bay warehouse sorting system. Addressing issues such as efficiency bottlenecks, information silos, and high labor costs in industry logistics operations, the study proposes an intelligent solution based on an industrial internet architecture. By introducing core modules such as an intelligent loading platform, full-process automation from warehousing management and sorting operations to loading and shipping has been achieved. Application results show that after system integration, loading efficiency has been significantly improved, while manual intervention and logistics operation costs have been effectively reduced.},
	booktitle = {Proceedings of the 2025 3rd {International} {Conference} on {Internet} of {Things} and {Cloud} {Computing} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Mao, Xinyan and Zeng, Qingjie and Chen, Enbo and Li, Lin and Yu, Baoyi and Wen, Yan},
	year = {2025},
	keywords = {Automated loading system, High-bay warehouse sorting system, Intelligent logistics, System integration},
	pages = {164--169},
}

@inproceedings{baumer_finding_2025,
	address = {New York, NY, USA},
	series = {{CCS} '25},
	title = {Finding {SSH} {Strict} {Key} {Exchange} {Violations} by {State} {Learning}},
	isbn = {979-8-4007-1525-9},
	url = {https://doi.org/10.1145/3719027.3765208},
	doi = {10.1145/3719027.3765208},
	abstract = {SSH is an important protocol for secure remote shell access to servers on the Internet. At USENIX 2024, Bäumer et al. presented the Terrapin attack on SSH, which relies on the attacker injecting optional messages during the key exchange. To mitigate this attack, SSH vendors adopted an extension developed by OpenSSH called strict key exchange (”strict KEX”). With strict KEX, optional messages are forbidden during the handshake, preventing the attack. In practice, this should simplify the state machine of an SSH handshake to a linear message flow similar to that of TLS. In this work, we analyze the design, implementation, and security of strict KEX in popular SSH servers, using black-box state learning, which can uncover the hidden state machine of an implementation. In practice, it is limited by the number of learned messages and the complexity of the state machine. Thus, learning the complete state machine of SSH is infeasible. Previous research on SSH, therefore, excluded optional messages, learning only a partial state machine. However, these messages are a critical part of the Terrapin attack. We propose to instead learn the complete state machine of the handshake phase of an SSH server, but with strict KEX enabled. We investigate the security of ten SSH implementations supporting strict KEX for up to five key exchange algorithms. In total, we learn 33 state machines, revealing significant differences in the implementations. We show that seven implementations violate the strict KEX specification and find two critical security vulnerabilities. One results in a rogue session attack in the proprietary Tectia SSH implementation. Another affects the official SSH implementation of the Erlang Open Telecom Platform, and enables unauthenticated remote code execution in the security context of the SSH server.},
	booktitle = {Proceedings of the 2025 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Bäumer, Fabian and Maehren, Marcel and Brinkmann, Marcus and Schwenk, Jörg},
	year = {2025},
	keywords = {man-in-the-middle, message injection, protocol violations, ssh, state learning, strict key exchange},
	pages = {246--260},
}

@inproceedings{chu_research_2024,
	address = {New York, NY, USA},
	series = {{ICRSA} '24},
	title = {Research on {Biped} {Robot} {Motion} {Control} and {Gait} {Based} on {CPG}},
	isbn = {979-8-4007-1703-1},
	url = {https://doi.org/10.1145/3702468.3702472},
	doi = {10.1145/3702468.3702472},
	abstract = {To address the limitations and instability against disturbances of the walking gait of existing bipedal robots based on the inverted pendulum model, this paper proposes a new control method based on the Central Pattern Generator (CPG) model. By incorporating Hopf oscillators and Kuramoto terms into the original inverted pendulum model, and by adjusting parameters, this method enables the controller to produce stable and gait-periodic oscillations that match the robot's stride, thereby making the robot's center of gravity more stable and enhancing its disturbance resistance during walking. This study also connects the improved controller with the robot's joint motors and adjusts the robot's leg-lifting height and foot trajectory using Bezier curve interpolation, based on the Zero Moment Point (ZMP) gait generation method, to plan a complete robot gait. Finally, through simulations and experiments of the bipedal humanoid robot walking continuously and under sinusoidal disturbances, it was found that the new model can maintain a smaller fluctuation range of the center of gravity during walking, stabilize more quickly against external disturbances, and produce less noise when disturbed. This verifies the feasibility and effectiveness of the proposed control strategy.},
	booktitle = {Proceedings of the 2024 7th {International} {Conference} on {Robot} {Systems} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Chu, Ruilin and Yu, Miao and Tan, Yingzi},
	year = {2024},
	keywords = {Central Pattern Generator, Gait Generation, Humanoid Robot, Inverted Pendulum},
	pages = {21--27},
}

@inproceedings{zhang_design_2025,
	address = {New York, NY, USA},
	series = {{ICCIR} '25},
	title = {Design of {Greenhouse} {Tomato} {Picking} {Robot} {Based} on intelligent control},
	isbn = {979-8-4007-1494-8},
	url = {https://doi.org/10.1145/3757940.3757945},
	doi = {10.1145/3757940.3757945},
	abstract = {At present, China's Tomato Picking methods mainly rely on labor, which has problems such as high labor intensity, high labor cost and high fruit damage rate. This paper designs a small greenhouse tomato picking machine, including visual recognition system, lifting system, picking system, storage system, automatic driving system, power supply system, control system and other parts. It is convenient for operation, use and maintenance, in order to realize the mechanization and automation of tomato picking process, reduce the picking cost and improve the work efficiency. In the greenhouse dynamic environment, the control variable method is used to control the light environment variables. A total of 8 rounds of tests were conducted, and each round of tests lasted for 5 minutes. Each round of test was conducted in the ridge section with different tomato distribution density to evaluate the universality of the system. The test scenarios include branch and leaf occlusion and artificial simulation of dynamic obstacles. The final result is taken as the average value of 8 rounds of test results. The core positioning scheme uses LIDAR and binocular vision. Through multi-level data fusion and dynamic interference suppression technology, the scheme improves the positioning accuracy of the system in the signal interference environment, and improves the perception ability in complex environment. Combined with the visual assisted LIDAR positioning scheme, the picking efficiency is improved by 15\%. The false cutting rate is stable at 6.5\%, and the developed picking prototype has reached an average picking rate of 28.4 pieces/minute, which basically meets the needs of automatic tomato picking. It has the advantages of low cost, integration of harvesting and collection, adaptability to different ridges, continuous operation throughout the day and so on. It can effectively assist fruit farmers, reduce labor burden and cost, and has a good application prospect.},
	booktitle = {Proceedings of the 2025 5th {International} {Conference} on {Control} and {Intelligent} {Robotics}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Shenghan and Meng, Fanzhao and Zhang, Rongji and Zhang, Yanshuo and Wang, Zhen and Wang, Guoqing},
	year = {2025},
	keywords = {Automated picking, Dynamic obstacle avoidance, Picking robot, SLAM algorithm},
	pages = {32--37},
}

@article{little_polymorphic_2024,
	address = {New York, NY, USA},
	title = {Polymorphic dynamic programming by algebraic shortcut fusion},
	volume = {36},
	issn = {0934-5043},
	url = {https://doi.org/10.1145/3664828},
	doi = {10.1145/3664828},
	abstract = {Dynamic programming (DP) is a broadly applicable algorithmic design paradigm for the efficient, exact solution of otherwise intractable, combinatorial problems. However, the design of such algorithms is often presented informally in an ad-hoc manner. It is sometimes difficult to justify the correctness of these DP algorithms. To address this issue, this article presents a rigorous algebraic formalism for systematically deriving DP algorithms, based on semiring polymorphism. We start with a specification, construct a (brute-force) algorithm to compute the required solution which is self-evidently correct because it exhaustively generates and evaluates all possible solutions meeting the specification. We then derive, primarily through the use of shortcut fusion, an implementation of this algorithm which is both efficient and correct. We also demonstrate how, with the use of semiring lifting, the specification can be augmented with combinatorial constraints and through semiring lifting, show how these constraints can also be fused with the derived algorithm. This article furthermore demonstrates how existing DP algorithms for a given combinatorial problem can be abstracted from their original context and re-purposed to solve other combinatorial problems.This approach can be applied to the full scope of combinatorial problems expressible in terms of semirings. This includes, for example: optimisation, optimal probability and Viterbi decoding, probabilistic marginalization, logical inference, fuzzy sets, differentiable softmax, and relational and provenance queries. The approach, building on many ideas from the existing literature on constructive algorithmics, exploits generic properties of (semiring) polymorphic functions, tupling and formal sums (lifting), and algebraic simplifications arising from constraint algebras. We demonstrate the effectiveness of this formalism for some example applications arising in signal processing, bioinformatics and reliability engineering. Python software implementing these algorithms can be downloaded from: .},
	number = {2},
	journal = {Form. Asp. Comput.},
	publisher = {Association for Computing Machinery},
	author = {Little, Max A and He, Xi and Kayas, Ugur},
	month = jun,
	year = {2024},
	keywords = {combinatorial optimisation, Dynamic programming, semiring, shortcut fusion},
}

@inproceedings{jiang_matcha_2022,
	address = {New York, NY, USA},
	series = {{DAC} '22},
	title = {{MATCHA}: a fast and energy-efficient accelerator for fully homomorphic encryption over the torus},
	isbn = {978-1-4503-9142-9},
	url = {https://doi.org/10.1145/3489517.3530435},
	doi = {10.1145/3489517.3530435},
	abstract = {Fully Homomorphic Encryption over the Torus (TFHE) allows arbitrary computations to happen directly on ciphertexts using homomorphic logic gates. However, each TFHE gate on state-of-the-art hardware platforms such as GPUs and FPGAs is extremely slow (\&gt; 0.2ms). Moreover, even the latest FPGA-based TFHE accelerator cannot achieve high energy efficiency, since it frequently invokes expensive double-precision floating point FFT and IFFT kernels. In this paper, we propose a fast and energy-efficient accelerator, MATCHA, to process TFHE gates. MATCHA supports aggressive bootstrapping key unrolling to accelerate TFHE gates without decryption errors by approximate multiplication-less integer FFTs and IFFTs, and a pipelined datapath. Compared to prior accelerators, MATCHA improves the TFHE gate processing throughput by 2.3x, and the throughput per Watt by 6.3x.},
	booktitle = {Proceedings of the 59th {ACM}/{IEEE} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Lei and Lou, Qian and Joshi, Nrushad},
	year = {2022},
	keywords = {accelerator, bootstrapping, fully homomorphic encryption, TFHE},
	pages = {235--240},
}

@article{bangaru_systematically_2021,
	address = {New York, NY, USA},
	title = {Systematically differentiating parametric discontinuities},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3450626.3459775},
	doi = {10.1145/3450626.3459775},
	abstract = {Emerging research in computer graphics, inverse problems, and machine learning requires us to differentiate and optimize parametric discontinuities. These discontinuities appear in object boundaries, occlusion, contact, and sudden change over time. In many domains, such as rendering and physics simulation, we differentiate the parameters of models that are expressed as integrals over discontinuous functions. Ignoring the discontinuities during differentiation often has a significant impact on the optimization process. Previous approaches either apply specialized hand-derived solutions, smooth out the discontinuities, or rely on incorrect automatic differentiation.We propose a systematic approach to differentiating integrals with discontinuous integrands, by developing a new differentiable programming language. We introduce integration as a language primitive and account for the Dirac delta contribution from differentiating parametric discontinuities in the integrand. We formally define the language semantics and prove the correctness and closure under the differentiation, allowing the generation of gradients and higher-order derivatives. We also build a system, Teg, implementing these semantics. Our approach is widely applicable to a variety of tasks, including image stylization, fitting shader parameters, trajectory optimization, and optimizing physical designs.},
	number = {4},
	journal = {ACM Trans. Graph.},
	publisher = {Association for Computing Machinery},
	author = {Bangaru, Sai Praveen and Michel, Jesse and Mu, Kevin and Bernstein, Gilbert and Li, Tzu-Mao and Ragan-Kelley, Jonathan},
	month = jul,
	year = {2021},
	keywords = {automatic differentiation, differentiable graphics, differentiable physics, differentiable programming, differentiable rendering, domain-specific language},
}

@article{sato_formal_2019,
	address = {New York, NY, USA},
	title = {Formal verification of higher-order probabilistic programs: reasoning about approximation, convergence, {Bayesian} inference, and optimization},
	volume = {3},
	url = {https://doi.org/10.1145/3290351},
	doi = {10.1145/3290351},
	abstract = {Probabilistic programming provides a convenient lingua franca for writing succinct and rigorous descriptions of probabilistic models and inference tasks. Several probabilistic programming languages, including Anglican, Church or Hakaru, derive their expressiveness from a powerful combination of continuous distributions, conditioning, and higher-order functions. Although very important for practical applications, these features raise fundamental challenges for program semantics and verification. Several recent works offer promising answers to these challenges, but their primary focus is on foundational semantics issues. In this paper, we take a step further by developing a suite of logics, collectively named PPV for proving properties of programs written in an expressive probabilistic higher-order language with continuous sampling operations and primitives for conditioning distributions. Our logics mimic the comfortable reasoning style of informal proofs using carefully selected axiomatizations of key results from probability theory. The versatility of our logics is illustrated through the formal verification of several intricate examples from statistics, probabilistic inference, and machine learning. We further show expressiveness by giving sound embeddings of existing logics. In particular, we do this in a parametric way by showing how the semantics idea of (unary and relational) ⊤⊤-lifting can be internalized in our logics. The soundness of PPV follows by interpreting programs and assertions in quasi-Borel spaces (QBS), a recently proposed variant of Borel spaces with a good structure for interpreting higher order probabilistic programs.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Sato, Tetsuya and Aguirre, Alejandro and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak and Hsu, Justin},
	month = jan,
	year = {2019},
	keywords = {probabilistic programming, formal reasoning, relational type systems},
}

@inproceedings{mio_beyond_2022,
	address = {New York, NY, USA},
	series = {{LICS} '22},
	title = {Beyond {Nonexpansive} {Operations} in {Quantitative} {Algebraic} {Reasoning}},
	isbn = {978-1-4503-9351-5},
	url = {https://doi.org/10.1145/3531130.3533366},
	doi = {10.1145/3531130.3533366},
	abstract = {The framework of quantitative equational logic has been successfully applied to reason about algebras whose carriers are metric spaces and operations are nonexpansive. We extend this framework in two orthogonal directions: algebras endowed with generalised metric space structures, and operations being nonexpansive up to a lifting. We apply our results to the algebraic axiomatisation of the Łukaszyk–Karmowski distance on probability distributions, which has recently found application in the field of representation learning on Markov processes.},
	booktitle = {Proceedings of the 37th {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Mio, Matteo and Sarkis, Ralph and Vignudelli, Valeria},
	year = {2022},
	keywords = {equational logic, free algebras, metrics, monads, probability distributions, quantitative reasoning},
}

@article{granskog_neural_2021,
	address = {New York, NY, USA},
	title = {Neural scene graph rendering},
	volume = {40},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3450626.3459848},
	doi = {10.1145/3450626.3459848},
	abstract = {We present a neural scene graph—a modular and controllable representation of scenes with elements that are learned from data. We focus on the forward rendering problem, where the scene graph is provided by the user and references learned elements. The elements correspond to geometry and material definitions of scene objects and constitute the leaves of the graph; we store them as high-dimensional vectors. The position and appearance of scene objects can be adjusted in an artist-friendly manner via familiar transformations, e.g. translation, bending, or color hue shift, which are stored in the inner nodes of the graph. In order to apply a (non-linear) transformation to a learned vector, we adopt the concept of linearizing a problem by lifting it into higher dimensions: we first encode the transformation into a high-dimensional matrix and then apply it by standard matrix-vector multiplication. The transformations are encoded using neural networks. We render the scene graph using a streaming neural renderer, which can handle graphs with a varying number of objects, and thereby facilitates scalability. Our results demonstrate a precise control over the learned object representations in a number of animated 2D and 3D scenes. Despite the limited visual complexity, our work presents a step towards marrying traditional editing mechanisms with learned representations, and towards high-quality, controllable neural rendering.},
	number = {4},
	journal = {ACM Trans. Graph.},
	publisher = {Association for Computing Machinery},
	author = {Granskog, Jonathan and Schnabel, Till N. and Rousselle, Fabrice and Novák, Jan},
	month = jul,
	year = {2021},
	keywords = {neural networks, generalization, modularity, neural scene representations, rendering},
}

@inproceedings{sharma_graspui_2024,
	address = {New York, NY, USA},
	series = {{DIS} '24},
	title = {{GraspUI}: {Seamlessly} {Integrating} {Object}-{Centric} {Gestures} within the {Seven} {Phases} of {Grasping}},
	isbn = {979-8-4007-0583-0},
	url = {https://doi.org/10.1145/3643834.3661551},
	doi = {10.1145/3643834.3661551},
	abstract = {Objects are indispensable tools in our daily lives. Recent research has demonstrated their potential to act as conduits for digital interactions with microgestures, however, the primary focus was on situations where the hand firmly grasps an object. We introduce GraspUI, an exploratory design space of object-centric gestures within the seven distinct phases of the grasping process, spanning pre-, during, and post-grasp movements. We conducted ideation sessions with mixed-reality designers from industry and academia to explore gesture integration throughout the entire grasping process. The outcome was 38 storyboards envisioning practical applications. To evaluate the design space’s utility, we performed a video-based assessment with end-users. We then implemented an interactive prototype and quantified the overhead cost of performing proposed gestures through a secondary study. Participants reacted positively to gestures and could integrate them into existing usage of objects. To conclude, we highlight technical and usability guidelines for implementing and extending GraspUI systems.},
	booktitle = {Proceedings of the 2024 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Sharma, Adwait and Ivanov, Alexander and Lai, Frances and Grossman, Tovi and Santosa, Stephanie},
	year = {2024},
	keywords = {design space, everyday objects, grasp, grasping process, hand-object manipulation, input},
	pages = {1275--1289},
}

@inproceedings{gevay_power_2021,
	address = {New York, NY, USA},
	series = {{SIGMOD} '21},
	title = {The {Power} of {Nested} {Parallelism} in {Big} {Data} {Processing}  {Hitting} {Three} {Flies} with {One} {Slap} },
	isbn = {978-1-4503-8343-1},
	url = {https://doi.org/10.1145/3448016.3457287},
	doi = {10.1145/3448016.3457287},
	abstract = {Many common data analysis tasks, such as performing hyperparameter optimization, processing a partitioned graph, and treating a matrix as a vector of vectors, offer natural opportunities for nested-parallel operations, i.e., launching parallel operations from inside other parallel operations. However, state-of-the-art dataflow engines, such as Spark and Flink, do not support nested parallelism. Users must implement workarounds, causing orders of magnitude slowdowns for their tasks, let alone the implementation effort.We present Matryoshka, a system that enables dataflow engines to support nested parallelism, even in the presence of control flow statements at inner nesting levels. Matryoshka achieves this via a novel two-phase flattening process, which translates nested-parallel programs to flat-parallel programs that can efficiently run on existing dataflow engines. The first phase introduces novel nesting primitives into the code, which allows for dynamic optimizations based on intermediate data characteristics in the second phase at runtime. We validate our system using several common data analysis tasks, such as PageRank and K-means.},
	booktitle = {Proceedings of the 2021 {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Gévay, Gábor E. and Quiané-Ruiz, Jorge-Arnulfo and Markl, Volker},
	year = {2021},
	keywords = {nested data, nested parallel collections, nested parallel operations},
	pages = {605--618},
}

@inproceedings{chen_enhancing_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {Enhancing {Home} {Exercise} {Experiences} with {Video} {Motion}-{Tracking} for {Automatic} {Display} {Height} {Adjustment}},
	isbn = {979-8-4007-0330-0},
	url = {https://doi.org/10.1145/3613904.3642936},
	doi = {10.1145/3613904.3642936},
	abstract = {The increasing demand for home fitness solutions underscores the need for interactive displays that enhance user experiences. This study introduces a technology that autonomously adjusts display height using the skeletal information of demonstrators from videos, catering to home fitness needs. A user study involving thirty participants compared fixed height, manual adjustment, and automatic adjustment conditions. Head flexion angles and NASA-TLX survey responses were used for evaluation. Results showed a significant reduction in head flexion angles with automatic adjustment, promoting proper spinal alignment. NASA-TLX responses indicated lower mental, effort, and frustration ratings, along with improved performance and perceived support in the automatic adjustment condition compared to other conditions. These findings confirm that motion-based height adjustment improves posture and enhances the overall interactive experience. This research demonstrates the feasibility of integrating responsive ergonomics into interactive displays and suggests the importance of further personalization, conducting diverse user studies, and refining algorithms to fully leverage the potential of this technology.},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Xinyu and Li, Yuqi and Chen, Jintao and Li, Jiabao and Wang, Chong and Tang, Pinyan},
	year = {2024},
	keywords = {Automatic Height Adjustment, Head Flexion Angle, Interactive Exercise Displays, NASA-TLX survey, User Experience Design},
}

@inproceedings{jiang_latent-space_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {Latent-space {Unfolding} for {MRI} {Reconstruction}},
	isbn = {979-8-4007-0108-5},
	url = {https://doi.org/10.1145/3581783.3613771},
	doi = {10.1145/3581783.3613771},
	abstract = {To circumvent the problems caused by prolonged acquisition periods, compressed sensing MRI enjoys a high usage profile to accelerate the recovery of high-quality images from under-sampled k-space data. Most current solutions dedicate to solving this issue with the pursuit of certain prior properties, yet the treatments are all enforced in the original space, resulting in limited feature information. To achieve a performance promotion yet with the guarantee of running efficiency, in this work, we propose a latent-space unfolding network (LsUNet). Specifically, by an elaborately designed reversible network, the inputs are first mapped to a channel-lifted latent space, which taps the potential of capturing spatial-invariant features sufficiently. Within the latent space, we then unfold an accelerated optimization algorithm to iterate an efficient and feasible solution, in which a parallelly dual-domain update is equipped for better feature fusion. Finally, an inverse embedding transformation of the recovered high-dimensional representation is applied to achieve the expected estimation. LsUNet enjoys high interpretability due to the physically induced modules, which not only facilitates an intuitive understanding of the internal operating mechanism but also endows it with high generalization ability. Comprehensive experiments on different datasets and various sampling rates/patterns demonstrate the advantages of our proposal over the latest methods both visually and numerically.},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Jiawei and Feng, Yuchao and Chen, Jiacheng and Guo, Dongyan and Zheng, Jianwei},
	year = {2023},
	keywords = {fast mri, latet-space unfolding, local and global features, parallelly dual-domain update},
	pages = {1294--1302},
}

@inproceedings{sun_sonispace_2023,
	address = {New York, NY, USA},
	series = {{DIS} '23 {Companion}},
	title = {{SoniSpace}: {Expressive} {Movement} {Interaction} to {Encourage} {Taking} {Up} {Space} with the {Body}},
	isbn = {978-1-4503-9898-5},
	url = {https://doi.org/10.1145/3563703.3596659},
	doi = {10.1145/3563703.3596659},
	abstract = {Movement forms the basis of our thoughts, emotions, and ways of being in the world. Informed by somaesthetics, we design for “taking up space” (e.g. encouraging expansive body movements), which may in turn alter our emotional experience. We demonstrate SoniSpace, an expressive movement interaction experience that uses movement sonification and visualization to encourage users to take up space with their body. We apply a first-person design approach to embed qualities of awareness, exploration, and comfort into the sound and visual design to promote authentic and enjoyable movement expression regardless of prior movement experience. Preliminary results from 20 user experiences with the system show that users felt more comfortable with taking up space and with movement in general following the interaction. We discuss our findings about designing for somatically-focused movement interactions and directions for future work.},
	booktitle = {Companion {Publication} of the 2023 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Ruojia and Wallop, Althea Vail and Leslie, Grace and Do, Ellen Yi-Luen},
	year = {2023},
	keywords = {interactive visuals, movement-based interaction, sensory feedback, soma design, sonification},
	pages = {279--283},
}

@inproceedings{ning_control_2023,
	address = {New York, NY, USA},
	series = {{RICAI} '22},
	title = {The {Control} of intelligent building robot patrol technology based on {Raspberry} {Pi}},
	isbn = {978-1-4503-9834-3},
	url = {https://doi.org/10.1145/3584376.3584422},
	doi = {10.1145/3584376.3584422},
	abstract = {With the rapid development of intelligent buildings, the intelligent research of mobile robots has set off an upsurge. In this paper, the research is carried out on building intelligent security robots, the wheeled robot platform is built, and an autonomous inspection system is designed. Through communication, electronics, Internet, Internet of Things, satellite remote sensing, laser ranging, radar ranging, satellite positioning and inertial navigation combination, computer, machinery manufacturing, and other technologies, the robot construction and equipment, hardware and circuit integration, embedded software development, etc. are designed and manufactured to realize intelligent work. Robot participation in inspection makes up for the empty period of manual inspection, avoids the risks and disadvantages of manual inspection, improves production efficiency, reduces the burden of operations and maintenance personnel, and saves costs.},
	booktitle = {Proceedings of the 2022 4th {International} {Conference} on {Robotics}, {Intelligent} {Control} and {Artificial} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Ning, Yingsong and Chen, Yifei and Xu, Zixuan and Ma, Wangran and Wang, Shoujin and Zhao, Shiyu},
	year = {2023},
	keywords = {Path planning, Radar scanning, Target identification, Target tracking},
	pages = {250--254},
}

@inproceedings{baronetto_simulation_2021,
	address = {New York, NY, USA},
	series = {{ISWC} '21},
	title = {Simulation of {Garment}-{Embedded} {Contact} {Sensor} {Performance} under {Motion} {Dynamics}},
	isbn = {978-1-4503-8462-9},
	url = {https://doi.org/10.1145/3460421.3480423},
	doi = {10.1145/3460421.3480423},
	abstract = {We propose a simulation method to evaluate the performance of garment-embedded contact sensors while performing common Activities of Daily Living (ADL). Our method comprises four steps: dynamic 3D human body model generation, automated smart garment design, ADL simulation, dynamic sensor fitting and sensor displacement evaluation. We generated 100 3D human body models with varying body shapes and virtually dressed them with three differently fitted smart T-Shirts. We then analysed the sensor-body distance and sensor displacement while performing common ADLs. An Electrocardiogram (ECG) smart shirt was considered as an example application. Results show a decrease in sensor distance while BMI increases for both sexes. Compared to females, males show higher sensor displacement and displacement variance, whereas women show higher distance variance compared to men for all ADLs, especially in the region below the breast. Our method can be used to evaluate contact sensor performance for different body shapes, ADLs, and garment designs.},
	booktitle = {Proceedings of the 2021 {ACM} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Baronetto, Annalisa and Uhlenberg, Lena and Wassermann, Dominik and Amft, Oliver},
	year = {2021},
	keywords = {contact sensors, garment fitting, smart garment design},
	pages = {73--77},
}

@inproceedings{bai_speech_2022,
	address = {New York, NY, USA},
	series = {{MM} '22},
	title = {Speech {Fusion} to {Face}: {Bridging} the {Gap} {Between} {Human}'s {Vocal} {Characteristics} and {Facial} {Imaging}},
	isbn = {978-1-4503-9203-7},
	url = {https://doi.org/10.1145/3503161.3547850},
	doi = {10.1145/3503161.3547850},
	abstract = {While deep learning technologies are now capable of generating realistic images confusing humans, the research efforts are turning to the synthesis of images for more concrete and application-specific purposes. Facial image generation based on vocal characteristics from speech is one of such important yet challenging tasks. It is the key enabler to influential use cases of image generation, especially for business in public security and entertainment. Existing solutions to the problem of speech2face renders limited image quality and fails to preserve facial similarity due to the lack of quality dataset for training and appropriate integration of vocal features. In this paper, we investigate these key technical challenges and propose Speech Fusion to Face, or SF2F in short, attempting to address the issue of facial image quality and the poor connection between vocal feature domain and modern image generation models. By adopting new strategies on data model and training, we demonstrate dramatic performance boost over state-of-the-art solution, by doubling the recall of individual identity, and lifting the quality score from 15 to 19 based on the mutual information score with VGGFace classifier.},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {BAI, Yeqi and Ma, Tao and Wang, Lipo and Zhang, Zhenjie},
	year = {2022},
	keywords = {multi-modal, video learning},
	pages = {2042--2050},
}

@inproceedings{bustany_specpart_2022,
	address = {New York, NY, USA},
	series = {{ICCAD} '22},
	title = {{SpecPart}: {A} {Supervised} {Spectral} {Framework} for {Hypergraph} {Partitioning} {Solution} {Improvement}},
	isbn = {978-1-4503-9217-4},
	url = {https://doi.org/10.1145/3508352.3549390},
	doi = {10.1145/3508352.3549390},
	abstract = {State-of-the-art hypergraph partitioners follow the multilevel paradigm that constructs multiple levels of progressively coarser hypergraphs that are used to drive cut refinements on each level of the hierarchy. Multilevel partitioners are subject to two limitations: (i) Hypergraph coarsening processes rely on local neighborhood structure without fully considering the global structure of the hypergraph. (ii) Refinement heuristics can stagnate on local minima. In this paper, we describe SpecPart, the first supervised spectral framework that directly tackles these two limitations. SpecPart solves a generalized eigenvalue problem that captures the balanced partitioning objective and global hypergraph structure in a low-dimensional vertex embedding while leveraging initial high-quality solutions from multilevel partitioners as hints. SpecPart further constructs a family of trees from the vertex embedding and partitions them with a tree-sweeping algorithm. Then, a novel overlay of multiple tree-based partitioning solutions, followed by lifting to a coarsened hypergraph, where an ILP partitioning instance is solved to alleviate local stagnation. We have validated SpecPart on multiple sets of benchmarks. Experimental results show that for some benchmarks, our SpecPart can substantially improve the cutsize by more than 50\% with respect to the best published solutions obtained with leading partitioners hMETIS and KaHyPar.},
	booktitle = {Proceedings of the 41st {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design}},
	publisher = {Association for Computing Machinery},
	author = {Bustany, Ismail and Kahng, Andrew B. and Koutis, Ioannis and Pramanik, Bodhisatta and Wang, Zhiang},
	year = {2022},
	keywords = {hypergraph partitioning, supervised spectral partitioning},
}

@inproceedings{aso_portable_2021,
	address = {New York, NY, USA},
	series = {{AHs} '21},
	title = {Portable {3D} {Human} {Pose} {Estimation} for {Human}-{Human} {Interaction} using a {Chest}-{Mounted} {Fisheye} {Camera}},
	isbn = {978-1-4503-8428-5},
	url = {https://doi.org/10.1145/3458709.3458986},
	doi = {10.1145/3458709.3458986},
	abstract = {We propose a system that estimates the 3D body pose of other parties using a single RGB chest-mounted ultra-wide fisheye camera. Although the fisheye camera can capture a wide field of view, it is difficult to apply image processing for perspective images because of its strong distortion. In our method, the input fisheye image is converted to an equirectangular image to detect another person and their 2D keypoints, and then convert them to a 3D pose. In order to adapt to the distortion of equirectangular images, we generate a synthetic dataset and fine-tune the model. We also estimate the location of the other person so that we can reconstruct the absolute camera-centered global pose. We evaluate the accuracy on real-world data and show that the fine-tuned model performs best.},
	booktitle = {Proceedings of the {Augmented} {Humans} {International} {Conference} 2021},
	publisher = {Association for Computing Machinery},
	author = {Aso, Kohei and Hwang, Dong-Hyun and Koike, Hideki},
	year = {2021},
	keywords = {Computer vision, Human pose estimation, Egocentric video, Fisheye camera, Human-human interaction, Mobile motion capture},
	pages = {116--120},
}

@article{chen_automata-based_2023,
	address = {New York, NY, USA},
	title = {An {Automata}-{Based} {Framework} for {Verification} and {Bug} {Hunting} in {Quantum} {Circuits}},
	volume = {7},
	url = {https://doi.org/10.1145/3591270},
	doi = {10.1145/3591270},
	abstract = {We introduce a new paradigm for analysing and finding bugs in quantum circuits. In our approach, the problem is given by a ‍triple P C Q and the question is whether, given a set P of quantum states on the input of a circuit C, the set of quantum states on the output is equal to (or included in) a set Q. While this is not suitable to specify, e.g., functional correctness of a quantum circuit, it is sufficient to detect many bugs in quantum circuits. We propose a technique based on tree automata to compactly represent sets of quantum states and develop transformers to implement the semantics of quantum gates over this representation. Our technique computes with an algebraic representation of quantum states, avoiding the inaccuracy of working with floating-point numbers. We implemented the proposed approach in a prototype tool and evaluated its performance against various benchmarks from the literature. The evaluation shows that our approach is quite scalable, e.g., we managed to verify a large circuit with 40 qubits and 141,527 gates, or catch bugs injected into a circuit with 320 qubits and 1,758 gates, where all tools we compared with failed. In addition, our work establishes a connection between quantum program verification and automata, opening new possibilities to exploit the richness of automata theory and automata-based verification in the world of quantum computing.},
	number = {PLDI},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Chen, Yu-Fang and Chung, Kai-Min and Lengál, Ondřej and Lin, Jyun-Ao and Tsai, Wei-Lun and Yen, Di-De},
	month = jun,
	year = {2023},
	keywords = {verification, quantum circuits, tree automata},
}

@article{colaco_constructive_2023,
	address = {New York, NY, USA},
	title = {A {Constructive} {State}-based {Semantics} and {Interpreter} for a {Synchronous} {Data}-flow {Language} with {State} {Machines}},
	volume = {22},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3609131},
	doi = {10.1145/3609131},
	abstract = {Scade is a domain-specific synchronous functional language used to implement safety-critical real-time software for more than twenty years. Two main approaches have been considered for its semantics: (i) an indirect collapsing semantics based on a source-to-source translation of high-level constructs into a data-flow core language whose semantics is precisely specified and is the entry for code generation; a relational synchronous semantics, akin to Esterel, that applies directly to the source. It defines what is a valid synchronous reaction but hides, on purpose, if a semantics exists, is unique and can be computed; hence, it is not executable.This paper presents, for the first time, an executable, state-based semantics for a language that has the key constructs of Scade all together, in particular the arbitrary combination of data-flow equations and hierarchical state machines. It can apply directly to the source language before static checks and compilation steps. It is constructive in the sense that the language in which the semantics is defined is a statically typed functional language with call-by-value and strong normalization, e.g., it is expressible in a proof-assistant where all functions terminate. It leads to a reference, purely functional, interpreter. This semantics is modular and can account for possible errors, allowing to establish what property is ensured by each static verification performed by the compiler. It also clarifies how causality is treated in Scade compared with Esterel.This semantics can serve as an oracle for compiler testing and validation; to prototype novel language constructs before they are implemented, to execute possibly unfinished models or that are correct but rejected by the compiler; to prove the correctness of compilation steps.The semantics given in the paper is implemented as an interpreter in a purely functional style, in OCaml.},
	number = {5s},
	journal = {ACM Trans. Embed. Comput. Syst.},
	publisher = {Association for Computing Machinery},
	author = {Colaço, Jean-Louis and Mendler, Michael and Pauget, Baptiste and Pouzet, Marc},
	month = sep,
	year = {2023},
	keywords = {Programming language, dynamic semantics, embedded software, synchronous programming},
}

@inproceedings{alexandru_intrinsically_2025,
	address = {New York, NY, USA},
	series = {{CPP} '25},
	title = {Intrinsically {Correct} {Sorting} in {Cubical} {Agda}},
	isbn = {979-8-4007-1347-7},
	url = {https://doi.org/10.1145/3703595.3705873},
	doi = {10.1145/3703595.3705873},
	abstract = {The paper "Sorting with Bialgebras and Distributive Laws" by Hinze et al. uses the framework of bialgebraic semantics to define sorting algorithms. From distributive laws between functors they construct pairs of sorting algorithms using both folds and unfolds. Pairs of sorting algorithms arising this way include insertion/selection sort and quick/tree sort. We extend this work to define intrinsically correct variants in cubical Agda. Our key idea is to index our data types by multisets, which concisely captures that a sorting algorithm terminates with an ordered permutation of its input list. By lifting bialgebraic semantics to the indexed setting, we obtain the correctness of sorting algorithms purely from the distributive law.},
	booktitle = {Proceedings of the 14th {ACM} {SIGPLAN} {International} {Conference} on {Certified} {Programs} and {Proofs}},
	publisher = {Association for Computing Machinery},
	author = {Alexandru, Cass and Choudhury, Vikraman and Rot, Jurriaan and van der Weide, Niels},
	year = {2025},
	keywords = {bialgebras, cubical Agda, distributive laws, intrinsic correctness, sorting},
	pages = {34--49},
}

@inproceedings{aranovich_beyond_2022,
	address = {New York, NY, USA},
	series = {{NSPW} '21},
	title = {Beyond {NVD}: {Cybersecurity} meets the {Semantic} {Web}.},
	isbn = {978-1-4503-8573-2},
	url = {https://doi.org/10.1145/3498891.3501259},
	doi = {10.1145/3498891.3501259},
	abstract = {Cybersecurity experts rely on the knowledge stored in databases like the NVD to do their work, but these are not the only sources of information about threats and vulnerabilities. Much of that information flows through social media channels. In this paper we argue that security experts and general users alike can benefit from the technologies of the Semantic Web, merging heterogeneous sources of knowledge in an ontological representation. We present a system that has an ontology of vulnerabilities at its core, but that is enhanced with NLP tools to identify cybersecurity-related information in social media and to launch queries over heterogeneous data sources. The transformative power of Semantic Web technologies for cybersecurity, which has been proven in the biomedical field, is evaluated and discussed.},
	booktitle = {Proceedings of the 2021 {New} {Security} {Paradigms} {Workshop}},
	publisher = {Association for Computing Machinery},
	author = {Aranovich, Raúl and Wu, Muting and Yu, Dian and Katsy, Katya and Ahmadnia, Benyamin and Bishop, Matthew and Filkov, Vladimir and Sagae, Kenji},
	year = {2022},
	keywords = {neural networks, nlp, cybersecurity, ontology, social media},
	pages = {59--69},
}

@inproceedings{futami_method_2022,
	address = {New York, NY, USA},
	series = {{iiWAS2021}},
	title = {A {Method} to {Recognize} {Facial} {Gesture} using {Infrared} {Distance} {Sensor} {Array} on {Ear} {Accessories}},
	isbn = {978-1-4503-9556-4},
	url = {https://doi.org/10.1145/3487664.3487761},
	doi = {10.1145/3487664.3487761},
	abstract = {Many hands-free input methods using ear accessories have been proposed. Although most of previous studies use canal type earphones, we focus on the following two points. 1) A method for using the hands-free input function with ear accessories that are not canal type. 2) A method for using the same hands-free input function across multiple ear accessories. Then, using an infrared distance sensor attached to the ear accessory, we propose a method for recognizing the gesture of the user’s facial expression. Based on the change in distance between the infrared distance sensor attached to the ear accessory and the skin, the proposed method detects skin movement around the ear, which differs for each facial expression gesture. We created a prototype system for the root of the ear, earlobe, and tragus ear accessories. The evaluation result for nine gestures and five subjects showed that F-value was 0.94 or more for one device alone, and the F-value was 0.97 or more for the pattern combining multiple devices.},
	booktitle = {The 23rd {International} {Conference} on {Information} {Integration} and {Web} {Intelligence}},
	publisher = {Association for Computing Machinery},
	author = {Futami, Kyosuke and Oyama, Kohei and Murao, Kazuya},
	year = {2022},
	keywords = {Ear accessories, Earphone, Facial gesture recognition, Hands-free input interface, Infrared distance sensor},
	pages = {650--654},
}

@inproceedings{heidari_allocating_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Allocating {Opportunities} in a {Dynamic} {Model} of {Intergenerational} {Mobility}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445867},
	doi = {10.1145/3442188.3445867},
	abstract = {Opportunities such as higher education can promote intergenerational mobility, leading individuals to achieve levels of socioeconomic status above that of their parents. We develop a dynamic model for allocating such opportunities in a society that exhibits bottlenecks in mobility; the problem of optimal allocation reflects a trade-off between the benefits conferred by the opportunities in the current generation and the potential to elevate the socioeconomic status of recipients, shaping the composition of future generations in ways that can benefit further from the opportunities. We show how optimal allocations in our model arise as solutions to continuous optimization problems over multiple generations, and we find in general that these optimal solutions can favor recipients of low socioeconomic status over slightly higher-performing individuals of high socioeconomic status — a form of socioeconomic affirmative action that the society in our model discovers in the pursuit of purely payoff-maximizing goals. We characterize how the structure of the model can lead to either temporary or persistent affirmative action, and we consider extensions of the model with more complex processes modulating the movement between different levels of socioeconomic status.},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Heidari, Hoda and Kleinberg, Jon},
	year = {2021},
	keywords = {continuous Markov Decision Processes (MDP), Intergenerational mobility, optimal allocation policy, socioeconomic affirmative action},
	pages = {15--25},
}

@article{yoon_formal_2022,
	address = {New York, NY, USA},
	title = {Formal reasoning about layered monadic interpreters},
	volume = {6},
	url = {https://doi.org/10.1145/3547630},
	doi = {10.1145/3547630},
	abstract = {Monadic computations built by interpreting, or handling, operations of a free monad are a compelling formalism for modeling language semantics and defining the behaviors of effectful systems. The resulting layered semantics offer the promise of modular reasoning principles based on the equational theory of the underlying monads. However, there are a number of obstacles to using such layered interpreters in practice. With more layers comes more boilerplate and glue code needed to define the monads and interpreters involved. That overhead is compounded by the need to define and justify the relational reasoning principles that characterize the equivalences at each layer. This paper addresses these problems by significantly extending the capabilities of the Coq interaction trees (ITrees) library, which supports layered monadic interpreters. We characterize a rich class of interpretable monads—obtained by applying monad transformers to ITrees—and show how to generically lift interpreters through them. We also introduce a corresponding framework for relational reasoning about "equivalence of monads up to a relation R". This collection of typeclasses, instances, new reasoning principles, and tactics greatly generalizes the existing theory of the ITree library, eliminating large amounts of unwieldy boilerplate code and dramatically simplifying proofs.},
	number = {ICFP},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Yoon, Irene and Zakowski, Yannick and Zdancewic, Steve},
	month = aug,
	year = {2022},
	keywords = {monads, coinduction, compiler correctness, Coq},
}

@article{aguirre_higher-order_2021,
	address = {New York, NY, USA},
	title = {Higher-order probabilistic adversarial computations: categorical semantics and program logics},
	volume = {5},
	url = {https://doi.org/10.1145/3473598},
	doi = {10.1145/3473598},
	abstract = {Adversarial computations are a widely studied class of computations where resource-bounded probabilistic adversaries have access to oracles, i.e., probabilistic procedures with private state. These computations arise routinely in several domains, including security, privacy and machine learning. In this paper, we develop program logics for reasoning about adversarial computations in a higher-order setting. Our logics are built on top of a simply typed λ-calculus extended with a graded monad for probabilities and state. The grading is used to model and restrict the memory footprint and the cost (in terms of oracle calls) of computations. Under this view, an adversary is a higher-order expression that expects as arguments the code of its oracles. We develop unary program logics for reasoning about error probabilities and expected values, and a relational logic for reasoning about coupling-based properties. All logics feature rules for adversarial computations, and yield guarantees that are valid for all adversaries that satisfy a fixed resource policy. We prove the soundness of the logics in the category of quasi-Borel spaces, using a general notion of graded predicate liftings, and we use logical relations over graded predicate liftings to establish the soundness of proof rules for adversaries. We illustrate the working of our logics with simple but illustrative examples.},
	number = {ICFP},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Aguirre, Alejandro and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak and Katsumata, Shin-ya and Sato, Tetsuya},
	month = aug,
	year = {2021},
	keywords = {Probabilistic programming, program logics, semantic models},
}

@article{arranz_olmos_preservation_2025,
	address = {New York, NY, USA},
	title = {Preservation of {Speculative} {Constant}-{Time} by {Compilation}},
	volume = {9},
	url = {https://doi.org/10.1145/3704880},
	doi = {10.1145/3704880},
	abstract = {Compilers often weaken or even discard software-based countermeasures commonly used to protect programs against side-channel attacks; worse, they may also introduce vulnerabilities that attackers can exploit. The solution to this problem is to develop compilers that preserve such countermeasures. Prior work establishes that (a mildly modified version of) the CompCert and Jasmin formally verified compilers preserve constant-time, an information flow policy that ensures that programs are protected against timing side-channel attacks. However, nothing is known about preservation of speculative constant-time, a strengthening of the constant-time policy that ensures that programs are protected against Spectre-v1 attacks. We first show that preservation of speculative constant-time fails in practice by providing examples of secure programs whose compilation is not speculative constant-time using GCC (GCC -O0 and GCC -O1) and Jasmin. Then, we define a proof-of-concept compiler that distills some of the critical passes of the Jasmin compiler and use the Coq proof assistant to prove that it preserves speculative constant-time. Finally, we patch the Jasmin speculative constant-time type checker and demonstrate that all cryptographic implementations written in Jasmin can be fixed with minimal impact.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Arranz Olmos, Santiago and Barthe, Gilles and Blatter, Lionel and Grégoire, Benjamin and Laporte, Vincent},
	month = jan,
	year = {2025},
	keywords = {Compilers, Formal security models, Formal software verification},
}

@inproceedings{carvalho_semantic_2023,
	address = {New York, NY, USA},
	series = {{SPLASH} 2023},
	title = {Semantic {Versioning} for {Python} {Programs}},
	isbn = {979-8-4007-0384-3},
	url = {https://doi.org/10.1145/3618305.3623589},
	doi = {10.1145/3618305.3623589},
	abstract = {We propose a language-based approach to software versioning. Unlike the traditional approach of mainstream version control systems, where each evolution step is represented by a textual diff, we treat versions as programming elements. Each evolution step, merge operation, and version relationship, is represented explicitly in the program. This provides compile time guarantees for safety code reuse from previous versions, as well as forward and backwards compatibility between versions, allowing clients to use newly introduced code without needing to refactor their program. By lifting the versioning to the language level, we pave the way for tools that interact with software repositories to have more insight regarding the evolution of the software semantics.},
	booktitle = {Companion {Proceedings} of the 2023 {ACM} {SIGPLAN} {International} {Conference} on {Systems}, {Programming}, {Languages}, and {Applications}: {Software} for {Humanity}},
	publisher = {Association for Computing Machinery},
	author = {Carvalho, Luís},
	year = {2023},
	keywords = {Software evolution, type theory},
	pages = {13--15},
}

@article{lu_grisette_2023,
	address = {New York, NY, USA},
	title = {Grisette: {Symbolic} {Compilation} as a {Functional} {Programming} {Library}},
	volume = {7},
	url = {https://doi.org/10.1145/3571209},
	doi = {10.1145/3571209},
	abstract = {The development of constraint solvers simplified automated reasoning about programs and shifted the engineering burden to implementing symbolic compilation tools that translate programs into efficiently solvable constraints. We describe Grisette, a reusable symbolic evaluation framework for implementing domain-specific symbolic compilers. Grisette evaluates all execution paths and merges their states into a normal form that avoids making guards mutually exclusive. This ordered-guards representation reduces the constraint size 5-fold and the solving time more than 2-fold. Grisette is designed entirely as a library, which sidesteps the complications of lifting the host language into the symbolic domain. Grisette is purely functional, enabling memoization of symbolic compilation as well as monadic integration with host libraries. Grisette is statically typed, which allows catching programming errors at compile time rather than delaying their detection to the constraint solver. We implemented Grisette in Haskell and evaluated it on benchmarks that stress both the symbolic evaluation and constraint solving.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Lu, Sirui and Bodík, Rastislav},
	month = jan,
	year = {2023},
	keywords = {State Merging, Symbolic Compilation},
}

@article{jeffrey_leaky_2022,
	address = {New York, NY, USA},
	title = {The leaky semicolon: compositional semantic dependencies for relaxed-memory concurrency},
	volume = {6},
	url = {https://doi.org/10.1145/3498716},
	doi = {10.1145/3498716},
	abstract = {Program logics and semantics tell a pleasant story about sequential composition: when executing (S1;S2), we first execute S1 then S2. To improve performance, however, processors execute instructions out of order, and compilers reorder programs even more dramatically. By design, single-threaded systems cannot observe these reorderings; however, multiple-threaded systems can, making the story considerably less pleasant. A formal attempt to understand the resulting mess is known as a “relaxed memory model.” Prior models either fail to address sequential composition directly, or overly restrict processors and compilers, or permit nonsense thin-air behaviors which are unobservable in practice. To support sequential composition while targeting modern hardware, we enrich the standard event-based approach with preconditions and families of predicate transformers. When calculating the meaning of (S1; S2), the predicate transformer applied to the precondition of an event e from S2 is chosen based on the set of events in S1 upon which e depends. We apply this approach to two existing memory models.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Jeffrey, Alan and Riely, James and Batty, Mark and Cooksey, Simon and Kaysin, Ilya and Podkopaev, Anton},
	month = jan,
	year = {2022},
	keywords = {Preconditions, Arm8, C11, Compiler Optimizations, Concurrency, Multi-Copy Atomicity, Pomsets, Predicate Transformers, Relaxed Memory Models, Thin-Air Reads},
}

@article{ullrich__2022,
	address = {New York, NY, USA},
	title = {‘do’ unchained: embracing local imperativity in a purely functional language (functional pearl)},
	volume = {6},
	url = {https://doi.org/10.1145/3547640},
	doi = {10.1145/3547640},
	abstract = {Purely functional programming languages pride themselves with reifying effects that are implicit in imperative languages into reusable and composable abstractions such as monads. This reification allows for more exact control over effects as well as the introduction of new or derived effects. However, despite libraries of more and more powerful abstractions over effectful operations being developed, syntactically the common 'do' notation still lags behind equivalent imperative code it is supposed to mimic regarding verbosity and code duplication. In this paper, we explore extending 'do' notation with other imperative language features that can be added to simplify monadic code: local mutation, early return, and iteration. We present formal translation rules that compile these features back down to purely functional code, show that the generated code can still be reasoned over using an implementation of the translation in the Lean 4 theorem prover, and formally prove the correctness of the translation rules relative to a simple static and dynamic semantics in Lean.},
	number = {ICFP},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Ullrich, Sebastian and de Moura, Leonardo},
	month = aug,
	year = {2022},
	keywords = {functional programming, interactive theorem proving, Lean},
}

@inproceedings{gu_3d_2024,
	address = {New York, NY, USA},
	series = {{MM} '24},
	title = {{3D} {Human} {Pose} {Estimation} from {Multiple} {Dynamic} {Views} via {Single}-view {Pretraining} with {Procrustes} {Alignment}},
	isbn = {979-8-4007-0686-8},
	url = {https://doi.org/10.1145/3664647.3680990},
	doi = {10.1145/3664647.3680990},
	abstract = {3D Human pose estimation from multiple cameras with unknown calibration has received less attention than it should. The few existing data-driven solutions do not fully exploit 3D training data that are available on the market, and typically train from scratch for every novel multi-view scene, which impedes both accuracy and efficiency. We show how to exploit 3D training data to the fullest and associate multiple dynamic views efficiently to achieve high precision on novel scenes using a simple yet effective framework, dubbed Multiple Dynamic View Pose estimation (MDVPose). MDVPose utilizes novel scenarios data to finetune a single-view pretrained motion encoder in multi-view setting, aligns arbitrary number of views in a unified coordinate via Procruste alignment, and imposes multi-view consistency. The proposed method achieves 22.1 mm P-MPJPE or 34.2 mm MPJPE on the challenging in-the-wild Ski-Pose PTZ dataset, which outperforms the state-of-the-art method by 24.8\% P-MPJPE (-7.3 mm) and 19.0\% MPJPE (-8.0 mm). It also outperforms the state-of-the-art methods by a large margin (-18.2mm P-MPJPE and -28.3mm MPJPE) on the EgoBody dataset. In addition, MDVPose achieves robust performance on the Human3.6M datasets featuring multiple static cameras. Code is available at https://github.com/iGame-Lab/MDVPose.},
	booktitle = {Proceedings of the 32nd {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Gu, Renshu and Zhu, Jiajun and Si, Yixuan and Gao, Fei and Xu, Jiamin and Xu, Gang},
	year = {2024},
	keywords = {3d human pose estimation, dynamic viewpoint, multi-view},
	pages = {10363--10372},
}

@article{spies_quiver_2024,
	address = {New York, NY, USA},
	title = {Quiver: {Guided} {Abductive} {Inference} of {Separation} {Logic} {Specifications} in {Coq}},
	volume = {8},
	url = {https://doi.org/10.1145/3656413},
	doi = {10.1145/3656413},
	abstract = {Over the past two decades, there has been a great deal of progress on verification of full functional correctness of programs using separation logic, sometimes even producing “foundational” proofs in proof assistants like Coq. Unfortunately, even though existing approaches to this problem provide significant support for automated verification, they still incur a significant specification overhead: the user must supply the specification against which the program is verified, and the specification may be long, complex, or tedious to formulate.In this paper, we introduce Quiver, the first technique for inferring functional correctness specifications in separation logic while simultaneously verifying foundationally that they are correct. To guide Quiver towards the final specification, we take hints from the user in the form of a specification sketch, and then complete the sketch using inference. To do so, Quiver introduces a new abductive deductive verification technique, which integrates ideas from abductive inference (for specification inference) together with deductive separation logic automation (for foundational verification). The result is that users have to provide some guidance, but significantly less than with traditional deductive verification techniques based on separation logic. We have evaluated Quiver on a range of case studies, including code from popular open-source libraries.},
	number = {PLDI},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Spies, Simon and Gäher, Lennard and Sammler, Michael and Dreyer, Derek},
	month = jun,
	year = {2024},
	keywords = {Coq, abduction, functional correctness, Iris, specification inference},
}

@article{trevithick_real-time_2023,
	address = {New York, NY, USA},
	title = {Real-{Time} {Radiance} {Fields} for {Single}-{Image} {Portrait} {View} {Synthesis}},
	volume = {42},
	issn = {0730-0301},
	url = {https://doi.org/10.1145/3592460},
	doi = {10.1145/3592460},
	abstract = {We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.},
	number = {4},
	journal = {ACM Trans. Graph.},
	publisher = {Association for Computing Machinery},
	author = {Trevithick, Alex and Chan, Matthew and Stengel, Michael and Chan, Eric and Liu, Chao and Yu, Zhiding and Khamis, Sameh and Chandraker, Manmohan and Ramamoorthi, Ravi and Nagano, Koki},
	month = jul,
	year = {2023},
	keywords = {inverse rendering, neural radiance field, view synthesis},
}

@article{jeong_massive_2022,
	address = {871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA},
	title = {Massive {IoT} {Malware} {Classification} {Method} {Using} {Binary} {Lifting}},
	volume = {32},
	issn = {1079-8587},
	doi = {10.32604/iasc.2022.021038},
	abstract = {Owing to the development of next-generation network and data processing technologies, massive Internet of Things (IoT) devices are becoming hyperconnected. As a result, Linux malware is being created to attack such hyperconnected networks by exploiting security threats in IoT devices. To determine the potential threats of such Linux malware and respond effectively, malware classification through an analysis of the executed code is required; however, a limitation exists in that each heterogeneous architecture must be analyzed separately. However, the binary codes of a heterogeneous architecture can be translated to a high-level intermediate representation (IR) of the same format using binary lifting and malicious behavior information can be identified because the functions and parameters of the assembly code are stored in the IR. Consequently, this study suggests a Linux malware classification method applicable to various architectures by converting Linux assembly codes into an IR using binary lifting and then learning the IR Sequence which reflects malicious behavior pattern using deep learning model for sequence learning.},
	language = {English},
	number = {1},
	journal = {IN℡LIGENT AUTOMATION AND SOFT COMPUTING},
	publisher = {TECH SCIENCE PRESS},
	author = {Jeong, Hae-Seon and Kwak, Jin},
	year = {2022},
	note = {Type: Article},
	keywords = {malware classification, binary lifting, Assembly code, Linux malware},
	pages = {467--481},
}

@article{ramamoorthy_linux_2024,
	address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
	title = {Linux {IoT} {Malware} {Variant} {Classification} {Using} {Binary} {Lifting} and {Opcode} {Entropy}},
	volume = {13},
	doi = {10.3390/electronics13122381},
	abstract = {Binary function analysis is fundamental in understanding the behavior and genealogy of malware. The detection, classification, and analysis of Linux IoT malware and its variants present significant challenges due to the wide range of architectures supported by the Linux IoT platform. This study concentrates on static analysis using binary lifting techniques to extract and analyze Intermediate Representation (IR) opcode sequences. We introduce a set of statistical entropy-based features derived from these IR opcode sequences, establishing a practical and straightforward methodology for machine learning classification models. By exclusively analyzing function metadata and opcode entropy, our architecture-agnostic approach not only efficiently detects malware but also classifies its variants with a high degree of accuracy, achieving an F1 score of 97\%. The proposed approach offers a robust alternative for enhancing malware detection and variant identification frameworks for IoT devices.},
	language = {English},
	number = {12},
	journal = {ELECTRONICS},
	publisher = {MDPI},
	author = {Ramamoorthy, Jayanthi and Gupta, Khushi and Shashidhar, Narasimha K. and Varol, Cihan},
	month = jun,
	year = {2024},
	note = {Type: Article},
	keywords = {machine learning, malware detection, malware classification, binary lifting, ELF static analysis, opcode sequence analysis},
}

@article{donisi_work-related_2021,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Work-{Related} {Risk} {Assessment} {According} to the {Revised} {NIOSH} {Lifting} {Equation}: {A} {Preliminary} {Study} {Using} a {Wearable} {Inertial} {Sensor} and {Machine} {Learning}},
	volume = {21},
	doi = {10.3390/s21082593},
	abstract = {Many activities may elicit a biomechanical overload. Among these, lifting loads can cause work-related musculoskeletal disorders. Aspiring to improve risk prevention, the National Institute for Occupational Safety and Health (NIOSH) established a methodology for assessing lifting actions by means of a quantitative method based on intensity, duration, frequency and other geometrical characteristics of lifting. In this paper, we explored the machine learning (ML) feasibility to classify biomechanical risk according to the revised NIOSH lifting equation. Acceleration and angular velocity signals were collected using a wearable sensor during lifting tasks performed by seven subjects and further segmented to extract time-domain features: root mean square, minimum, maximum and standard deviation. The features were fed to several ML algorithms. Interesting results were obtained in terms of evaluation metrics for a binary risk/no-risk classification; specifically, the tree-based algorithms reached accuracies greater than 90\% and Area under the Receiver operating curve characteristics curves greater than 0.9. In conclusion, this study indicates the proposed combination of features and algorithms represents a valuable approach to automatically classify work activities in two NIOSH risk groups. These data confirm the potential of this methodology to assess the biomechanical risk to which subjects are exposed during their work activity.},
	language = {English},
	number = {8},
	journal = {SENSORS},
	publisher = {MDPI},
	author = {Donisi, Leandro and Cesarelli, Giuseppe and Coccia, Armando and Panigazzi, Monica and Capodaglio, Edda Maria and D'Addio, Giovanni},
	month = apr,
	year = {2021},
	note = {Type: Article},
	keywords = {machine learning, feature extraction, ergonomics, lifting, work-related musculoskeletal disorders, health monitoring, biomechanical risk assessment, IMUs, NIOSH, wearable device},
}

@article{prisco_capability_2024,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Capability of {Machine} {Learning} {Algorithms} to {Classify} {Safe} and {Unsafe} {Postures} during {Weight} {Lifting} {Tasks} {Using} {Inertial} {Sensors}},
	volume = {14},
	doi = {10.3390/diagnostics14060576},
	abstract = {Occupational ergonomics aims to optimize the work environment and to enhance both productivity and worker well-being. Work-related exposure assessment, such as lifting loads, is a crucial aspect of this discipline, as it involves the evaluation of physical stressors and their impact on workers' health and safety, in order to prevent the development of musculoskeletal pathologies. In this study, we explore the feasibility of machine learning (ML) algorithms, fed with time- and frequency-domain features extracted from inertial signals (linear acceleration and angular velocity), to automatically and accurately discriminate safe and unsafe postures during weight lifting tasks. The signals were acquired by means of one inertial measurement unit (IMU) placed on the sternums of 15 subjects, and subsequently segmented to extract several time- and frequency-domain features. A supervised dataset, including the extracted features, was used to feed several ML models and to assess their prediction power. Interesting results in terms of evaluation metrics for a binary safe/unsafe posture classification were obtained with the logistic regression algorithm, which outperformed the others, with accuracy and area under the receiver operating characteristic curve values of up to 96\% and 99\%, respectively. This result indicates the feasibility of the proposed methodology-based on a single inertial sensor and artificial intelligence-to discriminate safe/unsafe postures associated with load lifting activities. Future investigation in a wider study population and using additional lifting scenarios could confirm the potentiality of the proposed methodology, supporting its applicability in the occupational ergonomics field.},
	language = {English},
	number = {6},
	journal = {DIAGNOSTICS},
	publisher = {MDPI},
	author = {Prisco, Giuseppe and Romano, Maria and Esposito, Fabrizio and Cesarelli, Mario and Santone, Antonella and Donisi, Leandro and Amato, Francesco},
	month = mar,
	year = {2024},
	note = {Type: Article},
	keywords = {machine learning, work-related musculoskeletal disorders, occupational ergonomics, safe/unsafe posture, wearable sensors, inertial signals, load lifting},
}

@article{falconer_condition_2022,
	address = {241 WOODLAND DR, STATE COLLEGE, PENNSYLVANIA, UNITED STATES},
	title = {Condition {Classification} of {Fibre} {Ropes} during {Cyclic} {Bend} over {Sheave} testing {Using} {Machine} {Learning}},
	volume = {13},
	issn = {2153-2648},
	doi = {10.36001/IJPHM.2022.v13i1.3105},
	abstract = {Fibre ropes have been shown to be a viable alternative to steel wire rope for offshore lifting operations. Visual inspection remains a common method of fibre rope condition monitoring and has the potential to be further automated by machine learning. This would provide a valuable aid to current inspection frameworks to make more accurate decisions on recertification or retirement of fibre ropes in operational use. Three different machine learning algorithms: decision tree, random forest and support vector machine are compared to classical statistical approaches such as logistic regression, k-nearest neighbours and Naive-Bayes for condition classification for fibre ropes under cyclic-bend-over-sheave (CBOS) testing. By measuring the rope global elongation throughout the CBOS tests, a binary classification system has been used to label recorded samples as healthy or close to rupture. Predictions are made on one rope through leave-one-out cross validation. The models are then assessed through calculating the accuracy, probability of detection, probability of false alarm and Matthew's Correlation Coefficient, and ranked based on the results. The results show that both machine learning and classical statistical methods are effective options for condition classification of fibre ropes under CBOS regimes. Typical values for Matthews Correlation Coefficient (MCC) were shown to exceed 0.8 for the best performing methods.},
	language = {English},
	number = {1},
	journal = {INTERNATIONAL JOURNAL OF PROGNOSTICS AND HEALTH MANAGEMENT},
	publisher = {PHM SOCIETY},
	author = {Falconer, Shaun and Krause, Peter and Back, Thomas and Nordgard-Hansen, Ellen and Grasmo, Geir},
	year = {2022},
	note = {Type: Article},
}

@article{stahl_toward_2025,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Toward the {Detection} of {Flow} {Separation} for {Operating} {Airfoils} {Using} {Machine} {Learning}},
	volume = {10},
	doi = {10.3390/ijtpp10040041},
	abstract = {Turbulent flow separation over lifting surfaces impacts high-lift systems such as aircraft, wind turbines, and turbomachinery, and contributes to noise, lift loss, and vibrations. Accurate detection of flow separation is therefore essential to enable active control strategies and to mitigate its adverse effects. Several machine learning models are compared for detecting flow separation from surface pressure fluctuations. The models were trained on experimental data covering various airfoils, angles of attack (0 degrees-23 degrees), and Reynolds numbers, with Rec=0.8-4.5x106. For supervised learning, the ground-truth binary labels (attached or separated flow) were derived from static pressure distributions, lift coefficients, and the power spectral densities of surface pressure fluctuations. Three machine learning techniques (multilayer perceptron, support vector machine, logistic regression) were utilized with fine-tuned hyperparameters. Promising results are obtained, with the support vector machine achieving the highest performance (accuracy 0.985, Matthews correlation coefficient 0.975), comparable to other models, with advantages in runtime and model size. However, most misclassifications occur near separation onset due to gradual transition, suggesting areas for model refinement. Sensitivity to database parameters is discussed alongside flow physics and data quality.},
	language = {English},
	number = {4},
	journal = {INTERNATIONAL JOURNAL OF TURBOMACHINERY PROPULSION AND POWER},
	publisher = {MDPI},
	author = {Stahl, Kathrin and Le Floc'h, Arnaud and Pester, Britta and Ebert, Paul L. and Suryadi, Alexandre and Hu, Nan and Herr, Michaela},
	month = nov,
	year = {2025},
	note = {Type: Article},
	keywords = {machine learning, aeroacoustic, aerodynamic, airfoil, data-driven approach, flow separation detection, high Reynolds number, trailing-edge noise, turbulent flow},
}

@article{haghighi_effective_2020,
	address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
	title = {An {Effective} {Semi}-fragile {Watermarking} {Method} for {Image} {Authentication} {Based} on {Lifting} {Wavelet} {Transform} and {Feed}-{Forward} {Neural} {Network}},
	volume = {12},
	issn = {1866-9956},
	doi = {10.1007/s12559-019-09700-9},
	abstract = {Digital watermarking is a significant issue in the field of information security and avoiding the misuse of images in the world of Internet and communication. This paper proposes a novel watermarking method for tamper detection and recovery using semi-fragile data hiding, based on lifting wavelet transform (LWT) and feed-forward neural network (FNN). In this work, first, the host image is decomposed up to one level using LWT, and the discrete cosine transform (DCT) is applied to each 2x2 blocks of diagonal details. Next, a random binary sequence is embedded in each block as the watermark by correlating DC coefficients. In the authentication stage, first, the geometry is analyzed by using speeded up robust features (SURF) algorithm and extract watermark bits by using FNN. Afterward, logical exclusive or operation between original and extracted watermark is applied to detect tampered region. Eventually, in the recovery stage, tampered regions are recovered using the inverse halftoning technique. The performance and efficiency of the method and its robustness against various geometric, non-geometric, and hybrid attacks are reported. From the experimental results, it can be seen that the proposed method is superior in terms of robustness and quality of the watermarked and recovered images, respectively, compared to the state-of-the-art methods. Besides, imperceptibility has been improved by using different correlation steps as the gain factor for flat (smooth) and texture (rough) blocks. Based on the advantages exhibited, the proposed method outperforms the related works, in terms of superiority, efficiency, and effectiveness for tamper detection and recovery-based applications.},
	language = {English},
	number = {4},
	journal = {COGNITIVE COMPUTATION},
	publisher = {SPRINGER},
	author = {Haghighi, Behrouz Bolourian and Taherinia, Amir Hossein and Monsefi, Reza},
	month = jul,
	year = {2020},
	note = {Type: Article},
	keywords = {Watermarking, Feed-forward neural network, Halftone technique, Image authentication and restoration, Lifting wavelet transform, Tamper detection and recovery},
	pages = {863--890},
}

@article{islam_neural_2018,
	address = {NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
	title = {Neural network based robust image watermarking technique in {LWT} domain},
	volume = {34},
	issn = {1064-1246},
	doi = {10.3233/JIFS-169462},
	abstract = {In this paper, a robust image watermarking technique has been proposed in lifting wavelet transform (LWT) domain. Neural network is incorporated in the watermark extraction process to achieve improved robustness against different attacks. The integration of neural network with LWT makes the system robust to various attacks maintaining an adequate level of imperceptibility. The 3-level LWT coefficients are randomized and arranged in 2x2 non-overlapping blocks. Each block is modified according to a binary watermark bit. Randomization of coefficients and blocks has been done to enhance the security of the system. The binary watermark bit is also encrypted using another key. The scheme provides an average imperceptibility of 43.88 dB for a watermark capacity of 512 bits. The robustness has been observed against all the intentional and non-intentional attacks. The technique provides satisfactory robustness against different attacks such as noising attacks, de-noising attacks, lossy compression attacks, image processing attacks and some geometric attacks. The algorithm has been tested on a large image database containing different class of images.},
	language = {English},
	number = {3},
	journal = {JOURNAL OF IN℡LIGENT \& FUZZY SYSTEMS},
	publisher = {IOS PRESS},
	author = {Islam, Mohiul and Roy, Amarjit and Laskar, Rabul Hussain},
	year = {2018},
	note = {Type: Article; Proceedings Paper},
	keywords = {artificial neural network (ANN), image watermarking, Lifting wavelet transform (LWT), normalized cross-correlation (NC), peak signal to noise ratio (PSNR)},
	pages = {1691--1700},
	annote = {3rd International Symposium on Intelligent Systems Technologies and Applications (ISTA), Manipal Univ, Manipal, INDIA, SEP 13-16, 2017},
}

@article{horvath_pragmatic_2023,
	address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
	title = {Pragmatic verification and validation of industrial executable {SysML} models},
	volume = {26},
	issn = {1098-1241},
	doi = {10.1002/sys.21679},
	abstract = {In recent years, Model-Based Systems Engineering (MBSE) practices have been applied in various industries to design, simulate and verify complex systems. The verification and validation (V\&V) of such systems engineering models are crucial to develop high-quality systems. However, this is a challenging problem due to the complexity of the models and semantic differences in how different tools interpret the models, which can undermine the validity of the obtained results if they go undiscovered. To address these issues, we propose (i) a subset of the SysML language for which the practical semantic integrity of tools can be achieved and (ii) a cloud-based V\&V framework for this subset, lifting verification to an industrial scale. We demonstrate the feasibility of our approach on an industrial-scale model from the aerospace domain and summarize the lessons learned during transitioning formal verification tools to an industrial context.},
	language = {English},
	number = {6},
	journal = {SYSTEMS ENGINEERING},
	publisher = {WILEY},
	author = {Horvath, Benedek and Molnar, Vince and Graics, Bence and Hajdu, Akos and Rath, Istvan and Horvath, Akos and Karban, Robert and Trancho, Gelys and Micskei, Zoltan},
	month = jan,
	year = {2023},
	note = {Type: Article},
	keywords = {formal verification, hidden formal methods, MBSE, model checking, SysML},
	pages = {693--714},
}

@article{jang_transformer-based_2025,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Transformer-based deep learning model and video dataset for installation action recognition in offsite projects},
	volume = {172},
	issn = {0926-5805},
	doi = {10.1016/j.autcon.2025.106042},
	abstract = {This paper developed and evaluated the Precast Concrete Installation Dataset (PCI-Dataset), a large-scale video dataset for automatically recognizing precast concrete (PC) installation activities. The dataset comprises 12,791 video clips (5 s each, 1080 x 1080 resolution, 30fps) from actual PC construction sites, including 12 balanced activity classes combining three component types and four work stages. Evaluation of six Transformer-based video classification models showed VideoMAE V2 achieved the highest overall accuracy of 98.10 \%, followed by UniFormer V2, Video Swin, MVIT, ViViT, and TimeSformer. VideoMAE V2 achieved F1 scores above 80 \% for most activities, with a peak of 92.20 \% for slab assembly. In a case study on a real PC construction site, the model demonstrated high recognition accuracies: 100 \% for lifting, 85.83-100 \% for rigging, and 93.75-100 \% for assembly operations. The paper contributes to PC construction management theory by applying computer vision for real-time and automated work recognition and analysis.},
	language = {English},
	journal = {AUTOMATION IN CONSTRUCTION},
	publisher = {ELSEVIER},
	author = {Jang, Junyoung and Jeong, Eunbeen and Kim, Tae Wan},
	month = apr,
	year = {2025},
	note = {Type: Article},
	keywords = {Computer vision, Off-site construction, Precast concrete installation, Transformer models, Video recognition},
}

@article{rhim_binary_2024,
	address = {POSTFACH 101161, 69451 WEINHEIM, GERMANY},
	title = {Binary {Addressable} {Optical} {Multiplexing} {Waveguides} via {Electrochromic} {Switching}},
	volume = {221},
	issn = {1862-6300},
	doi = {10.1002/pssa.202300177},
	abstract = {Photonic circuits attract much attention as promising candidates to overcome the drawbacks of their electronic counterparts. By utilizing the broad bandwidth and low energy consumption of optical communication, hybrid circuits can provide a comprehensive platform for the era beyond Moore's law. In particular, parallel matrix operations, the heavy lifting behind neural networks, remain challenging for traditional electronics due to high heat dissipation. To enable these parallel computations optically, (de-)multiplexing is crucial to address the different channels. Previously this has been accomplished with complex spectral or time encodings in wave division or time division methods. However, herein, a simple method to address parallel optical channels exclusively with 2-bit signals is presented. By using PEDOT:PSS as electrochromic material for intensity modulation, light transmission or absorption is controlled by oxidation and reduction with an electrolyte. Y-branch structures are used to design the multiplexing layout and to assign the 2-bit states to the channels. This binary addressable optical multiplexer, therefore, combines optical communication with electronic signals into a hybrid circuit.},
	language = {English},
	number = {1, SI},
	journal = {PHYSICA STATUS SOLIDI A-APPLICATIONS AND MATERIALS SCIENCE},
	publisher = {WILEY-V C H VERLAG GMBH},
	author = {Rhim, Seon-Young and Heyl, Max and Busch, Kurt and List-Kratochvil, Emil J. W.},
	month = jan,
	year = {2024},
	note = {Type: Article},
	keywords = {electrochromic, electrolyte, multiplexing, photonics, waveguide},
}

@article{zhao_research_2024-1,
	address = {PO BOX 564, 1001 LAUSANNE, SWITZERLAND},
	title = {Research and analysis of the thermal and control characteristics of the plate-type fuel assembly for the supercritical {CO2} reactor},
	volume = {416},
	issn = {0029-5493},
	doi = {10.1016/j.nucengdes.2023.112757},
	abstract = {Small nuclear reactors can be applied to micro grid power generation, island and space nuclear power systems. The plate-type fuel assembly has a compact design structure, low fuel core temperature, large heat exchange area, and high heat exchange efficiency, which is widely used in small nuclear reactors. To supplement the lack of research in S-CO2 small nuclear reactor, The Brayton cycle reactor system analysis program of S-CO2 plate-type fuel assemblies (BRESA-PFA) is developed, and the reactor control system program is developed to simulate the transient condition in this paper. This article establishes a fuel assembly flow and heat transfer model and a control rod control model using Modelica language, achieving physical and thermal coupling and power control of the reactor. The steady-state operating parameters of the developed program BRESA-PFA were studied, and the flow distribution, coolant and fuel temperature distribution of BRESA-PFA were obtained. The flow distribution conforms to the experimental parameters of CARR, and the fuel assembly radial temperature distribution is high at the center and low at the edge. After verification, the maximum fuel temperature did not exceed the limit value and met the design requirements. Transient characteristic analysis was conducted to study the rod lifting strategy during the startup process, the start-up and rod lifting process of BRESA-PFA is N2-N1-G2-G1, using the logic of step control. Research on reactor control system response under loss of coolant accident, after the accident, the coolant flow suddenly decreased to 65 \% of the rated value, and the reactor power also decreased to 65 \% FP, G2 and G1 control rods were inserted downwards, the coolant temperature fluctuates by 1 \%similar to 2\%. The reactor control system was able to respond quickly, ensuring the stability of coolant temperature, proving the safety and reliability of BRESA-PFA, and providing theoretical reference and scheme support for the research of S-CO2 small nuclear reactors.},
	language = {English},
	journal = {NUCLEAR ENGINEERING AND DESIGN},
	publisher = {ELSEVIER SCIENCE SA},
	author = {Zhao, Fulong and Guo, Jia and Xie, Lin and Tian, Ruifeng and Tan, Sichao},
	month = jan,
	year = {2024},
	note = {Type: Article},
	keywords = {Loss of coolant accident, Plate -type fuel assembly, Reactor control system, Reactor start -up, Supercritical carbon dioxide coolant},
}

@article{zhou_modeling_2024,
	address = {1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND},
	title = {Modeling and controlling of ship general section attitude adjustment process based on {RBF} neural network coupled with sliding mode algorithm},
	volume = {238},
	issn = {1475-0902},
	doi = {10.1177/14750902241227301},
	abstract = {Due to the advantages such as high efficiency, high precision, and the ability to reduce welding distortion, the block assembly method in shipbuilding possesses currently holds a dominant position in shipbuilding engineering. However, some key issues including low adjustment precision and slow control response speed urgently need to be resolved for the block assembly adjustment technology. This paper committed to solving the problems of inaccurate tracking of target displacement and slow control response speed in the vertical motion axis of the ship block joining equipment. A docking equipment control method based on the RBF neural network coupled with adaptive sliding mode algorithm was proposed. Firstly, an overview of the overall mechanics and control architecture of the ship block joining equipment was provided. Subsequently, a mathematical model for the transmission at the lifting mechanism was established. A sliding mode controller based on position control for the ship block joining equipment was designed for the transmission system. Then, the RBF neural network was employed to adjust the switching gain of the sliding mode controller and develop a self-adaptive sliding mode controller. Finally, simulations and verifications were conducted for multiple sets of input trajectories with different types. The results demonstrated that the combination of the neural network algorithm and the sliding mode control algorithm model presented in this paper reduces the system response time by 28.125\% and improves the average motion tracking accuracy by 30.76\%.},
	language = {English},
	number = {4},
	journal = {PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART M-JOURNAL OF ENGINEERING FOR THE MARITIME ENVIRONMENT},
	publisher = {SAGE PUBLICATIONS LTD},
	author = {Zhou, Honggen and Bao, Chaojie and Deng, Bo and Li, Lei},
	month = jan,
	year = {2024},
	note = {Type: Article},
	keywords = {attitude tracking, Posture adjustment mechanism, RBF neural networks, slip mold control},
	pages = {778--791},
}

@article{he_formalizing_2025,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {Formalizing {Linear} {Motion} {G}-{Code} for {Invariant} {Checking} and {Differential} {Testing} of {Fabrication} {Tools}},
	volume = {9},
	doi = {10.1145/3763106},
	abstract = {The computational fabrication pipeline for 3D printing is much like a compiler - users design models in Computer Aided Design (CAD) tools that are lowered to polygon meshes to be ultimately compiled to machine code by 3D slicers. For traditional compilers and programming languages, techniques for checking program invariants are well-established. Similarly, methods like differential testing are frequently used to uncover bugs in compilers themselves, which makes them more reliable. The fabrication pipeline would benefit from similar techniques but traditional approaches do not directly apply to the representations used in this domain. Unlike traditional programs, 3D models exist both as geometric objects (a CAD model or a polygon mesh) as well as machine code that ultimately runs on the hardware. The machine code, like in traditional compiling, is affected by many factors like the model, the slicer being used, and numerous user-configurable parameters that control the slicing process. In this work, we propose a new algorithm for lifting G-code (a common language used in many fabrication pipelines) by denoting a G-code program to a set of cuboids, and then defining an approximate point cloud representation for efficiently operating on these cuboids. Our algorithm opens up new opportunities: we show three use cases that demonstrate how it enables (1) error localization in CAD models through invariant checking, (2) quantitative comparisons between slicers, and (3) evaluating the efficacy of mesh repair tools. We present a prototype implementation of our algorithm in a tool, GLITCHFINDER, and evaluate it on 58 real-world CAD models. Our results show that GLITCHFINDER is particularly effective in identifying slicing issues due to small features, can highlight differences in how popular slicers (Cura and PrusaSlicer) slice the same model, and can identify cases where mesh repair tools (MeshLab and Meshmixer) introduce new errors during repair.},
	language = {English},
	number = {OOPSLA2, OOPLA2},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {He, Yumeng and Nandi, Chandrakana and Pai, Sreepathi},
	month = oct,
	year = {2025},
	note = {Type: Article},
	keywords = {differential testing, G-code, invariant checking, operational semantics},
}

@article{agrawal_leveraging_2025,
	address = {125 London Wall, London, ENGLAND},
	title = {Leveraging linked data for space constraints checking of mobile cranes in modular construction assembly lookahead planning},
	volume = {68},
	issn = {1474-0346},
	doi = {10.1016/j.aei.2025.103778},
	abstract = {Preparing constraint-free lookahead schedules (LAS) in the assembly stage of dynamic modular construction (MC) projects requires checking space availability for mobile crane operation using heterogeneous, distributed information sources. Current automated crane space evaluation methods rely on centralized information databases, whereas linked data based approaches are limited by insufficient geometric computation capabilities. This study proposes a framework to model and validate the space constraints for mobile crane operations using the semantic web. It starts with developing an ontology to represent crane lifting space requirements on the semantic web. Information sources, including construction site point clouds, 4D building information models, and crane specifications, are semantically interconnected using linked data. Shapes Constraint Language JavaScript Extension performs constraint validation through JavaScript-based mathematical computations utilizing the Separating Axis Theorem and a triangulation-based approach to check space for crane placement and rotation, respectively. Validation on two MC sites demonstrated the framework's effectiveness in identifying space constraint violations.},
	language = {English},
	number = {C},
	journal = {ADVANCED ENGINEERING INFORMATICS},
	publisher = {ELSEVIER SCI LTD},
	author = {Agrawal, Ajay Kumar and Zou, Yang and Chen, Long and Abdelmegid, Mohammed and Gonzalez, Vicente A. and Jin, Hongyu},
	month = jan,
	year = {2025},
	note = {Type: Article},
	keywords = {Linked data, Lookahead schedule, Modular construction, Shapes Constraint Language, Space constraint checking},
}

@article{liew_interpretable_2020,
	address = {ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES},
	title = {Interpretable machine learning models for classifying low back pain status using functional physiological variables},
	volume = {29},
	issn = {0940-6719},
	doi = {10.1007/s00586-020-06356-0},
	abstract = {Purpose To evaluate the predictive performance of statistical models which distinguishes different low back pain (LBP) sub-types and healthy controls, using as input predictors the time-varying signals of electromyographic and kinematic variables, collected during low-load lifting. Methods Motion capture with electromyography (EMG) assessment was performed on 49 participants [healthy control (con) = 16, remission LBP (rmLBP) = 16, current LBP (LBP) = 17], whilst performing a low-load lifting task, to extract a total of 40 predictors (kinematic and electromyographic variables). Three statistical models were developed using functional data boosting (FDboost), for binary classification of LBP statuses (model 1: con vs. LBP; model 2: con vs. rmLBP; model 3: rmLBP vs. LBP). After removing collinear predictors (i.e. a correlation of {\textgreater} 0.7 with other predictors) and inclusion of the covariate sex, 31 predictors were included for fitting model 1, 31 predictors for model 2, and 32 predictors for model 3. Results Seven EMG predictors were selected in model 1 (area under the receiver operator curve [AUC] of 90.4\%), nine predictors in model 2 (AUC of 91.2\%), and seven predictors in model 3 (AUC of 96.7\%). The most influential predictor was the biceps femoris muscle (peak beta = 0.047) in model 1, the deltoid muscle (peak beta = 0.052) in model 2, and the iliocostalis muscle (peak beta = 0.16) in model 3. Conclusion The ability to transform time-varying physiological differences into clinical differences could be used in future prospective prognostic research to identify the dominant movement impairments that drive the increased risk.},
	language = {English},
	number = {8},
	journal = {EUROPEAN SPINE JOURNAL},
	publisher = {SPRINGER},
	author = {Liew, Bernard X. W. and Rugamer, David and De Nunzio, Alessandro Marco and Falla, Deborah},
	month = aug,
	year = {2020},
	note = {Type: Article},
	keywords = {Machine learning, Biomechanics, Lifting, Functional regression, Low back pain, Motor control},
	pages = {1845--1859},
}

@article{qin_directed_2025,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Directed grey box fuzzy testing for power terminal device firmware with intermediate representation similarity comparison},
	volume = {90},
	issn = {2214-2126},
	doi = {10.1016/j.jisa.2025.104038},
	abstract = {The proliferation of heterogeneous devices in power IoT terminals significantly increases security risks due to firmware vulnerabilities, thereby threatening the stability and reliability of power systems. However, existing Directed Greybox Fuzzing (DGF) methods face challenges, such as the need for manual identification of vulnerable code and limitations to specific architectures. This paper proposes a DGF approach, guided by intermediate representation similarity comparison, comprising two main components: objective function localization and directed greybox fuzzing. In the objective function localization phase, support for multiple architectures is achieved by lifting the binary code to LLVM Intermediate Representation (IR). Given that functions may vary in both structure and semantics, we represent functions using both structural and semantic features. We employ word embedding techniques based on Natural Language Processing (NLP) and graph neural network models to construct feature vectors. By calculating the feature similarity between each function and known vulnerable functions, we automatically identify highly similar functions as targets. In the directed greybox fuzzing phase, to address issues like high false positive rates and unreachable targets, we designed a target scheduling mechanism. This mechanism permanently blocks targets that have been sufficiently covered and periodically blocks those that have not been covered, thereby further improving the efficiency of fuzzing. Experimental results on two datasets demonstrate the effectiveness of this method in identifying vulnerabilities in power terminal equipment.},
	language = {English},
	journal = {JOURNAL OF INFORMATION SECURITY AND APPLICATIONS},
	publisher = {ELSEVIER},
	author = {Qin, Zhongyuan and Chen, Jiaqi and Sun, Xin and Song, Yubo and Dai, Hua and Chen, Weiwei and Lv, Bang and Wang, Kanghui},
	month = may,
	year = {2025},
	note = {Type: Article},
	keywords = {Feature extraction, Binary code similarity, Directed grey box fuzzing(DGF), Graph auto-encoder, Intermediate representation(IR), Target scheduling mechanism},
}

@article{liu_headland_2024,
	address = {MDPI AG, Grosspeteranlage 5, CH-4052 BASEL, SWITZERLAND},
	title = {Headland {Identification} and {Ranging} {Method} for {Autonomous} {Agricultural} {Machines}},
	volume = {14},
	doi = {10.3390/agriculture14020243},
	abstract = {Headland boundary identification and ranging are the key supporting technologies for the automatic driving of intelligent agricultural machinery, and they are also the basis for controlling operational behaviors such as autonomous turning and machine lifting. The complex, unstructured environments of farmland headlands render traditional image feature extraction methods less accurate and adaptable. This study utilizes deep learning and binocular vision technologies to develop a headland boundary identification and ranging system built upon the existing automatic guided tractor test platform. A headland image annotation dataset was constructed, and the MobileNetV3 network, notable for its compact model structure, was employed to achieve binary classification recognition of farmland and headland images. An improved MV3-DeeplabV3+ image segmentation network model, leveraging an attention mechanism, was constructed, achieving a high mean intersection over union (MIoU) value of 92.08\% and enabling fast and accurate detection of headland boundaries. Following the detection of headland boundaries, binocular stereo vision technology was employed to measure the boundary distances. Field experiment results indicate that the system's average relative errors of distance in ranging at distances of 25 m, 20 m, and 15 m are 6.72\%, 4.80\%, and 4.35\%, respectively. This system is capable of meeting the real-time detection requirements for headland boundaries.},
	language = {English},
	number = {2},
	journal = {AGRICULTURE-BASEL},
	publisher = {MDPI},
	author = {Liu, Hui and Li, Kun and Ma, Luyao and Meng, Zhijun},
	month = feb,
	year = {2024},
	note = {Type: Article},
	keywords = {deep learning, autonomous agricultural machinery, binocular vision, headland, image recognition},
}

@article{bu_overview_2021,
	address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
	title = {An overview of testing methods for aeroengine fan noise},
	volume = {124},
	issn = {0376-0421},
	doi = {10.1016/j.paerosci.2021.100722},
	abstract = {We are now at a point where airframes, aeroengines and their integration require radical changes to meet increasingly aggressive environmental targets. Generally, fan noise emitted from rotor-stator assemblies would be one of the dominant noise sources for modern and next-generation aircraft engines, which shall direct most research interest into fans in the coming decade and, therefore, the associated measurement constitutes the main focus of the current review. Amongst various approaches, an experimental study of fan noise is usually efficient and of high-fidelity, but the associated cost is expensive. Moreover, the design of a test would be time-consuming due to the adequate choice of testing methods with the adaption to the testing case and rig. In addition, in most cases the experimental setup requires some sort of optimization to achieve high measurement accuracy. This article provides a contemporary review of the most well-known and state-of-the-art testing methods with the focus on fan noise problems. More specifically, the acoustic mode detection and noise source reconstruction methods are most relevant to the understanding of noise generation mechanism and propagating characteristics, which are useful for further noise reduction studies, and therefore are extensively reviewed in this article. As timely guidance to potential interested readers, this paper also provides an overview of recent developments based on the compressive sensing and machine learning techniques that have enabled disruptive innovations by fundamentally changing the testing practices of conventional measurement methods, thus constituting the continued research direction for next-generation aeroengines.},
	language = {English},
	journal = {PROGRESS IN AEROSPACE SCIENCES},
	publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
	author = {Bu, Huanxian and Huang, Xun and Zhang, Xin},
	month = jul,
	year = {2021},
	note = {Type: Review},
	keywords = {Acoustic mode detection, Aeroengine fan noise, Compressive sensing, In-duct beamforming},
}

@article{schwarz_co-evolutionary_2022,
	address = {GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND},
	title = {Co-evolutionary distance predictions contain flexibility information},
	volume = {38},
	issn = {1367-4803},
	doi = {10.1093/bioinformatics/btab562},
	abstract = {Motivation: Co-evolution analysis can be used to accurately predict residue-residue contacts from multiple sequence alignments. The introduction of machine-learning techniques has enabled substantial improvements in precision and a shift from predicting binary contacts to predict distances between pairs of residues. These developments have significantly improved the accuracy of de novo prediction of static protein structures. With AlphaFold2 lifting the accuracy of some predicted protein models close to experimental levels, structure prediction research will move on to other challenges. One of those areas is the prediction of more than one conformation of a protein. Here, we examine the potential of residue-residue distance predictions to be informative of protein flexibility rather than simply static structure. Results: We used DMPfold to predict distance distributions for every residue pair in a set of proteins that showed both rigid and flexible behaviour. Residue pairs that were in contact in at least one reference structure were classified as rigid, flexible or neither. The predicted distance distribution of each residue pair was analysed for local maxima of probability indicating the most likely distance or distances between a pair of residues. We found that rigid residue pairs tended to have only a single local maximum in their predicted distance distributions while flexible residue pairs more often had multiple local maxima. These results suggest that the shape of predicted distance distributions contains information on the rigidity or flexibility of a protein and its constituent residues. Supplementary information: Supplementary data are available at Bioinformatics},
	language = {English},
	number = {1},
	journal = {BIOINFORMATICS},
	publisher = {OXFORD UNIV PRESS},
	author = {Schwarz, Dominik and Georges, Guy and Kelm, Sebastian and Shi, Jiye and Vangone, Anna and Deane, Charlotte M.},
	month = jan,
	year = {2022},
	note = {Type: Article},
	pages = {65--72},
}

@article{khan_plane_2023,
	address = {TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY},
	title = {Plane invariant segmentation of computed tomography images through weighted cross entropy optimized conditional {GANs} in compressed formats},
	issn = {0140-0118},
	doi = {10.1007/s11517-023-02846-7},
	abstract = {Computed tomography (CT) scan provides first-hand knowledge to doctors to identify an ailment. Deep neural networks help enhance image understanding through segmentation and labeling. In this work, we implement two variants of Pix2Pix generative adversarial networks (GANs) with varying complexities of generator and discriminator networks for plane invariant segmentation of CT scan images and subsequently propose an effective generative adversarial network with a suitably weighted binary cross-entropy loss function followed by image processing layer necessary for getting high-quality output segmentation. Our conditional GAN is powered by a unique set of an encoder-decoder network that coupled with the image processing layer produces enhanced segmentation. The network can be extended to the complete set of Hounsfield units and can also be implemented on smartphones. Furthermore, we also demonstrate effects on accuracy, F-1 score, and Jaccard index by using the conditional GAN networks on the spine vertebrae dataset, thus achieving an average of 86.28 \% accuracy, 90.5 \% Jaccard index score, and 89.9 \% F-1 score in predicting segmented maps for validation input images. In addition, an overall lifting of accuracy, F-1 score, and Jaccard index graph for validation images with better continuity has also been highlighted.},
	language = {English},
	journal = {MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING},
	publisher = {SPRINGER HEIDELBERG},
	author = {Khan, Usman and Yasin, Amanullah},
	month = jul,
	year = {2023},
	note = {Type: Article; Early Access},
	keywords = {Deep CNN, Generative adversarial networks, Image enhancement, Image segmentation, Image-to-image translation, Mode failure, Tomography imaging},
}

@article{castellanos_fault_2020,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {Fault identification using a chain of decision trees in an electrical submersible pump operating in a liquid-gas flow},
	volume = {184},
	issn = {0920-4105},
	doi = {10.1016/j.petrol.2019.106490},
	abstract = {The monitoring of centrifugal pumps is essential for the suitable operation of several industrial applications. The reliability of petroleum artificial lifting systems that use Electrical Submersible Pumps (ESP) depends substantially on the performance of these pumps. ESP can operate subjected to severe operating conditions such as viscous and two-phase flow. In recent years, real-time technologies based on machine learning algorithms have gained importance due to the capability to take advantage of historical data for future predictions. The present work proposes a particular assembly of Classification and Regression Trees (CART) for the detection and classification of incipient faults in a pumping system. Experiments were carried out on a ten-stage ESP to simulate, monitor and label the faults. The pump worked at 1800, 2400, 3000 and 3500 rpm, with a two-phase liquid-gas mixture. The gas-phase was nitrogen, and the liquid-phase was a heavy oil with a viscosity between 200 and 1000 cP. The proposed methodology, named here as Chain of Decision Trees, observe the system behavior based on the monitoring of the pressure, flow, torque, and temperature only. The algorithm has two steps. The first determines whether the system is in a fault state; if it is, the second determines the type of fault. The failures considered were the unexpected closure of the choke valve, the input pressure decreasing, the fluid viscosity increasing and the gas flow rate increasing. The proposed approach intends to improve the balance in classification and the interpretation of the cause of failure. The Chain of Decision Trees and the Decision Tree were compared regarding the overall accuracy and the individual fault misclassification getting a reduction in individual misclassification and better comprehensibility for the Chain of Decision Tree.},
	language = {English},
	journal = {JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING},
	publisher = {ELSEVIER},
	author = {Castellanos, Mauricio Barrios and Serpa, Alberto Luiz and Biazussi, Jorge Luiz and Verde, William Monte and Dias Arrifano Sassim, Natache do Socorro},
	month = jan,
	year = {2020},
	note = {Type: Article},
	keywords = {Machine learning, Decision tree classifier, Electrical submersible pump, Fault detection},
}

@article{yin_artificial_2024,
	address = {111 RIVER ST, HOBOKEN 07030-5774, NJ USA},
	title = {An {Artificial} {Intelligence} {Approach} for {Test}-{Free} {Identification} of {Sarcopenia}},
	volume = {15},
	issn = {2190-5991},
	doi = {10.1002/jcsm.13627},
	abstract = {BackgroundThe diagnosis of sarcopenia relies extensively on human and equipment resources and requires individuals to personally visit medical institutions. The objective of this study was to develop a test-free, self-assessable approach to identify sarcopenia by utilizing artificial intelligence techniques and representative real-world data.MethodsThis multicentre study enrolled 11 661 middle-aged and older adults from a national survey initialized in 2011. Follow-up data from the baseline cohort collected in 2013 (n = 9403) and 2015 (n = 10 356) were used for validation. Sarcopenia was retrospectively diagnosed using the Asian Working Group for Sarcopenia 2019 framework. Baseline age, sex, height, weight and 20 functional capacity (FC)-related binary indices (activities of daily living = 6, instrumental activities of daily living = 5 and other FC indices = 9) were considered as predictors. Multiple machine learning (ML) models were trained and cross-validated using 70\% of the baseline data to predict sarcopenia. The remaining 30\% of the baseline data, along with two follow-up datasets (n = 9403 and n = 10 356, respectively), were used to assess model performance.ResultsThe study included 5634 men and 6027 women (median age = 57.0 years). Sarcopenia was identified in 1288 (11.0\%) individuals. Among the 20 FC indices, the running/jogging 1 km item showed the highest predictive value for sarcopenia (AUC [95\%CI] = 0.633 [0.620-0.647]). From the various ML models assessed, a 24-variable gradient boosting classifier (GBC) model was selected. This GBC model demonstrated favourable performance in predicting sarcopenia in the holdout data (AUC [95\%CI] = 0.831 [0.808-0.853], accuracy = 0.889, recall = 0.441, precision = 0.475, F1 score = 0.458, Kappa = 0.396 and Matthews correlation coefficient = 0.396). Further model validation on the temporal scale using two longitudinal datasets also demonstrated good performance (AUC [95\%CI]: 0.833 [0.818-0.848] and 0.852 [0.840-0.865], respectively). The model's built-in feature importance ranking and the SHapley Additive exPlanations method revealed that lifting 5 kg and running/jogging 1 km were relatively important variables among the 20 FC items contributing to the model's predictive capacity, respectively. The calibration curve of the model indicated good agreement between predictions and actual observations (Hosmer and Lemeshow p = 0.501, 0.451 and 0.374 for the three test sets, respectively), and decision curve analysis supported its clinical usefulness. The model was implemented as an online web application and exported as a deployable binary file, allowing for flexible, individualized risk assessment.ConclusionsWe developed an artificial intelligence model that can assist in the identification of sarcopenia, particularly in settings lacking the necessary resources for a comprehensive diagnosis. These findings offer potential for improving decision-making and facilitating the development of novel management strategies of sarcopenia.},
	language = {English},
	number = {6},
	journal = {JOURNAL OF CACHEXIA SARCOPENIA AND MUSCLE},
	publisher = {WILEY},
	author = {Yin, Liangyu and Zhao, Jinghong},
	month = feb,
	year = {2024},
	note = {Type: Article},
	keywords = {machine learning, artificial intelligence, functional capacity, sarcopenia},
	pages = {2765--2780},
}

@article{guan_study_2025,
	address = {RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS},
	title = {A study on the identification of factors related to depression in a population with an increasing number of chronic diseases in the short term in {China} based on a health ecology model},
	volume = {368},
	issn = {0165-0327},
	doi = {10.1016/j.jad.2024.09.090},
	abstract = {Background: The rapid increase in the number of patients with chronic diseases and depression, as well as the rapid spread of their effects, have led to these two health problems gradually developing into major public health issues in China and around the world. Currently, many individuals with chronic diseases are experiencing depressive symptoms one after another. Therefore, it is imperative to conduct research on how to prevent depression in this growing population of individuals with chronic diseases in a timely manner. Methods: Based on the data of the 2015 and 2018 national follow-up surveys of the China Health and Retirement Longitudinal Study, a total of 7641 patients with short-term increase in the number of chronic diseases were selected as the study objects, and a binary logistic regression model was constructed according to the five dimensions of the health ecology model. The neural network model was used to explore the main (first two) factors affecting the increase in the number of chronic diseases in China in the short term, and the random forest and extreme value gradient lifting algorithm were used to verify them, and effective suggestions were put forward. Results: The detection rate of depression in the population with increasing number of chronic diseases from 2015 to 2018 was 42.13 \%. The model was established based on five dimensions of the health ecology model: Model 1 (Personal trait layer), Model 2 (Personal trait layer plus Behavioral feature layer), Model 3 (Personal trait layer plus Behavioral feature layer plus Living and working conditions layer), Model 4 (Personal trait layer plus Behavioral feature layer plus Living and working conditions layer plus Networking layer) and Model 5 (Personal trait layer plus Behavioral feature layer plus Living and working conditions layer plus Networking layer plus Policy environment layer).The prediction accuracy of the five models was 66.4 \%, 68.3 \%, 70.7 \%, 71.6 \% and 71.6 \%, respectively, and Model 5 showed that the P values of gender, self-rated health, night's sleep time (h), disability, life satisfaction, child satisfaction, place of residence and highest level of education were all {\textless}0.05, life satisfaction and self-rated health importance were 0.249 (100 \%) and 0.226 (90.8 \%). Conclusion: Gender, self-rated health, night sleep duration, disability, satisfaction with life, satisfaction with children, place of residence and highest level of education were the main influencing factors for the increase of depressive symptoms in the population with chronic diseases in the short term, among which life satisfaction and self-rated health have the greatest impact on depressive symptoms, and there is an interaction between the two.},
	language = {English},
	journal = {JOURNAL OF AFFECTIVE DISORDERS},
	publisher = {ELSEVIER},
	author = {Guan, Weimin and Su, Wenyu and Ge, Huaiju and Dong, Shihong and Jia, Huiyu and Liu, Yan and Yu, Qing and Qi, Yuantao and Zhang, Huiqing and Ma, Guifeng},
	month = jan,
	year = {2025},
	note = {Type: Article},
	keywords = {Chronic disease, Depressive symptoms, Health ecology model, Influencing factors},
	pages = {838--846},
}
