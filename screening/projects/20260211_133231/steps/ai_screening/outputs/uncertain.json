[
  {
    "year": "2024",
    "url": "https://doi.org/10.1109/ASE56229.2023.00099",
    "title": "HexT5: Unified Pre-Training for Stripped Binary Code Information Inference",
    "series": "ASE '23",
    "publisher": "IEEE Press",
    "pages": "774–786",
    "numpages": "13",
    "location": "Echternach, Luxembourg",
    "keywords": "reverse engineering, deep learning, binary diffing, information inference, programming language model",
    "isbn": "9798350329964",
    "doi": "10.1109/ASE56229.2023.00099",
    "booktitle": "Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering",
    "author": "Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming",
    "abstract": "Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pretraining objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.",
    "ENTRYTYPE": "inproceedings",
    "ID": "10.1109/ASE56229.2023.00099"
  },
  {
    "year": "2025",
    "url": "https://doi.org/10.1145/3719027.3765089",
    "title": "Recover Function Signature from Combined Constraints",
    "series": "CCS '25",
    "publisher": "Association for Computing Machinery",
    "pages": "3386–3400",
    "numpages": "15",
    "location": "Taipei, Taiwan",
    "keywords": "function signature recovery, probabilistic constraint, reverse engineering",
    "isbn": "9798400715259",
    "doi": "10.1145/3719027.3765089",
    "booktitle": "Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security",
    "author": "Huang, Haohui and Liu, Yue and Cheng, Yuxi and Wei, Haiyang and Liu, Jiamu and Wang, Yu and Wang, Linzhang",
    "address": "New York, NY, USA",
    "abstract": "Recovering function signatures is a cornerstone of binary program analysis, yet it remains a challenging task. Existing methods either rely on disassembly-based constraints, which struggle with cross-architecture compatibility and scalability, or adopt learning-based approaches that are resource-intensive and often inaccurate. In this paper, we present CDA, a novel decompilation-based method for recovering function signatures that combines the strengths of multiple decompilers while mitigating their limitations. The core idea behind CDA is leveraging probabilistic constraints to estimate the likelihood of each function signature recovery result produced by decompilers, guided by inference rules specifically designed to address the limitations of decompilers. Based on these probabilities, CDA selects the recovery results with the highest likelihood as the final outcomes. We extensively evaluate CDA across five tasks --- variadic function/position detection, parameter identification, return value detection, and parameter type recovery --- comparing it against state-of-the-art tools, including IDA, Ghidra, Binary Ninja, and TYGR. Experimental results show that CDA outperforms baseline tools across multiple architectures (x64, x86, AArch64, Arm, and Mips) and optimization levels (O0-O3), highlighting its robustness and reliability in diverse compilation environments.",
    "ENTRYTYPE": "inproceedings",
    "ID": "10.1145/3719027.3765089"
  },
  {
    "year": "2022",
    "title": "Pop Quiz! Can a Large Language Model Help With Reverse Engineering?",
    "note": "18 pages, 19 figures. Linked dataset: https://doi.org/10.5281/zenodo.5949075",
    "month": "feb",
    "howpublished": "\\url{https://arxiv.org/pdf/2202.01142}",
    "doi": "https://doi.org/10.48550/arXiv.2202.01142",
    "author": "Pearce, Hammond AND Tan, Benjamin AND Krishnamurthy, Prashanth AND Khorrami, Farshad AND Karri, Ramesh AND Dolan-Gavitt, Brendan",
    "abstract": "Large language models (such as OpenAI's Codex) have demonstrated impressive zero-shot multi-task capabilities in the software domain, including code explanation. In this work, we examine if this ability can be used to help with reverse engineering. Specifically, we investigate prompting Codex to identify the purpose, capabilities, and important variable names or values from code, even when the code is produced through decompilation. Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model. We present an extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks: of the 136,260 questions we posed, it answered 72,754 correctly. A key takeaway is that while promising, LLMs are not yet ready for zero-shot reverse engineering. ",
    "ENTRYTYPE": "article",
    "ID": "Pearce2022"
  },
  {
    "year": "2020",
    "web-of-science-index": "Science Citation Index Expanded (SCI-EXPANDED)",
    "web-of-science-categories": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials\nScience, Multidisciplinary; Physics, Applied",
    "volume": "10",
    "usage-count-since-2013": "3",
    "usage-count-last-180-days": "0",
    "unique-id": "WOS:000569691600001",
    "type": "Article",
    "title": "Preliminary Results on Different Text Processing Tasks Using\nEncoder-Decoder Networks and the Causal Feature Extractor",
    "times-cited": "4",
    "researcherid-numbers": "García-Mateos, Ginés/G-7779-2015",
    "research-areas": "Chemistry; Engineering; Materials Science; Physics",
    "publisher": "MDPI",
    "orcid-numbers": "García-Mateos, Ginés/0000-0003-2521-4454\nJavaloy, Aián/0000-0002-5184-4460",
    "oa": "Green Submitted, gold",
    "number-of-cited-references": "23",
    "number": "17",
    "month": "SEP",
    "language": "English",
    "keywords-plus": "DROPOUT",
    "keywords": "natural language processing; deep neural networks; causal encoder;\nbilingual translation; speech-to-text; LaTeX decompilation",
    "journal-iso": "Appl. Sci.-Basel",
    "journal": "APPLIED SCIENCES-BASEL",
    "funding-text": "This research was funded by Spanish Ministry of Science, Innovation and\nUniversities, FEDER funds, under grant RTI2018-095855-B-I00 (G.G.-M.).",
    "funding-acknowledgement": "Spanish Ministry of Science, Innovation and Universities, FEDER funds\n{[}RTI2018-095855-B-I00]",
    "eissn": "2076-3417",
    "doi": "10.3390/app10175772",
    "doc-delivery-number": "NO7TQ",
    "da": "2026-02-04",
    "cited-references": "{[}Anonymous], 2019, ARXIV190202181.\nBahdanau D., 2014, P 3 INT C LEARNING R.\nBaldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004.\nBergstra J, 2012, J MACH LEARN RES, V13, P281.\nChan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621.\nDaudaravicius V., 2019, P WORKSH EXTR STRUCT, P72.\nDeng Y, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0400-9.\nGehring J, 2017, PR MACH LEARN RES, V70.\nHochreiter S., 1997, Neural Computation, V9, P1735, DOI DOI 10.1162/NECO.1997.9.8.1735.2.\nJavaloy A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10134551.\nKameoka H, 2020, IEEE-ACM T AUDIO SPE, V28, P1849, DOI 10.1109/TASLP.2020.3001456.\nLeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6\\_19.\nLipton ZC, 2019, Queue, V17, P45, DOI {[}10.1145/3317287.3328534, 10.1145/3317287.3328534].\nNabhan AR, 2005, PROCEEDINGS OF THE 2005 IEEE INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION, P338.\nOord A. v. d., 2016, SPEECH SYNTH WORKSH.\nPapineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135.\nPascanu R., 2013, PROC 30 INT C MACHIN, P1310, DOI {[}DOI 10.48550/ARXIV.1211.5063, 10.48550/arXiv.1211.5063].\nRiezler Stefan., 2005, P ACL WORKSHOP INTRI, P57.\nSalimans T, 2016, ADV NEUR IN, V29.\nShrestha A, 2019, IEEE ACCESS, V7, P53040, DOI 10.1109/ACCESS.2019.2912200.\nSrivastava N, 2014, J MACH LEARN RES, V15, P1929.\nYang S., 2020, A survey of deep learning techniques for neural machine translation.\nZhou P, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P207, DOI 10.18653/v1/p16-2034.",
    "author-email": "adrian.javaloy@tuebingen.mpg.de\nginesgm@um.es",
    "author": "Javaloy, Adrian and Garcia-Mateos, Gines",
    "article-number": "5772",
    "affiliations": "Max Planck Society; University of Murcia",
    "affiliation": "García-Mateos, G (Corresponding Author), Univ Murcia, Dept Comp Sci \\& Syst, Murcia 30100, Spain.\nJavaloy, Adrian, Max Planck Inst Intelligent Syst, D-72076 Tubingen, Germany.\nGarcia-Mateos, Gines, Univ Murcia, Dept Comp Sci \\& Syst, Murcia 30100, Spain.",
    "address": "ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND",
    "abstract": "Deep learning methods are gaining popularity in different application\ndomains, and especially in natural language processing. It is commonly\nbelieved that using a large enough dataset and an adequate network\narchitecture, almost any processing problem can be solved. A frequent\nand widely used typology is the encoder-decoder architecture, where the\ninput data is transformed into an intermediate code by means of an\nencoder, and then a decoder takes this code to produce its output.\nDifferent types of networks can be used in the encoder and the decoder,\ndepending on the problem of interest, such as convolutional neural\nnetworks (CNN) or long-short term memories (LSTM). This paper uses for\nthe encoder a method recently proposed, called Causal Feature Extractor\n(CFE). It is based on causal convolutions (i.e., convolutions that\ndepend only on one direction of the input), dilatation (i.e., increasing\nthe aperture size of the convolutions) and bidirectionality (i.e.,\nindependent networks in both directions). Some preliminary results are\npresented on three different tasks and compared with state-of-the-art\nmethods: bilingual translation, LaTeX decompilation and audio\ntranscription. The proposed method achieves promising results, showing\nits ubiquity to work with text, audio and images. Moreover, it has a\nshorter training time, requiring less time per iteration, and a good use\nof the attention mechanisms based on attention matrices.",
    "ENTRYTYPE": "article",
    "ID": "WOS:000569691600001"
  },
  {
    "year": "2025",
    "web-of-science-index": "Science Citation Index Expanded (SCI-EXPANDED)",
    "web-of-science-categories": "Computer Science, Artificial Intelligence; Computer Science, Software\nEngineering; Engineering, Electrical \\& Electronic",
    "volume": "35",
    "usage-count-since-2013": "8",
    "usage-count-last-180-days": "8",
    "unique-id": "WOS:001539888300001",
    "type": "Article",
    "title": "Modernizing 90s Era Software to a New Language and Environment Using\nLLMs - An Empirical Investigation",
    "times-cited": "0",
    "researcherid-numbers": "ašković, ažen/U-1113-2018\nDraskovic, Drazen/U-1113-2018",
    "research-areas": "Computer Science; Engineering",
    "publisher": "WORLD SCIENTIFIC PUBL CO PTE LTD",
    "pages": "1099-1119",
    "orcid-numbers": "Bojic, agan/0009-0001-3025-5886\našković, ažen/0000-0003-2564-4526",
    "number-of-cited-references": "23",
    "number": "08",
    "month": "AUG",
    "language": "English",
    "keywords": "Decompilation; software translation; machine learning",
    "journal-iso": "Int. J. Softw. Eng. Knowl. Eng.",
    "journal": "INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING",
    "issn": "0218-1940",
    "funding-text": "This research was supported by the Science Fund of the Republic of\nSerbia, Grant No 11113, Software for Text Offences Prevention in\nSerbian: Al-driven Hate Speech Detection - STOP.",
    "funding-acknowledgement": "Science Fund of the Republic of Serbia {[}11113]; Software for Text\nOffences Prevention in Serbian: Al-driven Hate Speech Detection - STOP",
    "eissn": "1793-6403",
    "earlyaccessdate": "JUL 2025",
    "doi": "10.1142/S021819402550024X",
    "doc-delivery-number": "6DL6W",
    "da": "2026-02-04",
    "cited-references": "{[}Anonymous], 2021, 2911942021EN ISOIECI.\n{[}Anonymous], 2019, ROCKET UNIVERSE DATA.\narchive.org, INTERNET ARCHIVE SOF.\nBoehm B., 2005, CROSSTALK J DEFENSE, V18, P20.\nBojic DM, 2016, INT J SOFTW ENG KNOW, V26, P953, DOI 10.1142/S0218194016500327.\nClaburn T., REGISTER.\ngithub.com, C2RUST TRANSPILER MI.\nIgnjatovic M., 2015, TELFOR J, V7.\ninformatik.hu-berlin, 2002, XCTL PROJEKT SOFTWAR.\nJang WS, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12189310.\nKrupalija E., 2022, 24 IEEE ACIS INT WIN, P162, DOI {[}10.1109/SNPD54884.2022.10051799, DOI 10.1109/SNPD54884.2022.10051799].\nLachaux M-A, 2020, ARXIV.\nMyers GJ, 2012, ART OF SOFTWARE TESTING, 3RD EDITION, P1.\nPARNAS DL, 1994, PROC INT CONF SOFTW, P279, DOI 10.1109/ICSE.1994.296790.\nRazavian M., 2013, THESIS VRIJE U AMSTE.\nRohleder R, 2019, SPRO'19: PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SOFTWARE PROTECTION, P77, DOI 10.1145/3338503.3357725.\nRoziere B., 2021, P INT C LEARN REPR.\nRoziere Baptiste, 2020, Advances in Neural Information Processing Systems, V33.\nSzafraniec M., 2022, P 11 INT C LEARN REP.\nVelasevic D., 1997, INFO SCI, V5, P4.\nWang YN, 2018, ADV NEUR IN, V31.\nZhen Yang, 2024, Proceedings of the ACM on Software Engineering, V1, DOI {[}10.1145/3660778, 10.1145/3660778].\nZhong H., 2010, P 32 ACMIEEE INT C S, P195, DOI {[}DOI 10.1145/1806799.1806831, 10.1145/1806799.1806831].",
    "author-email": "bojic@etf.bg.ac.rs\ndrazen.draskovic@etf.bg.ac.rs",
    "author": "Bojic, Dragan and Draskovic, Drazen",
    "affiliations": "University of Belgrade",
    "affiliation": "Draskovic, D (Corresponding Author), Univ Belgrade, Sch Elect Engn, Bulevar Kralja Aleksandra 73, Belgrade 11120, Serbia.\nBojic, Dragan; Draskovic, Drazen, Univ Belgrade, Sch Elect Engn, Bulevar Kralja Aleksandra 73, Belgrade 11120, Serbia.",
    "address": "5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE",
    "abstract": "Legacy software, particularly from the 1990s, often becomes obsolete due\nto aging hardware and outdated software environments. Traditionally,\nsoftware modernization required extensive manual effort, involving\nreverse engineering, code rewriting, and re-architecting. However,\nadvancements in large language models (LLMs) have introduced new\npossibilities for automating software translation and modernization.\nThis paper explores the feasibility of using LLMs for modernizing\n90s-era Windows applications, specifically migrating legacy C and C++\ncode to Python. Our methodology includes decompilation, source code\nanalysis, automated translation using ChatGPT, and user interface\nreconstruction. We empirically evaluate three software projects by\nanalyzing LLM-based translation accuracy across different code\nstructures, including algorithmic logic, file handling, and graphical\ninterfaces. Results indicate that while LLMs achieve high translation\naccuracy (-88\\%) for structured code, challenges persist in handling\ndecompiled code and user interface generation. The study provides\ninsights into the effectiveness and limitations of LLMs in real-world\nsoftware renovation, offering guidelines for leveraging machine learning\nin legacy system modernization. These findings contribute to both\nacademic research and practical applications, suggesting a pathway for\ncost-effective and scalable legacy software migration.",
    "ENTRYTYPE": "article",
    "ID": "WOS:001539888300001"
  },
  {
    "year": "2025",
    "title": "Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges",
    "note": "",
    "month": "jul",
    "howpublished": "\\url{https://arxiv.org/pdf/2507.18792}",
    "doi": "https://doi.org/10.48550/arXiv.2507.18792",
    "author": "Zhou, Zixu",
    "abstract": "Decompiling Rust binaries is challenging due to the language's rich type system, aggressive compiler optimizations, and widespread use of high-level abstractions. In this work, we conduct a benchmark-driven evaluation of decompilation quality across core Rust features and compiler build modes. Our automated scoring framework shows that generic types, trait methods, and error handling constructs significantly reduce decompilation quality, especially in release builds. Through representative case studies, we analyze how specific language constructs affect control flow, variable naming, and type information recovery. Our findings provide actionable insights for tool developers and highlight the need for Rust-aware decompilation strategies. ",
    "ENTRYTYPE": "article",
    "ID": "Zhou2025"
  }
]