@inproceedings{10.1109/ASE56229.2023.00099,
  abstract = {Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pretraining objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
  author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
  booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
  doi = {10.1109/ASE56229.2023.00099},
  isbn = {9798350329964},
  keywords = {reverse engineering, deep learning, binary diffing, information inference, programming language model},
  location = {Echternach, Luxembourg},
  numpages = {13},
  pages = {774–786},
  publisher = {IEEE Press},
  series = {ASE '23},
  title = {HexT5: Unified Pre-Training for Stripped Binary Code Information Inference},
  url = {https://doi.org/10.1109/ASE56229.2023.00099},
  year = {2024}
}

@inproceedings{10.1145/3719027.3765089,
  abstract = {Recovering function signatures is a cornerstone of binary program analysis, yet it remains a challenging task. Existing methods either rely on disassembly-based constraints, which struggle with cross-architecture compatibility and scalability, or adopt learning-based approaches that are resource-intensive and often inaccurate. In this paper, we present CDA, a novel decompilation-based method for recovering function signatures that combines the strengths of multiple decompilers while mitigating their limitations. The core idea behind CDA is leveraging probabilistic constraints to estimate the likelihood of each function signature recovery result produced by decompilers, guided by inference rules specifically designed to address the limitations of decompilers. Based on these probabilities, CDA selects the recovery results with the highest likelihood as the final outcomes. We extensively evaluate CDA across five tasks --- variadic function/position detection, parameter identification, return value detection, and parameter type recovery --- comparing it against state-of-the-art tools, including IDA, Ghidra, Binary Ninja, and TYGR. Experimental results show that CDA outperforms baseline tools across multiple architectures (x64, x86, AArch64, Arm, and Mips) and optimization levels (O0-O3), highlighting its robustness and reliability in diverse compilation environments.},
  address = {New York, NY, USA},
  author = {Huang, Haohui and Liu, Yue and Cheng, Yuxi and Wei, Haiyang and Liu, Jiamu and Wang, Yu and Wang, Linzhang},
  booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
  doi = {10.1145/3719027.3765089},
  isbn = {9798400715259},
  keywords = {function signature recovery, probabilistic constraint, reverse engineering},
  location = {Taipei, Taiwan},
  numpages = {15},
  pages = {3386–3400},
  publisher = {Association for Computing Machinery},
  series = {CCS '25},
  title = {Recover Function Signature from Combined Constraints},
  url = {https://doi.org/10.1145/3719027.3765089},
  year = {2025}
}

@article{Pearce2022,
  abstract = {Large language models (such as OpenAI's Codex) have demonstrated impressive zero-shot multi-task capabilities in the software domain, including code explanation. In this work, we examine if this ability can be used to help with reverse engineering. Specifically, we investigate prompting Codex to identify the purpose, capabilities, and important variable names or values from code, even when the code is produced through decompilation. Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model. We present an extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks: of the 136,260 questions we posed, it answered 72,754 correctly. A key takeaway is that while promising, LLMs are not yet ready for zero-shot reverse engineering. },
  author = {Pearce, Hammond AND Tan, Benjamin AND Krishnamurthy, Prashanth AND Khorrami, Farshad AND Karri, Ramesh AND Dolan-Gavitt, Brendan},
  doi = {https://doi.org/10.48550/arXiv.2202.01142},
  howpublished = {\url{https://arxiv.org/pdf/2202.01142}},
  month = {feb},
  note = {18 pages, 19 figures. Linked dataset: https://doi.org/10.5281/zenodo.5949075},
  title = {Pop Quiz! Can a Large Language Model Help With Reverse Engineering?},
  year = {2022}
}

@article{WOS:000569691600001,
  abstract = {Deep learning methods are gaining popularity in different application
domains, and especially in natural language processing. It is commonly
believed that using a large enough dataset and an adequate network
architecture, almost any processing problem can be solved. A frequent
and widely used typology is the encoder-decoder architecture, where the
input data is transformed into an intermediate code by means of an
encoder, and then a decoder takes this code to produce its output.
Different types of networks can be used in the encoder and the decoder,
depending on the problem of interest, such as convolutional neural
networks (CNN) or long-short term memories (LSTM). This paper uses for
the encoder a method recently proposed, called Causal Feature Extractor
(CFE). It is based on causal convolutions (i.e., convolutions that
depend only on one direction of the input), dilatation (i.e., increasing
the aperture size of the convolutions) and bidirectionality (i.e.,
independent networks in both directions). Some preliminary results are
presented on three different tasks and compared with state-of-the-art
methods: bilingual translation, LaTeX decompilation and audio
transcription. The proposed method achieves promising results, showing
its ubiquity to work with text, audio and images. Moreover, it has a
shorter training time, requiring less time per iteration, and a good use
of the attention mechanisms based on attention matrices.},
  address = {ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND},
  affiliation = {García-Mateos, G (Corresponding Author), Univ Murcia, Dept Comp Sci \& Syst, Murcia 30100, Spain.
Javaloy, Adrian, Max Planck Inst Intelligent Syst, D-72076 Tubingen, Germany.
Garcia-Mateos, Gines, Univ Murcia, Dept Comp Sci \& Syst, Murcia 30100, Spain.},
  affiliations = {Max Planck Society; University of Murcia},
  article-number = {5772},
  author = {Javaloy, Adrian and Garcia-Mateos, Gines},
  author-email = {adrian.javaloy@tuebingen.mpg.de
ginesgm@um.es},
  cited-references = {{[}Anonymous], 2019, ARXIV190202181.
Bahdanau D., 2014, P 3 INT C LEARNING R.
Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004.
Bergstra J, 2012, J MACH LEARN RES, V13, P281.
Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621.
Daudaravicius V., 2019, P WORKSH EXTR STRUCT, P72.
Deng Y, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0400-9.
Gehring J, 2017, PR MACH LEARN RES, V70.
Hochreiter S., 1997, Neural Computation, V9, P1735, DOI DOI 10.1162/NECO.1997.9.8.1735.2.
Javaloy A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10134551.
Kameoka H, 2020, IEEE-ACM T AUDIO SPE, V28, P1849, DOI 10.1109/TASLP.2020.3001456.
LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6\_19.
Lipton ZC, 2019, Queue, V17, P45, DOI {[}10.1145/3317287.3328534, 10.1145/3317287.3328534].
Nabhan AR, 2005, PROCEEDINGS OF THE 2005 IEEE INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION, P338.
Oord A. v. d., 2016, SPEECH SYNTH WORKSH.
Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135.
Pascanu R., 2013, PROC 30 INT C MACHIN, P1310, DOI {[}DOI 10.48550/ARXIV.1211.5063, 10.48550/arXiv.1211.5063].
Riezler Stefan., 2005, P ACL WORKSHOP INTRI, P57.
Salimans T, 2016, ADV NEUR IN, V29.
Shrestha A, 2019, IEEE ACCESS, V7, P53040, DOI 10.1109/ACCESS.2019.2912200.
Srivastava N, 2014, J MACH LEARN RES, V15, P1929.
Yang S., 2020, A survey of deep learning techniques for neural machine translation.
Zhou P, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2016), VOL 2, P207, DOI 10.18653/v1/p16-2034.},
  da = {2026-02-04},
  doc-delivery-number = {NO7TQ},
  doi = {10.3390/app10175772},
  eissn = {2076-3417},
  funding-acknowledgement = {Spanish Ministry of Science, Innovation and Universities, FEDER funds
{[}RTI2018-095855-B-I00]},
  funding-text = {This research was funded by Spanish Ministry of Science, Innovation and
Universities, FEDER funds, under grant RTI2018-095855-B-I00 (G.G.-M.).},
  journal = {APPLIED SCIENCES-BASEL},
  journal-iso = {Appl. Sci.-Basel},
  keywords = {natural language processing; deep neural networks; causal encoder;
bilingual translation; speech-to-text; LaTeX decompilation},
  keywords-plus = {DROPOUT},
  language = {English},
  month = {SEP},
  number = {17},
  number-of-cited-references = {23},
  oa = {Green Submitted, gold},
  orcid-numbers = {García-Mateos, Ginés/0000-0003-2521-4454
Javaloy, Aián/0000-0002-5184-4460},
  publisher = {MDPI},
  research-areas = {Chemistry; Engineering; Materials Science; Physics},
  researcherid-numbers = {García-Mateos, Ginés/G-7779-2015},
  times-cited = {4},
  title = {Preliminary Results on Different Text Processing Tasks Using
Encoder-Decoder Networks and the Causal Feature Extractor},
  type = {Article},
  unique-id = {WOS:000569691600001},
  usage-count-last-180-days = {0},
  usage-count-since-2013 = {3},
  volume = {10},
  web-of-science-categories = {Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
Science, Multidisciplinary; Physics, Applied},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
  year = {2020}
}

@article{WOS:001539888300001,
  abstract = {Legacy software, particularly from the 1990s, often becomes obsolete due
to aging hardware and outdated software environments. Traditionally,
software modernization required extensive manual effort, involving
reverse engineering, code rewriting, and re-architecting. However,
advancements in large language models (LLMs) have introduced new
possibilities for automating software translation and modernization.
This paper explores the feasibility of using LLMs for modernizing
90s-era Windows applications, specifically migrating legacy C and C++
code to Python. Our methodology includes decompilation, source code
analysis, automated translation using ChatGPT, and user interface
reconstruction. We empirically evaluate three software projects by
analyzing LLM-based translation accuracy across different code
structures, including algorithmic logic, file handling, and graphical
interfaces. Results indicate that while LLMs achieve high translation
accuracy (-88\%) for structured code, challenges persist in handling
decompiled code and user interface generation. The study provides
insights into the effectiveness and limitations of LLMs in real-world
software renovation, offering guidelines for leveraging machine learning
in legacy system modernization. These findings contribute to both
academic research and practical applications, suggesting a pathway for
cost-effective and scalable legacy software migration.},
  address = {5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE},
  affiliation = {Draskovic, D (Corresponding Author), Univ Belgrade, Sch Elect Engn, Bulevar Kralja Aleksandra 73, Belgrade 11120, Serbia.
Bojic, Dragan; Draskovic, Drazen, Univ Belgrade, Sch Elect Engn, Bulevar Kralja Aleksandra 73, Belgrade 11120, Serbia.},
  affiliations = {University of Belgrade},
  author = {Bojic, Dragan and Draskovic, Drazen},
  author-email = {bojic@etf.bg.ac.rs
drazen.draskovic@etf.bg.ac.rs},
  cited-references = {{[}Anonymous], 2021, 2911942021EN ISOIECI.
{[}Anonymous], 2019, ROCKET UNIVERSE DATA.
archive.org, INTERNET ARCHIVE SOF.
Boehm B., 2005, CROSSTALK J DEFENSE, V18, P20.
Bojic DM, 2016, INT J SOFTW ENG KNOW, V26, P953, DOI 10.1142/S0218194016500327.
Claburn T., REGISTER.
github.com, C2RUST TRANSPILER MI.
Ignjatovic M., 2015, TELFOR J, V7.
informatik.hu-berlin, 2002, XCTL PROJEKT SOFTWAR.
Jang WS, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12189310.
Krupalija E., 2022, 24 IEEE ACIS INT WIN, P162, DOI {[}10.1109/SNPD54884.2022.10051799, DOI 10.1109/SNPD54884.2022.10051799].
Lachaux M-A, 2020, ARXIV.
Myers GJ, 2012, ART OF SOFTWARE TESTING, 3RD EDITION, P1.
PARNAS DL, 1994, PROC INT CONF SOFTW, P279, DOI 10.1109/ICSE.1994.296790.
Razavian M., 2013, THESIS VRIJE U AMSTE.
Rohleder R, 2019, SPRO'19: PROCEEDINGS OF THE 3RD ACM WORKSHOP ON SOFTWARE PROTECTION, P77, DOI 10.1145/3338503.3357725.
Roziere B., 2021, P INT C LEARN REPR.
Roziere Baptiste, 2020, Advances in Neural Information Processing Systems, V33.
Szafraniec M., 2022, P 11 INT C LEARN REP.
Velasevic D., 1997, INFO SCI, V5, P4.
Wang YN, 2018, ADV NEUR IN, V31.
Zhen Yang, 2024, Proceedings of the ACM on Software Engineering, V1, DOI {[}10.1145/3660778, 10.1145/3660778].
Zhong H., 2010, P 32 ACMIEEE INT C S, P195, DOI {[}DOI 10.1145/1806799.1806831, 10.1145/1806799.1806831].},
  da = {2026-02-04},
  doc-delivery-number = {6DL6W},
  doi = {10.1142/S021819402550024X},
  earlyaccessdate = {JUL 2025},
  eissn = {1793-6403},
  funding-acknowledgement = {Science Fund of the Republic of Serbia {[}11113]; Software for Text
Offences Prevention in Serbian: Al-driven Hate Speech Detection - STOP},
  funding-text = {This research was supported by the Science Fund of the Republic of
Serbia, Grant No 11113, Software for Text Offences Prevention in
Serbian: Al-driven Hate Speech Detection - STOP.},
  issn = {0218-1940},
  journal = {INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING},
  journal-iso = {Int. J. Softw. Eng. Knowl. Eng.},
  keywords = {Decompilation; software translation; machine learning},
  language = {English},
  month = {AUG},
  number = {08},
  number-of-cited-references = {23},
  orcid-numbers = {Bojic, agan/0009-0001-3025-5886
ašković, ažen/0000-0003-2564-4526},
  pages = {1099-1119},
  publisher = {WORLD SCIENTIFIC PUBL CO PTE LTD},
  research-areas = {Computer Science; Engineering},
  researcherid-numbers = {ašković, ažen/U-1113-2018
Draskovic, Drazen/U-1113-2018},
  times-cited = {0},
  title = {Modernizing 90s Era Software to a New Language and Environment Using
LLMs - An Empirical Investigation},
  type = {Article},
  unique-id = {WOS:001539888300001},
  usage-count-last-180-days = {8},
  usage-count-since-2013 = {8},
  volume = {35},
  web-of-science-categories = {Computer Science, Artificial Intelligence; Computer Science, Software
Engineering; Engineering, Electrical \& Electronic},
  web-of-science-index = {Science Citation Index Expanded (SCI-EXPANDED)},
  year = {2025}
}

@article{Zhou2025,
  abstract = {Decompiling Rust binaries is challenging due to the language's rich type system, aggressive compiler optimizations, and widespread use of high-level abstractions. In this work, we conduct a benchmark-driven evaluation of decompilation quality across core Rust features and compiler build modes. Our automated scoring framework shows that generic types, trait methods, and error handling constructs significantly reduce decompilation quality, especially in release builds. Through representative case studies, we analyze how specific language constructs affect control flow, variable naming, and type information recovery. Our findings provide actionable insights for tool developers and highlight the need for Rust-aware decompilation strategies. },
  author = {Zhou, Zixu},
  doi = {https://doi.org/10.48550/arXiv.2507.18792},
  howpublished = {\url{https://arxiv.org/pdf/2507.18792}},
  month = {jul},
  note = {},
  title = {Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges},
  year = {2025}
}
