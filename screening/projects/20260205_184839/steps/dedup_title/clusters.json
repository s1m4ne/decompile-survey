{
  "clusters": [
    {
      "id": "cluster-8",
      "size": 3,
      "representative_id": "10.1145/3196321.3196330",
      "representative_title": "Meaningful variable names for decompiled code: a machine translation approach",
      "average_similarity": 0.9775051124744376,
      "members": [
        {
          "id": "10.1145/3106237.3121274",
          "title": "Suggesting meaningful variable names for decompiled code: a machine translation approach",
          "authors": "Jaffe, Alan",
          "year": "2017",
          "abstract": "Decompiled code lacks meaningful variable names. We used statistical machine translation to suggest variable names that are natural given the context. This technique has previously been successfully applied to obfuscated JavaScript code, but decompiled C code poses unique challenges in constructing an aligned corpus and selecting the best translation from among several candidates.",
          "similarity": 0.9325153374233128,
          "action": "keep"
        },
        {
          "id": "10.1145/3196321.3196330",
          "title": "Meaningful variable names for decompiled code: a machine translation approach",
          "authors": "Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan",
          "year": "2018",
          "abstract": "When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2% of the time, which represents a lower bound for performance.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "8973072",
          "title": "Meaningful Variable Names for Decompiled Code: A Machine Translation Approach",
          "authors": "Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan",
          "year": "2018",
          "abstract": "When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2% of the time, which represents a lower bound for performance.",
          "similarity": 1,
          "action": "remove"
        }
      ],
      "reviewed": false
    },
    {
      "id": "cluster-184",
      "size": 2,
      "representative_id": "10911787",
      "representative_title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion",
      "average_similarity": 1,
      "members": [
        {
          "id": "10911787",
          "title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion",
          "authors": "Zhang, Lixia and Liu, Tianxu and Shen, Kaihui and Chen, Cheng",
          "year": "2024",
          "abstract": "With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness.This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware.In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability.The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Zhang2024",
          "title": "A Novel Approach to Malicious Code Detection Using CNN-BiLSTM and Feature Fusion",
          "authors": "Zhang, Lixia AND Liu, Tianxu AND Shen, Kaihui AND Chen, Cheng",
          "year": "2024",
          "abstract": "With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness. This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware. In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability. The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-237",
      "size": 2,
      "representative_id": "8987703",
      "representative_title": "Applications of Graph Integration to Function Comparison and Malware Classification",
      "average_similarity": 1,
      "members": [
        {
          "id": "8987703",
          "title": "Applications of Graph Integration to Function Comparison and Malware Classification",
          "authors": "Slawinski, Michael and Wortman, Andy",
          "year": "2019",
          "abstract": "We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled. NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms. 11Code available at https://github.com/gtownrocks/grafuple",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Slawinski2019",
          "title": "Applications of Graph Integration to Function Comparison and Malware Classification",
          "authors": "Slawinski, A., Michael AND Wortman, Andy",
          "year": "2019",
          "abstract": "We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled .NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-310",
      "size": 2,
      "representative_id": "WOS:001639031400025",
      "representative_title": "Augmenting Smart Contract Decompiler Output Through Fine-Grained\nDependency Analysis and LLM-Facilitated Semantic Recovery",
      "average_similarity": 1,
      "members": [
        {
          "id": "Liao2025",
          "title": "Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery",
          "authors": "Liao, Zeqin AND Nan, Yuhong AND Gao, Zixu AND Liang, Henglong AND Hao, Sicheng AND Reng, Peifan AND Zheng, Zibin",
          "year": "2025",
          "abstract": "Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \\system\\{\\} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes. ",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:001639031400025",
          "title": "Augmenting Smart Contract Decompiler Output Through Fine-Grained\nDependency Analysis and LLM-Facilitated Semantic Recovery",
          "authors": "Liao, Zeqin and Nan, Yuhong and Gao, Zixu and Liang, Henglong and Hao,\nSicheng and Ren, Peifan and Zheng, Zibin",
          "year": "2025",
          "abstract": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program\ncomprehension and vulnerability detection. However, current Solidity\nsmart contract decompilers face significant limitations in\nreconstructing the original source code. In particular, the bottleneck\nof SOTA decompilers lies in inaccurate function identification,\nincorrect variable type recovery, and missing contract attributes. These\ndeficiencies hinder downstream tasks and understanding of the program\nlogic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis\n(SA) and large language models (LLM). SmartHalo leverages the\ncomplementary strengths of SA's accuracy in control and data flow\nanalysis and LLM's capability in semantic prediction. More specifically,\nSmartHalo constructs a new data structure - Dependency Graph (DG), to\nextract semantic dependencies via static analysis. Then, it takes DG to\ncreate prompts for LLM optimization. Finally, the correctness of LLM\noutputs is validated through symbolic execution and formal verification.\nEvaluation on a dataset consisting of 465 randomly selected smart\ncontract functions shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g.,\nGigahorse). Notably, integrating GPT-4o mini with SmartHalo further\nenhances its performance, achieving a precision of 91.32\\% and a recall\nof 87.38\\% for function boundaries, a precision of 90.40\\% and a recall\nof 88.82\\% for variable types, and a precision of 80.66\\% and a recall\nof 91.78\\% for contract attributes.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-325",
      "size": 2,
      "representative_id": "WOS:001488140900040",
      "representative_title": "Automatically Mitigating Vulnerabilities in Binary Programs via\nPartially Recompilable Decompilation",
      "average_similarity": 1,
      "members": [
        {
          "id": "Reiter2023",
          "title": "Automatically Mitigating Vulnerabilities in Binary Programs via Partially Recompilable Decompilation",
          "authors": "Reiter, Pemma AND Tay, Jun, Hui AND Weimer, Westley AND Doupé, Adam AND Wang, Ruoyu AND Forrest, Stephanie",
          "year": "2023",
          "abstract": "Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD). PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically work on an entire binary, our approach succeeds because it is limited to a few functions, like those identified by our binary fault localization. We evaluate these assumptions and find that, without any grammar or compilation restrictions, 70-89% of individual functions are successfully decompiled and recompiled with sufficient type recovery. In comparison, only 1.7% of the full C-binaries succeed. When decompilation succeeds, PRD produces test-equivalent binaries 92.9% of the time. In addition, we evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, performs as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with these same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, our evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries. ",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:001488140900040",
          "title": "Automatically Mitigating Vulnerabilities in Binary Programs via\nPartially Recompilable Decompilation",
          "authors": "Reiter, Pemma and Tay, Hui Jun and Weimer, Westley and Doupe, Adam and\nWang, Ruoyu and Forrest, Stephanie",
          "year": "2025",
          "abstract": "Vulnerabilities are challenging to locate and repair, especially when\nsource code is unavailable and binary patching is required. Manual\nmethods are time-consuming, require significant expertise, and do not\nscale to the rate at which new vulnerabilities are discovered. Automated\nmethods are an attractive alternative, and we propose Partially\nRecompilable Decompilation (PRD) to help automate the process. PRD lifts\nsuspect binary functions to source, available for analysis, revision, or\nreview, and creates a patched binary using source- and binary-level\ntechniques. Although decompilation and recompilation do not typically\nsucceed on an entire binary, our approach does because it is limited to\na few functions, such as those identified by our binary fault\nlocalization. We evaluate the assumptions underlying our approach and\nfind that, without any grammar or compilation restrictions, up to 79\\%\nof individual functions are successfully decompiled and recompiled. In\ncomparison, only 1.7\\% of the full C-binaries succeed. When\nrecompilation succeeds, PRD produces test-equivalent binaries 93.0\\% of\nthe time. We evaluate PRD in two contexts: a fully automated process\nincorporating source-level Automated Program Repair (APR) methods; and\nhuman-edited source-level repairs. When evaluated on DARPA Cyber Grand\nChallenge (CGC) binaries, we find that PRD-enabled APR tools, operating\nonly on binaries, perform as well as, and sometimes better than\nfull-source tools, collectively mitigating 85 of the 148 scenarios, a\nsuccess rate consistent with the same tools operating with access to the\nentire source code. PRD achieves similar success rates as the winning\nCGC entries, sometimes finding higher-quality mitigations than those\nproduced by top CGC teams. For generality, the evaluation includes two\nindependently developed APR tools and C++, Rode0day, and real-world\nbinaries.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-67",
      "size": 2,
      "representative_id": "Cao2023",
      "representative_title": "Boosting Neural Networks to Decompile Optimized Binaries",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3564625.3567998",
          "title": "Boosting Neural Networks to Decompile Optimized Binaries",
          "authors": "Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei",
          "year": "2022",
          "abstract": "Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "Cao2023",
          "title": "Boosting Neural Networks to Decompile Optimized Binaries",
          "authors": "Cao, Ying AND Liang, Ruigang AND Chen, Kai AND Hu, Peiwei",
          "year": "2023",
          "abstract": "Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks. ",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-123",
      "size": 2,
      "representative_id": "10.1145/3722041.3723097",
      "representative_title": "Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3722041.3723097",
          "title": "Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?",
          "authors": "Cotroneo, Domenico and Grasso, Francesco C. and Natella, Roberto and Orbinato, Vittorio",
          "year": "2025",
          "abstract": "Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Cotroneo2025",
          "title": "Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?",
          "authors": "Cotroneo, D. AND Grasso, C., F. AND Natella, R. AND Orbinato, V.",
          "year": "2025",
          "abstract": "Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-438",
      "size": 2,
      "representative_id": "WOS:001470367300014",
      "representative_title": "CF-GKAT: Efficient Validation of Control-Flow Transformations",
      "average_similarity": 1,
      "members": [
        {
          "id": "WOS:001470367300014",
          "title": "CF-GKAT: Efficient Validation of Control-Flow Transformations",
          "authors": "Zhang, Cheng and Kappe, Tobias and Narvaez, David E. and Naus, Nico",
          "year": "2025",
          "abstract": "Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete\nframework to reason about trace equivalence between simple imperative\nprograms. However, there are still several notable limitations. First,\nGKAT is completely agnostic with respect to the meaning of primitives,\nto keep equivalence decidable. Second, GKAT excludes non-local control\nflow such as goto, break, and return. To overcome these limitations, we\nintroduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning\nabout programs that include non-local control flow as well as hardcoded\nvalues. CF-GKAT is able to soundly and completely verify trace\nequivalence of a larger class of programs, while preserving the\nnearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the\nverification of control-flow manipulating procedures, such as\ndecompilation and goto-elimination. To demonstrate CF-GKAT's abilities,\nwe validated the output of several highly non-trivial program\ntransformations, such as Erosa and Hendren's goto-elimination procedure\nand the output of Ghidra decompiler. CF-GKAT opens up the application of\nKleene Algebra to a wider set of challenges, and provides an important\nverification tool that can be applied to the field of decompilation and\ncontrol-flow transformation.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Zhang2025",
          "title": "CF-GKAT: Efficient Validation of Control-Flow Transformations",
          "authors": "Zhang, Cheng AND Kappé, Tobias AND Narváez, E., David AND Naus, Nico",
          "year": "2025",
          "abstract": "Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT's abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren's goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-17",
      "size": 2,
      "representative_id": "10.1145/3321705.3329833",
      "representative_title": "DeClassifier: Class-Inheritance Inference Engine for Optimized C++ Binaries",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3321705.3329833",
          "title": "DeClassifier: Class-Inheritance Inference Engine for Optimized C++ Binaries",
          "authors": "Erinfolami, Rukayat Ayomide and Prakash, Aravind",
          "year": "2019",
          "abstract": "Recovering class inheritance from C++ binaries has several security benefits including in solving problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining---or worse exclusion---due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5% and 71.4% true positive directed edges in the class hierarchy tree (CHT) under O0 and O2 optimizations respectively, (2) a combination of constructor-destructor (ctor-dtor) analysis provides a substantial improvement in inheritance inference than constructor-only (ctor-only) analysis.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Erinfolami2019",
          "title": "DeClassifier: Class-Inheritance Inference Engine for Optimized C++ Binaries",
          "authors": "Erinfolami, Ayomide, Rukayat AND Prakash, Aravind",
          "year": "2019",
          "abstract": "Recovering class inheritance from C++ binaries has several security benefits including problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining--or worse exclusion--due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5% and 71.4% true positive directed edges in the class hierarchy tree under O0 and O2 optimizations respectively, (2) a combination of ctor+dtor analysis provides much better inference than ctor only analysis. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-25",
      "size": 2,
      "representative_id": "10.1145/3372297.3417251",
      "representative_title": "Devil is Virtual: Reversing Virtual Inheritance in C++ Binaries",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3372297.3417251",
          "title": "Devil is Virtual: Reversing Virtual Inheritance in C++ Binaries",
          "authors": "Erinfolami, Rukayat Ayomide and Prakash, Aravind",
          "year": "2020",
          "abstract": "The complexities that arise from the implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious of virtual inheritance. The conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10% on Linux and 12.5% on Windows) of real-world C++ programs including Mysql and Libstdc++. (2) We show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These falses either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95% and 95.5% (GCC -O0) and a minimum of 77.5% and 73.8% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Erinfolami2020",
          "title": "Devil is Virtual: Reversing Virtual Inheritance in C++ Binaries",
          "authors": "Erinfolami, Ayomide, Rukayat AND Prakash, Aravind",
          "year": "2020",
          "abstract": "Complexities that arise from implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious to virtual inheritance. Conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10\\% on Linux and 12.5\\% on Windows) of real-world C++ programs including Mysql and libstdc++. (2) we show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These false positves and negatives either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95\\% and 95.5\\% (GCC -O0) and a minimum of 77.5\\% and 73.8\\% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-1",
      "size": 2,
      "representative_id": "10.1109/ASE.2019.00064",
      "representative_title": "DIRE: a neural approach to decompiled identifier naming",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1109/ASE.2019.00064",
          "title": "DIRE: a neural approach to decompiled identifier naming",
          "authors": "Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan",
          "year": "2020",
          "abstract": "The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Lacomis2019",
          "title": "DIRE: A Neural Approach to Decompiled Identifier Naming",
          "authors": "Lacomis, Jeremy AND Yin, Pengcheng AND Schwartz, J., Edward AND Allamanis, Miltiadis AND Goues, Le, Claire AND Neubig, Graham AND Vasilescu, Bogdan",
          "year": "2019",
          "abstract": "The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-66",
      "size": 2,
      "representative_id": "WOS:001027789500001",
      "representative_title": "E-APK: Energy pattern detection in decompiled android applications",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3561320.3561328",
          "title": "E-APK: Energy Pattern Detection in Decompiled Android Applications",
          "authors": "Greg\\'{o}rio, Nelson and Fernandes, Jo\\~{a}o Paulo and Bispo, Jo\\~{a}o and Medeiros, S\\'{e}rgio",
          "year": "2022",
          "abstract": "Energy efficiency is a non-functional requirement that developers must consider. This requirement is particularly relevant when building software for battery-operated devices like mobile ones: a long-lasting battery is an essential requirement for an enjoyable user experience. It has been shown that many mobile applications include inefficiencies that cause battery to be drained faster than necessary. Some of these inefficiencies result from software patterns that have been catalogued in the literature. The catalogues often provide more energy-efficient alternatives. While the related literature is vast, most approaches so far assume as a fundamental requirement that one has access to the source code of an application in order to be able to analyse it. This requirement makes independent energy analysis challenging, or even impossible, e.g. for a mobile user or, most significantly, an App Store trying to provide information on how efficient an application being submitted for publication is. Our work studies the viability of looking for known energy patterns in applications by decompiling them and analysing the resulting code. For this, we decompiled and analysed 236 open-source applications. We extended an existing tool to aid in this process, making it capable of seamlessly decompiling and analysing android applications. With the collected data, we performed a comparative analysis of the presence of energy patterns between the source code and the decompiled code. While further research is required to more assertively say if this type of static analysis is viable, our results point in a promising direction with 163 applications, approximately 69%, containing the same number of detected patterns in both source code and the release APK.",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:001027789500001",
          "title": "E-APK: Energy pattern detection in decompiled android applications",
          "authors": "Gregorio, Nelson and Bispo, Joao and Fernandes, Joao Paulo and de\nMedeiros, Sergio Queiroz",
          "year": "2023",
          "abstract": "Energy efficiency is a non-functional requirement that developers must\nconsider, particularly when building software for battery-operated\ndevices like mobile ones: a long-lasting battery is an essential\nrequirement for an enjoyable user experience.In previous studies, it has\nbeen shown that many mobile applications include inefficiencies that\ncause battery to be drained faster than necessary. Some of these\ninefficiencies result from software patterns that have been catalogued,\nand for which more energy-efficient alternatives are also known.The\nexisting catalogues, however, assume as a fundamental requirement that\none has access to the source code of an application in order to be able\nto analyse it. This requirement makes independent energy analysis\nchallenging, or even impossible, e.g. for a mobile user or, most\nsignificantly, an App Store trying to provide information on how\nefficient an application being submitted for publication is.We study the\nviability of looking for known energy patterns in applications by\ndecompiling them and analysing the resulting code. For this, we\ndecompiled and analysed 420 open-source applications by extending an\nexisting tool, which is now capable of transparently decompiling and\nanalysing android applications. With the collected data, we performed a\ncomparative study of the presence of four energy patterns between the\nsource code and the decompiled code.We performed two types of analysis:\n(i) comparing the total number of energy pattern detections; (ii)\ncomparing the similarity between energy pattern detections. When\ncomparing the total number of detections in source code against\ndecompiled code, we found that 79.29\\% of the applications reported the\nsame number of energy pattern detections.To test the similarity between\nsource code and APKs, we calculated, for each application, a similarity\nscore based on our four implemented detectors. Of all applications,\n35.76\\% achieved a perfect similarity score of 4, and 89.40\\% got a\nscore of 3 or more out of 4. Furthermore, only two applications got a\nscore of 0.When viewed in tandem, the results of the two analyses we\nperformed point in a promising direction. They provide initial evidence\nthat static analysis techniques, typically used in source code, can be a\nviable method to inspect APKs when access to source code is restricted,\nand further research in this area is worthwhile.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-153",
      "size": 2,
      "representative_id": "10123452",
      "representative_title": "Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries",
      "average_similarity": 1,
      "members": [
        {
          "id": "10123452",
          "title": "Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries",
          "authors": "Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie",
          "year": "2023",
          "abstract": "Binary reverse engineering is used to understand and analyse programs for which the source code is unavailable. Decompilers can help, transforming opaque binaries into a more readable source code-like representation. Still, reverse engineering is difficult and costly, involving considering effort in labelling code with helpful summaries. While the automated summarisation of decompiled code can help reverse engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise de-compiled binary functions. Further-more, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by removing identifiers, and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82 and, 44.21 for summarising source, decompiled, and obfuscated decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Al-Kaswan2023",
          "title": "Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries",
          "authors": "Al-Kaswan, Ali AND Ahmed, Toufique AND Izadi, Maliheh AND Sawant, Ashok, Anand AND Devanbu, Premkumar AND Deursen, van, Arie",
          "year": "2023",
          "abstract": "Reverse engineering binaries is required to understand and analyse programs for which the source code is unavailable. Decompilers can transform the largely unreadable binaries into a more readable source code-like representation. However, reverse engineering is time-consuming, much of which is taken up by labelling the functions with semantic information. While the automated summarisation of decompiled code can help Reverse Engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise decompiled binary functions. Furthermore, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by generating synthetic datasets and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82, and 44.21 for summarising source, decompiled, and synthetically stripped decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-147",
      "size": 2,
      "representative_id": "10.1145/3772368",
      "representative_title": "Fast, Fine-Grained Equivalence Checking for Neural Decompilers",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3772368",
          "title": "Fast, Fine-Grained Equivalence Checking for Neural Decompilers",
          "authors": "Dramko, Luke and Le Goues, Claire and Schwartz, Edward J.",
          "year": "2025",
          "abstract": "Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models are generally inadequate, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign,1 a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides—which parts of the functions are equivalent and how well the variable names match—is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Dramko2025",
          "title": "Fast, Fine-Grained Equivalence Checking for Neural Decompilers",
          "authors": "Dramko, Luke AND Goues, Le, Claire AND Schwartz, J., Edward",
          "year": "2025",
          "abstract": "Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models have substantial weaknesses, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign, a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides-which parts of the functions are equivalent and how well the variable names match-is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-5",
      "size": 2,
      "representative_id": "10.1109/ICSE55347.2025.00231",
      "representative_title": "Formally Verified Binary-Level Pointer Analysis",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1109/ICSE55347.2025.00231",
          "title": "Formally Verified Binary-Level Pointer Analysis",
          "authors": "Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy",
          "year": "2025",
          "abstract": "Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the tradeoff between scalability and precision, such customization allows \"meaningful\" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Verbeek2025",
          "title": "Formally Verified Binary-level Pointer Analysis",
          "authors": "Verbeek, Freek AND Shokri, Ali AND Engel, Daniel AND Ravindran, Binoy",
          "year": "2025",
          "abstract": "Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows \"meaningful\" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-173",
      "size": 2,
      "representative_id": "10596484",
      "representative_title": "GraphBinMatch: Graph-Based Similarity Learning for Cross-Language Binary and Source Code Matching",
      "average_similarity": 1,
      "members": [
        {
          "id": "10596484",
          "title": "GraphBinMatch: Graph-Based Similarity Learning for Cross-Language Binary and Source Code Matching",
          "authors": "TehraniJamsaz, Ali and Chen, Hanze and Jannesari, Ali",
          "year": "2024",
          "abstract": "Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate the performance of our approach on single-language binary-to-source code matching. Experimental results show that GraphBinMatch significantly outperforms state-of-the-art, with improvements as high as 15% over the F1 score.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "arXiv:2304.04658",
          "title": "GraphBinMatch: Graph-based Similarity Learning for Cross-Language Binary and Source Code Matching",
          "authors": "Ali TehraniJamsaz and Hanze Chen and Ali Jannesari",
          "year": "2023",
          "abstract": "Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate our approach performance on single-language binary-to-source code matching. Experimental results show that GraphBinMatch outperforms state-of-the-art significantly, with improvements as high as 15\\% over the F1 score.",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-296",
      "size": 2,
      "representative_id": "WOS:000557871300009",
      "representative_title": "Java decompiler diversity and its application to meta-decompilation",
      "average_similarity": 1,
      "members": [
        {
          "id": "Harrand2020",
          "title": "Java Decompiler Diversity and its Application to Meta-decompilation",
          "authors": "Harrand, Nicolas AND Soto-Valero, César AND Monperrus, Martin AND Baudry, Benoit",
          "year": "2020",
          "abstract": "During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84%, respectively 78%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler. ",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:000557871300009",
          "title": "Java decompiler diversity and its application to meta-decompilation",
          "authors": "Harrand, Nicolas and Soto-Valero, Cesar and Monperrus, Martin and\nBaudry, Benoit",
          "year": "2020",
          "abstract": "During compilation from Java source code to bytecode, some information\nis irreversibly lost. In other words, compilation and decompilation of\nJava code is not symmetric. Consequently, decompilation, which aims at\nproducing source code from bytecode, relies on strategies to reconstruct\nthe information that has been lost. Different Java decompilers use\ndistinct strategies to achieve proper decompilation. In this work, we\nhypothesize that the diverse ways in which bytecode can be decompiled\nhas a direct impact on the quality of the source code produced by\ndecompilers.\nIn this paper, we assess the strategies of eight Java decompilers with\nrespect to three quality indicators: syntactic correctness, syntactic\ndistortion and semantic equivalence modulo inputs. Our results show that\nno single modern decompiler is able to correctly handle the variety of\nbytecode structures coming from real-world programs. The highest ranking\ndecompiler in this study produces syntactically correct, and\nsemantically equivalent code output for 84\\%, respectively 78\\%, of the\nclasses in our dataset. Our results demonstrate that each decompiler\ncorrectly handles a different set of bytecode classes.\nWe propose a new decompiler called Arlecchino that leverages the\ndiversity of existing decompilers. To do so, we merge partial\ndecompilation into a new one based on compilation errors. Arlecchino\nhandles 37.6\\% of bytecode classes that were previously handled by no\ndecompiler. We publish the sources of this new bytecode decompiler. (C)\n2020 Published by Elsevier Inc.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-266",
      "size": 2,
      "representative_id": "WOS:000870301800008",
      "representative_title": "Learning to Find Usages of Library Functions in Optimized Binaries",
      "average_similarity": 1,
      "members": [
        {
          "id": "Ahmed2021",
          "title": "Learning to Find Usages of Library Functions in Optimized Binaries",
          "authors": "Ahmed, Toufique AND Devanbu, Premkumar AND Sawant, Ashok, Anand",
          "year": "2021",
          "abstract": "Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, \"natural\" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to \"decompile\" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization. ",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:000870301800008",
          "title": "Learning to Find Usages of Library Functions in Optimized Binaries",
          "authors": "Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok",
          "year": "2022",
          "abstract": "Much software, whether beneficent or malevolent, is distributed only as\nbinaries, sans source code. Absent source code, understanding binaries'\nbehavior can be quite challenging, especially when compiled under higher\nlevels of compiler optimization. These optimizations can transform\ncomprehensible, ``natural{''} source constructions into something\nentirely unrecognizable. Reverse engineering binaries, especially those\nsuspected of being malevolent or guilty of intellectual property theft,\nare important and time-consuming tasks. There is a great deal of\ninterest in tools to ``decompile{''} binaries back into more natural\nsource code to aid reverse engineering. Decompilation involves several\ndesirable steps, including recreating source-language constructions,\nvariable names, and perhaps even comments. One central step in creating\nbinaries is optimizing function calls, using steps such as inlining.\nRecovering these (possibly inlined) function calls from optimized\nbinaries is an essential task that most state-of-the-art decompiler\ntools try to do but do not perform very well. In this paper, we evaluate\na supervised learning approach to the problem of recovering optimized\nfunction calls. We leverage open-source software and develop an\nautomated labeling scheme to generate a reasonably large dataset of\nbinaries labeled with actual function usages. We augment this large but\nlimited labeled dataset with a pre-training step, which learns the\ndecompiled code statistics from a much larger unlabeled dataset. Thus\naugmented, our learned labeling model can be combined with an existing\ndecompilation tool, Ghidra, to achieve substantially improved\nperformance in function call recovery, especially at higher levels of\noptimization.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-114",
      "size": 2,
      "representative_id": "10.1145/3713081.3731745",
      "representative_title": "On Benchmarking Code LLMs for Android Malware Analysis",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3713081.3731745",
          "title": "On Benchmarking Code LLMs for Android Malware Analysis",
          "authors": "He, Yiling and She, Hongyu and Qian, Xingzhi and Zheng, Xinran and Chen, Zhuo and Qin, Zhan and Cavallaro, Lorenzo",
          "year": "2025",
          "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names.This paper presents Cama, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. Cama specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics—consistency, fidelity, and semantic relevance—enabling rigorous stability and effectiveness assessment and cross-model comparison.We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use Cama to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "He2025",
          "title": "On Benchmarking Code LLMs for Android Malware Analysis",
          "authors": "He, Yiling AND She, Hongyu AND Qian, Xingzhi AND Zheng, Xinran AND Chen, Zhuo AND Qin, Zhan AND Cavallaro, Lorenzo",
          "year": "2025",
          "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-37",
      "size": 2,
      "representative_id": "10.1145/3453483.3454033",
      "representative_title": "Proof repair across type equivalences",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3453483.3454033",
          "title": "Proof repair across type equivalences",
          "authors": "Ringer, Talia and Porter, RanDair and Yazdani, Nathaniel and Leo, John and Grossman, Dan",
          "year": "2021",
          "abstract": "We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to suggested tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in Pumpkin Pi, an extension to the Pumpkin Patch Coq plugin suite for proof repair. We demonstrate Pumpkin Pi’s flexibility on eight case studies, including supporting a benchmark from a user study,easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Ringer2021",
          "title": "Proof Repair across Type Equivalences",
          "authors": "Ringer, Talia AND Porter, RanDair AND Yazdani, Nathaniel AND Leo, John AND Grossman, Dan",
          "year": "2021",
          "abstract": "We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in PUMPKIN Pi, an extension to the PUMPKIN PATCH Coq plugin suite for proof repair. We demonstrate PUMPKIN Pi's flexibility on eight case studies, including supporting a benchmark from a user study, easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-336",
      "size": 2,
      "representative_id": "WOS:001368390400002",
      "representative_title": "REMaQE: Reverse Engineering Math Equations from Executables",
      "average_similarity": 1,
      "members": [
        {
          "id": "Udeshi2024",
          "title": "REMaQE: Reverse Engineering Math Equations from Executables",
          "authors": "Udeshi, Meet AND Krishnamurthy, Prashanth AND Pearce, Hammond AND Karri, Ramesh AND Khorrami, Farshad",
          "year": "2024",
          "abstract": "Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This paper proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool \"tmon\". To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow. ",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:001368390400002",
          "title": "REMaQE: Reverse Engineering Math Equations from Executables",
          "authors": "Udeshi, Meet and Krishnamurthy, Prashanth and Pearce, Hammond and Karri,\nRamesh and Khorrami, Farshad",
          "year": "2024",
          "abstract": "Cybersecurity attacks on embedded devices for industrial control systems\nand cyber-physical systems may cause catastrophic physical damage as\nwell as economic loss. This could be achieved by infecting device\nbinaries with malware that modifies the physical characteristics of the\nsystem operation. Mitigating such attacks benefits from reverse\nengineering tools that recover sufficient semantic knowledge in terms of\nmathematical equations of the implemented algorithm. Conventional\nreverse engineering tools can decompile binaries to low-level code, but\noffer little semantic insight. This article proposes the REMaQE\nautomated framework for reverse engineering of math equations from\nbinary executables. Improving over state-of-the-art, REMaQE handles\nequation parameters accessed via registers, the stack, global memory, or\npointers, and can reverse engineer equations from object-oriented\nimplementations such as C++ classes. Using REMaQE, we discovered a bug\nin the Linux kernel thermal monitoring tool ``tmon.{''} To evaluate\nREMaQE, we generate a dataset of 25,096 binaries with math equations\nimplemented in C and Simulink. REMaQE successfully recovers a\nsemantically matching equation for all 25,096 binaries. REMaQE executes\nin 0.48 seconds on average and in up to 2 seconds for complex equations.\nReal-time execution enables integration in an interactive math-oriented\nreverse engineering workflow.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-156",
      "size": 2,
      "representative_id": "10174218",
      "representative_title": "Revisiting Deep Learning for Variable Type Recovery",
      "average_similarity": 1,
      "members": [
        {
          "id": "10174218",
          "title": "Revisiting Deep Learning for Variable Type Recovery",
          "authors": "Cao, Kevin and Leach, Kevin",
          "year": "2023",
          "abstract": "Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Cao2023",
          "title": "Revisiting Deep Learning for Variable Type Recovery",
          "authors": "Cao, Kevin AND Leach, Kevin",
          "year": "2023",
          "abstract": "Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-273",
      "size": 2,
      "representative_id": "arXiv:2509.14646",
      "representative_title": "SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation",
      "average_similarity": 1,
      "members": [
        {
          "id": "arXiv:2509.14646",
          "title": "SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation",
          "authors": "Yongpan Wang and Xin Xu and Xiaojie Zhu and Xiaodong Gu and Beijun Shen",
          "year": "2025",
          "abstract": "Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose \\\\saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of \\\\saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, \\\\saltm constructs a Source-level Abstract Logic Tree (\\\\salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed \\\\salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare \\\\saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that \\\\saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\\\\\\% TCP rate on Decompile-Eval with a 10.6\\\\\\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Wang2025",
          "title": "SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation",
          "authors": "Wang, Yongpan AND Xu, Xin AND Zhu, Xiaojie AND Gu, Xiaodong AND Shen, Beijun",
          "year": "2025",
          "abstract": "Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose \\saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of \\saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed \\salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare \\saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that \\saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\\% TCP rate on Decompile-Eval with a 10.6\\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-283",
      "size": 2,
      "representative_id": "WOS:000459709500083",
      "representative_title": "Security and Privacy Analyses of Internet of Things Children's Toys",
      "average_similarity": 1,
      "members": [
        {
          "id": "Chu2018",
          "title": "Security and Privacy Analyses of Internet of Things Children's Toys",
          "authors": "Chu, Gordon AND Apthorpe, Noah AND Feamster, Nick",
          "year": "2018",
          "abstract": "This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially-available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule (COPPA) as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many IoT toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks. ",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "WOS:000459709500083",
          "title": "Security and Privacy Analyses of Internet of Things Children's Toys",
          "authors": "Chu, Gordon and Apthorpe, Noah and Feamster, Nick",
          "year": "2019",
          "abstract": "This paper investigates the security and privacy of Internet-connected\nchildren's smart toys through case studies of three commercially\navailable products. We conduct network and application vulnerability\nanalyses of each toy using static and dynamic analysis techniques,\nincluding application binary decompilation and network monitoring. We\ndiscover several publicly undisclosed vulnerabilities that violate the\nChildren's Online Privacy Protection Rule as well as the toys'\nindividual privacy policies. These vulnerabilities, especially security\nflaws in network communications with first-party servers, are indicative\nof a disconnect between many Internet of Things toy developers and\nsecurity and privacy best practices despite increased attention to\nInternet-connected toy hacking risks.",
          "similarity": 1,
          "action": "keep"
        }
      ]
    },
    {
      "id": "cluster-3",
      "size": 2,
      "representative_id": "10.1109/CGO57630.2024.10444788",
      "representative_title": "SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1109/CGO57630.2024.10444788",
          "title": "SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly",
          "authors": "Armengol-Estap\\'{e}, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F. P.",
          "year": "2024",
          "abstract": "Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect.This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code.We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6\\texttimes{} more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4\\texttimes{} more accurate than the large language model ChatGPT and generates significantly more readable code than both.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Armengol-Estapé2024",
          "title": "SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly",
          "authors": "Armengol-Estapé, Jordi AND Woodruff, Jackson AND Cummins, Chris AND O'Boyle, P., F., Michael",
          "year": "2024",
          "abstract": "Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from ExeBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-242",
      "size": 2,
      "representative_id": "9282282",
      "representative_title": "STAN: Towards Describing Bytecodes of Smart Contract",
      "average_similarity": 1,
      "members": [
        {
          "id": "9282282",
          "title": "STAN: Towards Describing Bytecodes of Smart Contract",
          "authors": "Li, Xiaoqi and Chen, Ting and Luo, Xiapu and Zhang, Tao and Yu, Le and Xu, Zhou",
          "year": "2020",
          "abstract": "More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named Stan to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, Stan can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that Stan can generate adequate, accurate and readable descriptions for contract’s bytecodes, which have practical value for users.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Li2020",
          "title": "STAN: Towards Describing Bytecodes of Smart Contract",
          "authors": "Li, Xiaoqi AND Chen, Ting AND Luo, Xiapu AND Zhang, Tao AND Yu, Le AND Xu, Zhou",
          "year": "2020",
          "abstract": "More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-106",
      "size": 2,
      "representative_id": "10.1145/3696410.3714790",
      "representative_title": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3696410.3714790",
          "title": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract",
          "authors": "Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Wu, Tingguan and Wang, Siyun and Chalkias, Kostas Kryptos",
          "year": "2025",
          "abstract": "The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization. Moreover, MAD's methodology could potentially extend to other smart contract languages, like Solidity, further enhancing Web3 transparency.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Chen2025",
          "title": "SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract",
          "authors": "Chen, Eason AND Tang, Xinyi AND Xiao, Zimo AND Li, Chuangji AND Li, Shizhuo AND Tingguan, Wu AND Wang, Siyun AND Chalkias, Kryptos, Kostas",
          "year": "2025",
          "abstract": "The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-27",
      "size": 2,
      "representative_id": "10.1145/3385412.3386012",
      "representative_title": "Synthesizing structured CAD models with equality saturation and inverse transformations",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3385412.3386012",
          "title": "Synthesizing structured CAD models with equality saturation and inverse transformations",
          "authors": "Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary",
          "year": "2020",
          "abstract": "Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly \"shrink\" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Nandi2020",
          "title": "Synthesizing Structured CAD Models with Equality Saturation and Inverse Transformations",
          "authors": "Nandi, Chandrakana AND Willsey, Max AND Anderson, Adam AND Wilcox, R., James AND Darulova, Eva AND Grossman, Dan AND Tatlock, Zachary",
          "year": "2020",
          "abstract": "Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly \"shrink\" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-219",
      "size": 2,
      "representative_id": "8080433",
      "representative_title": "Text-based adventures of the golovin AI agent",
      "average_similarity": 1,
      "members": [
        {
          "id": "8080433",
          "title": "Text-based adventures of the golovin AI agent",
          "authors": "Kostka, Bartosz and Kwiecieli, Jaroslaw and Kowalski, Jakub and Rychlikowski, Pawel",
          "year": "2017",
          "abstract": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Kostka2017",
          "title": "Text-based Adventures of the Golovin AI Agent",
          "authors": "Kostka, Bartosz AND Kwiecien, Jaroslaw AND Kowalski, Jakub AND Rychlikowski, Pawel",
          "year": "2017",
          "abstract": "The domain of text-based adventure games has been recently established as a new challenge of creating the agent that is both able to understand natural language, and acts intelligently in text-described environments. In this paper, we present our approach to tackle the problem. Our agent, named Golovin, takes advantage of the limited game domain. We use genre-related corpora (including fantasy books and decompiled games) to create language models suitable to this domain. Moreover, we embed mechanisms that allow us to specify, and separately handle, important tasks as fighting opponents, managing inventory, and navigating on the game map. We validated usefulness of these mechanisms, measuring agent's performance on the set of 50 interactive fiction games. Finally, we show that our agent plays on a level comparable to the winner of the last year Text-Based Adventure AI Competition. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-128",
      "size": 2,
      "representative_id": "10.1145/3728935",
      "representative_title": "The Incredible Shrinking Context... in a Decompiler Near You",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3728935",
          "title": "The Incredible Shrinking Context... in a Decompiler Near You",
          "authors": "Lagouvardos, Sifis and Bollanos, Yannis and Grech, Neville and Smaragdakis, Yannis",
          "year": "2025",
          "abstract": "Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrnkr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrnkr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly “forgetting” control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5% of contracts (compared to ∼95% for Elipmoc), covers (i.e., reaches and manages to decompile) 67% more code than Heimdall-rs, and reduces key imprecision metrics by over 65%, compared again to Elipmoc.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Lagouvardos2025",
          "title": "The Incredible Shrinking Context... in a Decompiler Near You",
          "authors": "Lagouvardos, Sifis AND Bollanos, Yannis AND Grech, Neville AND Smaragdakis, Yannis",
          "year": "2025",
          "abstract": "Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrknr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrknr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly \"forgetting\" control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5% of contracts (compared to ~95%), covers (i.e., reaches and manages to decompile) 67% more code, and reduces key imprecision metrics by over 65%. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-233",
      "size": 2,
      "representative_id": "8930870",
      "representative_title": "The Strengths and Behavioral Quirks of Java Bytecode Decompilers",
      "average_similarity": 1,
      "members": [
        {
          "id": "8930870",
          "title": "The Strengths and Behavioral Quirks of Java Bytecode Decompilers",
          "authors": "Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit",
          "year": "2019",
          "abstract": "During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84% of classes of our dataset and semantically equivalent code output for 78% of classes.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "Harrand2019",
          "title": "The Strengths and Behavioral Quirks of Java Bytecode Decompilers",
          "authors": "Harrand, Nicolas AND Soto-Valero, César AND Monperrus, Martin AND Baudry, Benoit",
          "year": "2019",
          "abstract": "During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84% of classes of our dataset and semantically equivalent code output for 78% of classes. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    },
    {
      "id": "cluster-103",
      "size": 3,
      "representative_id": "10.1145/3691620.3695020",
      "representative_title": "WaDec: Decompiling WebAssembly Using Large Language Model",
      "average_similarity": 1,
      "members": [
        {
          "id": "10.1145/3691620.3695020",
          "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
          "authors": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
          "year": "2024",
          "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
          "similarity": 1,
          "action": "keep"
        },
        {
          "id": "10764949",
          "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
          "authors": "She, Xinyu and Zhao, Yanjie and Wang, Haoyu",
          "year": "2024",
          "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm’s binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art’s 116.94%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. In summary, WaDec enhances understanding of the code’s structure and execution flow, facilitating automated code analysis, optimization, and security auditing.",
          "similarity": 1,
          "action": "remove"
        },
        {
          "id": "She2024",
          "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
          "authors": "She, Xinyu AND Zhao, Yanjie AND Wang, Haoyu",
          "year": "2024",
          "abstract": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. ",
          "similarity": 1,
          "action": "remove"
        }
      ]
    }
  ]
}