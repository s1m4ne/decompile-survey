@article{Ahmed2021,
  abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, "natural" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to "decompile" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization. },
  author = {Ahmed, Toufique AND Devanbu, Premkumar AND Sawant, Ashok, Anand},
  doi = {https://doi.org/10.48550/arXiv.2103.05221},
  howpublished = {\url{https://arxiv.org/pdf/2103.05221}},
  month = {sep},
  note = {},
  title = {Learning to Find Usages of Library Functions in Optimized Binaries},
  year = {2021}
}

@article{Arasteh2025,
  abstract = {The software compilation process has a tendency to obscure the original design of the system and makes it difficult both to identify individual components and discern their purpose simply by examining the resulting binary code. Although decompilation techniques attempt to recover higher-level source code from the machine code in question, they are not fully able to restore the semantics of the original functions. Furthermore, binaries are often stripped of metadata, and this makes it challenging to reverse engineer complex binary software. In this paper we show how a combination of binary decomposition techniques, decompilation passes, and LLM-powered function summarization can be used to build an economical engine to identify modules in stripped binaries and associate them with high-level natural language descriptions. We instantiated this technique with three underlying open-source LLMs -- CodeQwen, DeepSeek-Coder and CodeStral -- and measured its effectiveness in identifying modules in robotics firmware. This experimental evaluation involved 467 modules from four devices from the ArduPilot software suite, and showed that CodeStral, the best-performing backend LLM, achieves an average F1-score of 0.68 with an online running time of just a handful of seconds. },
  author = {Arasteh, Sima AND Jandaghi, Pegah AND Weideman, Nicolaas AND Perepech, Dennis AND Raghothaman, Mukund AND Hauser, Christophe AND Garcia, Luis},
  doi = {https://doi.org/10.48550/arXiv.2503.03969},
  howpublished = {\url{https://arxiv.org/pdf/2503.03969}},
  month = {mar},
  note = {11 pages, 5 figures},
  title = {Trim My View: An LLM-Based Code Query System for Module Retrieval in Robotic Firmware},
  year = {2025}
}

@article{Chukkol2024,
  abstract = {Binary program vulnerability detection is critical for software security, yet existing deep learning approaches often rely on source code analysis, limiting their ability to detect unknown vulnerabilities. To address this, we propose VulCatch, a binary-level vulnerability detection framework. VulCatch introduces a Synergy Decompilation Module (SDM) and Kolmogorov-Arnold Networks (KAN) to transform raw binary code into pseudocode using CodeT5, preserving high-level semantics for deep analysis with tools like Ghidra and IDA. KAN further enhances feature transformation, enabling the detection of complex vulnerabilities. VulCatch employs word2vec, Inception Blocks, BiLSTM Attention, and Residual connections to achieve high detection accuracy (98.88%) and precision (97.92%), while minimizing false positives (1.56%) and false negatives (2.71%) across seven CVE datasets. },
  author = {Chukkol, Adama, Hamman, Abdulrahman AND Luo, Senlin AND Sharif, Kashif AND Haruna, Yunusa AND Abdullahi, Muhammad, Muhammad},
  doi = {https://doi.org/10.48550/arXiv.2408.07181},
  howpublished = {\url{https://arxiv.org/pdf/2408.07181}},
  month = {aug},
  note = {},
  title = {VulCatch: Enhancing Binary Vulnerability Detection through CodeT5 Decompilation and KAN Advanced Feature Extraction},
  year = {2024}
}

@article{Dramko2025,
  abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models have substantial weaknesses, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign, a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides-which parts of the functions are equivalent and how well the variable names match-is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity. },
  author = {Dramko, Luke AND Goues, Le, Claire AND Schwartz, J., Edward},
  doi = {https://doi.org/10.48550/arXiv.2501.04811},
  howpublished = {\url{https://arxiv.org/pdf/2501.04811}},
  month = {jan},
  note = {},
  title = {Fast, Fine-Grained Equivalence Checking for Neural Decompilers},
  year = {2025}
}

@article{Escalada2021,
  abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1% F1-measure, whereas the best decompiler obtains a 30% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers. },
  author = {Escalada, Javier AND Scully, Ted AND Ortin, Francisco},
  doi = {https://doi.org/10.48550/arXiv.2101.08116},
  howpublished = {\url{https://arxiv.org/pdf/2101.08116}},
  month = {feb},
  note = {},
  title = {Improving type information inferred by decompilers with supervised machine learning},
  year = {2021}
}

@article{Green2024,
  abstract = {Decompilers are widely used by security researchers and developers to reverse engineer executable code. While modern decompilers are adept at recovering instructions, control flow, and function boundaries, some useful information from the original source code, such as variable types and names, is lost during the compilation process. Our work aims to predict these variable types and names from the remaining information. We propose STRIDE, a lightweight technique that predicts variable names and types by matching sequences of decompiler tokens to those found in training data. We evaluate it on three benchmark datasets and find that STRIDE achieves comparable performance to state-of-the-art machine learning models for both variable retyping and renaming while being much simpler and faster. We perform a detailed comparison with two recent SOTA transformer-based models in order to understand the specific factors that make our technique effective. We implemented STRIDE in fewer than 1000 lines of Python and have open-sourced it under a permissive license at https://github.com/hgarrereyn/STRIDE. },
  author = {Green, Harrison AND Schwartz, J., Edward AND Goues, Le, Claire AND Vasilescu, Bogdan},
  doi = {https://doi.org/10.48550/arXiv.2407.02733},
  howpublished = {\url{https://arxiv.org/pdf/2407.02733}},
  month = {jul},
  note = {},
  title = {STRIDE: Simple Type Recognition In Decompiled Executables},
  year = {2024}
}

@article{Jiang2025,
  abstract = {Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58% (absolute percentage point improvement) higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17% Recall@1, showing promising abilities on both assembly generation and understanding tasks. },
  author = {Jiang, Nan AND Wang, Chengxiao AND Liu, Kevin AND Xu, Xiangzhe AND Tan, Lin AND Zhang, Xiangyu AND Babkin, Petr},
  doi = {https://doi.org/10.48550/arXiv.2311.13721},
  howpublished = {\url{https://arxiv.org/pdf/2311.13721}},
  month = {nov},
  note = {Published as a conference paper at ICLR 2025},
  title = {Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning},
  year = {2025}
}

@article{Li2025,
  abstract = {Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \textbf\{19,448\} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \textbf\{19\} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation. },
  author = {Li, Qingyuan AND Li, Binchang AND Gao, Cuiyun AND Gao, Shuzheng AND Li, Zongjie},
  doi = {https://doi.org/10.48550/arXiv.2509.06052},
  howpublished = {\url{https://arxiv.org/pdf/2509.06052}},
  month = {sep},
  note = {},
  title = {Empirical Study of Code Large Language Models for Binary Security Patch Detection},
  year = {2025}
}

@article{Manuel2025,
  abstract = {The generation of large, high-quality datasets for code understanding and generation remains a significant challenge, particularly when aligning decompiled binaries with their original source code. To address this, we present CodableLLM, a Python framework designed to automate the creation and curation of datasets by mapping decompiled functions to their corresponding source functions. This process enhances the alignment between decompiled and source code representations, facilitating the development of large language models (LLMs) capable of understanding and generating code across multiple abstraction levels. CodableLLM supports multiple programming languages and integrates with existing decompilers and parsers to streamline dataset generation. This paper presents the design and implementation of CodableLLM, evaluates its performance in dataset creation, and compares it to existing tools in the field. The results demonstrate that CodableLLM offers a robust and efficient solution for generating datasets tailored for code-focused LLMS. },
  author = {Manuel, Dylan AND Rad, Paul},
  doi = {https://doi.org/10.48550/arXiv.2507.22066},
  howpublished = {\url{https://arxiv.org/pdf/2507.22066}},
  month = {jul},
  note = {},
  title = {CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation},
  year = {2025}
}

@article{Szafraniec2023,
  abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11% on average, and up to 79% for the Java -> Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation. },
  author = {Szafraniec, Marc AND Roziere, Baptiste AND Leather, Hugh AND Charton, Francois AND Labatut, Patrick AND Synnaeve, Gabriel},
  doi = {https://doi.org/10.48550/arXiv.2207.03578},
  howpublished = {\url{https://arxiv.org/pdf/2207.03578}},
  month = {apr},
  note = {9 pages},
  title = {Code Translation with Compiler Representations},
  year = {2023}
}

@article{Tan2025,
  abstract = {Recent advances in LLM-based decompilers have been shown effective to convert low-level binaries into human-readable source code. However, there still lacks a comprehensive benchmark that provides large-scale binary-source function pairs, which is critical for advancing the LLM decompilation technology. Creating accurate binary-source mappings incurs severe issues caused by complex compilation settings and widespread function inlining that obscure the correspondence between binaries and their original source code. Previous efforts have either relied on used contest-style benchmarks, synthetic binary-source mappings that diverge significantly from the mappings in real world, or partially matched binaries with only code lines or variable names, compromising the effectiveness of analyzing the binary functionality. To alleviate these issues, we introduce Decompile-Bench, the first open-source dataset comprising two million binary-source function pairs condensed from 100 million collected function pairs, i.e., 450GB of binaries compiled from permissively licensed GitHub projects. For the evaluation purposes, we also developed a benchmark Decompile-Bench-Eval including manually crafted binaries from the well-established HumanEval and MBPP, alongside the compiled GitHub repositories released after 2025 to mitigate data leakage issues. We further explore commonly-used evaluation metrics to provide a thorough assessment of the studied LLM decompilers and find that fine-tuning with Decompile-Bench causes a 20% improvement over previous benchmarks in terms of the re-executability rate. Our code and data has been released in HuggingFace and Github. https://github.com/albertan017/LLM4Decompile },
  author = {Tan, Hanzhuo AND Tian, Xiaolong AND Qi, Hanrui AND Liu, Jiaming AND Gao, Zuchen AND Wang, Siyi AND Luo, Qi AND Li, Jing AND Zhang, Yuqun},
  doi = {https://doi.org/10.48550/arXiv.2505.12668},
  howpublished = {\url{https://arxiv.org/pdf/2505.12668}},
  month = {oct},
  note = {},
  title = {Decompile-Bench: Million-Scale Binary-Source Function Pairs for Real-World Binary Decompilation},
  year = {2025}
}

@article{Thurnherr2024,
  abstract = {Recently, the transformer architecture has enabled substantial progress in many areas of pattern recognition and machine learning. However, as with other neural network models, there is currently no general method available to explain their inner workings. The present paper represents a first step towards this direction. We utilize \textit\{Transformer Compiler for RASP\} (Tracr) to generate a large dataset of pairs of transformer weights and corresponding RASP programs. Based on this dataset, we then build and train a model, with the aim of recovering the RASP code from the compiled model. We demonstrate that the simple form of Tracr compiled transformer weights is interpretable for such a decompiler model. In an empirical evaluation, our model achieves exact reproductions on more than 30\% of the test objects, while the remaining 70\% can generally be reproduced with only few errors. Additionally, more than 70\% of the programs, produced by our model, are functionally equivalent to the ground truth, and therefore a valid decompilation of the Tracr compiled transformer weights. },
  author = {Thurnherr, Hannes AND Riesen, Kaspar},
  doi = {https://doi.org/10.48550/arXiv.2410.00061},
  howpublished = {\url{https://arxiv.org/pdf/2410.00061}},
  month = {sep},
  note = {},
  title = {Neural Decompiling of Tracr Transformers},
  year = {2024}
}

@article{Wan2025,
  abstract = {While Vision-language Models (VLMs) have demonstrated strong semantic capabilities, their ability to interpret the underlying geometric structure of visual information is less explored. Pictographic characters, which combine visual form with symbolic structure, provide an ideal test case for this capability. We formulate this visual recognition challenge in the mathematical domain, where each character is represented by an executable program of geometric primitives. This is framed as a program synthesis task, training a VLM to decompile raster images into programs composed of Bézier curves. Our model, acting as a "visual decompiler", demonstrates performance superior to strong zero-shot baselines, including GPT-4o. The most significant finding is that when trained solely on modern Chinese characters, the model is able to reconstruct ancient Oracle Bone Script in a zero-shot context. This generalization provides strong evidence that the model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to a more structured form of visual understanding. },
  author = {Wan, Zihao AND Xu, Lin, Tong, Pau AND Luo, Fuwen AND Wang, Ziyue AND Li, Peng AND Liu, Yang},
  doi = {https://doi.org/10.48550/arXiv.2511.00076},
  howpublished = {\url{https://arxiv.org/pdf/2511.00076}},
  month = {oct},
  note = {},
  title = {Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves},
  year = {2025}
}

@article{Wang2025,
  abstract = {Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet comprehensive security analysis remains difficult due to unverified code, proxy-based architectures, and the reliance on manual inspection of complex execution traces. Existing approaches fall into two main categories: anomaly transaction detection, which flags suspicious transactions but offers limited insight into specific attack strategies hidden in execution traces inside transactions, and code vulnerability detection, which cannot analyze unverified contracts and struggles to show how identified flaws are exploited in real incidents. As a result, analysts must still manually align transaction traces with contract code to reconstruct attack scenarios and conduct forensics. To address this gap, TraceLLM is proposed as a framework that leverages LLMs to integrate execution trace-level detection with decompiled contract code. We introduce a new anomaly execution path identification algorithm and an LLM-refined decompile tool to identify vulnerable functions and provide explicit attack paths to LLM. TraceLLM establishes the first benchmark for joint trace and contract code-driven security analysis. For comparison, proxy baselines are created by jointly transmitting the results of three representative code analysis along with raw traces to LLM. TraceLLM identifies attacker and victim addresses with 85.19\% precision and produces automated reports with 70.37\% factual precision across 27 cases with ground truth expert reports, achieving 25.93\% higher accuracy than the best baseline. Moreover, across 148 real-world Ethereum incidents, TraceLLM automatically generates reports with 66.22\% expert-verified accuracy, demonstrating strong generalizability. },
  author = {Wang, Shuzheng AND Huang, Yue AND Xu, Zhuoer AND Huang, Yuming AND Tang, Jing},
  doi = {https://doi.org/10.48550/arXiv.2509.03037},
  howpublished = {\url{https://arxiv.org/pdf/2509.03037}},
  month = {sep},
  note = {},
  title = {TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum},
  year = {2025}
}

@article{Zhou2025,
  abstract = {Decompiling Rust binaries is challenging due to the language's rich type system, aggressive compiler optimizations, and widespread use of high-level abstractions. In this work, we conduct a benchmark-driven evaluation of decompilation quality across core Rust features and compiler build modes. Our automated scoring framework shows that generic types, trait methods, and error handling constructs significantly reduce decompilation quality, especially in release builds. Through representative case studies, we analyze how specific language constructs affect control flow, variable naming, and type information recovery. Our findings provide actionable insights for tool developers and highlight the need for Rust-aware decompilation strategies. },
  author = {Zhou, Zixu},
  doi = {https://doi.org/10.48550/arXiv.2507.18792},
  howpublished = {\url{https://arxiv.org/pdf/2507.18792}},
  month = {jul},
  note = {},
  title = {Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges},
  year = {2025}
}
