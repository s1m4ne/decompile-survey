@article{Ahmed2021,
  abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, "natural" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to "decompile" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization. },
  author = {Ahmed, Toufique AND Devanbu, Premkumar AND Sawant, Ashok, Anand},
  doi = {https://doi.org/10.48550/arXiv.2103.05221},
  howpublished = {\url{https://arxiv.org/pdf/2103.05221}},
  month = {sep},
  note = {},
  title = {Learning to Find Usages of Library Functions in Optimized Binaries},
  year = {2021}
}

@article{Arasteh2025,
  abstract = {The software compilation process has a tendency to obscure the original design of the system and makes it difficult both to identify individual components and discern their purpose simply by examining the resulting binary code. Although decompilation techniques attempt to recover higher-level source code from the machine code in question, they are not fully able to restore the semantics of the original functions. Furthermore, binaries are often stripped of metadata, and this makes it challenging to reverse engineer complex binary software. In this paper we show how a combination of binary decomposition techniques, decompilation passes, and LLM-powered function summarization can be used to build an economical engine to identify modules in stripped binaries and associate them with high-level natural language descriptions. We instantiated this technique with three underlying open-source LLMs -- CodeQwen, DeepSeek-Coder and CodeStral -- and measured its effectiveness in identifying modules in robotics firmware. This experimental evaluation involved 467 modules from four devices from the ArduPilot software suite, and showed that CodeStral, the best-performing backend LLM, achieves an average F1-score of 0.68 with an online running time of just a handful of seconds. },
  author = {Arasteh, Sima AND Jandaghi, Pegah AND Weideman, Nicolaas AND Perepech, Dennis AND Raghothaman, Mukund AND Hauser, Christophe AND Garcia, Luis},
  doi = {https://doi.org/10.48550/arXiv.2503.03969},
  howpublished = {\url{https://arxiv.org/pdf/2503.03969}},
  month = {mar},
  note = {11 pages, 5 figures},
  title = {Trim My View: An LLM-Based Code Query System for Module Retrieval in Robotic Firmware},
  year = {2025}
}

@article{Armengol-Estapé2024,
  abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from ExeBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both. },
  author = {Armengol-Estapé, Jordi AND Woodruff, Jackson AND Cummins, Chris AND O'Boyle, P., F., Michael},
  doi = {https://doi.org/10.48550/arXiv.2305.12520},
  howpublished = {\url{https://arxiv.org/pdf/2305.12520}},
  month = {feb},
  note = {},
  title = {SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly},
  year = {2024}
}

@article{Banerjee2021,
  abstract = {Decompilation is the procedure of transforming binary programs into a high-level representation, such as source code, for human analysts to examine. While modern decompilers can reconstruct and recover much information that is discarded during compilation, inferring variable names is still extremely difficult. Inspired by recent advances in natural language processing, we propose a novel solution to infer variable names in decompiled code based on Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as Transformers and BERT. Our solution takes \textit\{raw\} decompiler output, the less semantically meaningful code, as input, and enriches it using our proposed \textit\{finetuning\} technique, Constrained Masked Language Modeling. Using Constrained Masked Language Modeling introduces the challenge of predicting the number of masked tokens for the original variable name. We address this \textit\{count of token prediction\} challenge with our post-processing algorithm. Compared to the state-of-the-art approaches, our trained VarBERT model is simpler and of much better performance. We evaluated our model on an existing large-scale data set with 164,632 binaries and showed that it can predict variable names identical to the ones present in the original source code up to 84.15\% of the time. },
  author = {Banerjee, Pratyay AND Pal, Kumar, Kuntal AND Wang, Fish AND Baral, Chitta},
  doi = {https://doi.org/10.48550/arXiv.2103.12801},
  howpublished = {\url{https://arxiv.org/pdf/2103.12801}},
  month = {mar},
  note = {Work In Progress},
  title = {Variable Name Recovery in Decompiled Binary Code using Constrained Masked Language Modeling},
  year = {2021}
}

@article{Cao2023,
  abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks. },
  author = {Cao, Ying AND Liang, Ruigang AND Chen, Kai AND Hu, Peiwei},
  doi = {https://doi.org/10.48550/arXiv.2301.00969},
  howpublished = {\url{https://arxiv.org/pdf/2301.00969}},
  month = {jan},
  note = {},
  title = {Boosting Neural Networks to Decompile Optimized Binaries},
  year = {2023}
}

@article{Chen2021,
  abstract = {A common tool used by security professionals for reverse-engineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4% of the time and the original types 75.8% of the time. },
  author = {Chen, Qibin AND Lacomis, Jeremy AND Schwartz, J., Edward AND Goues, Le, Claire AND Neubig, Graham AND Vasilescu, Bogdan},
  doi = {https://doi.org/10.48550/arXiv.2108.06363},
  howpublished = {\url{https://arxiv.org/pdf/2108.06363}},
  month = {aug},
  note = {17 pages to be published in USENIX Security '22},
  title = {Augmenting Decompiler Output with Learned Variable Names and Types},
  year = {2021}
}

@article{Chen2025,
  abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization },
  author = {Chen, Eason AND Tang, Xinyi AND Xiao, Zimo AND Li, Chuangji AND Li, Shizhuo AND Tingguan, Wu AND Wang, Siyun AND Chalkias, Kryptos, Kostas},
  doi = {https://doi.org/10.48550/arXiv.2410.15275},
  howpublished = {\url{https://arxiv.org/pdf/2410.15275}},
  month = {jan},
  note = {Paper accepted at ACM The Web Conference 2025},
  title = {SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract},
  year = {2025}
}

@article{Chukkol2024,
  abstract = {Binary program vulnerability detection is critical for software security, yet existing deep learning approaches often rely on source code analysis, limiting their ability to detect unknown vulnerabilities. To address this, we propose VulCatch, a binary-level vulnerability detection framework. VulCatch introduces a Synergy Decompilation Module (SDM) and Kolmogorov-Arnold Networks (KAN) to transform raw binary code into pseudocode using CodeT5, preserving high-level semantics for deep analysis with tools like Ghidra and IDA. KAN further enhances feature transformation, enabling the detection of complex vulnerabilities. VulCatch employs word2vec, Inception Blocks, BiLSTM Attention, and Residual connections to achieve high detection accuracy (98.88%) and precision (97.92%), while minimizing false positives (1.56%) and false negatives (2.71%) across seven CVE datasets. },
  author = {Chukkol, Adama, Hamman, Abdulrahman AND Luo, Senlin AND Sharif, Kashif AND Haruna, Yunusa AND Abdullahi, Muhammad, Muhammad},
  doi = {https://doi.org/10.48550/arXiv.2408.07181},
  howpublished = {\url{https://arxiv.org/pdf/2408.07181}},
  month = {aug},
  note = {},
  title = {VulCatch: Enhancing Binary Vulnerability Detection through CodeT5 Decompilation and KAN Advanced Feature Extraction},
  year = {2024}
}

@article{David2025,
  abstract = {The widespread lack of broad source code verification on blockchain explorers such as Etherscan, where despite 78,047,845 smart contracts deployed on Ethereum (as of May 26, 2025), a mere 767,520 (< 1%) are open source, presents a severe impediment to blockchain security. This opacity necessitates the automated semantic analysis of on-chain smart contract bytecode, a fundamental research challenge with direct implications for identifying vulnerabilities and understanding malicious behavior. Prevailing decompilers struggle to reverse bytecode in a readable manner, often yielding convoluted code that critically hampers vulnerability analysis and thwarts efforts to dissect contract functionalities for security auditing. This paper addresses this challenge by introducing a pioneering decompilation pipeline that, for the first time, successfully leverages Large Language Models (LLMs) to transform Ethereum Virtual Machine (EVM) bytecode into human-readable and semantically faithful Solidity code. Our novel methodology first employs rigorous static program analysis to convert bytecode into a structured three-address code (TAC) representation. This intermediate representation then guides a Llama-3.2-3B model, specifically fine-tuned on a comprehensive dataset of 238,446 TAC-to-Solidity function pairs, to generate high-quality Solidity. This approach uniquely recovers meaningful variable names, intricate control flow, and precise function signatures. Our extensive empirical evaluation demonstrates a significant leap beyond traditional decompilers, achieving an average semantic similarity of 0.82 with original source and markedly superior readability. The practical viability and effectiveness of our research are demonstrated through its implementation in a publicly accessible system, available at https://evmdecompiler.com. },
  author = {David, Isaac AND Zhou, Liyi AND Song, Dawn AND Gervais, Arthur AND Qin, Kaihua},
  doi = {https://doi.org/10.48550/arXiv.2506.19624},
  howpublished = {\url{https://arxiv.org/pdf/2506.19624}},
  month = {jun},
  note = {},
  title = {Decompiling Smart Contracts with a Large Language Model},
  year = {2025}
}

@article{Dramko2025,
  abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models have substantial weaknesses, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign, a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides-which parts of the functions are equivalent and how well the variable names match-is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity. },
  author = {Dramko, Luke AND Goues, Le, Claire AND Schwartz, J., Edward},
  doi = {https://doi.org/10.48550/arXiv.2501.04811},
  howpublished = {\url{https://arxiv.org/pdf/2501.04811}},
  month = {jan},
  note = {},
  title = {Fast, Fine-Grained Equivalence Checking for Neural Decompilers},
  year = {2025}
}

@article{Escalada2021,
  abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1% F1-measure, whereas the best decompiler obtains a 30% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers. },
  author = {Escalada, Javier AND Scully, Ted AND Ortin, Francisco},
  doi = {https://doi.org/10.48550/arXiv.2101.08116},
  howpublished = {\url{https://arxiv.org/pdf/2101.08116}},
  month = {feb},
  note = {},
  title = {Improving type information inferred by decompilers with supervised machine learning},
  year = {2021}
}

@article{Fang2024,
  abstract = {WebAssembly enables near-native execution in web applications and is increasingly adopted for tasks that demand high performance and robust security. However, its assembly-like syntax, implicit stack machine, and low-level data types make it extremely difficult for human developers to understand, spurring the need for effective WebAssembly reverse engineering techniques. In this paper, we propose StackSight, a novel neurosymbolic approach that combines Large Language Models (LLMs) with advanced program analysis to decompile complex WebAssembly code into readable C++ snippets. StackSight visualizes and tracks virtual stack alterations via a static analysis algorithm and then applies chain-of-thought prompting to harness LLM's complex reasoning capabilities. Evaluation results show that StackSight significantly improves WebAssembly decompilation. Our user study also demonstrates that code snippets generated by StackSight have significantly higher win rates and enable a better grasp of code semantics. },
  author = {Fang, Weike AND Zhou, Zhejian AND He, Junzhou AND Wang, Weihang},
  doi = {https://doi.org/10.48550/arXiv.2406.04568},
  howpublished = {\url{https://arxiv.org/pdf/2406.04568}},
  month = {jun},
  note = {9 pages. In the Proceedings of the 41st International Conference on Machine Learning (ICML' 24)},
  title = {StackSight: Unveiling WebAssembly through Large Language Models and Neurosymbolic Chain-of-Thought Decompilation},
  year = {2024}
}

@article{Feng2024,
  abstract = {Decompilation transforms compiled code back into a high-level programming language for analysis when source code is unavailable. Previous work has primarily focused on enhancing decompilation performance by increasing the scale of model parameters or training data for pre-training. Based on the characteristics of the decompilation task, we propose two methods: (1) Without fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method recompiles the LLM's decompilation results to construct pairs for in-context learning, helping the model improve decompilation performance. (2) Fine-grained Alignment Enhancement (FAE), which meticulously aligns assembly code with source code at the statement level by leveraging debugging information, is employed during the fine-tuning phase to achieve further improvements in decompilation. By integrating these two methods, we achieved a Re-Executability performance improvement of approximately 3.90% on the Decompile-Eval benchmark, establishing a new state-of-the-art performance of 52.41%. The code, data, and models are available at https://github.com/AlongWY/sccdec. },
  author = {Feng, Yunlong AND Teng, Dechuan AND Xu, Yang AND Mu, Honglin AND Xu, Xiao AND Qin, Libo AND Zhu, Qingfu AND Che, Wanxiang},
  doi = {https://doi.org/10.48550/arXiv.2406.17233},
  howpublished = {\url{https://arxiv.org/pdf/2406.17233}},
  month = {oct},
  note = {EMNLP 2024 Findings},
  title = {Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement},
  year = {2024}
}

@article{Feng2025,
  abstract = {The goal of decompilation is to convert compiled low-level code (e.g., assembly code) back into high-level programming languages, enabling analysis in scenarios where source code is unavailable. This task supports various reverse engineering applications, such as vulnerability identification, malware analysis, and legacy software migration. The end-to-end decompile method based on large langauge models (LLMs) reduces reliance on additional tools and minimizes manual intervention due to its inherent properties. However, previous end-to-end methods often lose critical information necessary for reconstructing control flow structures and variables when processing binary files, making it challenging to accurately recover the program's logic. To address these issues, we propose the \textbf\{ReF Decompile\} method, which incorporates the following innovations: (1) The Relabelling strategy replaces jump target addresses with labels, preserving control flow clarity. (2) The Function Call strategy infers variable types and retrieves missing variable information from binary files. Experimental results on the Humaneval-Decompile Benchmark demonstrate that ReF Decompile surpasses comparable baselines and achieves state-of-the-art (SOTA) performance of $61.43\%$. },
  author = {Feng, Yunlong AND Li, Bohan AND Shi, Xiaoming AND Zhu, Qingfu AND Che, Wanxiang},
  doi = {https://doi.org/10.48550/arXiv.2502.12221},
  howpublished = {\url{https://arxiv.org/pdf/2502.12221}},
  month = {feb},
  note = {},
  title = {ReF Decompile: Relabeling and Function Call Enhanced Decompile},
  year = {2025}
}

@article{Fu2019,
  abstract = {Reverse engineering of binary executables is a critical problem in the computer security domain. On the one hand, malicious parties may recover interpretable source codes from the software products to gain commercial advantages. On the other hand, binary decompilation can be leveraged for code vulnerability analysis and malware detection. However, efficient binary decompilation is challenging. Conventional decompilers have the following major limitations: (i) they are only applicable to specific source-target language pair, hence incurs undesired development cost for new language tasks; (ii) their output high-level code cannot effectively preserve the correct functionality of the input binary; (iii) their output program does not capture the semantics of the input and the reversed program is hard to interpret. To address the above problems, we propose Coda, the first end-to-end neural-based framework for code decompilation. Coda decomposes the decompilation task into two key phases: First, Coda employs an instruction type-aware encoder and a tree decoder for generating an abstract syntax tree (AST) with attention feeding during the code sketch generation stage. Second, Coda then updates the code sketch using an iterative error correction machine guided by an ensembled neural error predictor. By finding a good approximate candidate and then fixing it towards perfect, Coda achieves superior performance compared to baseline approaches. We assess Coda's performance with extensive experiments on various benchmarks. Evaluation results show that Coda achieves an average of 82\% program recovery accuracy on unseen binary samples, where the state-of-the-art decompilers yield 0\% accuracy. Furthermore, Coda outperforms the sequence-to-sequence model with attention by a margin of 70\% program accuracy. },
  author = {Fu, Cheng AND Chen, Huili AND Liu, Haolan AND Chen, Xinyun AND Tian, Yuandong AND Koushanfar, Farinaz AND Zhao, Jishen},
  doi = {https://doi.org/10.48550/arXiv.1906.12029},
  howpublished = {\url{https://arxiv.org/pdf/1906.12029}},
  month = {jun},
  note = {},
  title = {A Neural-based Program Decompiler},
  year = {2019}
}

@article{Green2024,
  abstract = {Decompilers are widely used by security researchers and developers to reverse engineer executable code. While modern decompilers are adept at recovering instructions, control flow, and function boundaries, some useful information from the original source code, such as variable types and names, is lost during the compilation process. Our work aims to predict these variable types and names from the remaining information. We propose STRIDE, a lightweight technique that predicts variable names and types by matching sequences of decompiler tokens to those found in training data. We evaluate it on three benchmark datasets and find that STRIDE achieves comparable performance to state-of-the-art machine learning models for both variable retyping and renaming while being much simpler and faster. We perform a detailed comparison with two recent SOTA transformer-based models in order to understand the specific factors that make our technique effective. We implemented STRIDE in fewer than 1000 lines of Python and have open-sourced it under a permissive license at https://github.com/hgarrereyn/STRIDE. },
  author = {Green, Harrison AND Schwartz, J., Edward AND Goues, Le, Claire AND Vasilescu, Bogdan},
  doi = {https://doi.org/10.48550/arXiv.2407.02733},
  howpublished = {\url{https://arxiv.org/pdf/2407.02733}},
  month = {jul},
  note = {},
  title = {STRIDE: Simple Type Recognition In Decompiled Executables},
  year = {2024}
}

@article{Hosseini2022,
  abstract = {The problem of reversing the compilation process, decompilation, is an important tool in reverse engineering of computer software. Recently, researchers have proposed using techniques from neural machine translation to automate the process in decompilation. Although such techniques hold the promise of targeting a wider range of source and assembly languages, to date they have primarily targeted C code. In this paper we argue that existing neural decompilers have achieved higher accuracy at the cost of requiring language-specific domain knowledge such as tokenizers and parsers to build an abstract syntax tree (AST) for the source language, which increases the overhead of supporting new languages. We explore a different tradeoff that, to the extent possible, treats the assembly and source languages as plain text, and show that this allows us to build a decompiler that is easily retargetable to new languages. We evaluate our prototype decompiler, Beyond The C (BTC), on Go, Fortran, OCaml, and C, and examine the impact of parameters such as tokenization and training data selection on the quality of decompilation, finding that it achieves comparable decompilation results to prior work in neural decompilation with significantly less domain knowledge. We will release our training data, trained decompilation models, and code to help encourage future research into language-agnostic decompilation. },
  author = {Hosseini, Iman AND Dolan-Gavitt, Brendan},
  doi = {https://doi.org/10.48550/arXiv.2212.08950},
  howpublished = {\url{https://arxiv.org/pdf/2212.08950}},
  month = {dec},
  note = {},
  title = {Beyond the C: Retargetable Decompilation using Neural Machine Translation},
  year = {2022}
}

@article{Jiang2025,
  abstract = {Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58% (absolute percentage point improvement) higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17% Recall@1, showing promising abilities on both assembly generation and understanding tasks. },
  author = {Jiang, Nan AND Wang, Chengxiao AND Liu, Kevin AND Xu, Xiangzhe AND Tan, Lin AND Zhang, Xiangyu AND Babkin, Petr},
  doi = {https://doi.org/10.48550/arXiv.2311.13721},
  howpublished = {\url{https://arxiv.org/pdf/2311.13721}},
  month = {nov},
  note = {Published as a conference paper at ICLR 2025},
  title = {Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning},
  year = {2025}
}

@article{Katz2019,
  abstract = {We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code. The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high. We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97% and 88% of our benchmarks respectively. },
  author = {Katz, Omer AND Olshaker, Yuval AND Goldberg, Yoav AND Yahav, Eran},
  doi = {https://doi.org/10.48550/arXiv.1905.08325},
  howpublished = {\url{https://arxiv.org/pdf/1905.08325}},
  month = {may},
  note = {},
  title = {Towards Neural Decompilation},
  year = {2019}
}

@article{Lacomis2019,
  abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time. },
  author = {Lacomis, Jeremy AND Yin, Pengcheng AND Schwartz, J., Edward AND Allamanis, Miltiadis AND Goues, Le, Claire AND Neubig, Graham AND Vasilescu, Bogdan},
  doi = {https://doi.org/10.48550/arXiv.1909.09029},
  howpublished = {\url{https://arxiv.org/pdf/1909.09029}},
  month = {oct},
  note = {2019 International Conference on Automated Software Engineering},
  title = {DIRE: A Neural Approach to Decompiled Identifier Naming},
  year = {2019}
}

@article{Li2019,
  abstract = {Reverse Engineering(RE) has been a fundamental task in software engineering. However, most of the traditional Java reverse engineering tools are strictly rule defined, thus are not fault-tolerant, which pose serious problem when noise and interference were introduced into the system. In this paper, we view reverse engineering as a statistical machine translation task instead of rule-based task, and propose a fault-tolerant Java decompiler based on machine translation models. Our model is based on attention-based Neural Machine Translation (NMT) and Transformer architectures. First, we measure the translation quality on both the redundant and purified datasets. Next, we evaluate the fault-tolerance(anti-noise ability) of our framework on test sets with different unit error probability (UEP). In addition, we compare the suitability of different word segmentation algorithms for decompilation task. Experimental results demonstrate that our model is more robust and fault-tolerant compared to traditional Abstract Syntax Tree (AST) based decompilers. Specifically, in terms of BLEU-4 and Word Error Rate (WER), our performance has reached 94.50% and 2.65% on the redundant test set; 92.30% and 3.48% on the purified test set. },
  author = {Li, Zhiming AND Wu, Qing AND Qian, Kun},
  doi = {https://doi.org/10.48550/arXiv.1908.06748},
  howpublished = {\url{https://arxiv.org/pdf/1908.06748}},
  month = {oct},
  note = {8 pages},
  title = {Adabot: Fault-Tolerant Java Decompiler},
  year = {2019}
}

@article{Li2025,
  abstract = {Security patch detection (SPD) is crucial for maintaining software security, as unpatched vulnerabilities can lead to severe security risks. In recent years, numerous learning-based SPD approaches have demonstrated promising results on source code. However, these approaches typically cannot be applied to closed-source applications and proprietary systems that constitute a significant portion of real-world software, as they release patches only with binary files, and the source code is inaccessible. Given the impressive performance of code large language models (LLMs) in code intelligence and binary analysis tasks such as decompilation and compilation optimization, their potential for detecting binary security patches remains unexplored, exposing a significant research gap between their demonstrated low-level code understanding capabilities and this critical security task. To address this gap, we construct a large-scale binary patch dataset containing \textbf\{19,448\} samples, with two levels of representation: assembly code and pseudo-code, and systematically evaluate \textbf\{19\} code LLMs of varying scales to investigate their capability in binary SPD tasks. Our initial exploration demonstrates that directly prompting vanilla code LLMs struggles to accurately identify security patches from binary patches, and even state-of-the-art prompting techniques fail to mitigate the lack of domain knowledge in binary SPD within vanilla models. Drawing on the initial findings, we further investigate the fine-tuning strategy for injecting binary SPD domain knowledge into code LLMs through two levels of representation. Experimental results demonstrate that fine-tuned LLMs achieve outstanding performance, with the best results obtained on the pseudo-code representation. },
  author = {Li, Qingyuan AND Li, Binchang AND Gao, Cuiyun AND Gao, Shuzheng AND Li, Zongjie},
  doi = {https://doi.org/10.48550/arXiv.2509.06052},
  howpublished = {\url{https://arxiv.org/pdf/2509.06052}},
  month = {sep},
  note = {},
  title = {Empirical Study of Code Large Language Models for Binary Security Patch Detection},
  year = {2025}
}

@article{Liang2021,
  abstract = {Decompilation transforms low-level program languages (PL) (e.g., binary code) into high-level PLs (e.g., C/C++). It has been widely used when analysts perform security analysis on software (systems) whose source code is unavailable, such as vulnerability search and malware analysis. However, current decompilation tools usually need lots of experts' efforts, even for years, to generate the rules for decompilation, which also requires long-term maintenance as the syntax of high-level PL or low-level PL changes. Also, an ideal decompiler should concisely generate high-level PL with similar functionality to the source low-level PL and semantic information (e.g., meaningful variable names), just like human-written code. Unfortunately, existing manually-defined rule-based decompilation techniques only functionally restore the low-level PL to a similar high-level PL and are still powerless to recover semantic information. In this paper, we propose a novel neural decompilation approach to translate low-level PL into accurate and user-friendly high-level PL, effectively improving its readability and understandability. Furthermore, we implement the proposed approaches called SEAM. Evaluations on four real-world applications show that SEAM has an average accuracy of 94.41%, which is much better than prior neural machine translation (NMT) models. Finally, we evaluate the effectiveness of semantic information recovery through a questionnaire survey, and the average accuracy is 92.64%, which is comparable or superior to the state-of-the-art compilers. },
  author = {Liang, Ruigang AND Cao, Ying AND Hu, Peiwei AND He, Jinwen AND Chen, Kai},
  doi = {https://doi.org/10.48550/arXiv.2112.15491},
  howpublished = {\url{https://arxiv.org/pdf/2112.15491}},
  month = {dec},
  note = {},
  title = {Semantics-Recovering Decompilation through Neural Machine Translation},
  year = {2021}
}

@article{Liao2025,
  abstract = {Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \system\{\} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes. },
  author = {Liao, Zeqin AND Nan, Yuhong AND Gao, Zixu AND Liang, Henglong AND Hao, Sicheng AND Reng, Peifan AND Zheng, Zibin},
  doi = {https://doi.org/10.48550/arXiv.2501.08670},
  howpublished = {\url{https://arxiv.org/pdf/2501.08670}},
  month = {oct},
  note = {This is the author version of the article accepted for publication in IEEE Transactions on Software Engineering},
  title = {Augmenting Smart Contract Decompiler Output through Fine-grained Dependency Analysis and LLM-facilitated Semantic Recovery},
  year = {2025}
}

@article{Liu2022,
  abstract = {Due to their widespread use on heterogeneous hardware devices, deep learning (DL) models are compiled into executables by DL compilers to fully leverage low-level hardware primitives. This approach allows DL computations to be undertaken at low cost across a variety of computing platforms, including CPUs, GPUs, and various hardware accelerators. We present BTD (Bin to DNN), a decompiler for deep neural network (DNN) executables. BTD takes DNN executables and outputs full model specifications, including types of DNN operators, network topology, dimensions, and parameters that are (nearly) identical to those of the input models. BTD delivers a practical framework to process DNN executables compiled by different DL compilers and with full optimizations enabled on x86 platforms. It employs learning-based techniques to infer DNN operators, dynamic analysis to reveal network architectures, and symbolic execution to facilitate inferring dimensions and parameters of DNN operators. Our evaluation reveals that BTD enables accurate recovery of full specifications of complex DNNs with millions of parameters (e.g., ResNet). The recovered DNN specifications can be re-compiled into a new DNN executable exhibiting identical behavior to the input executable. We show that BTD can boost two representative attacks, adversarial example generation and knowledge stealing, against DNN executables. We also demonstrate cross-architecture legacy code reuse using BTD, and envision BTD being used for other critical downstream tasks like DNN security hardening and patching. },
  author = {Liu, Zhibo AND Yuan, Yuanyuan AND Wang, Shuai AND Xie, Xiaofei AND Ma, Lei},
  doi = {https://doi.org/10.48550/arXiv.2210.01075},
  howpublished = {\url{https://arxiv.org/pdf/2210.01075}},
  month = {oct},
  note = {The extended version of a paper to appear in the Proceedings of the 32nd USENIX Security Symposium, 2023, (USENIX Security '23), 25 pages},
  title = {Decompiling x86 Deep Neural Network Executables},
  year = {2022}
}

@article{Liu2025,
  abstract = {Binary decompilation plays a vital role in various cybersecurity and software engineering tasks. Recently, end-to-end decompilation methods powered by large language models (LLMs) have garnered significant attention due to their ability to generate highly readable source code with minimal human intervention. However, existing LLM-based approaches face several critical challenges, including limited capability in reconstructing code structure and logic, low accuracy in data recovery, concerns over data security and privacy, and high computational resource requirements. To address these issues, we develop the CodeInverter Suite, making three contributions: (1) the CodeInverter Workflow (CIW) is a novel prompt engineering workflow that incorporates control flow graphs (CFG) and explicit data mappings to improve LLM-based decompilation. (2) Using CIW on well-known source code datasets, we curate the CodeInverter Dataset (CID), a domain-specific dataset containing 8.69 million samples that contains CFGs and data mapping tables. (3) We train the CoderInverter Models (CIMs) on CID, generating two lightweight LLMs (with 1.3B and 6.7B parameters) intended for efficient inference in privacy-sensitive or resource-constrained environments. Extensive experiments on two benchmarks demonstrate that the CIW substantially enhances the performance of various LLMs across multiple metrics. Our CIM-6.7B can achieve state-of-the-art decompilation performance, outperforming existing LLMs even with over 100x more parameters in decompilation tasks, an average improvement of 11.03% in re-executability, 6.27% in edit similarity. },
  author = {Liu, Peipei AND Sun, Jian AND Sun, Rongkang AND Chen, Li AND Yan, Zhaoteng AND Zhang, Peizheng AND Sun, Dapeng AND Wang, Dawei AND Zhang, Xiaoling AND Li, Dan},
  doi = {https://doi.org/10.48550/arXiv.2503.07215},
  howpublished = {\url{https://arxiv.org/pdf/2503.07215}},
  month = {may},
  note = {},
  title = {The CodeInverter Suite: Control-Flow and Data-Mapping Augmented Binary Decompilation with LLMs},
  year = {2025}
}

@article{Manuel2025,
  abstract = {The generation of large, high-quality datasets for code understanding and generation remains a significant challenge, particularly when aligning decompiled binaries with their original source code. To address this, we present CodableLLM, a Python framework designed to automate the creation and curation of datasets by mapping decompiled functions to their corresponding source functions. This process enhances the alignment between decompiled and source code representations, facilitating the development of large language models (LLMs) capable of understanding and generating code across multiple abstraction levels. CodableLLM supports multiple programming languages and integrates with existing decompilers and parsers to streamline dataset generation. This paper presents the design and implementation of CodableLLM, evaluates its performance in dataset creation, and compares it to existing tools in the field. The results demonstrate that CodableLLM offers a robust and efficient solution for generating datasets tailored for code-focused LLMS. },
  author = {Manuel, Dylan AND Rad, Paul},
  doi = {https://doi.org/10.48550/arXiv.2507.22066},
  howpublished = {\url{https://arxiv.org/pdf/2507.22066}},
  month = {jul},
  note = {},
  title = {CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation},
  year = {2025}
}

@article{She2024,
  abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. },
  author = {She, Xinyu AND Zhao, Yanjie AND Wang, Haoyu},
  doi = {https://doi.org/10.48550/arXiv.2406.11346},
  howpublished = {\url{https://arxiv.org/pdf/2406.11346}},
  month = {sep},
  note = {This paper was accepted by ASE 2024},
  title = {WaDec: Decompiling WebAssembly Using Large Language Model},
  year = {2024}
}

@article{Szafraniec2023,
  abstract = {In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11% on average, and up to 79% for the Java -> Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation. },
  author = {Szafraniec, Marc AND Roziere, Baptiste AND Leather, Hugh AND Charton, Francois AND Labatut, Patrick AND Synnaeve, Gabriel},
  doi = {https://doi.org/10.48550/arXiv.2207.03578},
  howpublished = {\url{https://arxiv.org/pdf/2207.03578}},
  month = {apr},
  note = {9 pages},
  title = {Code Translation with Compiler Representations},
  year = {2023}
}

@article{Tan2024,
  abstract = {Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100% in terms of re-executability rate. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results. Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile },
  author = {Tan, Hanzhuo AND Luo, Qi AND Li, Jing AND Zhang, Yuqun},
  doi = {https://doi.org/10.48550/arXiv.2403.05286},
  howpublished = {\url{https://arxiv.org/pdf/2403.05286}},
  month = {oct},
  note = {},
  title = {LLM4Decompile: Decompiling Binary Code with Large Language Models},
  year = {2024}
}

@article{Tan2025,
  abstract = {Recent advances in LLM-based decompilers have been shown effective to convert low-level binaries into human-readable source code. However, there still lacks a comprehensive benchmark that provides large-scale binary-source function pairs, which is critical for advancing the LLM decompilation technology. Creating accurate binary-source mappings incurs severe issues caused by complex compilation settings and widespread function inlining that obscure the correspondence between binaries and their original source code. Previous efforts have either relied on used contest-style benchmarks, synthetic binary-source mappings that diverge significantly from the mappings in real world, or partially matched binaries with only code lines or variable names, compromising the effectiveness of analyzing the binary functionality. To alleviate these issues, we introduce Decompile-Bench, the first open-source dataset comprising two million binary-source function pairs condensed from 100 million collected function pairs, i.e., 450GB of binaries compiled from permissively licensed GitHub projects. For the evaluation purposes, we also developed a benchmark Decompile-Bench-Eval including manually crafted binaries from the well-established HumanEval and MBPP, alongside the compiled GitHub repositories released after 2025 to mitigate data leakage issues. We further explore commonly-used evaluation metrics to provide a thorough assessment of the studied LLM decompilers and find that fine-tuning with Decompile-Bench causes a 20% improvement over previous benchmarks in terms of the re-executability rate. Our code and data has been released in HuggingFace and Github. https://github.com/albertan017/LLM4Decompile },
  author = {Tan, Hanzhuo AND Tian, Xiaolong AND Qi, Hanrui AND Liu, Jiaming AND Gao, Zuchen AND Wang, Siyi AND Luo, Qi AND Li, Jing AND Zhang, Yuqun},
  doi = {https://doi.org/10.48550/arXiv.2505.12668},
  howpublished = {\url{https://arxiv.org/pdf/2505.12668}},
  month = {oct},
  note = {},
  title = {Decompile-Bench: Million-Scale Binary-Source Function Pairs for Real-World Binary Decompilation},
  year = {2025}
}

@article{Thurnherr2024,
  abstract = {Recently, the transformer architecture has enabled substantial progress in many areas of pattern recognition and machine learning. However, as with other neural network models, there is currently no general method available to explain their inner workings. The present paper represents a first step towards this direction. We utilize \textit\{Transformer Compiler for RASP\} (Tracr) to generate a large dataset of pairs of transformer weights and corresponding RASP programs. Based on this dataset, we then build and train a model, with the aim of recovering the RASP code from the compiled model. We demonstrate that the simple form of Tracr compiled transformer weights is interpretable for such a decompiler model. In an empirical evaluation, our model achieves exact reproductions on more than 30\% of the test objects, while the remaining 70\% can generally be reproduced with only few errors. Additionally, more than 70\% of the programs, produced by our model, are functionally equivalent to the ground truth, and therefore a valid decompilation of the Tracr compiled transformer weights. },
  author = {Thurnherr, Hannes AND Riesen, Kaspar},
  doi = {https://doi.org/10.48550/arXiv.2410.00061},
  howpublished = {\url{https://arxiv.org/pdf/2410.00061}},
  month = {sep},
  note = {},
  title = {Neural Decompiling of Tracr Transformers},
  year = {2024}
}

@article{Wan2025,
  abstract = {While Vision-language Models (VLMs) have demonstrated strong semantic capabilities, their ability to interpret the underlying geometric structure of visual information is less explored. Pictographic characters, which combine visual form with symbolic structure, provide an ideal test case for this capability. We formulate this visual recognition challenge in the mathematical domain, where each character is represented by an executable program of geometric primitives. This is framed as a program synthesis task, training a VLM to decompile raster images into programs composed of Bézier curves. Our model, acting as a "visual decompiler", demonstrates performance superior to strong zero-shot baselines, including GPT-4o. The most significant finding is that when trained solely on modern Chinese characters, the model is able to reconstruct ancient Oracle Bone Script in a zero-shot context. This generalization provides strong evidence that the model acquires an abstract and transferable geometric grammar, moving beyond pixel-level pattern recognition to a more structured form of visual understanding. },
  author = {Wan, Zihao AND Xu, Lin, Tong, Pau AND Luo, Fuwen AND Wang, Ziyue AND Li, Peng AND Liu, Yang},
  doi = {https://doi.org/10.48550/arXiv.2511.00076},
  howpublished = {\url{https://arxiv.org/pdf/2511.00076}},
  month = {oct},
  note = {},
  title = {Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves},
  year = {2025}
}

@article{Wang2025,
  abstract = {Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet comprehensive security analysis remains difficult due to unverified code, proxy-based architectures, and the reliance on manual inspection of complex execution traces. Existing approaches fall into two main categories: anomaly transaction detection, which flags suspicious transactions but offers limited insight into specific attack strategies hidden in execution traces inside transactions, and code vulnerability detection, which cannot analyze unverified contracts and struggles to show how identified flaws are exploited in real incidents. As a result, analysts must still manually align transaction traces with contract code to reconstruct attack scenarios and conduct forensics. To address this gap, TraceLLM is proposed as a framework that leverages LLMs to integrate execution trace-level detection with decompiled contract code. We introduce a new anomaly execution path identification algorithm and an LLM-refined decompile tool to identify vulnerable functions and provide explicit attack paths to LLM. TraceLLM establishes the first benchmark for joint trace and contract code-driven security analysis. For comparison, proxy baselines are created by jointly transmitting the results of three representative code analysis along with raw traces to LLM. TraceLLM identifies attacker and victim addresses with 85.19\% precision and produces automated reports with 70.37\% factual precision across 27 cases with ground truth expert reports, achieving 25.93\% higher accuracy than the best baseline. Moreover, across 148 real-world Ethereum incidents, TraceLLM automatically generates reports with 66.22\% expert-verified accuracy, demonstrating strong generalizability. },
  author = {Wang, Shuzheng AND Huang, Yue AND Xu, Zhuoer AND Huang, Yuming AND Tang, Jing},
  doi = {https://doi.org/10.48550/arXiv.2509.03037},
  howpublished = {\url{https://arxiv.org/pdf/2509.03037}},
  month = {sep},
  note = {},
  title = {TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum},
  year = {2025}
}

@article{Wong2023,
  abstract = {A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly. This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable. We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler -- IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions. },
  author = {Wong, Kin, Wai AND Wang, Huaijin AND Li, Zongjie AND Liu, Zhibo AND Wang, Shuai AND Tang, Qiyi AND Nie, Sen AND Wu, Shi},
  doi = {https://doi.org/10.48550/arXiv.2310.06530},
  howpublished = {\url{https://arxiv.org/pdf/2310.06530}},
  month = {nov},
  note = {},
  title = {Refining Decompiled C Code with Large Language Models},
  year = {2023}
}

@article{Xu2024,
  abstract = {Decompilation aims to recover the source code form of a binary executable. It has many security applications, such as malware analysis, vulnerability detection, and code hardening. A prominent challenge in decompilation is to recover variable names. We propose a novel technique that leverages the strengths of generative models while mitigating model biases. We build a prototype, GenNm, from pre-trained generative models CodeGemma-2B, CodeLlama-7B, and CodeLlama-34B. We finetune GenNm on decompiled functions and teach models to leverage contextual information. GenNm includes names from callers and callees while querying a function, providing rich contextual information within the model's input token limitation. We mitigate model biases by aligning the output distribution of models with symbol preferences of developers. Our results show that GenNm improves the state-of-the-art name recovery precision by 5.6-11.4 percentage points on two commonly used datasets and improves the state-of-the-art by 32% (from 17.3% to 22.8%) in the most challenging setup where ground-truth variable names are not seen in the training dataset. },
  author = {Xu, Xiangzhe AND Zhang, Zhuo AND Su, Zian AND Huang, Ziyang AND Feng, Shiwei AND Ye, Yapeng AND Jiang, Nan AND Xie, Danning AND Cheng, Siyuan AND Tan, Lin AND Zhang, Xiangyu},
  doi = {https://doi.org/10.48550/arXiv.2306.02546},
  howpublished = {\url{https://arxiv.org/pdf/2306.02546}},
  month = {dec},
  note = {},
  title = {Symbol Preference Aware Generative Models for Recovering Variable Names from Stripped Binary},
  year = {2024}
}

@article{Zhou2025,
  abstract = {Decompiling Rust binaries is challenging due to the language's rich type system, aggressive compiler optimizations, and widespread use of high-level abstractions. In this work, we conduct a benchmark-driven evaluation of decompilation quality across core Rust features and compiler build modes. Our automated scoring framework shows that generic types, trait methods, and error handling constructs significantly reduce decompilation quality, especially in release builds. Through representative case studies, we analyze how specific language constructs affect control flow, variable naming, and type information recovery. Our findings provide actionable insights for tool developers and highlight the need for Rust-aware decompilation strategies. },
  author = {Zhou, Zixu},
  doi = {https://doi.org/10.48550/arXiv.2507.18792},
  howpublished = {\url{https://arxiv.org/pdf/2507.18792}},
  month = {jul},
  note = {},
  title = {Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges},
  year = {2025}
}

@article{Zou2025,
  abstract = {As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2% of all the functions produced by the native decompiler. },
  author = {Zou, Muqi AND Cai, Hongyu AND Wu, Hongwei AND Basque, Leonahenahe, Zion AND Khan, Arslan AND Celik, Berkay AND Dave AND Tian AND Bianchi, Antonio AND Ruoyu AND Wang AND Xu, Dongyan},
  doi = {https://doi.org/10.48550/arXiv.2506.10125},
  howpublished = {\url{https://arxiv.org/pdf/2506.10125}},
  month = {aug},
  note = {},
  title = {D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning},
  year = {2025}
}
