@article{arXiv:1802.07119,
 abstract = {This paper proposes a novel method for tamper detection and recovery using semi-fragile data hiding, based on Lifting Wavelet Transform (LWT) and Feed-Forward Neural Network (FNN). In TRLF, first, the host image is decomposed up to one level using LWT, and the Discrete Cosine Transform (DCT) is applied to each 2*2 blocks of diagonal details. Next, a random binary sequence is embedded in each block as the watermark by correlating \$DC\$ coefficients. In authentication stage, first, the watermarked image geometry is reconstructed by using Speeded Up Robust Features (SURF) algorithm and extract watermark bits by using FNN. Afterward, logical exclusive-or operation between original and extracted watermark is applied to detect tampered region. Eventually, in the recovery stage, tampered regions are recovered by image digest which is generated by inverse halftoning technique. The performance and efficiency of TRLF and its robustness against various geometric, non-geometric and hybrid attacks are reported. From the experimental results, it can be seen that TRLF is superior in terms of robustness and quality of the digest and watermarked image respectively, compared to the-state-of-the-art fragile and semi-fragile watermarking methods. In addition, imperceptibility has been improved by using different correlation steps as the gain factor for flat (smooth) and texture (rough) blocks.},
 archiveprefix = {arXiv},
 author = {Behrouz Bolourian Haghighi and Amir Hossein Taherinia and Reza Monsefi},
 categories = {cs.CR cs.MM},
 eprint = {1802.07119},
 journal = {arXiv preprint arXiv:1802.07119},
 month = {02},
 pdf = {https://arxiv.org/pdf/1802.07119v1},
 primaryclass = {cs.CR},
 published = {2018-02-18T18:36:35Z},
 title = {TRLF: An Effective Semi-fragile Watermarking Method for Tamper Detection and Recovery based on LWT and FNN},
 updated = {2018-02-18T18:36:35Z},
 url = {https://arxiv.org/abs/1802.07119v1},
 year = {2018}
}

@article{arXiv:1809.04193,
 abstract = {Variational execution is a novel dynamic analysis technique for exploring highly configurable systems and accurately tracking information flow. It is able to efficiently analyze many configurations by aggressively sharing redundancies of program executions. The idea of variational execution has been demonstrated to be effective in exploring variations in the program, especially when the configuration space grows out of control. Existing implementations of variational execution often require heavy lifting of the runtime interpreter, which is painstaking and error-prone. Furthermore, the performance of this approach is suboptimal. For example, the state-of-the-art variational execution interpreter for Java, VarexJ, slows down executions by 100 to 800 times over a single execution for small to medium size Java programs. Instead of modifying existing JVMs, we propose to transform existing bytecode to make it variational, so it can be executed on an unmodified commodity JVM. Our evaluation shows a dramatic improvement on performance over the state-of-the-art, with a speedup of 2 to 46 times, and high efficiency in sharing computations.},
 archiveprefix = {arXiv},
 author = {Chu-Pan Wong and Jens Meinicke and Lukas Lazarek and Christian Kästner},
 categories = {cs.PL cs.SE},
 eprint = {1809.04193},
 journal = {arXiv preprint arXiv:1809.04193},
 month = {09},
 pdf = {https://arxiv.org/pdf/1809.04193v1},
 primaryclass = {cs.PL},
 published = {2018-09-11T22:54:37Z},
 title = {Faster Variational Execution with Transparent Bytecode Transformation},
 updated = {2018-09-11T22:54:37Z},
 url = {https://arxiv.org/abs/1809.04193v1},
 year = {2018}
}

@article{arXiv:1901.06540,
 abstract = {Sensitivity properties describe how changes to the input of a program affect the output, typically by upper bounding the distance between the outputs of two runs by a monotone function of the distance between the corresponding inputs. When programs are probabilistic, the distance between outputs is a distance between distributions. The Kantorovich lifting provides a general way of defining a distance between distributions by lifting the distance of the underlying sample space; by choosing an appropriate distance on the base space, one can recover other usual probabilistic distances, such as the Total Variation distance. We develop a relational pre-expectation calculus to upper bound the Kantorovich distance between two executions of a probabilistic program. We illustrate our methods by proving algorithmic stability of a machine learning algorithm, convergence of a reinforcement learning algorithm, and fast mixing for card shuffling algorithms. We also consider some extensions: proving lower bounds on the Total Variation distance and convergence to the uniform distribution. Finally, we describe an asynchronous extension of our calculus to reason about pairs of program executions with different control flow.},
 archiveprefix = {arXiv},
 author = {Alejandro Aguirre and Gilles Barthe and Justin Hsu and Benjamin Lucien Kaminski and Joost-Pieter Katoen and Christoph Matheja},
 categories = {cs.LO cs.PL},
 comment = {Major revision},
 eprint = {1901.06540},
 journal = {arXiv preprint arXiv:1901.06540},
 month = {01},
 pdf = {https://arxiv.org/pdf/1901.06540v2},
 primaryclass = {cs.LO},
 published = {2019-01-19T15:23:19Z},
 title = {A Pre-Expectation Calculus for Probabilistic Sensitivity},
 updated = {2020-08-10T07:57:37Z},
 url = {https://arxiv.org/abs/1901.06540v2},
 year = {2019}
}

@article{arXiv:1907.12475,
 abstract = {Edge machine learning can deliver low-latency and private artificial intelligent (AI) services for mobile devices by leveraging computation and storage resources at the network edge. This paper presents an energy-efficient edge processing framework to execute deep learning inference tasks at the edge computing nodes whose wireless connections to mobile devices are prone to channel uncertainties. Aimed at minimizing the sum of computation and transmission power consumption with probabilistic quality-of-service (QoS) constraints, we formulate a joint inference tasking and downlink beamforming problem that is characterized by a group sparse objective function. We provide a statistical learning based robust optimization approach to approximate the highly intractable probabilistic-QoS constraints by nonconvex quadratic constraints, which are further reformulated as matrix inequalities with a rank-one constraint via matrix lifting. We design a reweighted power minimization approach by iteratively reweighted \$\\ell\_1\$ minimization with difference-of-convex-functions (DC) regularization and updating weights, where the reweighted approach is adopted for enhancing group sparsity whereas the DC regularization is designed for inducing rank-one solutions. Numerical results demonstrate that the proposed approach outperforms other state-of-the-art approaches.},
 archiveprefix = {arXiv},
 author = {Kai Yang and Yuanming Shi and Wei Yu and Zhi Ding},
 categories = {cs.IT cs.LG eess.SP},
 comment = {This paper has been accepted by IEEE Internet of Things Journal},
 eprint = {1907.12475},
 journal = {arXiv preprint arXiv:1907.12475},
 month = {07},
 pdf = {https://arxiv.org/pdf/1907.12475v2},
 primaryclass = {cs.IT},
 published = {2019-07-29T15:24:58Z},
 title = {Energy-Efficient Processing and Robust Wireless Cooperative Transmission for Edge Inference},
 updated = {2020-03-02T09:19:35Z},
 url = {https://arxiv.org/abs/1907.12475v2},
 year = {2019}
}

@article{arXiv:1910.10398,
 abstract = {Convolutional neural networks are state-of-the-art for various segmentation tasks. While for 2D images these networks are also computationally efficient, 3D convolutions have huge storage requirements and therefore, end-to-end training is limited by GPU memory and data size. To overcome this issue, we introduce a network structure for volumetric data without 3D convolution layers. The main idea is to include projections from different directions to transform the volumetric data to a sequence of images, where each image contains information of the full data. We then apply 2D convolutions to these projection images and lift them again to volumetric data using a trainable reconstruction algorithm. The proposed architecture can be applied end-to-end to very large data volumes without cropping or sliding-window techniques. For a tested sparse binary segmentation task, it outperforms already known standard approaches and is more resistant to generation of artefacts.},
 archiveprefix = {arXiv},
 author = {Christoph Angermann and Markus Haltmeier},
 categories = {cs.CV cs.LG eess.IV stat.ML},
 comment = {Submission for joint MICCAI-Workshops on Computing and Visualization for Intravascular Imaging and Computer Assisted Stenting (CVII-STENT) 2019},
 doi = {10.1007/978-3-030-33327-0\_19},
 eprint = {1910.10398},
 journal = {arXiv preprint arXiv:1910.10398},
 month = {10},
 pdf = {https://arxiv.org/pdf/1910.10398v1},
 primaryclass = {cs.CV},
 published = {2019-10-23T08:02:09Z},
 title = {Random 2.5D U-net for Fully 3D Segmentation},
 updated = {2019-10-23T08:02:09Z},
 url = {https://arxiv.org/abs/1910.10398v1},
 year = {2019}
}

@article{arXiv:1910.11717,
 abstract = {Lambda lifting is a well-known transformation, traditionally employed for compiling functional programs to supercombinators. However, more recent abstract machines for functional languages like OCaml and Haskell tend to do closure conversion instead for direct access to the environment, so lambda lifting is no longer necessary to generate machine code. We propose to revisit selective lambda lifting in this context as an optimising code generation strategy and conceive heuristics to identify beneficial lifting opportunities. We give a static analysis for estimating impact on heap allocations of a lifting decision. Performance measurements of our implementation within the Glasgow Haskell Compiler on a large corpus of Haskell benchmarks suggest modest speedups.},
 archiveprefix = {arXiv},
 author = {Sebastian Graf and Simon Peyton Jones},
 categories = {cs.PL},
 comment = {Rejected from ICFP 2019},
 eprint = {1910.11717},
 journal = {arXiv preprint arXiv:1910.11717},
 month = {10},
 pdf = {https://arxiv.org/pdf/1910.11717v2},
 primaryclass = {cs.PL},
 published = {2019-10-25T13:31:21Z},
 title = {Selective Lambda Lifting},
 updated = {2019-10-28T14:53:56Z},
 url = {https://arxiv.org/abs/1910.11717v2},
 year = {2019}
}

@article{arXiv:2006.04847,
 abstract = {Hashing is one of the most popular methods for similarity search because of its speed and efficiency. Dense binary hashing is prevalent in the literature. Recently, insect olfaction was shown to be structurally and functionally analogous to sparse hashing [6]. Here, we prove that this biological mechanism is the solution to a well-posed optimization problem. Furthermore, we show that orthogonality increases the accuracy of sparse hashing. Next, we present a novel method, Procrustean Orthogonal Sparse Hashing (POSH), that unifies these findings, learning an orthogonal transform from training data compatible with the sparse hashing mechanism. We provide theoretical evidence of the shortcomings of Optimal Sparse Lifting (OSL) [22] and BioHash [30], two related olfaction-inspired methods, and propose two new methods, Binary OSL and SphericalHash, to address these deficiencies. We compare POSH, Binary OSL, and SphericalHash to several state-of-the-art hashing methods and provide empirical results for the superiority of the proposed methods across a wide range of standard benchmarks and parameter settings.},
 archiveprefix = {arXiv},
 author = {Mariano Tepper and Dipanjan Sengupta and Ted Willke},
 categories = {cs.LG stat.ML},
 eprint = {2006.04847},
 journal = {arXiv preprint arXiv:2006.04847},
 month = {06},
 pdf = {https://arxiv.org/pdf/2006.04847v1},
 primaryclass = {cs.LG},
 published = {2020-06-08T18:09:33Z},
 title = {Procrustean Orthogonal Sparse Hashing},
 updated = {2020-06-08T18:09:33Z},
 url = {https://arxiv.org/abs/2006.04847v1},
 year = {2020}
}

@article{arXiv:2012.15860,
 abstract = {Strong, electromagnetic, and weak forces were unified in the Standard Model (SM) with spontaneous gauge symmetry breaking. These forces were further conjectured to be unified in a simple Lie group gauge interaction in the Grand Unification (GUT). In this work, we propose a theory beyond the SM and GUT by adding new gapped Topological Phase Sectors consistent with the nonperturbative global anomaly cancellation and cobordism constraints (especially from the baryon minus lepton number \$\{\\bf B\}-\{\\bf L\}\$, the electroweak hypercharge \$Y\$, and the mixed gauge-gravitational anomaly). Gapped Topological Phase Sectors are constructed via symmetry extension, whose low energy contains unitary Lorentz invariant topological quantum field theories (TQFTs): either 3+1d non-invertible TQFT, or 4+1d invertible or non-invertible TQFT (short-range or long-range entangled gapped phase). Alternatively, there could also be right-handed "sterile" neutrinos, gapless unparticle physics, more general interacting conformal field theories, or gravity with topological cobordism constraints, or their combinations to altogether cancel the mixed gauge-gravitational anomaly. We propose that a new high-energy physics frontier beyond the conventional 0d particle physics relies on the new Topological Force and Topological Matter including gapped extended objects (gapped 1d line and 2d surface operators or defects, etc., whose open ends carry deconfined fractionalized particle or anyonic string excitations) or gapless conformal matter. Physical characterizations of these gapped extended objects require the mathematical theories of cohomology, cobordism, or category. Although weaker than the weak force, Topological Force is infinite-range or long-range which does not decay in the distance, and mediates between the linked worldvolume trajectories via fractional or categorical statistical interactions.},
 archiveprefix = {arXiv},
 author = {Juven Wang},
 categories = {hep-th cond-mat.str-el hep-ph},
 comment = {38 pages. Sequel to: arXiv:1910.14668, arXiv:2006.16996, arXiv:2008.06499. Supplementary: arXiv:1809.11171, arXiv:1705.06728. Physical Review journal version accepted subtitle: Unified model beyond grand unification. v3: refinement, and more clarifications on math/physics and neutrino masses. Related talks: https://www.youtube.com/results?search\_query=Ultra+Unification+Quantum+Criticality+Wang},
 doi = {10.1103/PhysRevD.103.105024},
 eprint = {2012.15860},
 journal = {arXiv preprint arXiv:2012.15860},
 month = {12},
 note = {Journal reference: Phys. Rev. D 103, 105024 (2021)},
 pdf = {https://arxiv.org/pdf/2012.15860v3},
 primaryclass = {hep-th},
 published = {2020-12-31T18:59:50Z},
 title = {Ultra Unification},
 updated = {2022-02-01T15:00:00Z},
 url = {https://arxiv.org/abs/2012.15860v3},
 year = {2020}
}

@article{arXiv:2106.16248,
 abstract = {Standard lore views our 4d quantum vacuum governed by one of the candidate Standard Models (SMs), while lifting towards some Grand Unification-like structure (GUT) at higher energy scales. In contrast, in our work, we introduce an alternative view that the SM arises from various neighbor vacua competition in a quantum phase diagram. In general, we regard the SM arising near the gapless quantum criticality (either critical points or critical regions) between the competing neighbor vacua. In particular, we demonstrate how the \$su(3)\\times su(2)\\times u(1)\$ SM with 16n Weyl fermions arises near the quantum criticality between the GUT competition of Georgi-Glashow (GG) \$su(5)\$ and Pati-Salam (PS) \$su(4)\\times su(2)\\times su(2)\$. We propose two enveloping toy models. Model I is a conventional \$so(10)\$ GUT with a Spin(10) gauge group plus GUT-Higgs potential inducing various Higgs transitions. Model II modifies Model I plus a 4d discrete torsion Wess-Zumino-Witten-like term built from GUT-Higgs field (that matches a nonperturbative global mixed gauge-gravity anomaly captured by a 5d invertible topological field theory \$w\_2w\_3\$), which manifests a Beyond-Landau-Ginzburg criticality between GG and PS models, with extra Beyond-the-Standard-Model (BSM) excitations emerging near a quantum critical region. If the internal symmetries were treated as global symmetries, we show a gapless 4d deconfined quantum criticality with new BSM fractionalized fragmentary excitations of Color-Flavor separation, and gauge enhancement including a Dark Gauge force sector, altogether requiring a double fermionic Spin structure named DSpin. If the internal symmetries are dynamically gauged, we show a 4d boundary criticality such that only appropriately gauge enhanced dynamical GUT gauge fields propagate into an extra-dimensional 5d bulk. The phenomena may be regarded as SM deformation or morphogenesis.},
 archiveprefix = {arXiv},
 author = {Juven Wang and Yi-Zhuang You},
 categories = {hep-th cond-mat.str-el hep-lat hep-ph},
 comment = {65 pages. Primers: https://www.youtube.com/results?search\_query=ultra+unification+quantum+crticiality+deformation+standard+model. Dedicate to Subir Sachdev (60), Xiao-Gang Wen (60), Edward Witten (70), Shing-Tung Yau (72). v3: Add more figures, motivations, comments on proton decay, non-perturbative effects on perturbatively irrelevant deformations. Sequel: arXiv:2111.10369, arXiv:2112.14765},
 doi = {10.1103/PhysRevD.106.025013},
 eprint = {2106.16248},
 journal = {arXiv preprint arXiv:2106.16248},
 month = {06},
 note = {Journal reference: Phys. Rev. D 106, 025013 (2022)},
 pdf = {https://arxiv.org/pdf/2106.16248v3},
 primaryclass = {hep-th},
 published = {2021-06-30T17:59:55Z},
 title = {Gauge Enhanced Quantum Criticality Beyond the Standard Model},
 updated = {2022-02-22T22:22:22Z},
 url = {https://arxiv.org/abs/2106.16248v3},
 year = {2021}
}

@article{arXiv:2112.14765,
 abstract = {'t Hooft anomalies of quantum field theories (QFTs) with an invertible global symmetry G (including spacetime and internal symmetries) in a \$d\$d spacetime are known to be classified by a \$d+1\$d cobordism group TP\$\_\{d+1\}\$(G), whose group generator is a \$d+1\$d cobordism invariant written as an invertible topological field theory (iTFT) Z\$\_\{d+1\}\$. The deformation class of QFT is recently proposed to be specified by its symmetry G and an iTFT Z\$\_\{d+1\}\$. Seemingly different QFTs of the same deformation class can be deformed to each other via quantum phase transitions. In this work, we ask which deformation class controls the 4d ungauged or gauged (SU(3)\$\\times\$SU(2)\$\\times\$U(1))/\$\\mathbb\{Z\}\_q\$ Standard Model (SM) for \$q=1,2,3,6\$ with a continuous or discrete \$(\\bf\{B\}-\\bf\{L\})\$ symmetry. We show that the answer contains some combination of 5d iTFTs: two \$\\mathbb\{Z\}\$ classes associated with \$(\\bf\{B\}-\\bf\{L\})\textasciicircum{}3\$ and \$(\\bf\{B\}-\\bf\{L\})\$-(gravity)\$\textasciicircum{}2\$ 4d perturbative local anomalies, a mod 16 class Atiyah-Patodi-Singer \$η\$ invariant and a mod 2 class Stiefel-Whitney \$w\_2w\_3\$ invariant associated with 4d nonperturbative global anomalies, and additional \$\\mathbb\{Z\}\_3\\times\\mathbb\{Z\}\_2\$ classes involving higher symmetries whose charged objects are Wilson electric or 't Hooft magnetic line operators. Out of \$\\mathbb\{Z\}\$ classes of local anomalies and 24576 classes of global anomalies, we pin down a deformation class of SM labeled by \$(N\_f,n\_\{ν\_\{R\}\},\$ p\$',q)\$, the family and "right-handed sterile" neutrino numbers, magnetic monopole datum, and mod \$q\$ relation. Grand Unifications and Ultra Unification that replaces sterile neutrinos with new exotic gapped/gapless sectors (e.g., topological or conformal field theory) or gravitational sectors with topological or cobordism constraints, all reside in an SM deformation class. Neighbor phases/transitions/critical regions near SM exhibit beyond SM phenomena.},
 archiveprefix = {arXiv},
 author = {Juven Wang and Zheyan Wan and Yi-Zhuang You},
 categories = {hep-th cond-mat.str-el hep-lat hep-ph math-ph},
 comment = {6 pages. Sequel to arXiv:1910.14668, arXiv:2006.16996, arXiv:2008.06499, arXiv:2012.15860, arXiv:2106.16248, arXiv:2111.10369, arXiv:2202.13498, arXiv:2204.08393. Related prior talks: https://www.youtube.com/results?search\_query=cobordism+deformation+standard+model+ultra+unification+crticiality. v4: Phys. Rev. D (Letter)},
 doi = {10.1103/PhysRevD.106.L041701},
 eprint = {2112.14765},
 journal = {arXiv preprint arXiv:2112.14765},
 month = {12},
 note = {Journal reference: Phys. Rev. D 106, L041701 (2022)},
 pdf = {https://arxiv.org/pdf/2112.14765v4},
 primaryclass = {hep-th},
 published = {2021-12-29T18:59:51Z},
 title = {Cobordism and Deformation Class of the Standard Model},
 updated = {2022-08-18T15:15:15Z},
 url = {https://arxiv.org/abs/2112.14765v4},
 year = {2021}
}

@article{arXiv:2203.11386,
 abstract = {The growing interest in explainable artificial intelligence (XAI) for critical decision making motivates the need for interpretable machine learning (ML) models. In fact, due to their structure (especially with small sizes), these models are inherently understandable by humans. Recently, several exact methods for computing such models are proposed to overcome weaknesses of traditional heuristic methods by providing more compact models or better prediction quality. Despite their compressed representation of Boolean functions, Binary decision diagrams (BDDs) did not gain enough interest as other interpretable ML models. In this paper, we first propose SAT-based models for learning optimal BDDs (in terms of the number of features) that classify all input examples. Then, we lift the encoding to a MaxSAT model to learn optimal BDDs in limited depths, that maximize the number of examples correctly classified. Finally, we tackle the fragmentation problem by introducing a method to merge compatible subtrees for the BDDs found via the MaxSAT model. Our empirical study shows clear benefits of the proposed approach in terms of prediction quality and intrepretability (i.e., lighter size) compared to the state-of-the-art approaches.},
 archiveprefix = {arXiv},
 author = {Hao Hu and Marie-José Huguet and Mohamed Siala},
 categories = {cs.AI cs.LG},
 comment = {This is the preprint version of the paper accepted in AAAI'22},
 eprint = {2203.11386},
 journal = {arXiv preprint arXiv:2203.11386},
 month = {03},
 pdf = {https://arxiv.org/pdf/2203.11386v1},
 primaryclass = {cs.AI},
 published = {2022-03-21T23:17:37Z},
 title = {Optimizing Binary Decision Diagrams with MaxSAT for classification},
 updated = {2022-03-21T23:17:37Z},
 url = {https://arxiv.org/abs/2203.11386v1},
 year = {2022}
}

@article{arXiv:2204.08393,
 abstract = {A proton is known for its longevity, but what is its lifetime? While many Grand Unified Theories predict the proton decay with a finite lifetime, we show that the Standard Model (SM) and some versions of Ultra Unification (which replace sterile neutrinos with new exotic gapped/gapless sectors, e.g., topological or conformal field theory under global anomaly cancellation constraints) with a discrete baryon plus lepton symmetry permit a stable proton. For the 4d SM with \$N\_f\$ families of 15 or 16 Weyl fermions, in addition to the continuous baryon minus lepton U(1)\$\_\{\\bf B - L\}\$ symmetry, there is also a compatible discrete baryon plus lepton \$\\mathbb\{Z\}\_\{2N\_f, \\bf B + L\}\$ symmetry. The \$\\mathbb\{Z\}\_\{2N\_f, \\bf B + L\}\$ is discrete due to the ABJ anomaly under the BPST SU(2) instanton. Although both U(1)\$\_\{\\bf B - L\}\$ and \$\\mathbb\{Z\}\_\{2N\_f, \\bf B + L\}\$ symmetries are anomaly-free under the dynamical SM gauge field, it is important to check whether they have mixed anomalies with the gravitational background field and higher symmetries (whose charged objects are Wilson electric or 't Hooft magnetic line operators) of SM. We can also replace the U(1)\$\_\{\\bf B - L\}\$ with a discrete variant \$\\mathbb\{Z\}\_\{4,X\}\$ for \$X \\equiv 5(\{\\bf B - L\})-\\frac\{2\}\{3\} \{\\tilde Y\}\$ of electroweak hypercharge \$\{\\tilde Y\}\$. We explore a systematic classification of candidate perturbative local and nonperturbative global anomalies of the 4d SM, including all these gauge and gravitational backgrounds, via a cobordism theory, which controls the SM's deformation class. We discuss the proton stability of the SM and Ultra Unification in the presence of discrete \$\{\\bf B + L\}\$ symmetry protection, in particular (U(1)\$\_\{\\bf B - L\} \\times \\mathbb\{Z\}\_\{2N\_f,\\bf B + L\})/\{\\mathbb\{Z\}\_2\textasciicircum{}\{\\rm F\}\}\$ or \$(\\mathbb\{Z\}\_\{4,X\} \\times \\mathbb\{Z\}\_\{2N\_f, \\bf B + L\})/\{\\mathbb\{Z\}\_2\textasciicircum{}\{\\rm F\}\}\$ symmetry with the fermion parity \$\\mathbb\{Z\}\_2\textasciicircum{}\{\\rm F\}\$.},
 archiveprefix = {arXiv},
 author = {Juven Wang and Zheyan Wan and Yi-Zhuang You},
 categories = {hep-ph cond-mat.str-el hep-lat hep-th},
 comment = {6 pages. Sequel to arXiv:2112.14765. v2: refinement. Related talks: https://www.youtube.com/results?search\_query=cobordism+deformation+standard+model+crticiality. The original title "Proton Stability: From the Standard Model to Ultra Unification" is adjusted in PRD},
 doi = {10.1103/PhysRevD.106.025016},
 eprint = {2204.08393},
 journal = {arXiv preprint arXiv:2204.08393},
 month = {04},
 note = {Journal reference: Phys. Rev. D 106, 025016 (2022)},
 pdf = {https://arxiv.org/pdf/2204.08393v2},
 primaryclass = {hep-ph},
 published = {2022-04-18T16:45:01Z},
 title = {Proton Stability: From the Standard Model to Beyond Grand Unification},
 updated = {2022-08-18T16:16:16Z},
 url = {https://arxiv.org/abs/2204.08393v2},
 year = {2022}
}

@article{arXiv:2212.09239,
 abstract = {The non-interactive source simulation (NISS) scenario is considered. In this scenario, a pair of distributed agents, Alice and Bob, observe a distributed binary memoryless source \$(X\textasciicircum{}d,Y\textasciicircum{}d)\$ generated based on joint distribution \$P\_\{X,Y\}\$. The agents wish to produce a pair of discrete random variables \$(U\_d,V\_d)\$ with joint distribution \$P\_\{U\_d,V\_d\}\$, such that \$P\_\{U\_d,V\_d\}\$ converges in total variation distance to a target distribution \$Q\_\{U,V\}\$ as the input blocklength \$d\$ is taken to be asymptotically large. Inner and outer bounds are obtained on the set of distributions \$Q\_\{U,V\}\$ which can be produced given an input distribution \$P\_\{X,Y\}\$. To this end, a bijective mapping from the set of distributions \$Q\_\{U,V\}\$ to a union of star-convex sets is provided. By leveraging proof techniques from discrete Fourier analysis along with a novel randomized rounding technique, inner and outer bounds are derived for each of these star-convex sets, and by inverting the aforementioned bijective mapping, necessary and sufficient conditions on \$Q\_\{U,V\}\$ and \$P\_\{X,Y\}\$ are provided under which \$Q\_\{U,V\}\$ can be produced from \$P\_\{X,Y\}\$. The bounds are applicable in NISS scenarios where the output alphabets \$\\mathcal\{U\}\$ and \$\\mathcal\{V\}\$ have arbitrary finite size. In case of binary output alphabets, the outer-bound recovers the previously best-known outer-bound.},
 archiveprefix = {arXiv},
 author = {Farhad Shirani and Mohsen Heidari},
 categories = {cs.IT cs.CR eess.SY math.PR},
 eprint = {2212.09239},
 journal = {arXiv preprint arXiv:2212.09239},
 month = {12},
 pdf = {https://arxiv.org/pdf/2212.09239v1},
 primaryclass = {cs.IT},
 published = {2022-12-19T04:25:49Z},
 title = {On Non-Interactive Source Simulation via Fourier Transform},
 updated = {2022-12-19T04:25:49Z},
 url = {https://arxiv.org/abs/2212.09239v1},
 year = {2022}
}

@article{arXiv:2302.08760,
 abstract = {Existing lifting networks for regressing 3D human poses from 2D single-view poses are typically constructed with linear layers based on graph-structured representation learning. In sharp contrast to them, this paper presents Grid Convolution (GridConv), mimicking the wisdom of regular convolution operations in image space. GridConv is based on a novel Semantic Grid Transformation (SGT) which leverages a binary assignment matrix to map the irregular graph-structured human pose onto a regular weave-like grid pose representation joint by joint, enabling layer-wise feature learning with GridConv operations. We provide two ways to implement SGT, including handcrafted and learnable designs. Surprisingly, both designs turn out to achieve promising results and the learnable one is better, demonstrating the great potential of this new lifting representation learning formulation. To improve the ability of GridConv to encode contextual cues, we introduce an attention module over the convolutional kernel, making grid convolution operations input-dependent, spatial-aware and grid-specific. We show that our fully convolutional grid lifting network outperforms state-of-the-art methods with noticeable margins under (1) conventional evaluation on Human3.6M and (2) cross-evaluation on MPI-INF-3DHP. Code is available at https://github.com/OSVAI/GridConv},
 archiveprefix = {arXiv},
 author = {Yangyuxuan Kang and Yuyang Liu and Anbang Yao and Shandong Wang and Enhua Wu},
 categories = {cs.CV cs.AI cs.LG},
 comment = {Oral paper at AAAI 2023. Project website: https://github.com/OSVAI/GridConv},
 eprint = {2302.08760},
 journal = {arXiv preprint arXiv:2302.08760},
 month = {02},
 pdf = {https://arxiv.org/pdf/2302.08760v1},
 primaryclass = {cs.CV},
 published = {2023-02-17T08:52:16Z},
 title = {3D Human Pose Lifting with Grid Convolution},
 updated = {2023-02-17T08:52:16Z},
 url = {https://arxiv.org/abs/2302.08760v1},
 year = {2023}
}

@article{arXiv:2304.09842,
 abstract = {Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54\% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37\%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0\%, lifting the state of the art to 98.78\%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.},
 archiveprefix = {arXiv},
 author = {Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Jianfeng Gao},
 categories = {cs.CL cs.AI cs.CV cs.LG},
 comment = {32 pages, 10 figures, 24 tables. Accepted to NeurIPS 2023},
 eprint = {2304.09842},
 journal = {arXiv preprint arXiv:2304.09842},
 month = {04},
 pdf = {https://arxiv.org/pdf/2304.09842v3},
 primaryclass = {cs.CL},
 published = {2023-04-19T17:47:47Z},
 title = {Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
 updated = {2023-10-31T17:43:39Z},
 url = {https://arxiv.org/abs/2304.09842v3},
 year = {2023}
}

@article{arXiv:2304.12866,
 abstract = {Deep learning needs high-precision handling of forwarding signals, backpropagating errors, and updating weights. This is inherently required by the learning algorithm since the gradient descent learning rule relies on the chain product of partial derivatives. However, it is challenging to implement deep learning in hardware systems that use noisy analog memristors as artificial synapses, as well as not being biologically plausible. Memristor-based implementations generally result in an excessive cost of neuronal circuits and stringent demands for idealized synaptic devices. Here, we demonstrate that the requirement for high precision is not necessary and that more efficient deep learning can be achieved when this requirement is lifted. We propose a binary stochastic learning algorithm that modifies all elementary neural network operations, by introducing (i) stochastic binarization of both the forwarding signals and the activation function derivatives, (ii) signed binarization of the backpropagating errors, and (iii) step-wised weight updates. Through an extensive hybrid approach of software simulation and hardware experiments, we find that binary stochastic deep learning systems can provide better performance than the software-based benchmarks using the high-precision learning algorithm. Also, the binary stochastic algorithm strongly simplifies the neural network operations in hardware, resulting in an improvement of the energy efficiency for the multiply-and-accumulate operations by more than three orders of magnitudes.},
 archiveprefix = {arXiv},
 author = {Yang Li and Wei Wang and Ming Wang and Chunmeng Dou and Zhengyu Ma and Huihui Zhou and Peng Zhang and Nicola Lepri and Xumeng Zhang and Qing Luo and Xiaoxin Xu and Guanhua Yang and Feng Zhang and Ling Li and Daniele Ielmini and Ming Liu},
 categories = {cs.NE cs.LG eess.SP physics.data-an},
 doi = {10.1002/aisy.202300399},
 eprint = {2304.12866},
 journal = {arXiv preprint arXiv:2304.12866},
 month = {04},
 note = {Journal reference: Adv. Intel. Sys., 6(1), 2300399, 2024},
 pdf = {https://arxiv.org/pdf/2304.12866v1},
 primaryclass = {cs.NE},
 published = {2023-04-25T14:38:36Z},
 title = {Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy},
 updated = {2023-04-25T14:38:36Z},
 url = {https://arxiv.org/abs/2304.12866v1},
 year = {2023}
}

@article{arXiv:2306.03542,
 abstract = {Machine learning is typically framed from a perspective of i.i.d., and more importantly, isolated data. In parts, federated learning lifts this assumption, as it sets out to solve the real-world challenge of collaboratively learning a shared model from data distributed across clients. However, motivated primarily by privacy and computational constraints, the fact that data may change, distributions drift, or even tasks advance individually on clients, is seldom taken into account. The field of continual learning addresses this separate challenge and first steps have recently been taken to leverage synergies in distributed supervised settings, in which several clients learn to solve changing classification tasks over time without forgetting previously seen ones. Motivated by these prior works, we posit that such federated continual learning should be grounded in unsupervised learning of representations that are shared across clients; in the loose spirit of how humans can indirectly leverage others' experience without exposure to a specific task. For this purpose, we demonstrate that masked autoencoders for distribution estimation are particularly amenable to this setup. Specifically, their masking strategy can be seamlessly integrated with task attention mechanisms to enable selective knowledge transfer between clients. We empirically corroborate the latter statement through several continual federated scenarios on both image and binary datasets.},
 archiveprefix = {arXiv},
 author = {Subarnaduti Paul and Lars-Joel Frey and Roshni Kamath and Kristian Kersting and Martin Mundt},
 categories = {cs.LG},
 eprint = {2306.03542},
 journal = {arXiv preprint arXiv:2306.03542},
 month = {06},
 pdf = {https://arxiv.org/pdf/2306.03542v2},
 primaryclass = {cs.LG},
 published = {2023-06-06T09:38:57Z},
 title = {Masked Autoencoders are Efficient Continual Federated Learners},
 updated = {2024-07-18T13:22:21Z},
 url = {https://arxiv.org/abs/2306.03542v2},
 year = {2023}
}

@article{arXiv:2308.04838,
 abstract = {Large language models (LLMs) have demonstrated impressive capabilities across various NLP tasks. Additionally, LLMs are also highly valuable in supporting software engineering tasks, particularly in the field of code generation. Automatic code generation is a process of automatically generating source code or executable code based on given specifications or requirements, improving developer productivity. In this study, we perform a systematic empirical assessment to the quality of code generation using ChatGPT. We leverage 728 algorithm problems in five languages (i.e., C, C++, Java, Python, and JavaScript) and 18 CWEs with 54 code scenarios for the code generation task. Our evaluation encompasses a comprehensive analysis of code snippets generated by ChatGPT, focusing on three critical aspects: correctness, complexity, and security. We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability) of facilitating code generation. By delving into the generated code and examining the experimental results, this work provides valuable insights into the performance of ChatGPT in tackling code generation tasks over the three critical aspects. Overall, our findings uncover potential issues and limitations that arise in the ChatGPT-based code generation and lay the groundwork for improving AI and LLM-based code generation techniques.},
 archiveprefix = {arXiv},
 author = {Zhijie Liu and Yutian Tang and Xiapu Luo and Yuming Zhou and Liang Feng Zhang},
 categories = {cs.SE},
 eprint = {2308.04838},
 journal = {arXiv preprint arXiv:2308.04838},
 month = {08},
 pdf = {https://arxiv.org/pdf/2308.04838v2},
 primaryclass = {cs.SE},
 published = {2023-08-09T10:01:09Z},
 title = {No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT},
 updated = {2024-04-13T04:58:47Z},
 url = {https://arxiv.org/abs/2308.04838v2},
 year = {2023}
}

@article{arXiv:2310.02295,
 abstract = {Advocating for a sustainable, resilient and human-centric industry, the three pillars of Industry 5.0 call for an increased understanding of industrial processes and manufacturing systems, as well as their energy sustainability. One of the most fundamental elements of comprehension is knowing when the systems are operated, as this is key to locating energy intensive subsystems and operations. Such knowledge is often lacking in practice. Activation statuses can be recovered from sensor data though. Some non-intrusive sensors (accelerometers, current sensors, etc.) acquire mixed signals containing information about multiple actuators at once. Despite their low cost as regards the fleet of systems they monitor, additional signal processing is required to extract the individual activation sequences. To that end, sparse regression techniques can extract leading dynamics in sequential data. Notorious dictionary learning algorithms have proven effective in this regard. This paper considers different industrial settings in which the identification of binary subsystem activation sequences is sought. In this context, it is assumed that each sensor measures an extensive physical property, source signals are periodic, quasi-stationary and independent, albeit these signals may be correlated and their noise distribution is arbitrary. Existing methods either restrict these assumptions, e.g., by imposing orthogonality or noise characteristics, or lift them using additional assumptions, typically using nonlinear transforms.},
 archiveprefix = {arXiv},
 author = {Romain Delabeye and Martin Ghienne and Olivia Penas and Jean-Luc Dion},
 author_affiliations = {Romain Delabeye: QUARTZ, ISAE-Supméca; Martin Ghienne: QUARTZ, ISAE-Supméca; Olivia Penas: QUARTZ, ISAE-Supméca; Jean-Luc Dion: QUARTZ, ISAE-Supméca},
 categories = {cs.LG eess.SP},
 eprint = {2310.02295},
 journal = {arXiv preprint arXiv:2310.02295},
 month = {10},
 pdf = {https://arxiv.org/pdf/2310.02295v1},
 primaryclass = {cs.LG},
 published = {2023-10-03T09:29:16Z},
 title = {Unsupervised Complex Semi-Binary Matrix Factorization for Activation Sequence Recovery of Quasi-Stationary Sources},
 updated = {2023-10-03T09:29:16Z},
 url = {https://arxiv.org/abs/2310.02295v1},
 year = {2023}
}

@article{arXiv:2403.01907,
 abstract = {In \\cite\{Hop82\}, Hopfield introduced a \\emph\{Hebbian\} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, \$m\$) scales linearly with each pattern's size, \$n\$. Moreover, he famously predicted \$α\_c=\\lim\_\{n\\rightarrow\\infty\}\\frac\{m\}\{n\}\\approx 0.14\$. We study this very same scenario with two famous pattern's basins of attraction: \\textbf\{\\emph\{(i)\}\} The AGS one from \\cite\{AmiGutSom85\}; and \\textbf\{\\emph\{(ii)\}\} The NLT one from \\cite\{Newman88,Louk94,Louk94a,Louk97,Tal98\}. Relying on the \\emph\{fully lifted random duality theory\} (fl RDT) from \\cite\{Stojnicflrdt23\}, we obtain the following explicit capacity characterizations on the first level of lifting: \\begin\{equation\} α\_c\textasciicircum{}\{(AGS,1)\} = \\left ( \\max\_\{δ\\in \\left ( 0,\\frac\{1\}\{2\}\\right ) \}\\frac\{1-2δ\}\{\\sqrt\{2\} \\mbox\{erfinv\} \\left ( 1-2δ\\right )\} - \\frac\{2\}\{\\sqrt\{2π\}\} e\textasciicircum{}\{-\\left ( \\mbox\{erfinv\}\\left ( 1-2δ\\right )\\right )\textasciicircum{}2\}\\right )\textasciicircum{}2 \\approx \\mathbf\{0.137906\} \\end\{equation\} \\begin\{equation\} α\_c\textasciicircum{}\{(NLT,1)\} = \\frac\{\\mbox\{erf\}(x)\textasciicircum{}2\}\{2x\textasciicircum{}2\}-1+\\mbox\{erf\}(x)\textasciicircum{}2 \\approx \\mathbf\{0.129490\}, \\quad 1-\\mbox\{erf\}(x)\textasciicircum{}2- \\frac\{2\\mbox\{erf\}(x)e\textasciicircum{}\{-x\textasciicircum{}2\}\}\{\\sqrtπx\}+\\frac\{2e\textasciicircum{}\{-2x\textasciicircum{}2\}\}π=0. \\end\{equation\} A substantial numerical work gives on the second level of lifting \$α\_c\textasciicircum{}\{(AGS,2)\} \\approx \\mathbf\{0.138186\}\$ and \$α\_c\textasciicircum{}\{(NLT,2)\} \\approx \\mathbf\{0.12979\}\$, effectively uncovering a remarkably fast lifting convergence. Moreover, the obtained AGS characterizations exactly match the replica symmetry based ones of \\cite\{AmiGutSom85\} and the corresponding symmetry breaking ones of \\cite\{SteKuh94\}.},
 archiveprefix = {arXiv},
 author = {Mihailo Stojnic},
 categories = {stat.ML cond-mat.dis-nn cs.IT cs.LG math.PR},
 eprint = {2403.01907},
 journal = {arXiv preprint arXiv:2403.01907},
 month = {03},
 pdf = {https://arxiv.org/pdf/2403.01907v1},
 primaryclass = {stat.ML},
 published = {2024-03-04T10:10:23Z},
 title = {Capacity of the Hebbian-Hopfield network associative memory},
 updated = {2024-03-04T10:10:23Z},
 url = {https://arxiv.org/abs/2403.01907v1},
 year = {2024}
}

@article{arXiv:2404.18098,
 abstract = {We present a theory of parameterized dynamic logic, namely DLp, for specifying and reasoning about a rich set of program models based on their transitional behaviours. Different from most dynamic logics that deal with regular expressions or a particular type of formalisms, DLp introduces a type of labels called "program configurations" as explicit program status for symbolic executions, allowing programs and formulas to be of arbitrary forms according to interested domains. This characteristic empowers dynamic logical formulas with a direct support of symbolic-execution-based reasoning, while still maintaining reasoning based on syntactic structures in traditional dynamic logics through a rule-lifting process. We propose a proof system and build a cyclic preproof structure special for DLp, which guarantees the soundness of infinite proof trees induced by symbolically executing programs with explicit/implicit loop structures. The soundness of DLp is formally analyzed and proved. DLp provides a flexible verification framework based on the theories of dynamic logics. It helps reduce the burden of developing different dynamic-logic theories for different programs, and save the additional transformations in the derivations of non-compositional programs. We give some examples of instantiations of DLp in particular domains, showing the potential and advantages of using DLp in practical usage.},
 archiveprefix = {arXiv},
 author = {Yuanrui Zhang},
 categories = {cs.LO cs.SE},
 comment = {Major revisions from last comments: 1. fix the whole proof system of DLp and its related proofs; 2. add additional two examples for illustrations of lifting processes and an implication of a more complex configuration; 3. further revise the introduction part to adapt these changes; 4. add a formal definition of while programs in the logic},
 eprint = {2404.18098},
 journal = {arXiv preprint arXiv:2404.18098},
 month = {04},
 pdf = {https://arxiv.org/pdf/2404.18098v4},
 primaryclass = {cs.LO},
 published = {2024-04-28T07:08:44Z},
 title = {Parameterized Dynamic Logic -- Towards A Cyclic Logical Framework for General Program Specification and Verification},
 updated = {2025-01-29T18:43:16Z},
 url = {https://arxiv.org/abs/2404.18098v4},
 year = {2024}
}

@article{arXiv:2404.18249,
 abstract = {Tensor processing infrastructures such as deep learning frameworks and specialized hardware accelerators have revolutionized how computationally intensive code from domains such as deep learning and image processing is executed and optimized. These infrastructures provide powerful and expressive abstractions while ensuring high performance. However, to utilize them, code must be written specifically using the APIs / ISAs of such software frameworks or hardware accelerators. Importantly, given the fast pace of innovation in these domains, code written today quickly becomes legacy as new frameworks and accelerators are developed, and migrating such legacy code manually is a considerable effort. To enable developers in leveraging such DSLs while preserving their current programming paradigm, we introduce Tenspiler, a verified lifting-based compiler that uses program synthesis to translate sequential programs written in general-purpose programming languages (e.g., C++ or Python code) into tensor operations. Central to Tenspiler is our carefully crafted yet simple intermediate language, named TensIR, that expresses tensor operations. TensIR enables efficient lifting, verification, and code generation. Currently, Tenspiler already supports \$\\textbf\{six\}\$ DSLs, spanning a broad spectrum of software and hardware environments. Furthermore, we show that new backends can be easily supported by Tenspiler by adding simple pattern-matching rules for TensIR. Using 10 real-world code benchmark suites, our experimental evaluation shows that by translating code to be executed on \$\\textbf\{6\}\$ different software frameworks and hardware devices, Tenspiler offers on average 105\$\\times\$ kernel and 9.65\$\\times\$ end-to-end execution time improvement over the fully-optimized sequential implementation of the same benchmarks.},
 archiveprefix = {arXiv},
 author = {Jie Qiu and Colin Cai and Sahil Bhatia and Niranjan Hasabnis and Sanjit A. Seshia and Alvin Cheung},
 categories = {cs.PL},
 eprint = {2404.18249},
 journal = {arXiv preprint arXiv:2404.18249},
 month = {04},
 pdf = {https://arxiv.org/pdf/2404.18249v3},
 primaryclass = {cs.PL},
 published = {2024-04-28T17:10:17Z},
 title = {Tenspiler: A Verified Lifting-Based Compiler for Tensor Operations (Extended Version)},
 updated = {2024-12-14T08:29:46Z},
 url = {https://arxiv.org/abs/2404.18249v3},
 year = {2024}
}

@article{arXiv:2406.10529,
 abstract = {Can a deep neural network be approximated by a small decision tree based on simple features? This question and its variants are behind the growing demand for machine learning models that are *interpretable* by humans. In this work we study such questions by introducing *interpretable approximations*, a notion that captures the idea of approximating a target concept \$c\$ by a small aggregation of concepts from some base class \$\\mathcal\{H\}\$. In particular, we consider the approximation of a binary concept \$c\$ by decision trees based on a simple class \$\\mathcal\{H\}\$ (e.g., of bounded VC dimension), and use the tree depth as a measure of complexity. Our primary contribution is the following remarkable trichotomy. For any given pair of \$\\mathcal\{H\}\$ and \$c\$, exactly one of these cases holds: (i) \$c\$ cannot be approximated by \$\\mathcal\{H\}\$ with arbitrary accuracy; (ii) \$c\$ can be approximated by \$\\mathcal\{H\}\$ with arbitrary accuracy, but there exists no universal rate that bounds the complexity of the approximations as a function of the accuracy; or (iii) there exists a constant \$κ\$ that depends only on \$\\mathcal\{H\}\$ and \$c\$ such that, for *any* data distribution and *any* desired accuracy level, \$c\$ can be approximated by \$\\mathcal\{H\}\$ with a complexity not exceeding \$κ\$. This taxonomy stands in stark contrast to the landscape of supervised classification, which offers a complex array of distribution-free and universally learnable scenarios. We show that, in the case of interpretable approximations, even a slightly nontrivial a-priori guarantee on the complexity of approximations implies approximations with constant (distribution-free and accuracy-free) complexity. We extend our trichotomy to classes \$\\mathcal\{H\}\$ of unbounded VC dimension and give characterizations of interpretability based on the algebra generated by \$\\mathcal\{H\}\$.},
 archiveprefix = {arXiv},
 author = {Marco Bressan and Nicolò Cesa-Bianchi and Emmanuel Esposito and Yishay Mansour and Shay Moran and Maximilian Thiessen},
 categories = {cs.LG cs.AI stat.ML},
 comment = {To appear at COLT 2024},
 eprint = {2406.10529},
 journal = {arXiv preprint arXiv:2406.10529},
 month = {06},
 pdf = {https://arxiv.org/pdf/2406.10529v1},
 primaryclass = {cs.LG},
 published = {2024-06-15T06:43:45Z},
 title = {A Theory of Interpretable Approximations},
 updated = {2024-06-15T06:43:45Z},
 url = {https://arxiv.org/abs/2406.10529v1},
 year = {2024}
}

@article{arXiv:2409.17115,
 abstract = {Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2\% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, FineWeb, FineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6\% over Mistral-7B, with 14.6\% for Llama-2-7B and 20.3\% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training. We are open-sourcing ProX with >500B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX},
 archiveprefix = {arXiv},
 author = {Fan Zhou and Zengzhi Wang and Qian Liu and Junlong Li and Pengfei Liu},
 categories = {cs.CL cs.AI cs.LG},
 comment = {47 pages, 13 figures, 34 tables},
 eprint = {2409.17115},
 journal = {arXiv preprint arXiv:2409.17115},
 month = {09},
 pdf = {https://arxiv.org/pdf/2409.17115v2},
 primaryclass = {cs.CL},
 published = {2024-09-25T17:28:13Z},
 title = {Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale},
 updated = {2025-02-14T16:44:08Z},
 url = {https://arxiv.org/abs/2409.17115v2},
 year = {2024}
}

@article{arXiv:2501.09201,
 abstract = {The rise of automated code generation tools, such as large language models (LLMs), has introduced new challenges in ensuring the correctness and efficiency of scientific software, particularly in complex kernels, where numerical stability, domain-specific optimizations, and precise floating-point arithmetic are critical. We propose a stepwise semantics lifting approach using an extended SPIRAL framework with symbolic execution and theorem proving to statically derive high-level code semantics from LLM-generated kernels. This method establishes a structured path for verifying the source code's correctness via a step-by-step lifting procedure to high-level specification. We conducted preliminary tests on the feasibility of this approach by successfully lifting GPT-generated fast Fourier transform code to high-level specifications.},
 archiveprefix = {arXiv},
 author = {Naifeng Zhang and Sanil Rao and Mike Franusich and Franz Franchetti},
 categories = {cs.PL cs.SC},
 comment = {Accepted at the Theory and Practice of Static Analysis Workshop (TPSA), in conjunction with the ACM SIGPLAN Symposium on Principles of Programming Languages (POPL), 2025},
 eprint = {2501.09201},
 journal = {arXiv preprint arXiv:2501.09201},
 month = {01},
 pdf = {https://arxiv.org/pdf/2501.09201v1},
 primaryclass = {cs.PL},
 published = {2025-01-15T23:24:32Z},
 title = {Towards Semantics Lifting for Scientific Computing: A Case Study on FFT},
 updated = {2025-01-15T23:24:32Z},
 url = {https://arxiv.org/abs/2501.09201v1},
 year = {2025}
}

@article{arXiv:2501.19259,
 abstract = {The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.},
 archiveprefix = {arXiv},
 author = {Amogh Joshi and Sourav Sanyal and Kaushik Roy},
 categories = {cs.RO cs.CV cs.LG cs.NE eess.SY},
 comment = {Accepted for publication at the International Joint Conference on Neural Networks (IJCNN) 2025},
 eprint = {2501.19259},
 journal = {arXiv preprint arXiv:2501.19259},
 month = {01},
 pdf = {https://arxiv.org/pdf/2501.19259v2},
 primaryclass = {cs.RO},
 published = {2025-01-31T16:17:03Z},
 title = {Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge},
 updated = {2025-04-26T18:37:29Z},
 url = {https://arxiv.org/abs/2501.19259v2},
 year = {2025}
}

@article{arXiv:2505.21577,
 abstract = {The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110\% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7\% to 62.9\% while reducing token usage by 95\%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster.},
 archiveprefix = {arXiv},
 author = {Huacan Wang and Ziyi Ni and Shuo Zhang and Shuo Lu and Sen Hu and Ziyang He and Chen Hu and Jiaye Lin and Yifu Guo and Ronghao Chen and Xin Li and Daxin Jiang and Yuntao Du and Pin Lyu},
 categories = {cs.SE cs.AI},
 comment = {A novel approach; Very practical},
 eprint = {2505.21577},
 journal = {arXiv preprint arXiv:2505.21577},
 month = {05},
 pdf = {https://arxiv.org/pdf/2505.21577v3},
 primaryclass = {cs.SE},
 published = {2025-05-27T08:35:05Z},
 title = {RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving},
 updated = {2025-08-25T13:40:36Z},
 url = {https://arxiv.org/abs/2505.21577v3},
 year = {2025}
}

@article{arXiv:2506.18240,
 abstract = {In this work, we introduce a novel Quadratic Binary Optimization (QBO) framework for training a quantized neural network. The framework enables the use of arbitrary activation and loss functions through spline interpolation, while Forward Interval Propagation addresses the nonlinearities and the multi-layered, composite structure of neural networks via discretizing activation functions into linear subintervals. This preserves the universal approximation properties of neural networks while allowing complex nonlinear functions accessible to quantum solvers, broadening their applicability in artificial intelligence. Theoretically, we derive an upper bound on the approximation error and the number of Ising spins required by deriving the sample complexity of the empirical risk minimization problem from an optimization perspective. A key challenge in solving the associated large-scale Quadratic Constrained Binary Optimization (QCBO) model is the presence of numerous constraints. To overcome this, we adopt the Quantum Conditional Gradient Descent (QCGD) algorithm, which solves QCBO directly on quantum hardware. We establish the convergence of QCGD under a quantum oracle subject to randomness, bounded variance, and limited coefficient precision, and further provide an upper bound on the Time-To-Solution. To enhance scalability, we further incorporate a decomposed copositive optimization scheme that replaces the monolithic lifted model with sample-wise subproblems. This decomposition substantially reduces the quantum resource requirements and enables efficient low-bit neural network training. We further propose the usage of QCGD and Quantum Progressive Hedging (QPH) algorithm to efficiently solve the decomposed problem.},
 archiveprefix = {arXiv},
 author = {Wenxin Li and Chuan Wang and Hongdong Zhu and Qi Gao and Yin Ma and Hai Wei and Kai Wen},
 categories = {cs.LG cs.AI physics.optics},
 eprint = {2506.18240},
 journal = {arXiv preprint arXiv:2506.18240},
 month = {06},
 pdf = {https://arxiv.org/pdf/2506.18240v4},
 primaryclass = {cs.LG},
 published = {2025-06-23T02:12:36Z},
 title = {Quantum-Classical Hybrid Quantized Neural Network},
 updated = {2025-12-08T06:41:28Z},
 url = {https://arxiv.org/abs/2506.18240v4},
 year = {2025}
}

@article{arXiv:2507.10624,
 abstract = {Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between \\textit\{comprehension\} and \\textit\{competence\}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying them--a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational \\textit\{split-brain syndrome\}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.},
 archiveprefix = {arXiv},
 author = {Zheng Zhang},
 categories = {cs.AI cs.LG},
 comment = {v2: Two TMLR revision rounds addressing reviewer feedback. Added real-world validation (3.4), interpretability analysis (7), computational hallucination framework, strengthened theory. v3: Sec 3.2 - added transformer architecture diagram, clarified UAT capacity vs computational limits, improved role specialization theorem presentation},
 eprint = {2507.10624},
 journal = {arXiv preprint arXiv:2507.10624},
 month = {07},
 pdf = {https://arxiv.org/pdf/2507.10624v3},
 primaryclass = {cs.AI},
 published = {2025-07-14T04:01:45Z},
 title = {Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning},
 updated = {2025-11-14T15:49:48Z},
 url = {https://arxiv.org/abs/2507.10624v3},
 year = {2025}
}

@article{arXiv:2507.11676,
 abstract = {Quantum programs today are written at a low level of abstraction - quantum circuits akin to assembly languages - and the unitary parts of even advanced quantum programming languages essentially function as circuit description languages. This state of affairs impedes scalability, clarity, and support for higher-level reasoning. More abstract and expressive quantum programming constructs are needed. To this end, we introduce a simple syntax for generating unitaries from "just a phase"; we combine a (global) phase operation that captures phase shifts with a quantum analogue of the "if let" construct that captures subspace selection via pattern matching. This minimal language lifts the focus from gates to eigendecomposition, conjugation, and controlled unitaries; common building blocks in quantum algorithm design. We demonstrate several aspects of the expressive power of our language in several ways. Firstly, we establish that our representation is universal by deriving a universal quantum gate set. Secondly, we show that important quantum algorithms can be expressed naturally and concisely, including Grover's search algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give clean denotational semantics grounded in categorical quantum mechanics. Finally, we implement a prototype compiler that efficiently translates terms of our language to quantum circuits, and prove that it is sound with respect to these semantics. Collectively, these contributions show that this construct offers a principled and practical step toward more abstract and structured quantum programming.},
 archiveprefix = {arXiv},
 author = {Chris Heunen and Louis Lemonnier and Christopher McNally and Alex Rice},
 categories = {cs.PL cs.LO quant-ph},
 comment = {42 pages, 5 figures},
 doi = {10.1145/3776731},
 eprint = {2507.11676},
 journal = {arXiv preprint arXiv:2507.11676},
 month = {07},
 pdf = {https://arxiv.org/pdf/2507.11676v2},
 primaryclass = {cs.PL},
 published = {2025-07-15T19:31:53Z},
 title = {Quantum Circuits Are Just a Phase},
 updated = {2025-12-01T16:26:39Z},
 url = {https://arxiv.org/abs/2507.11676v2},
 year = {2025}
}

@article{arXiv:2507.14570,
 abstract = {Graph Neural Networks (GNNs) have emerged as powerful tools for various graph mining tasks, yet existing scalable solutions often struggle to balance execution efficiency with prediction accuracy. These difficulties stem from iterative message-passing techniques, which place significant computational demands and require extensive GPU memory, particularly when dealing with the neighbor explosion issue inherent in large-scale graphs. This paper introduces a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN, which can perform representation learning on 100 billion graphs with a single GPU in 10 hours and shows a 13.8\% improvement in User Acquisition scenarios. We examine existing graph partitioning methods and design a superior graph partition algorithm named LPMetis. In particular, LPMetis outperforms current state-of-the-art (SOTA) approaches on various evaluation metrics. In addition, our paper proposes a subgraph augmentation strategy to enhance the model's predictive performance. It exhibits excellent compatibility, allowing the entire framework to accommodate various GNN algorithms. Successfully deployed on the Tencent platform, LPS-GNN has been tested on public and real-world datasets, achieving performance lifts of 8. 24\% to 13. 89\% over SOTA models in online applications.},
 archiveprefix = {arXiv},
 author = {Xu Cheng and Liang Yao and Feng He and Yukuo Cen and Yufei He and Chenhui Zhang and Wenzheng Feng and Hongyun Cai and Jie Tang},
 categories = {cs.LG cs.AI},
 eprint = {2507.14570},
 journal = {arXiv preprint arXiv:2507.14570},
 month = {07},
 pdf = {https://arxiv.org/pdf/2507.14570v1},
 primaryclass = {cs.LG},
 published = {2025-07-19T10:44:26Z},
 title = {LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges},
 updated = {2025-07-19T10:44:26Z},
 url = {https://arxiv.org/abs/2507.14570v1},
 year = {2025}
}

@article{arXiv:2509.24507,
 abstract = {Large Language Models (LLMs) can translate natural language requirements into code, yet empirical analyses of representative models reveal that semantic errors-programs that compile but behave incorrectly-constitute the majority of observed faults (e.g., >60\% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc repair pipelines detect such faults only after execution, incurring latency, relying on incomplete test suites, and often mis-localizing the defect. Since semantic drift originates in the autoregressive decoding process, intervening while the code is being generated is a direct way to stop error propagation. Constrained-decoding approaches such as ROCODE attempt this, but still wait until the entire program runs to obtain feedback and use entropy heuristics that do not truly capture semantics. A more effective solution must inject semantic signals-early and precisely-into the decoding process.We present SemGuard, a semantic-evaluator-driven framework that performs real-time, line-level semantic supervision. To train the evaluator, we build SemDiff, the first dataset with fine-grained annotations that mark the exact line where a correct and an incorrect implementation diverge. The evaluator, once embedded in the LLM's decoder, flags deviations on partial code, rolls back to the faulty line, and guides regeneration-without executing the program or requiring test cases. Across four benchmarks, SemGuard consistently outperforms state-of-the-art baselines. It lowers the semantic error rate by 19.86\% on SemDiff relative to ROCODE, and lifts Pass@1 by 48.92\% on the real-world LiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP and for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating model- and language-agnostic effectiveness.},
 archiveprefix = {arXiv},
 author = {Qinglin Wang and Zhihong Sun and Ruyun Wang and Tao Huang and Zhi Jin and Ge Li and Chen Lyu},
 categories = {cs.SE},
 comment = {Accepted by the 40th IEEE/ACM Automated Software Engineering Conference (ASE 2025)},
 eprint = {2509.24507},
 journal = {arXiv preprint arXiv:2509.24507},
 month = {09},
 pdf = {https://arxiv.org/pdf/2509.24507v1},
 primaryclass = {cs.SE},
 published = {2025-09-29T09:21:32Z},
 title = {SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code},
 updated = {2025-09-29T09:21:32Z},
 url = {https://arxiv.org/abs/2509.24507v1},
 year = {2025}
}

@article{arXiv:2511.05791,
 abstract = {Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.},
 archiveprefix = {arXiv},
 author = {Manav Kulshrestha and S. Talha Bukhari and Damon Conover and Aniket Bera},
 categories = {cs.RO cs.AI cs.LG},
 comment = {8 pages, 4 figures, under review},
 eprint = {2511.05791},
 journal = {arXiv preprint arXiv:2511.05791},
 month = {11},
 pdf = {https://arxiv.org/pdf/2511.05791v1},
 primaryclass = {cs.RO},
 published = {2025-11-08T01:47:40Z},
 title = {VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models},
 updated = {2025-11-08T01:47:40Z},
 url = {https://arxiv.org/abs/2511.05791v1},
 year = {2025}
}

@article{arXiv:2512.06155,
 abstract = {Security research is fundamentally a problem of resource constraint and consequent prioritization. There is simply too much attack surface and too little time and energy to spend analyzing it all. The most effective security researchers are often those who are most skilled at intuitively deciding which part of an expansive attack surface to investigate. We demonstrate that this problem of selecting the most promising option from among many possibilities can be reframed as an information retrieval problem, and solved using document ranking techniques with LLMs performing the heavy lifting as general-purpose rankers. We present SiftRank, a ranking algorithm achieving O(n) complexity through three key mechanisms: listwise ranking using an LLM to order documents in small batches of approximately 10 items at a time; inflection-based convergence detection that adaptively terminates ranking when score distributions have stabilized; and iterative refinement that progressively focuses ranking effort on the most relevant documents. Unlike existing reranking approaches that require a separate first-stage retrieval step to narrow datasets to approximately 100 candidates, SiftRank operates directly on thousands of items, with each document evaluated across multiple randomized batches to mitigate inconsistent judgments by an LLM. We demonstrate practical effectiveness on N-day vulnerability analysis, successfully identifying a vulnerability-fixing function among 2,197 changed functions in a stripped binary firmware patch within 99 seconds at an inference cost of \$0.82. Our approach enables scalable security prioritization for problems that are generally constrained by manual analysis, requiring only standard LLM API access without specialized infrastructure, embedding, or domain-specific fine-tuning. An open-source implementation of SiftRank may be found at https://github.com/noperator/siftrank.},
 archiveprefix = {arXiv},
 author = {Caleb Gross},
 categories = {cs.CR cs.IR},
 eprint = {2512.06155},
 journal = {arXiv preprint arXiv:2512.06155},
 month = {12},
 pdf = {https://arxiv.org/pdf/2512.06155v1},
 primaryclass = {cs.CR},
 published = {2025-12-05T21:09:32Z},
 title = {Sift or Get Off the PoC: Applying Information Retrieval to Vulnerability Research with SiftRank},
 updated = {2025-12-05T21:09:32Z},
 url = {https://arxiv.org/abs/2512.06155v1},
 year = {2025}
}

@article{arXiv:2512.24296,
 abstract = {After a brief historical perspective, we introduce the key notions of work and heat for quantum systems, to then apply them to quantum engines operating on quantum Otto and Carnot cycles. The irreversible and dissipative character of the quantum Otto cycle is briefly analyzed, contrasting with the energetic optimality of the quantum Carnot cycle. The central question of quantum effects is also addressed and illustrated with several examples. Finally, the last part strives to explain the role that quantum thermodynamics plays for quantum applications and quantum technologies, particularly in relation to energy optimization and the trade-off between performances and energy costs.},
 archiveprefix = {arXiv},
 author = {Camille L Latune},
 categories = {quant-ph},
 comment = {in French language, Contribution to the celebration of the 200 years of Sadi Carnot's book "Reflections on the Motive Power of Fire", in French and part of the book "Chaleur, énergie, thermodynamique: le message de Carnot aujourd'hui... 200 ans après", direction G. Bertrand, Ed. Univ. de Dijon (2025). https://eud.ube.fr/sciences/891-chaleur-energie-thermodynamique-9782364415829.html?search\_query=carnot\&results=3},
 eprint = {2512.24296},
 journal = {arXiv preprint arXiv:2512.24296},
 month = {12},
 note = {Journal reference: "Chaleur, Energie, Thermodynamique: le message de Carnot aujourd'hui... 200 ans apres", sous la direction de Gilles Bertrand - Editions universitaires de Dijon (2025)},
 pdf = {https://arxiv.org/pdf/2512.24296v1},
 primaryclass = {quant-ph},
 published = {2025-12-30T15:36:24Z},
 title = {Quantum Thermodynamics and Quantum Perspectives},
 updated = {2025-12-30T15:36:24Z},
 url = {https://arxiv.org/abs/2512.24296v1},
 year = {2025}
}

@article{arXiv:2601.05887,
 abstract = {AI-driven penetration testing now executes thousands of actions per hour but still lacks the strategic intuition humans apply in competitive security. To build cybersecurity superintelligence --Cybersecurity AI exceeding best human capability-such strategic intuition must be embedded into agentic reasoning processes. We present Generative Cut-the-Rope (G-CTR), a game-theoretic guidance layer that extracts attack graphs from agent's context, computes Nash equilibria with effort-aware scoring, and feeds a concise digest back into the LLM loop \\emph\{guiding\} the agent's actions. Across five real-world exercises, G-CTR matches 70--90\% of expert graph structure while running 60--245x faster and over 140x cheaper than manual analysis. In a 44-run cyber-range, adding the digest lifts success from 20.0\% to 42.9\%, cuts cost-per-success by 2.7x, and reduces behavioral variance by 5.2x. In Attack-and-Defense exercises, a shared digest produces the Purple agent, winning roughly 2:1 over the LLM-only baseline and 3.7:1 over independently guided teams. This closed-loop guidance is what produces the breakthrough: it reduces ambiguity, collapses the LLM's search space, suppresses hallucinations, and keeps the model anchored to the most relevant parts of the problem, yielding large gains in success rate, consistency, and reliability.},
 archiveprefix = {arXiv},
 author = {Víctor Mayoral-Vilches and María Sanz-Gómez and Francesco Balassone and Stefan Rass and Lidia Salas-Espejo and Benjamin Jablonski and Luis Javier Navarrete-Lozano and Maite del Mundo de Torres and Cristóbal R. J. Veas Chavez},
 categories = {cs.CR},
 eprint = {2601.05887},
 journal = {arXiv preprint arXiv:2601.05887},
 month = {01},
 pdf = {https://arxiv.org/pdf/2601.05887v1},
 primaryclass = {cs.CR},
 published = {2026-01-09T16:06:10Z},
 title = {Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense},
 updated = {2026-01-09T16:06:10Z},
 url = {https://arxiv.org/abs/2601.05887v1},
 year = {2026}
}
