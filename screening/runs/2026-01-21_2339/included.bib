@article{arXiv:2101.08116,
 abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1\% F1-measure, whereas the best decompiler obtains a 30\% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers.},
 archiveprefix = {arXiv},
 author = {Javier Escalada and Ted Scully and Francisco Ortin},
 author_affiliations = {Javier Escalada: University of Oviedo; Ted Scully: Cork Institute of Technology; Francisco Ortin: University of Oviedo},
 categories = {cs.SE cs.LG cs.PL},
 eprint = {2101.08116},
 journal = {arXiv preprint arXiv:2101.08116},
 month = {01},
 pdf = {https://arxiv.org/pdf/2101.08116v2},
 primaryclass = {cs.SE},
 published = {2021-01-19T11:45:46Z},
 title = {Improving type information inferred by decompilers with supervised machine learning},
 updated = {2021-02-24T11:01:27Z},
 url = {https://arxiv.org/abs/2101.08116v2},
 year = {2021}
}

@article{arXiv:2103.05221,
 abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, "natural" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to "decompile" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
 archiveprefix = {arXiv},
 author = {Toufique Ahmed and Premkumar Devanbu and Anand Ashok Sawant},
 categories = {cs.SE},
 doi = {10.1109/TSE.2021.3106572},
 eprint = {2103.05221},
 journal = {arXiv preprint arXiv:2103.05221},
 month = {03},
 note = {Journal reference: Transactions on Software Engineering (2021)},
 pdf = {https://arxiv.org/pdf/2103.05221v2},
 primaryclass = {cs.SE},
 published = {2021-03-09T04:48:03Z},
 title = {Learning to Find Usages of Library Functions in Optimized Binaries},
 updated = {2021-09-17T01:34:54Z},
 url = {https://arxiv.org/abs/2103.05221v2},
 year = {2021}
}

@article{arXiv:2105.05159,
 abstract = {There is increasing interest in applying verification tools to programs that have bitvector operations (eg., binaries). SMT solvers, which serve as a foundation for these tools, have thus increased support for bitvector reasoning through bit-blasting and linear arithmetic approximations. In this paper we show that similar linear arithmetic approximation of bitvector operations can be done at the source level through transformations. Specifically, we introduce new paths that over-approximate bitvector operations with linear conditions/constraints, increasing branching but allowing us to better exploit the well-developed integer reasoning and interpolation of verification tools. We show that, for reachability of bitvector programs, increased branching incurs negligible overhead yet, when combined with integer interpolation optimizations, enables more programs to be verified. We further show this exploitation of integer interpolation in the common case also enables competitive termination verification of bitvector programs and leads to the first effective technique for LTL verification of bitvector programs. Finally, we provide an in-depth case study of decompiled ("lifted") binary programs, which emulate X86 execution through frequent use of bitvector operations. We present a new tool DarkSea, the first tool capable of verifying reachability, termination, and LTL of lifted binaries.},
 archiveprefix = {arXiv},
 author = {Yuandong Cyrus Liu and Chengbin Pang and Daniel Dietsch and Eric Koskinen and Ton-Chanh Le and Georgios Portokalidis and Jun Xu},
 author_affiliations = {Yuandong Cyrus Liu: Stevens Institute of Technology; Chengbin Pang: Stevens Institute of Technology; Daniel Dietsch: University of Freiburg; Eric Koskinen: Stevens Institute of Technology; Ton-Chanh Le: Stevens Institute of Technology; Georgios Portokalidis: Stevens Institute of Technology; Jun Xu: Stevens Institute of Technology},
 categories = {cs.PL cs.FL cs.SE eess.SY},
 comment = {39 pages(including Appendix), 10 tables, 4 Postscript figures, accepted to APLAS 2021},
 eprint = {2105.05159},
 journal = {arXiv preprint arXiv:2105.05159},
 month = {05},
 pdf = {https://arxiv.org/pdf/2105.05159v2},
 primaryclass = {cs.PL},
 published = {2021-05-11T16:12:02Z},
 title = {Proving LTL Properties of Bitvector Programs and Decompiled Binaries (Extended)},
 updated = {2021-08-28T05:44:26Z},
 url = {https://arxiv.org/abs/2105.05159v2},
 year = {2021}
}

@article{arXiv:2108.07639,
 abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
 archiveprefix = {arXiv},
 author = {Jordi Armengol-Estapé and Michael F. P. O'Boyle},
 categories = {cs.AI cs.PL},
 comment = {Published in AIPLANS 2021},
 eprint = {2108.07639},
 journal = {arXiv preprint arXiv:2108.07639},
 month = {08},
 note = {Journal reference: Armengol-Estapé, J. and O'Boyle, M. Learning C to x86 translation: An experiment in neural compilation. In Advances in Programming Languages and Neurosymbolic Systems Workshop, 2021. URL \\url\{https://openreview.net/forum?id=444ug\_EYXet\}},
 pdf = {https://arxiv.org/pdf/2108.07639v2},
 primaryclass = {cs.AI},
 published = {2021-08-17T14:11:15Z},
 title = {Learning C to x86 Translation: An Experiment in Neural Compilation},
 updated = {2022-12-16T11:21:46Z},
 url = {https://arxiv.org/abs/2108.07639v2},
 year = {2021}
}

@article{arXiv:2201.07420,
 abstract = {Binary-source code matching plays an important role in many security and software engineering related tasks such as malware detection, reverse engineering and vulnerability assessment. Currently, several approaches have been proposed for binary-source code matching by jointly learning the embeddings of binary code and source code in a common vector space. Despite much effort, existing approaches target on matching the binary code and source code written in a single programming language. However, in practice, software applications are often written in different programming languages to cater for different requirements and computing platforms. Matching binary and source code across programming languages introduces additional challenges when maintaining multi-language and multi-platform applications. To this end, this paper formulates the problem of cross-language binary-source code matching, and develops a new dataset for this new problem. We present a novel approach XLIR, which is a Transformer-based neural network by learning the intermediate representations for both binary and source code. To validate the effectiveness of XLIR, comprehensive experiments are conducted on two tasks of cross-language binary-source code matching, and cross-language source-source code matching, on top of our curated dataset. Experimental results and analysis show that our proposed XLIR with intermediate representations significantly outperforms other state-of-the-art models in both of the two tasks.},
 archiveprefix = {arXiv},
 author = {Yi Gui and Yao Wan and Hongyu Zhang and Huifang Huang and Yulei Sui and Guandong Xu and Zhiyuan Shao and Hai Jin},
 categories = {cs.SE cs.AI},
 comment = {SANER2022},
 eprint = {2201.07420},
 journal = {arXiv preprint arXiv:2201.07420},
 month = {01},
 pdf = {https://arxiv.org/pdf/2201.07420v1},
 primaryclass = {cs.SE},
 published = {2022-01-19T05:17:02Z},
 title = {Cross-Language Binary-Source Code Matching with Intermediate Representations},
 updated = {2022-01-19T05:17:02Z},
 url = {https://arxiv.org/abs/2201.07420v1},
 year = {2022}
}

@article{arXiv:2304.01102,
 abstract = {Recently, we can notice a transition to data-driven techniques in Automated Program Repair (APR), in particular towards deep neural networks. This entails training on hundreds of thousands or even millions of non-executable code fragments. We would like to bring more attention to an aspect of code often neglected in Neural Program Repair (NPR), namely its execution. Code execution has several significant advantages. It allows for test-based evaluation of candidate fixes and can provide valuable information to aid repair. In this work we present a fully executable dataset of 450,000 small buggy/fixed program pairs originally submitted to programming competition websites written in eight different programming languages. Along with the dataset we provide infrastructure to compile, safely execute and test programs as well as fine-grained bug-type labels. To give a point of reference, we provide basic evaluation results for two baselines, one based on a generate-and-validate approach and one on deep learning. With this dataset we follow several goals: we want to lift Neural Program Repair beyond fully static code representations, foster the use of execution-based features and, by including several different languages, counterbalance the predominance of Java in the current landscape of APR datasets and benchmarks.},
 archiveprefix = {arXiv},
 author = {Julian Aron Prenner and Romain Robbes},
 categories = {cs.SE cs.LG},
 eprint = {2304.01102},
 journal = {arXiv preprint arXiv:2304.01102},
 month = {04},
 pdf = {https://arxiv.org/pdf/2304.01102v1},
 primaryclass = {cs.SE},
 published = {2023-04-03T16:02:00Z},
 title = {RunBugRun -- An Executable Dataset for Automated Program Repair},
 updated = {2023-04-03T16:02:00Z},
 url = {https://arxiv.org/abs/2304.01102v1},
 year = {2023}
}

@article{arXiv:2304.04658,
 abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate our approach performance on single-language binary-to-source code matching. Experimental results show that GraphBinMatch outperforms state-of-the-art significantly, with improvements as high as 15\% over the F1 score.},
 archiveprefix = {arXiv},
 author = {Ali TehraniJamsaz and Hanze Chen and Ali Jannesari},
 categories = {cs.SE},
 eprint = {2304.04658},
 journal = {arXiv preprint arXiv:2304.04658},
 month = {04},
 pdf = {https://arxiv.org/pdf/2304.04658v1},
 primaryclass = {cs.SE},
 published = {2023-04-10T15:36:31Z},
 title = {GraphBinMatch: Graph-based Similarity Learning for Cross-Language Binary and Source Code Matching},
 updated = {2023-04-10T15:36:31Z},
 url = {https://arxiv.org/abs/2304.04658v1},
 year = {2023}
}

@article{arXiv:2401.11161,
 abstract = {While third-party libraries are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis, proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@1 and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product.},
 archiveprefix = {arXiv},
 author = {Ling Jiang and Junwen An and Huihui Huang and Qiyi Tang and Sen Nie and Shi Wu and Yuqun Zhang},
 categories = {cs.SE},
 comment = {In Proceedings of the 46th International Conference on Software Engineering (ICSE'24)},
 eprint = {2401.11161},
 journal = {arXiv preprint arXiv:2401.11161},
 month = {01},
 pdf = {https://arxiv.org/pdf/2401.11161v3},
 primaryclass = {cs.SE},
 published = {2024-01-20T07:57:57Z},
 title = {BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching},
 updated = {2024-08-26T03:11:04Z},
 url = {https://arxiv.org/abs/2401.11161v3},
 year = {2024}
}

@article{arXiv:2404.16041,
 abstract = {The escalating demand to migrate legacy software across different Instruction Set Architectures (ISAs) has driven the development of assembly-to-assembly translators to map between their respective assembly languages. However, the development of these tools requires substantial engineering effort. State-of-the-art approaches use lifting, a technique where source assembly code is translated to an architecture-independent intermediate representation (IR) (for example, the LLVM IR) and use a pre-existing compiler to recompile the IR to the target ISA. However, the hand-written rules these lifters employ are sensitive to the particular compiler and optimization level used to generate the code and require significant engineering effort to support each new ISA. We propose Forklift, the first neural lifter that learns how to translate assembly to LLVM IR using a token-level encoder-decoder Transformer. We show how to incrementally add support to new ISAs by fine tuning the assembly encoder and freezing the IR decoder, improving the overall accuracy and efficiency. We collect millions of parallel LLVM IR, x86, ARM, and RISC-V programs across compilers and optimization levels to train Forklift and set up an input/output-based accuracy harness. We evaluate Forklift on two challenging benchmark suites and translate 2.5x more x86 programs than a state-of-the-art hand-written lifter and 4.4x more x86 programs than GPT-4 as well as enabling translation from new ISAs.},
 archiveprefix = {arXiv},
 author = {Jordi Armengol-Estapé and Rodrigo C. O. Rocha and Jackson Woodruff and Pasquale Minervini and Michael F. P. O'Boyle},
 categories = {cs.PL cs.AI cs.LG},
 eprint = {2404.16041},
 journal = {arXiv preprint arXiv:2404.16041},
 month = {04},
 pdf = {https://arxiv.org/pdf/2404.16041v1},
 primaryclass = {cs.PL},
 published = {2024-04-01T17:27:58Z},
 title = {Forklift: An Extensible Neural Lifter},
 updated = {2024-04-01T17:27:58Z},
 url = {https://arxiv.org/abs/2404.16041v1},
 year = {2024}
}

@article{arXiv:2405.19581,
 abstract = {Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3\% relative gain in CHRF and a 16.7\% relative gain in a GPT4-based metric for summarization, as well as a 6.7\% and 7.4\% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.},
 archiveprefix = {arXiv},
 author = {Zian Su and Xiangzhe Xu and Ziyang Huang and Kaiyuan Zhang and Xiangyu Zhang},
 categories = {cs.SE cs.AI cs.CL},
 eprint = {2405.19581},
 journal = {arXiv preprint arXiv:2405.19581},
 month = {05},
 pdf = {https://arxiv.org/pdf/2405.19581v2},
 primaryclass = {cs.SE},
 published = {2024-05-30T00:17:44Z},
 title = {Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases},
 updated = {2024-10-30T16:12:36Z},
 url = {https://arxiv.org/abs/2405.19581v2},
 year = {2024}
}

@article{arXiv:2405.20611,
 abstract = {Detecting vulnerabilities within compiled binaries is challenging due to lost high-level code structures and other factors such as architectural dependencies, compilers, and optimization options. To address these obstacles, this research explores vulnerability detection using natural language processing (NLP) embedding techniques with word2vec, BERT, and RoBERTa to learn semantics from intermediate representation (LLVM IR) code. Long short-term memory (LSTM) neural networks were trained on embeddings from encoders created using approximately 48k LLVM functions from the Juliet dataset. This study is pioneering in its comparison of word2vec models with multiple bidirectional transformers (BERT, RoBERTa) embeddings built using LLVM code to train neural networks to detect vulnerabilities in compiled binaries. Word2vec Skip-Gram models achieved 92\% validation accuracy in detecting vulnerabilities, outperforming word2vec Continuous Bag of Words (CBOW), BERT, and RoBERTa. This suggests that complex contextual embeddings may not provide advantages over simpler word2vec models for this task when a limited number (e.g. 48K) of data samples are used to train the bidirectional transformer-based models. The comparative results provide novel insights into selecting optimal embeddings for learning compiler-independent semantic code representations to advance machine learning detection of vulnerabilities in compiled binaries.},
 archiveprefix = {arXiv},
 author = {Gary A. McCully and John D. Hastings and Shengjie Xu and Adam Fortier},
 categories = {cs.CR cs.CL cs.LG cs.SE},
 comment = {Updated with improvements},
 doi = {10.1109/CARS61786.2024.10778724},
 eprint = {2405.20611},
 journal = {arXiv preprint arXiv:2405.20611},
 month = {05},
 note = {Journal reference: 2024 IEEE Cyber Awareness and Research Symposium (CARS), Grand Forks, ND, USA, 2024, pp. 1-8},
 pdf = {https://arxiv.org/pdf/2405.20611v3},
 primaryclass = {cs.CR},
 published = {2024-05-31T03:57:19Z},
 title = {Bi-Directional Transformers vs. word2vec: Discovering Vulnerabilities in Lifted Compiled Code},
 updated = {2024-09-27T13:29:00Z},
 url = {https://arxiv.org/abs/2405.20611v3},
 year = {2024}
}

@article{arXiv:2412.20992,
 abstract = {Deep learning operators are fundamental components of modern deep learning frameworks. With the growing demand for customized operators, it has become increasingly common for developers to create their own. However, designing and implementing operators is complex and error-prone, due to hardware-specific optimizations and the need for numerical stability. There is a pressing need for tools that can summarize the functionality of both existing and user-defined operators. To address this gap, this work introduces a novel framework for the verified lifting of deep learning operators, which synthesizes high-level mathematical formulas from low-level implementations. Our approach combines symbolic execution, syntax-guided synthesis, and SMT-based verification to produce readable and formally verified mathematical formulas. In synthesis, we employ a combination of top-down and bottom-up strategies to explore the vast search space efficiently; In verification, we design invariant synthesis patterns and leverage SMT solvers to validate the correctness of the derived summaries; In simplification, we use egraph-based techniques with custom rules to restore complex formulas to their natural, intuitive forms. Evaluated on a dataset of deep learning operators implemented in Triton from the real world, our method demonstrates the effectiveness of synthesis and verification compared to existing techniques. This framework bridges the gap between low-level implementations and high-level abstractions, improving understanding and reliability in deep learning operator development.},
 archiveprefix = {arXiv},
 author = {Qi Zhan and Xing Hu and Xin Xia and Shanping Li},
 categories = {cs.LG cs.PL stat.ML},
 eprint = {2412.20992},
 journal = {arXiv preprint arXiv:2412.20992},
 month = {12},
 pdf = {https://arxiv.org/pdf/2412.20992v1},
 primaryclass = {cs.LG},
 published = {2024-12-30T14:57:32Z},
 title = {Verified Lifting of Deep learning Operators},
 updated = {2024-12-30T14:57:32Z},
 url = {https://arxiv.org/abs/2412.20992v1},
 year = {2024}
}

@article{arXiv:2503.23748,
 abstract = {On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models. To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14\%, showing the practicality of THEMIS.},
 archiveprefix = {arXiv},
 author = {Yujin Huang and Zhi Zhang and Qingchuan Zhao and Xingliang Yuan and Chunyang Chen},
 categories = {cs.CR cs.LG cs.SE},
 comment = {To Appear in the 34th USENIX Security Symposium, August 13-15, 2025},
 eprint = {2503.23748},
 journal = {arXiv preprint arXiv:2503.23748},
 month = {03},
 pdf = {https://arxiv.org/pdf/2503.23748v1},
 primaryclass = {cs.CR},
 published = {2025-03-31T05:58:57Z},
 title = {THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models},
 updated = {2025-03-31T05:58:57Z},
 url = {https://arxiv.org/abs/2503.23748v1},
 year = {2025}
}

@article{arXiv:2505.04852,
 abstract = {There has been a growing interest in translating C code to Rust due to Rust's robust memory and thread safety guarantees. Tools such as C2RUST enable syntax-guided transpilation from C to semantically equivalent Rust code. However, the resulting Rust programs often rely heavily on unsafe constructs--particularly raw pointers--which undermines Rust's safety guarantees. This paper aims to improve the memory safety of Rust programs generated by C2RUST by eliminating raw pointers. Specifically, we propose a peephole raw pointer rewriting technique that lifts raw pointers in individual functions to appropriate Rust data structures. Technically, PR2 employs decision-tree-based prompting to guide the pointer lifting process. Additionally, it leverages code change analysis to guide the repair of errors introduced during rewriting, effectively addressing errors encountered during compilation and test case execution. We implement PR2 as a prototype and evaluate it using gpt-4o-mini on 28 real-world C projects. The results show that PR2 successfully eliminates 13.22\% of local raw pointers across these projects, significantly enhancing the safety of the translated Rust code. On average, PR2 completes the transformation of a project in 5.44 hours, at an average cost of \$1.46.},
 archiveprefix = {arXiv},
 author = {Yifei Gao and Chengpeng Wang and Pengxiang Huang and Xuwei Liu and Mingwei Zheng and Xiangyu Zhang},
 categories = {cs.SE cs.AI cs.PL},
 eprint = {2505.04852},
 journal = {arXiv preprint arXiv:2505.04852},
 month = {05},
 pdf = {https://arxiv.org/pdf/2505.04852v2},
 primaryclass = {cs.SE},
 published = {2025-05-07T23:30:27Z},
 title = {PR2: Peephole Raw Pointer Rewriting with LLMs for Translating C to Safer Rust},
 updated = {2025-05-09T06:32:08Z},
 url = {https://arxiv.org/abs/2505.04852v2},
 year = {2025}
}

@article{arXiv:2505.07360,
 abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
 archiveprefix = {arXiv},
 author = {Xiuwei Shang and Guoqiang Chen and Shaoyin Cheng and Benlong Wu and Li Hu and Gangyang Li and Weiming Zhang and Nenghai Yu},
 categories = {cs.SE},
 comment = {23 pages, 5 figures, to be published in IJCAI 2025},
 eprint = {2505.07360},
 journal = {arXiv preprint arXiv:2505.07360},
 month = {05},
 pdf = {https://arxiv.org/pdf/2505.07360v1},
 primaryclass = {cs.SE},
 published = {2025-05-12T08:54:07Z},
 title = {BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models},
 updated = {2025-05-12T08:54:07Z},
 url = {https://arxiv.org/abs/2505.07360v1},
 year = {2025}
}

@article{arXiv:2506.10125,
 abstract = {As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3\% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2\% of all the functions produced by the native decompiler.},
 archiveprefix = {arXiv},
 author = {Muqi Zou and Hongyu Cai and Hongwei Wu and Zion Leonahenahe Basque and Arslan Khan and Berkay Celik and Dave and Tian and Antonio Bianchi and Ruoyu and Wang and Dongyan Xu},
 author_affiliations = {Muqi Zou: Jing; Hongyu Cai: Jing; Hongwei Wu: Jing; Zion Leonahenahe Basque: Jing; Arslan Khan: Jing; Berkay Celik: Jing; Dave: Jing; Tian: Fish; Antonio Bianchi: Fish; Ruoyu: Fish},
 categories = {cs.CR cs.SE},
 eprint = {2506.10125},
 journal = {arXiv preprint arXiv:2506.10125},
 month = {06},
 pdf = {https://arxiv.org/pdf/2506.10125v2},
 primaryclass = {cs.CR},
 published = {2025-06-11T19:09:08Z},
 title = {D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning},
 updated = {2025-08-15T18:26:50Z},
 url = {https://arxiv.org/abs/2506.10125v2},
 year = {2025}
}

@article{arXiv:2507.04931,
 abstract = {Dynamic Symbolic Execution (DSE) is a key technique in program analysis, widely used in software testing, vulnerability discovery, and formal verification. In distributed AI systems, DSE plays a crucial role in identifying hard-to-detect bugs, especially those arising from complex network communication patterns. However, traditional approaches to symbolic execution are often hindered by scalability issues and inefficiencies, particularly in large-scale systems. This paper introduces LIFT (Large-language-model Integrated Functional-equivalent-IR Transformation), a novel framework that leverages Large Language Models (LLMs) to automate the optimization of Intermediate Representations (IRs) in symbolic execution. LIFT addresses the challenges of symbolic execution by providing a scalable, context-sensitive solution for IR transformation. The framework consists of two phases: IR Analysis and Optimization, where LLMs optimize time-intensive IR blocks, and Symbolic Execution and Validation, which includes benchmarking and semantic verification to ensure correctness and generalizability. Experiments on real-world binaries demonstrated significant performance improvements, including a 53.5\\\% reduction in execution time for bigtest and a 10.24\\\% reduction for random, along with reductions in IR statements, PUT instructions, and temporary variables. These results demonstrate that LLMs simplify IRs while maintaining functional correctness, enhancing symbolic execution in distributed AI systems.},
 archiveprefix = {arXiv},
 author = {Ruoxi Wang and Kun Li and Minghui Xu and Yue Zhang and Kaidi Xu and Chunchi Liu and Yinhao Xiao and Xiuzhen Cheng},
 categories = {cs.CR},
 comment = {Accepted by ACM SIGCOMM 2025 - 2nd Workshop on Networks for AI Computing (NAIC). 7 pages, 2 figures, 2 tables},
 eprint = {2507.04931},
 journal = {arXiv preprint arXiv:2507.04931},
 month = {07},
 pdf = {https://arxiv.org/pdf/2507.04931v1},
 primaryclass = {cs.CR},
 published = {2025-07-07T12:26:56Z},
 title = {LIFT: Automating Symbolic Execution Optimization with Large Language Models for AI Networks},
 updated = {2025-07-07T12:26:56Z},
 url = {https://arxiv.org/abs/2507.04931v1},
 year = {2025}
}

@article{arXiv:2509.14646,
 abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose \\saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of \\saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed \\salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare \\saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that \\saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\\\% TCP rate on Decompile-Eval with a 10.6\\\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
 archiveprefix = {arXiv},
 author = {Yongpan Wang and Xin Xu and Xiaojie Zhu and Xiaodong Gu and Beijun Shen},
 categories = {cs.SE cs.PL},
 comment = {13 pages, 7 figures},
 eprint = {2509.14646},
 journal = {arXiv preprint arXiv:2509.14646},
 month = {09},
 pdf = {https://arxiv.org/pdf/2509.14646v1},
 primaryclass = {cs.SE},
 published = {2025-09-18T05:57:15Z},
 title = {SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation},
 updated = {2025-09-18T05:57:15Z},
 url = {https://arxiv.org/abs/2509.14646v1},
 year = {2025}
}

@article{arXiv:2601.05772,
 abstract = {Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \\textbf\{\\textit\{StriderSPD\}\}, a \\underline\{Str\}ucture-gu\\underline\{ide\}d joint \\underline\{r\}epresentation \\underline\{SPD\} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.},
 archiveprefix = {arXiv},
 author = {Qingyuan Li and Chenchen Yu and Chuanyi Li and Xin-Cheng Wen and Cheryl Lee and Cuiyun Gao and Bin Luo},
 categories = {cs.SE cs.CR},
 eprint = {2601.05772},
 journal = {arXiv preprint arXiv:2601.05772},
 month = {01},
 pdf = {https://arxiv.org/pdf/2601.05772v1},
 primaryclass = {cs.SE},
 published = {2026-01-09T12:55:29Z},
 title = {StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection},
 updated = {2026-01-09T12:55:29Z},
 url = {https://arxiv.org/abs/2601.05772v1},
 year = {2026}
}
