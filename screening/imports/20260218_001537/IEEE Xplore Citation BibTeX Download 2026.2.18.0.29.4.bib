@INPROCEEDINGS{926965,
  author={Johnstone, A. and Scott, E. and Womack, T.},
  booktitle={Proceedings of the 33rd Annual Hawaii International Conference on System Sciences}, 
  title={Reverse compilation for digital signal processors: a working example}, 
  year={2000},
  volume={},
  number={},
  pages={10 pp. vol.1-},
  abstract={We describe the implementation and use of a reverse compiler from Analog Devices 21xx assembler source to ANSI-C (with optional use of the language extensions for the TMS320C6x processors) which has been used to port substantial applications. The main results of this work are that reverse compilation is feasible and that some of the features that make small DSPs hard to compile for, actually assist the process of reverse compilation compared to that of a general purpose processor. The article presents statistics on the occurrence of non-statically visible features of handwritten assembler code. It also looks at the quality of the code generated by an optimising ANSI-C compiler from our reverse compiled source and compares it to code generated from conventionally authored ANSI-C programs.},
  keywords={Digital signal processors;High level languages;Assembly systems;Digital signal processing;Programming profession;Optimizing compilers;Program processors;Embedded system;Productivity;Runtime},
  doi={10.1109/HICSS.2000.926965},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{4032166,
  author={Yang, Xiaoqi and Zheng, Qilong and Chen, Guoliang and Yao, Zhen},
  booktitle={2006 Seventh International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT'06)}, 
  title={Reverse Compilation for Speculative Parallel Threading}, 
  year={2006},
  volume={},
  number={},
  pages={138-143},
  abstract={Multi-core processors can easily provide benefits for multithreaded workloads, but many applications written for uniprocessors cannot automatically benefit from chip multiprocessors (CMP) designs. This paper presents a reverse compilation framework, which translates existing binary code without source code to the static single assignment (SSA) form, and then the internal SSA form is applied by the compilation phase to generate the speculative parallel threading (SPT) code. A profiler is applied to optimize the code dynamically during execution. The evaluation results show that these existing binary codes without source codes execute on CMP with performance improved, due to taking advantage of the speculative parallel threading support provided by the processor},
  keywords={Yarn;Application software;Parallel processing;Binary codes;Hardware;Parallel programming;Concurrent computing;High performance computing;Computer science;Multicore processing},
  doi={10.1109/PDCAT.2006.94},
  ISSN={2379-5352},
  month={Dec},}@INPROCEEDINGS{4268259,
  author={Naeem, Nomair A. and Batchelder, Michael and Hendren, Laurie},
  booktitle={15th IEEE International Conference on Program Comprehension (ICPC '07)}, 
  title={Metrics for Measuring the Effectiveness of Decompilers and Obfuscators}, 
  year={2007},
  volume={},
  number={},
  pages={253-258},
  abstract={Java developers often use decompilers to aid reverse engineering and obfuscators to prevent it. Decompilers translate low-level class files to Java source and can produce "good" output. Obfuscators transform class files into semantically-equivalent versions that are either: (1) difficult to decompile, or (2) decompilable, but result in "hard- to-understand" Java source. We present a set of metrics developed to quantify the effectiveness of decompilers and obfuscators. The metrics include some selective size and counting metrics and an expression complexity metric. We have applied these metrics to evaluate a collection of decompilers and obfuscators. By quantitatively comparing original Java source against decompiled and obfuscated code respectively, we show which decompilers produce "good" code and whether obfuscations result in "hard-to-understand" code.},
  keywords={Java;Reverse engineering;Software engineering;Computer science;Particle measurements;Software measurement;Software metrics;Displays;Programming profession;Sun},
  doi={10.1109/ICPC.2007.27},
  ISSN={1092-8138},
  month={June},}@INPROCEEDINGS{5645445,
  author={Chen, Gengbiao and Wang, Zhuo and Zhang, Ruoyu and Zhou, Kan and Huang, Shiqiu and Ni, Kangqi and Qi, Zhengwei and Chen, Kai and Guan, Haibing},
  booktitle={2010 17th Working Conference on Reverse Engineering}, 
  title={A Refined Decompiler to Generate C Code with High Readability}, 
  year={2010},
  volume={},
  number={},
  pages={150-154},
  abstract={As a key part of reverse engineering, decompilation plays a very important role in software security and maintenance. Unfortunately, most existing decompilation tools suffer from the low accuracy in identifying variables, functions and composite structures, which results in poor readability. To address these limitations, we present a practical decompiler called C-Decompiler for Windows C programs that (1) uses a shadow stack to perform refined data flow analysis, and (2) adopts inter-basic-block register propagation to reduce redundant variables. Our experimental results illustrate that on average C-Decompiler has the highest total percentage reduction of 55.91%, lowest variable expansion rate of 55.79% in the three tools, and the same Cyclomatic Complexity as the original source code for each test application. Furthermore, in our experiment, C-Decompiler is able to recognize functions with lower false positive and false negative rate. In the studies, we show that C-Decompiler is a practical tool to produce highly readable C code.},
  keywords={Registers;Algorithm design and analysis;Benchmark testing;Accuracy;Educational institutions;Software;Binary codes;Reverse Engineering;Decompilation},
  doi={10.1109/WCRE.2010.24},
  ISSN={2375-5369},
  month={Oct},}@INPROCEEDINGS{882387,
  author={Tam, V. and Gupta, R.K.},
  booktitle={Proceedings of the First International Conference on Web Information Systems Engineering}, 
  title={Using class decompilers to facilitate the security of Java applications!}, 
  year={2000},
  volume={1},
  number={},
  pages={153-158 vol.1},
  abstract={Undoubtedly, Java/sup TM/ has become a very popular choice of Internet programming language for developing many Web applications. However, few engineers or researchers questioned Java security problems due to its informative classfiles in which hackers can easily use most available decompilers to reverse-engineer targeted applications. We investigate an interesting proposal of the innovative combination of class decompilers and obfuscators as a feedback-and-control system to secure Java applications. Unlike ordinary obfuscation techniques which always require prior knowledge about the Java source files, our approach can start from the compiled Java classfiles, especially useful when the original source is partially or completely lost. Moreover, the obfuscated codes can also use back the class decompiler as a tester to check if the final product is sufficiently secured. In general, our contribution is two-fold. First, our proposal demonstrated the first constructive use of class decompilers to facilitate the security of Java applications. Decompilers are combined with visualization techniques to deduce useful information for obfuscation. More importantly, with component-based approach, our implemented system can actually be extended as a centralized Web-based testing center with a library of obfuscators to secure most real-life Java applications against a collection of class decompilers.},
  keywords={Java;Application software;National security;Computer languages;Computer hacking;Proposals;Visualization;Sun;Software maintenance;Computer science},
  doi={10.1109/WISE.2000.882387},
  ISSN={},
  month={June},}@INPROCEEDINGS{7113109,
  author={Ďurfina, Lukáš and Křoustek, Jakub and Zemek, Petr},
  booktitle={2013 Third World Congress on Information and Communication Technologies (WICT 2013)}, 
  title={Retargetable machine-code decompilation in your web browser}, 
  year={2013},
  volume={},
  number={},
  pages={57-62},
  abstract={Machine-code decompilation, belonging to the area of reverse engineering, has found its applications in many real-world areas. Analysis of malicious software, search for vulnerabilities, and source-code recovery are some of the most important uses. As there exists a diversity of different platforms on which software can be run, an existence of a generic decompiler would be highly appreciated. The present paper describes such a tool. In this paper, we provide a description of a retargetable decompiler that is being developed within the Lissom project. First, we give an introduction into the area of machine-code decompilation, including a brief discussion of existing tools. Then, we describe the concept and architecture of the decompiler. As it is available in the form of a web service, we also provide its description. Finally, we summarize our results, present a case study of using the tool for analysing malicious software, and conclude the paper by several remarks on future research.},
  keywords={World Wide Web;reverse engineering;decompilation;retargetable decompiler;web service;Lissom},
  doi={10.1109/WICT.2013.7113109},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{792629,
  author={Johnstone, A. and Scott, E. and Womack, T.},
  booktitle={Proceedings IEEE International Conference on Software Maintenance - 1999 (ICSM'99). 'Software Maintenance for Business Change' (Cat. No.99CB36360)}, 
  title={Reverse compilation of digital signal processor assembler source to ANSI-C}, 
  year={1999},
  volume={},
  number={},
  pages={316-325},
  abstract={Digital signal processors (DSPs) are special purpose microprocessors optimised for embedded applications that require high arithmetic rates. These devices are often difficult to compile for; compared to modern general purpose processors, DSPs often have very small address spaces. In addition they contain unusual hardware features and they require correct scheduling of operands against pipeline registers to extract the highest levels of available performance. As a result, high level language compilers for these devices generate poor quality code, and are rarely used in practice. Recently, new generation processors have been launched that are very hard to program by hand in assembler because of the complexity of their internal pipelines and arithmetic structures. DSP users are therefore having to migrate to using high level language compilers since this is the only practical development environment. However, there exist large quantities of legacy code written in assembler which represent a significant investment to the user who would like to be able to deploy core algorithms on the new processors without having to re-code from scratch. The article discusses the development and use of a tool to automatically reverse-compile assembler source for the ADSP-21xx family of DSPs to ANSI-C. We include a discussion of the architectural features of the ADSP-21xx processors and the ways in which they assist the translation process. We also identify a series of translation challenges which, in the limit, can only be handled with manual intervention and give some statistics for the frequency with which these pathological cases appear in real applications.},
  keywords={Digital signal processors;Assembly;Digital signal processing;Pipelines;High level languages;Microprocessors;Digital arithmetic;Hardware;Processor scheduling;Registers},
  doi={10.1109/ICSM.1999.792629},
  ISSN={1063-6773},
  month={Aug},}@INPROCEEDINGS{8109451,
  author={Kostelanský, Jozef and Dedera, Ľubomír},
  booktitle={2017 Communication and Information Technologies (KIT)}, 
  title={An evaluation of output from current Java bytecode decompilers: Is it Android which is responsible for such quality boost?}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  abstract={Countless various malware families provide huge variety of functionalities which allow them to do many malicious activities. This conditions led to development of many different analysis methods. In this paper, we focused on reverse engineering, which is elementary part of static analysis. We evaluate current Java bytecode decompilers. We evaluate the output from current Java bytecode decompilers in this paper using test samples and metrics from previous surveys in 2003 and in 2009. Quality boost is really significant between actual results and the results based on research from 2009, in contrast with only slight improvement between researches in 2003 and 2009. Even though we were witnesses of rapid quality boost, still any of the decompilers pass all the tests acceptably. We also give reasons why outdated decompilers from previous research perform almost same.},
  keywords={Java;Malware;Casting;Androids;Humanoid robots;Measurement;java;reverse engineering;bytecode;decompilation},
  doi={10.23919/KIT.2017.8109451},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8054016,
  author={Lv, Xuefeng and Xie, Yaobin and Zhu, Xiaodong and Ren, Lu},
  booktitle={2017 IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, 
  title={A technique for bytecode decompilation of PLC program}, 
  year={2017},
  volume={},
  number={},
  pages={252-257},
  abstract={Program logical controllers (PLCs) are the kernel equipment of industrial control system (ICS) as they directly monitor and control industrial processes. Recently, ICS is suffering from various cyber threats, which may lead to significant consequences due to its inherent characteristics. In IT system, decompilation is a useful method to detect intrusion or to discovery vulnerabilities, however, it has yet not been developed in ICS. In this work, we present a technique to decompile the bytecode of PLC program. By introducing the instruction template and operand template, we propose a decompiling framework, which is validated by 11 PLC programs. In disassembling experiments, the present framework can cover all instructions with disassembling accuracy reaching 100%, this fully shows that our framework is able to effectively decompile the bytecode of PLC programs.},
  keywords={Integrated circuits;Computer languages;Instruction sets;Algorithm design and analysis;Encoding;Libraries;Industrial control;programmable logical controller;bytecode;decompilation;mapping rules},
  doi={10.1109/IAEAC.2017.8054016},
  ISSN={},
  month={March},}@INPROCEEDINGS{11208400,
  author={Duţu, Teodor-Ştefan and Deaconescu, Adrian-Răzvan and Peca, Ludmila},
  booktitle={2025 24th RoEduNet Conference: Networking in Education and Research (RoEduNet)}, 
  title={Improving iOS Sandbox Profile Decompilation Accuracy}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={Mobile devices have become ubiquitous, with iOS being the second most popular mobile operating system on the market [1]. One method iOS uses to ensure the security of its apps is through sandboxing. This mechanism is implemented as a set of rules compiled into binary files that lie inside the OS firmware and which are not made public by Apple. Thus, security engineers require third-party tools to decompile and then visualize the contents of the profiles mentioned above. This paper presents a validation framework for iOS sandbox profile decompilers, specifically targeting the SandBlaster tool. Our approach represents sandbox profiles as dependency graphs and compares decompiled profiles with reference implementations compiled from Sandbox Profile Language (SBPL) representations using SandScout. We evaluated our framework in iOS versions 7–10, analyzing both individual profiles and bundled profile collections. The results demonstrate 100% precision and recall for iOS 7–8 profiles, 90-100% for iOS 9, and 75-100% for iOS 10. We also optimised a performance bottleneck in SandBlaster's node matching algorithm, reducing decompilation time from over 7 hours to under 5 minutes.},
  keywords={Visualization;Accuracy;Operating systems;Education;Mobile communication;Security;Smart phones;Microprogramming;iOS;Sandboxing},
  doi={10.1109/RoEduNet68395.2025.11208400},
  ISSN={2247-5443},
  month={Sep.},}@INPROCEEDINGS{7790233,
  author={Guo, Yanhui and Hou, Jianpeng and Chen, Weiping and Li, Qi},
  booktitle={2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)}, 
  title={A method to detect Android ad plugins based on decompiled digital sequence}, 
  year={2016},
  volume={},
  number={},
  pages={104-108},
  abstract={As the Android mobile terminals have become prevalent, an increasing number of developers build Android applications and get benefits from bonus of advertising sharing. However, some mobile advertisements have serious security problems, such as traffic consumption, malicious deduction and privacy leak. White list and semantic analysis are proposed to defect the problem recently. In this paper, we put forward the work further by analyzing the decompiled digital sequence of ad plugins. We select appropriate decompiling tools to decompile the ad plugins to get the Java files that we need. And then we traverse the Java files to count the unique word of each file. The number of the unique word is the digital sequence of the file which could be applied as the feature of machine learning classifier to detect ad plugins. The experimental results show that the decompiled digital sequence has very good performance in terms of detect ad plugins, which cannot only avoid the situation of getting less semantic features, but also solve white list method inadequate.},
  keywords={Sequences;Feature extraction;Semantics;Java;Androids;Humanoid robots;Classification algorithms;Ad Plugins;Decompiled Digital Sequence;Classifier},
  doi={10.1109/CCIS.2016.7790233},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{957844,
  author={Mycroft, A. and Ohori, A. and Katsumata, S.},
  booktitle={Proceedings Eighth Working Conference on Reverse Engineering}, 
  title={Comparing type-based and proof-directed decompilation}, 
  year={2001},
  volume={},
  number={},
  pages={362-367},
  abstract={In the past couple of years interest in decompilation has widened from its initial concentration on reconstruction of control flow into well-founded-in-theory methods to reconstruct type information. A. Mycroft (1999) described Type-Based Decompilation and S. Katsumata and A. Ohori (2001) described Proof-Directed Decompilation. The article summarises the two approaches and identifies their commonality, strengths and weaknesses; it concludes by suggesting how they may be integrated.},
  keywords={Laboratories;Internet telephony;Information science;Computer science;Assembly;Data mining;Intellectual property;Web server;Logic;Calculus},
  doi={10.1109/WCRE.2001.957844},
  ISSN={1095-1350},
  month={Oct},}@INPROCEEDINGS{6410138,
  author={Shudrak, Maxim and Zolotarev, Vyacheslav},
  booktitle={2012 Sixth UKSim/AMSS European Symposium on Computer Modeling and Simulation}, 
  title={The New Technique of Decompilation and Its Application in Information Security}, 
  year={2012},
  volume={},
  number={},
  pages={115-120},
  abstract={The article describes a new technique of binary code decompilation and its applicability in information security such as software protection against reverse engineering and code obfuscation analysis in malware. The basic idea of the article is the fact that the process of binary code decompilation doesn't require a machine code representation in source code view. The authors propose original decompilation technique based on restoration of code section algorithm. The technique uses a serial interpretation of binary code into intermediate code, analysis of program control flow and code transformation in the algorithmic form. The results are applied to the problem of malware obfuscation analyze and software protection against unauthorized reverse engineering. To solve the first problem, we used a serial algorithm for analysis of the resources used by the virus and the functional control flow separation. For solving the second problem we was also used analysis of the basic program control flow algorithm and additional threads to obfuscate the program and protect it against unauthorized reverse engineering.},
  keywords={Binary codes;Algorithm design and analysis;Software;Registers;Software algorithms;Malware;Generators;decomilation;binary code;malware;software protection},
  doi={10.1109/EMS.2012.20},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{5614983,
  author={Ribić, Samir and Salihbegović, Adnan},
  booktitle={2010 Fifth International Conference on Software Engineering Advances}, 
  title={Adding Optimization to the Decompilable Code Editor}, 
  year={2010},
  volume={},
  number={},
  pages={88-93},
  abstract={The authors of this paper recently researched the possibility of developing programming language implementation, that is neither compiler, nor interpreter. The concept is based on keeping the complete program in native machine code, but the specialized editor can 'on the fly' decompile the machine code and display it as high level language. The displayed code can be edited and saved again as pure machine code. This paper reviews the possibility of optimizing generated code, while still retaining the possibility of decompilation. We found many important code sequences which can be replaced with shorter ones while keeping the code in decompilable executable format.},
  keywords={Artificial neural networks;Optimization;Program processors;Arrays;Testing;High level languages;compilation;decompilation;programming languages},
  doi={10.1109/ICSEA.2010.21},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{6671321,
  author={Ďurfina, Lukás and Křoustek, Jakub and Zemek, Petr},
  booktitle={2013 20th Working Conference on Reverse Engineering (WCRE)}, 
  title={PsybOt malware: A step-by-step decompilation case study}, 
  year={2013},
  volume={},
  number={},
  pages={449-456},
  abstract={Decompilation (i.e. reverse compilation) represents one of the most toughest and challenging tasks in reverse engineering. Even more difficult task is the decompilation of malware because it typically does not follow standard application binary interface conventions, has stripped symbols, is obfuscated, and can contain polymorphic code. Moreover, in the recent years, there is a rapid expansion of various smart devices, running different types of operating systems on many types of processors, and malware targeting these platforms. These facts, combined with the boundedness of standard decompilation tools to a particular platform, imply that a considerable amount of effort is needed when decompiling malware for such a diversity of platforms. This is an experience paper reporting the decompilation of a real-world malware. We give a step-by-step case study of decompiling a MIPS worm called psyb0t by using a retargetable decompiler that is being developed within the Lissom project. First, we describe the decompiler in detail. Then, we present the case study. After that, we analyse the results obtained during the decompilation and present our personal experience. The paper is concluded by discussing future research possibilities.},
  keywords={Malware;Ground penetrating radar;Geophysical measurement techniques;Registers;Program processors;Standards;Delays;Reverse engineering;decompilation;retargetable decompiler;Lissom;malware;psyb0t;experience},
  doi={10.1109/WCRE.2013.6671321},
  ISSN={2375-5369},
  month={Oct},}@INPROCEEDINGS{9425937,
  author={Mauthe, Noah and Kargén, Ulf and Shahmehri, Nahid},
  booktitle={2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={A Large-Scale Empirical Study of Android App Decompilation}, 
  year={2021},
  volume={},
  number={},
  pages={400-410},
  abstract={Decompilers are indispensable tools in Android malware analysis and app security auditing. Numerous academic works also employ an Android decompiler as the first step in a program analysis pipeline. In such settings, decompilation is frequently regarded as a "solved" problem, in that it is simply expected that source code can be accurately recovered from an app. While a large proportion of methods in an app can typically be decompiled successfully, it is common that at least some methods fail to decompile. In order to better understand the practical applicability of techniques in which decompilation is used as part of an automated analysis, it is important to know the actual expected failure rate of Android decompilation. To this end, we have performed what is, to the best of our knowledge, the first large-scale study of Android decompilation failure rates. We have used three sets of apps, consisting of, respectively, 3,018 open-source apps, 13,601 apps from a recent crawl of Google Play, and a collection of 24,553 malware samples. In addition to the state-of-the-art Dalvik bytecode decompiler jadx, we used three popular Java decompilers. While jadx achieves an impressively low failure rate of only 0.02% failed methods per app on average, we found that it manages to recover source code for all methods in only 21% of the Google Play apps.We have also sought to better understand the degree to which in-the-wild obfuscation techniques can prevent decompilation. Our empirical evaluation, complemented with an indepth manual analysis of a number of apps, indicate that code obfuscation is quite rarely encountered, even in malicious apps. Moreover, decompilation failures mostly appear to be caused by technical limitations in decompilers, rather than by deliberate attempts to thwart source-code recovery by obfuscation. This is an encouraging finding, as it indicates that near-perfect Android decompilation is, at least in theory, achievable, with implementation-level improvements to decompilation tools.},
  keywords={Java;Conferences;Pipelines;Process control;Manuals;Tools;Malware;Android;mobile apps;decompilation;obfuscation;reverse engineering;malware},
  doi={10.1109/SANER50967.2021.00044},
  ISSN={1534-5351},
  month={March},}@INPROCEEDINGS{4637556,
  author={Gómez-Zamalloa, Miguel and Albert, Elvira and Puebla, Germán},
  booktitle={2008 Eighth IEEE International Working Conference on Source Code Analysis and Manipulation}, 
  title={Modular Decompilation of Low-Level Code by Partial Evaluation}, 
  year={2008},
  volume={},
  number={},
  pages={239-248},
  abstract={Decompiling low-level code to a high-level intermediate representation facilitates the development of analyzers, model checkers, etc. which reason about properties of the low-level code (e.g., bytecode, .NET). Interpretive decompilation consists in partially evaluating an interpreter for the low-level language (written in the high-level language) w.r.t. the code to be decompiled. There have been proofs-of-concept that interpretive decompilation is feasible, butt here remain important open issues when it comes to decompile a real language: does the approach scale up? is the quality of decompiled programs comparable to that obtained by ad-hoc decompilers? do decompiled programs preserve the structure of the original programs?  This paper addresses these issues by presenting, to the best of our knowledge, the first modular scheme to enable interpretive decompilation of low-level code to a high-level representation, namely, we decompile bytecode into Prolog. We introduce two notions of optimality. The first one requires that each method/block is decompiled just once. The second one requires that each program point is traversed at most once during decompilation. We demonstrate the impact of our modular approach and optimality issues on a series of realistic benchmarks.  Decompilation times and decompiled program sizes are linear with the size of the input bytecode program. This demostrates empirically the scalability of modular decompilation of low-level code by partial evaluation.},
  keywords={Superluminescent diodes;Java;Analytical models;Radiation detectors;Scalability;Logic programming;Indexes;decompilation;partial evaluation;bytecode},
  doi={10.1109/SCAM.2008.35},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{957845,
  author={Miecznikowski, J. and Hendren, L.},
  booktitle={Proceedings Eighth Working Conference on Reverse Engineering}, 
  title={Decompiling Java using staged encapsulation}, 
  year={2001},
  volume={},
  number={},
  pages={368-374},
  abstract={The paper presents an approach to program structuring for use in decompiling Java bytecode to Java source. The structuring approach uses three intermediate representations: (1) a list of typed, aggregated statements with an associated exception table, (2) a control flow graph, and (3) a structure encapsulation tree. The approach works in six distinct stages, with each stage focusing on a specific family of Java constructs, and each stage contributing more detail to the structure encapsulation tree. After completion of all stages the structure encapsulation tree contains enough information to allow a simple extraction of a structured Java program. The approach targets general Java bytecode including bytecode that may be the result of front-ends for languages other than Java, and also bytecode that has been produced by a bytecode optimizer. Thus, the techniques have been designed to work for bytecode that may not exhibit the typical structured patterns of bytecode produced by a standard Java compiler. The structuring techniques have been implemented as part of the Dava decompiler which has been built using the Soot framework.},
  keywords={Java;Encapsulation;Tree graphs;Pattern matching;Computer science;Data mining;Design optimization;Visualization;Constraint optimization;High level languages},
  doi={10.1109/WCRE.2001.957845},
  ISSN={1095-1350},
  month={Oct},}@INPROCEEDINGS{5090054,
  author={Troshina, K. and Chernov, A. and Fokin, A.},
  booktitle={2009 IEEE 17th International Conference on Program Comprehension}, 
  title={Profile-based type reconstruction for decompilation}, 
  year={2009},
  volume={},
  number={},
  pages={263-267},
  abstract={Decompilation is reconstruction of a program in a high-level language from a program in a low-level language. In most cases static decompilation is unable to completely reconstruct high-level data types due to loss of typing information during compilation. We present several profile-based techniques that help to recover high-level types. The techniques include pointer/integer determination by value profiling and composite type identification by heap profiling.},
  keywords={Reconstruction algorithms;Registers;Reverse engineering;Cybernetics;Assembly;Equations;Mathematical programming;Application software;Computer bugs;Computer languages},
  doi={10.1109/ICPC.2009.5090054},
  ISSN={1092-8138},
  month={May},}@INPROCEEDINGS{6462558,
  author={Myreen, Magnus O. and Gordon, Michael J. C. and Slind, Konrad},
  booktitle={2012 Formal Methods in Computer-Aided Design (FMCAD)}, 
  title={Decompilation into logic — Improved}, 
  year={2012},
  volume={},
  number={},
  pages={78-81},
  abstract={This paper presents improvements to a technique which aids verification of machine-code programs. This technique, called decompilation into logic, allows the verifier to only deal with tractable extracted models of the machine code rather than the concrete code itself. Our improvements make decompilation simpler, faster and more generally applicable. In particular, the new technique allows the verifier to avoid tedious reasoning directly in the underlying machine-code Hoare logic or the model of the instruction set architecture. The method described in this paper has been implemented in the HOL4 theorem prover.},
  keywords={Arrays;Assembly;Design automation;Concrete;Cognition;Computational modeling},
  doi={},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9631736,
  author={Pavel, Sharikov and Andrey, Krasov and Artem, Gelfand and Ernest, Birikh},
  booktitle={2021 13th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)}, 
  title={A Technique for Detecting the Substitution of a Java-Module of an Information System Prone to Pharming with Using a Hidden Embedding of a Digital Watermark Resistant to Decompilation}, 
  year={2021},
  volume={},
  number={},
  pages={219-223},
  abstract={The relevance of the problem under study is due to the rapid development of the java programming language, a large number of commercial applications written in this programming language. The purpose of the article is to develop a methodology that allows you to determine the java-module of the information system, which does not contain a hidden digital watermark. It is necessary that the digital watermark is not damaged during the decompilation of the entire system in order to replace the java module, and that the class files and the entire information system function unchanged. Thus, at first, a digital watermark is hidden in all java-modules of the information system. Next, we check the resistance of the digital watermark to decompilation attacks with block substitution. The materials of the article can be useful in the implementation of hidden embedding of a digital watermark in the class files of a large java module.},
  keywords={Resistance;Java;Computer languages;Codes;Watermarking;Writing;Virtual machining;bytecode;java-module;java;digital watermark;decompilation attack;decompilation;java module substitution;intruder model},
  doi={10.1109/ICUMT54235.2021.9631736},
  ISSN={2157-023X},
  month={Oct},}@INPROCEEDINGS{5362985,
  author={Zhou, Lina and Qing, Yin and Jiang, Liehui and Yin, Wenjian and Liu, Tieming},
  booktitle={2009 International Conference on Computational Intelligence and Software Engineering}, 
  title={A Method of Type Inference Based on Dataflow Analysis for Decompilation}, 
  year={2009},
  volume={},
  number={},
  pages={1-4},
  abstract={Type analysis is an important part of decompilation, which has a great impact on the readability and the veracity of the output of decompilation. In this paper we discussed a method of type analysis which was based on dataflow analysis .We inference the type of data during the calculating of the dataflow equations, by constructing the type of multiplicate kinds data of the hierarchical structure for the type lattice and designing dataflow equations which have type-related information.},
  keywords={Data analysis;Information analysis;Equations;Lattices;High level languages;Algorithm design and analysis;Switching systems;Data engineering;Systems engineering and theory;Reverse engineering},
  doi={10.1109/CISE.2009.5362985},
  ISSN={},
  month={Dec},}@INBOOK{10649756,
  author={Domas, Stephanie and Domas, Christopher},
  booktitle={x86 Software Reverse-Engineering, Cracking, and Counter-Measures}, 
  title={Decompilation and Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={1-12},
  abstract={Summary <p>This chapter explores the steps necessary to get started reverse engineering an application. Decompilation is crucial to transforming an application from machine code to something that can be read and understood by humans. For many programming languages, full decompilation is impossible. These languages build code directly to machine code, and some information, such as variable names, is lost in the process. JIT compilation also makes reverse engineering these applications much easier. Unlike true machine code programs, JIT&#x2010;compiled programs can often be converted to source code. All high&#x2010;level languages are eventually converted into a series of bits called machine code. Assembly code is designed to be a human&#x2010;readable version of machine code. A microarchitecture describes how a particular ISA is implemented on a processor. Reduced instruction set computing architectures define a small number of simpler instructions.</p>},
  keywords={Codes;Computer architecture;Source coding;Reverse engineering;Java;Computer languages;Computers},
  doi={10.1002/9781394277131.ch1},
  ISSN={},
  publisher={Wiley},
  isbn={9781394199907},
  url={https://ieeexplore.ieee.org/document/10649756},}@INPROCEEDINGS{8811905,
  author={Grech, Neville and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
  booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)}, 
  title={Gigahorse: Thorough, Declarative Decompilation of Smart Contracts}, 
  year={2019},
  volume={},
  number={},
  pages={1176-1186},
  abstract={The rise of smart contracts - autonomous applications running on blockchains - has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a “batteries included” approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.},
  keywords={Smart contracts;Blockchain;Virtual machining;Task analysis;Java;Security;Ethereum;Blockchain;Decompilation;Program Analysis;Security},
  doi={10.1109/ICSE.2019.00120},
  ISSN={1558-1225},
  month={May},}@INPROCEEDINGS{10123564,
  author={Liu, Xia and Hua, Baojian and Wang, Yang and Pan, Zhizhong},
  booktitle={2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={An Empirical Study of Smart Contract Decompilers}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={Smart contract decompilers, converting smart contract bytecode into smart contract source code, have been used extensively in many scenarios such as binary code analysis, reverse engineering, and security studies. However, existing studies, as well as industrial engineering practices, all assumed that smart contract decompilers are reliable and trustworthy, to generate correct and semantically equivalent source code from binaries. Unfortunately, whether such an assumption truly holds in practice is still unknown.In this paper, we conduct, to the best of our knowledge, the first and most comprehensive large-scale empirical study of smart contract decompilers, to gain an understanding of the reliability, limitations, and remaining research challenges of state-of-the-art smart contract decompilation tools. We first designed and implemented a software prototype SOLINSIGHT, then used it to study 5 state-of-the-art smart contract decompilers. We obtained important findings and insights from empirical results, such as: 1) we proposed 3 root causes leading to decompiler failures; 2) we revealed 2 reasons hurting performance; 3) we identified 3 root causes affecting decompilation effectiveness; 4) we proposed a measurement metric for completeness; and 5) we investigated the resilience of contract decompilers against program transformations. We suggest that: 1) decompiler builders should enhance decompilers in terms of effectiveness, performance, and completeness; and 2) security researchers should select appropriate decompilers based on the suggestions in this study. We believe these findings and suggestions will help decompiler builders, contract developers, and security researchers, by providing better guidelines for contract decompiler studies.},
  keywords={Measurement;Source coding;Smart contracts;Reverse engineering;Prototypes;Reliability engineering;Software;Empirical study;Smart contracts;Decompilation},
  doi={10.1109/SANER56733.2023.00011},
  ISSN={2640-7574},
  month={March},}@INPROCEEDINGS{1395592,
  author={Stitt, G. and Vahid, F.},
  booktitle={Design, Automation and Test in Europe}, 
  title={A decompilation approach to partitioning software for microprocessor/FPGA platforms}, 
  year={2005},
  volume={},
  number={},
  pages={396-397 Vol. 1},
  abstract={We present a software compilation approach for microprocessor/FPGA platforms that partitions a software binary onto custom hardware implemented in the FPGA. Our approach imposes fewer restrictions on software tool flow than previous compiler approaches, allowing software designers to use any software language and compiler. Our approach uses a back-end partitioning tool that utilizes decompilation techniques to recover important high-level information, resulting in performance comparable to high-level compiler-based approaches.},
  keywords={Microprocessors;Field programmable gate arrays;Software tools;Hardware;Circuit synthesis;Application software;Embedded software;Computer architecture;Flow graphs;Arithmetic},
  doi={10.1109/DATE.2005.9},
  ISSN={1558-1101},
  month={March},}@INPROCEEDINGS{7860070,
  author={Tiwari, Pankaj and Tere, Girish and Singh, Pooja},
  booktitle={2016 International Conference on Computing Communication Control and automation (ICCUBEA)}, 
  title={Malware detection in android application by rigorous analysis of decompiled source code}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={Android platform implements permissions to guard sensitive information from untrusted apps. Android's permission system agreements an all-or-nothing choice when installing an app in smart phones. However, after permissions are approved by users at installation time, applications can use these permissions with no further restrictions to access personal information. Thus, contemporary years have perceived the detonation of uninvited activities in Android apps. An indispensable part in the resistance is the truthful analysis of Android apps. However, traditional system call based study practices are not suitable for Android based OS, because they are unable to detect serious communications between the application and the Android system. In this paper we have analyzed an android application by a mechanism which rigorously examine the decompiled source code which we will get after reverse engineering the android apk files.},
  keywords={Smart phones;Malware;Java;Security;Virtual machining;Androids;Malware;Android;Permission;Virus;Security},
  doi={10.1109/ICCUBEA.2016.7860070},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{4024006,
  author={Ribic, Samir},
  booktitle={2006 13th Working Conference on Reverse Engineering}, 
  title={Concept and implementation of the programming language and translator, for embedded systems, based on machine code decompilation and equivalence between source and executable code}, 
  year={2006},
  volume={},
  number={},
  pages={307-308},
  abstract={In this thesis it will be investigated the possibility of developing the programming language translator, heavily based on decompilation. Instead of keeping program in source code, it will be kept in native machine code, but it will be transparently visible as high level language program, with the help of the specialized editor.},
  keywords={Computer languages;Embedded system;High level languages;Algorithms;Program processors;Libraries;Computer architecture;Assembly;Application software;Computer applications},
  doi={10.1109/WCRE.2006.20},
  ISSN={2375-5369},
  month={Oct},}@INPROCEEDINGS{9464996,
  author={Broukhis, Leonid A.},
  booktitle={2020 Fifth International Conference “History of Computing in the Russia, former Soviet Union and Council for Mutual Economic Assistance countries” (SORUCOM)}, 
  title={Notes on Simulating the BESM-6 and Decompiling Pascal Programs}, 
  year={2020},
  volume={},
  number={},
  pages={67-71},
  abstract={The article describes the author's source of interest in the BESM-6 computer, its software, and the history of development of simulators of the BESM-6 architecture, and provides comments and observations regarding decompilation of a Pascal compiler for the BESM-6 and of the strategy game Kalah.},
  keywords={Program processors;Software algorithms;Software;History;Software measurement;Task analysis;Optimization;BESM-6;simulation;DISPAK;Dubna monitor system;ALGOL-60;FORTRAN;Pascal},
  doi={10.1109/SORUCOM51654.2020.9464996},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{7513642,
  author={Ilić, Stefan and Đukić, Slavica},
  booktitle={2016 Zooming Innovation in Consumer Electronics International Conference (ZINC)}, 
  title={Protection of Android applications from decompilation using class encryption and native code}, 
  year={2016},
  volume={},
  number={},
  pages={10-11},
  abstract={In this paper we analyzed how different methods of protection, namely class encryption and usage of native code, affect decompilation of Android applications. This is important in consumer electronics as it can be used to stop repackaging and spreading of maliciously modified applications. We analyzed two different methods of protection, their benefits and flaws.},
  keywords={Decision support systems;Android;decompilation;class encryption;native code},
  doi={10.1109/ZINC.2016.7513642},
  ISSN={},
  month={June},}@INPROCEEDINGS{4731463,
  author={Longjie, Zhang and Xiaofang, Xie and Shengzhi, Yuan and Jiang, Tang},
  booktitle={2008 International Symposium on Computer Science and Computational Technology}, 
  title={Research on the Composite Arithmetic of Logic Compound Sentences in Decompilation}, 
  year={2008},
  volume={1},
  number={},
  pages={442-446},
  abstract={This article had a formalizing description to the structure of logic compound sentences (LCS) in the course of decompilation. It applied the Logic diagram to the recognition of LCS which belongs to graph theory. The recognition arithmetic overcame the shortcomings of some typical current approaches. Furthermore, this article introduced logical correlation matrices in the solution to the Logic diagrams, and a dynamic pruning policy was executed in this course by which the novel arithmetic was greatly predigested. In the end of the article, it offered a specific instance and had the complexity analyses, which proved the validity and sophistication of the new arithmetic separately.},
  keywords={Boolean functions;Logic programming;Assembly;Data analysis;Computer science;Military computing;Digital arithmetic;Weapons;Aerospace engineering;Graph theory;software reverse engineering;decompilation;software reuse;logic compound condition sentences;logic diagram;logical correlation matrices},
  doi={10.1109/ISCSCT.2008.130},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{5370155,
  author={Zhang, Jingbo and Zhao, Rongcai and Pang, Jianmin and Fu, Wen},
  booktitle={2009 Third International Symposium on Intelligent Information Technology Application}, 
  title={Decompiling High-level Control Structures with Propositions}, 
  year={2009},
  volume={3},
  number={},
  pages={592-595},
  abstract={In recent years, there has been a growing need for analyst to explore inside the binary executables for the reasons of decompilation, security analysis, reverse engineering, etc. It is very helpful to recovery the high-level control structure information, such as loops and conditionals, from arbitrary control-flow of low-level code. This paper presents a novel approach to structure control-flow graphs in binary executables, which are normally represented by unconditional or conditional jumps. We firstly formalize control flow information of the instructions into expressions of propositional calculus. Then the control flow information can be propagated along the execution path. At last, high-level control structures are identified and recovered through the result of calculation. We have implemented our method in RADUX, a statical malicious code detector based on semantic analysis. Our experimental result shows that this method can recognize and recovery loops and conditionals effectively, and have the ability of analyzing the predicated instructions.},
  keywords={Binary codes;Information analysis;Reverse engineering;Application software;Documentation;Intelligent structures;Information technology;Control systems;Switching systems;Systems engineering and theory},
  doi={10.1109/IITA.2009.474},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8668959,
  author={Yoo, Dongkyun and Shin, Yeonghun and Kim, SungJin and Kim, HyunJin and Kwon, SungMoon and Shon, Taeshik},
  booktitle={2019 International Conference on Platform Technology and Service (PlatCon)}, 
  title={Digital Forensic Artifact Collection Technique using Application Decompilation}, 
  year={2019},
  volume={},
  number={},
  pages={1-3},
  abstract={Nowadays, many applications tend to collect user profile, such as location, usage trace and so on, even if it is not malicious. This information can be important clues in the criminal investigation. So, the technique is needed which extract artifacts from applications using decompilation. We describe a method for selecting and analyzing forensic artifacts from the Android application with a share of over 80% of mobile devices. Based on the static analysis method, we propose a method for automatically collecting forensic artifact. The effectiveness of the proposed idea is proved by simulation.},
  keywords={Smart phones;Java;Static analysis;Tools;Servers;Digital forensics;Reverse Engineering;APK Decompilation},
  doi={10.1109/PlatCon.2019.8668959},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{11381092,
  author={Chawla, Aniesh and Prasad, Udbhav},
  booktitle={2025 IEEE 16th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, 
  title={A Decompilation-Driven Framework for Malware Detection with Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={0403-0408},
  abstract={The parallel evolution of Large Language Models (LLMs) with advanced code-understanding capabilities and the increasing sophistication of malware presents a new frontier for cybersecurity research. This paper evaluates the efficacy of state-of-the-art LLMs in classifying executable code as either benign or malicious. We introduce an automated pipeline that first decompiles Windows executables into a C code using Ghidra disassembler and then leverages LLMs to perform the classification. Our evaluation reveals that while standard LLMs show promise, they are not yet robust enough to replace traditional anti-virus software. We demonstrate that a fine-tuned model, trained on curated malware and benign datasets, significantly outperforms its vanilla counterpart. However, the performance of even this specialized model degrades notably when encountering newer malware. This finding demonstrates the critical need for continuous fine-tuning with emerging threats to maintain model effectiveness against the changing coding patterns and behaviors of malicious software.},
  keywords={Codes;Machine learning algorithms;Large language models;Pipelines;Mobile communication;Malware;Encoding;Computer security;Information technology;Standards;Malware;Ghidra;Cybersecurity;LLMs;GenAI;Machine Learning Algorithms;LLMs Code development},
  doi={10.1109/IEMCON67450.2025.11381092},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{827315,
  author={Johnstone, A. and Scott, E. and Womack, T.},
  booktitle={Proceedings of the Fourth European Conference on Software Maintenance and Reengineering}, 
  title={What assembly language programmers get up to: control flow challenges in reverse compilation}, 
  year={2000},
  volume={},
  number={},
  pages={83-92},
  abstract={The analysis of assembly code to provide a high-level control flow view in terms of the usual high-level looping and selection constructs is of great assistance to high-level language programmers who are attempting to understand and port low-level code as part of a system re-engineering project. This paper describes the control flow analyser component of our asm21toc reverse compiler from assembly language programs for the ADSP-21xx family of digital signal processors to ANSI C. We give a brief overview of the class of processors and programs that we have studied so as to motivate the design of our reverse compiler. We describe the merged call-graph/dataflow representation that supports our analyses and the way in which hierarchical structural control flow information is extracted and stored. We give summary statistics showing the usage of various classes of control flow structure, along with occurrences of non-disjoint functions, self-modifying code and non-reducible control flow constructs.},
  keywords={Programming profession;Program processors;Assembly systems;Control systems;High level languages;Signal analysis;Digital signal processors;Signal design;Information analysis;Data mining},
  doi={10.1109/CSMR.2000.827315},
  ISSN={},
  month={March},}@INPROCEEDINGS{8330222,
  author={Katz, Deborah S. and Ruchti, Jason and Schulte, Eric},
  booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={Using recurrent neural networks for decompilation}, 
  year={2018},
  volume={},
  number={},
  pages={346-356},
  abstract={Decompilation, recovering source code from binary, is useful in many situations where it is necessary to analyze or understand software for which source code is not available. Source code is much easier for humans to read than binary code, and there are many tools available to analyze source code. Existing decompilation techniques often generate source code that is difficult for humans to understand because the generated code often does not use the coding idioms that programmers use. Differences from human-written code also reduce the effectiveness of analysis tools on the decompiled source code. To address the problem of differences between decompiled code and human-written code, we present a novel technique for decompiling binary code snippets using a model based on Recurrent Neural Networks. The model learns properties and patterns that occur in source code and uses them to produce decompilation output. We train and evaluate our technique on snippets of binary machine code compiled from C source code. The general approach we outline in this paper is not language-specific and requires little or no domain knowledge of a language and its properties or how a compiler operates, making the approach easily extensible to new languages and constructs. Furthermore, the technique can be extended and applied in situations to which traditional decompilers are not targeted, such as for decompilation of isolated binary snippets; fast, on-demand decompilation; domain-specific learned decompilation; optimizing for readability of decompilation; and recovering control flow constructs, comments, and variable or function names. We show that the translations produced by this technique are often accurate or close and can provide a useful picture of the snippet's behavior.},
  keywords={Recurrent neural networks;Tools;Training;Binary codes;Natural languages;Decoding;Data models;decompilation;recurrent neural networks;translation;deep learning},
  doi={10.1109/SANER.2018.8330222},
  ISSN={},
  month={March},}@INPROCEEDINGS{7880502,
  author={Ragkhitwetsagul, Chaiyong and Krinke, Jens},
  booktitle={2017 IEEE 11th International Workshop on Software Clones (IWSC)}, 
  title={Using compilation/decompilation to enhance clone detection}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  abstract={We study effects of compilation and decompilation to code clone detection in Java. Compilation/decompilation canonicalise syntactic changes made to source code and can be used as source code normalisation. We used NiCad to detect clones before and after decompilation in three open source software systems, JUnit, JFreeChart, and Tomcat. We filtered and compared the clones in the original and decompiled clone set and found that 1,201 clone pairs (78.7%) are common between the two sets while 326 pairs (21.3%) are only in one of the sets. A manual investigation identified 325 out of the 326 pairs as true clones. The 252 original-only clone pairs contain a single false positive while the 74 decompiled-only clone pairs are all true positives. Many clones in the original source code that are detected only after decompilation are type-3 clones that are dicult to detect due to added or deleted statements, keywords, package names; flipped if-else statements; or changed loops. We suggest to use decompilation as normalisation to compliment clone detection. By combining clones found before and after decompilation, one can achieve higher recall without losing precision.},
  keywords={Cloning;Java;Detectors;Software systems;Manuals;Open source software},
  doi={10.1109/IWSC.2017.7880502},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{6079860,
  author={Fokin, Alexander and Derevenetc, Egor and Chernov, Alexander and Troshina, Katerina},
  booktitle={2011 18th Working Conference on Reverse Engineering}, 
  title={SmartDec: Approaching C++ Decompilation}, 
  year={2011},
  volume={},
  number={},
  pages={347-356},
  abstract={Decompilation is a reconstruction of a program in a high-level language from a program in a low-level language. Typical applications of decompilation are software security assessment, malware analysis, error correction and reverse engineering for interoperability. Native code decompilation is traditionally considered in the context of the C programming language. C++ presents new challenges for decompilation, since the rules of translation from C++ to assembly language are far more complex than those of C. In addition, when decompiling a program that was originally written in C++, reconstruction of C++ specific constructs is desired. In this paper we discuss new methods that allow partial recovery of C++ specific language constructs from a low-level code provided that this code was obtained from a C++ compiler. The challenges that arise when decompiling such code are described. These challenges include reconstruction of polymorphic classes, class hierarchies, member functions and exception handling constructs. An approach to decompilation that is used to overcome these challenges is presented. Smart Dec, a native code to C++ decompiler that is being developed by the authors at Select LTD is presented. It reconstructs expressions, function arguments, local and global variables, integral and composite types, loops and compound conditional statements, C++ class hierarchies and exception handling constructs. An empirical study of the decompiler is provided.},
  keywords={Software;Assembly;Reverse engineering;Layout;Runtime;Computer languages;Lattices;C++;Decompilation;Reverse Engineering;Class Hierarchy Reconstruction;Exception Reconstruction},
  doi={10.1109/WCRE.2011.49},
  ISSN={2375-5369},
  month={Oct},}@INPROCEEDINGS{1631140,
  author={Naeem, N.A. and Hendren, L.},
  booktitle={14th IEEE International Conference on Program Comprehension (ICPC'06)}, 
  title={Programmer-friendly Decompiled Java}, 
  year={2006},
  volume={},
  number={},
  pages={327-336},
  abstract={Java decompilers convert Java class files to Java source. Java class files may be created by a number of different tools including standard Java compilers, compilers for other languages such as AspectJ, or other tools such as optimizers or obfuscators. There are two kinds of Java decompilers, Javac-specific decompilers that assume that the class file was created by a standard Javac compiler and tool-independent decompilers that can decompile arbitrary class files, independent of the tool that created the class files. Typically Javac-specific decompilers produce more readable code, but they fail to decompile many class files produced by other tools. This paper tackles the problem of how to make a tool-independent decompiler, Dava, produce Java source code that is programmer-friendly. In past work it has been shown that Dava can decompile arbitrary class files, but often the output, although correct, is very different from what a programmer would write and is hard to understand. Furthermore, tools like obfuscators intentionally confuse the class files and this also leads to confusing decompiled source files. Given that Dava already produces correct Java abstract syntax trees (ASTs) for arbitrary class files, we provide a new back-end for Dava. The back-end rewrites the ASTs to semantically equivalent ASTs that correspond to code that is easier for programmers to understand. Our new back-end includes a new AST traversal framework, a set of simple pattern-based transformations, a structure-based data flow analysis framework and a collection of more advanced AST transformations that use flow analysis information. We include several illustrative examples including the use of advanced transformations to clean up obfuscated code},
  keywords={Java;Program processors;Optimizing compilers;Programming profession;Information analysis;Pattern analysis;Code standards;Computer science;Data analysis;Internet},
  doi={10.1109/ICPC.2006.40},
  ISSN={1092-8138},
  month={June},}@INPROCEEDINGS{8930870,
  author={Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit},
  booktitle={2019 19th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
  title={The Strengths and Behavioral Quirks of Java Bytecode Decompilers}, 
  year={2019},
  volume={},
  number={},
  pages={92-102},
  abstract={During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84% of classes of our dataset and semantically equivalent code output for 78% of classes.},
  keywords={Java;Syntactics;Semantics;Distortion;Uniform resource locators;Measurement;Program processors;Java bytecode;decompilation;reverse engineering;source code analysis},
  doi={10.1109/SCAM.2019.00019},
  ISSN={2470-6892},
  month={Sep.},}@INPROCEEDINGS{11023256,
  author={Wiedemeier, Josh and Tarbet, Elliot and Zheng, Max and Ko, Sangsoo and Ouyang, Jessica and Cha, Sang Kil and Jee, Kangkook},
  booktitle={2025 IEEE Symposium on Security and Privacy (SP)}, 
  title={PyLingual: Toward Perfect Decompilation of Evolving High-Level Languages}, 
  year={2025},
  volume={},
  number={},
  pages={2976-2994},
  abstract={Python is one of the most popular programming languages among both industry developers and malware authors. Despite demand for Python decompilers, community efforts to maintain automatic Python decompilation tools have been hindered by Python's aggressive language improvements and unstable bytecode specification. Every year, language features are added, code generation undergoes significant changes, and opcodes are added, deleted, and modified. Our research aims to integrate Natural Language Processing (NLP) techniques with classical Programming Language (PL) theory to create a Python decompiler that accomodates evolving language features and changes to the bytecode specification with minimal human maintenance effort. PyLINGUAL plugs in data-driven NLP components to a version-agnostic core to automatically absorb superficial bytecode and compiler changes, while leveraging programmatic components for abstract control flow reconstruction. To establish trust in the decompilation results, we introduce a stringent correctness measure based on “perfect decompilation”, a statically verifiable refinement of semantic equivalence. We demonstrate the efficacy of our approach with extensive real-world datasets of benign and malicious Python source code and their corresponding compiled PYC binaries. Our research makes three major contributions: (1) we present PyLINGUAL, a scalable, data-driven decompilation framework with state-of-the-art support for Python versions 3.6 through 3.12, improving the perfect decompilation rate by an average of 45% over the best results of existing decompiler across four datasets; (2) we provide a Python decompiler evaluation framework that verifies decompilation results with perfect decompilation; and (3) we launch PyLINGUAL as a public online service.},
  keywords={Codes;Translation;Program processors;Source coding;Semantics;Reverse engineering;Syntactics;Natural language processing;Security;Python;python;decompiler;nlp;reverse engineering},
  doi={10.1109/SP61157.2025.00052},
  ISSN={2375-1207},
  month={May},}@INPROCEEDINGS{6644218,
  author={Křoustek, Jakub and Pokorný, Fridolín},
  booktitle={2013 Federated Conference on Computer Science and Information Systems}, 
  title={Reconstruction of instruction idioms in a retargetable decompiler}, 
  year={2013},
  volume={},
  number={},
  pages={1519-1526},
  abstract={Machine-code decompilation is a reverse-engineering discipline focused on reverse compilation. It performs an application recovery from binary executable files back into the high level language (HLL) representation. One of its critical tasks is to produce an accurate and well-readable code. However, this is a challenging task since the executable code may be produced by one of the modern compilers that use advanced optimizations. One type of such an optimization is usage of so-called instruction idioms. These idioms are used to produce faster or even smaller executable files. On the other hand, decompilation of instruction idioms without any advanced analysis produces almost unreadable HLL code that may confuse the user of a decompiler. In this paper, we present a method of instruction-idioms detection and reconstruction back into a readable form with the same meaning. This approach is adapted in an existing retargetable decompiler developed within the Lissom project. The implementation has been tested on several modern compilers and target architectures. According to our experimental results, the proposed solution is highly accurate on the RISC (Reduced Instruction Set Computer) processor families, but it should be further improved on the CISC (Complex Instruction Set Computer) architectures.},
  keywords={Registers;Computer architecture;Optimized production technology;Computers;Operating systems;Detection algorithms;compiler optimizations;reverse engineering;decompiler;Lissom;instruction idioms;bit twiddling hacks},
  doi={},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{7546501,
  author={Yakdan, Khaled and Dechand, Sergej and Gerhards-Padilla, Elmar and Smith, Matthew},
  booktitle={2016 IEEE Symposium on Security and Privacy (SP)}, 
  title={Helping Johnny to Analyze Malware: A Usability-Optimized Decompiler and Malware Analysis User Study}, 
  year={2016},
  volume={},
  number={},
  pages={158-177},
  abstract={Analysis of malicious software is an essential task in computer security, it provides the necessary understanding to devise effective countermeasures and mitigation strategies. The level of sophistication and complexity of current malware continues to evolve significantly, as the recently discovered "Regin" malware family strikingly illustrates. This complexity makes the already tedious and time-consuming task of manual malware reverse engineering even more difficult and challenging. Decompilation can accelerate this process by enabling analysts to reason about a high-level, more abstract from of binary code. While significant advances have been made, state-of-the-art decompilers still produce very complex and unreadable code and malware analysts still frequently go back to analyzing the assembly code. In this paper, we present several semantics-preserving code transformations to make the decompiled code more readable, thus helping malware analysts understand and combat malware. We have implemented our optimizations as extensions to the academic decompiler DREAM. To evaluate our approach, we conducted the first user study to measure the quality of decompilers for malware analysis. Our study includes 6 analysis tasks based on real malware samples we obtained from independent malware experts. We evaluate three decompilers: the leading industry decompiler Hex-Rays, the state-of-the-art academic decompiler DREAM, and our usability-optimized decompiler DREAM++. The results show that our readability improvements had a significant effect on how well our participants could analyze the malware samples. DREAM++ outperforms both Hex-Rays and DREAM significantly. Using DREAM++ participants solved 3x more tasks than when using Hex-Rays and 2x more tasks than when using DREAM.},
  keywords={Malware;Reverse engineering;Binary codes;Manuals;Optimization;Security;Acceleration},
  doi={10.1109/SP.2016.18},
  ISSN={2375-1207},
  month={May},}@INPROCEEDINGS{5279917,
  author={Hamilton, James and Danicic, Sebastian},
  booktitle={2009 Ninth IEEE International Working Conference on Source Code Analysis and Manipulation}, 
  title={An Evaluation of Current Java Bytecode Decompilers}, 
  year={2009},
  volume={},
  number={},
  pages={129-136},
  abstract={Decompilation of Java bytecode is the act of transforming Java bytecode to Java source code. Although easier than that of decompilation of machine code, problems still arise in Java bytecode decompilation. These include type inference of local variables and exception-handling. Since the last such evaluation (2003) several new commercial, free and open-source Java decompilers have appeared and some of the older ones have been updated. In this paper, we evaluate the currently available Java bytecode decompilers using an extension of the criteria that were used in the original study. Although there has been a slight improvement since this study, it was found that none passed all the tests, each of which were designed to target different problem areas. We give reasons for this lack of success and suggest methods by which future Java bytecode decompilers could be improved.},
  keywords={Java;Testing;Application software;Open source software;High level languages;Virtual machining;Sun;Program processors;Optimizing compilers;Performance evaluation;java;bytecode;decompilation},
  doi={10.1109/SCAM.2009.24},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10179314,
  author={Han, HyungSeok and Kyea, JeongOh and Jin, Yonghwi and Kang, Jinoh and Pak, Brian and Yun, Insu},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)}, 
  title={QueryX: Symbolic Query on Decompiled Code for Finding Bugs in COTS Binaries}, 
  year={2023},
  volume={},
  number={},
  pages={3279-3295},
  abstract={Extensible static checking tools, such as Sys and CodeQL, have successfully discovered bugs in source code. These tools allow analysts to write application-specific rules, referred to as queries. These queries can leverage the domain knowledge of analysts, thereby making the analysis more accurate and scalable. However, the majority of these tools are inapplicable to binary-only analysis. One exception, joern, translates a binary code into decompiled code and feeds the decompiled code into an ordinary C code analyzer. However, this approach is not sufficiently precise for symbolic analysis, as it overlooks the unique characteristics of decompiled code. While binary analysis platforms, such as angr, support symbolic analysis, analysts must understand their intermediate representations (IRs) although they are mostly working with decompiled code.In this paper, we propose a precise and scalable symbolic analysis called fearless symbolic analysis that uses intuitive queries for binary code and implement this in QueryX. To make the query intuitive, QueryX enables analysts to write queries on top of decompiled code instead of IRs. In particular, QueryX supports callbacks on decompiled code, using which analysts can control symbolic analysis to discover bugs in the code. For precise analysis, we lift decompiled code into our IR named DNR and perform symbolic analysis on DNR while considering the characteristics of the decompiled code. Notably, DNR is only used internally such that it allows analysts to write queries regardless of using DNR. For scalability, QueryX automatically reduces control-flow graphs using callbacks and ordering dependencies between callbacks that are specified in the queries. We applied QueryX to the Windows kernel, the Windows system service, and an automotive binary. As a result, we found 15 unique bugs including 10 CVEs and earned $180,000 from the Microsoft bug bounty program.},
  keywords={Privacy;Codes;Scalability;Source coding;Computer bugs;Binary codes;Security},
  doi={10.1109/SP46215.2023.10179314},
  ISSN={2375-1207},
  month={May},}@INPROCEEDINGS{10179370,
  author={Ahad, Ali and Jung, Chijung and Askar, Ammar and Kim, Doowon and Kim, Taesoo and Kwon, Yonghwi},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)}, 
  title={Pyfet: Forensically Equivalent Transformation for Python Binary Decompilation}, 
  year={2023},
  volume={},
  number={},
  pages={3296-3313},
  abstract={Decompilation is a crucial capability in forensic analysis, facilitating analysis of unknown binaries. The recent rise of Python malware has brought attention to Python decompilers that aim to obtain source code representation from a Python binary. However, Python decompilers fail to handle various binaries, limiting their capabilities in forensic analysis.This paper proposes a novel solution that transforms a decompilation error-inducing Python binary into a decompilable binary. Our key intuition is that we can resolve the decompilation errors by transforming error-inducing code blocks in the input binary into another form. The core of our approach is the concept of Forensically Equivalent Transformation (FET) which allows non-semantic preserving transformation in the context of forensic analysis. We carefully define the FETs to minimize their undesirable consequences while fixing various error-inducing instructions that are difficult to solve when preserving the exact semantics. We evaluate the prototype of our approach with 17,117 real-world Python malware samples causing decompilation errors in five popular decompilers. It successfully identifies and fixes 77,022 errors. Our approach also handles anti-analysis techniques, including opcode remapping, and helps migrate Python 3.9 binaries to 3.8 binaries.},
  keywords={Privacy;Limiting;Forensics;Source coding;Semantics;Field effect transistors;Prototypes;Reverse-Engineering;Decompilation;Binary-Transformation;Python-Malware},
  doi={10.1109/SP46215.2023.10179370},
  ISSN={2375-1207},
  month={May},}@ARTICLE{10967090,
  author={Sateanpattanakul, Siwadol and Jetpipattanapong, Duangpen and Mathulaprangsan, Seksan},
  journal={Journal of Mobile Multimedia}, 
  title={Java Bytecode Control Flow Classification: Framework for Guiding Java Decompilation}, 
  year={2022},
  volume={18},
  number={2},
  pages={179-202},
  abstract={Decompilation is the main process of software development, which is very important when a program tries to retrieve lost source codes. Although decompiling Java bytecode is easier than bytecode, many Java decompilers cannot recover originally lost sources, especially the selection statement, i.e., if statement. This deficiency affects directly decompilation performance. In this paper, we propose the methodology for guiding Java decompiler to deal with the aforementioned problem. In the framework, Java bytecode is transformed into two kinds of features called frame feature and latent semantic feature. The former is extracted directly from the bytecode. The latter is achieved by two-step transforming the Java bytecode to bigram and then term frequency-inverse document frequency (TFIDF). After that, both of them are fed to the genetic algorithm to reduce their dimensions. The proposed feature is achieved by converting the selected TFIDF to a latent semantic feature and concatenating it with the selected frame feature. Finally, KNN is used to classify the proposed feature. The experimental results show that the decompilation accuracy is 93.68 percent, which is obviously better than Java Decompiler.},
  keywords={Java;Accuracy;Source coding;Semantics;Nearest neighbor methods;Feature extraction;Genetic algorithms;Software development management;Indexing;Decompilation;feature selection;latent semantic indexing;genetic algorithm},
  doi={10.13052/jmm1550-4646.1822},
  ISSN={1550-4654},
  month={March},}@INPROCEEDINGS{10986108,
  author={Sharikov, Pavel and Krasov, Andrey and Chechulin, Andrey},
  booktitle={2025 International Russian Smart Industry Conference (SmartIndustryCon)}, 
  title={Methodology for Embedding a Digital Watermark in Java Application Class Files Resistant to Decompilation Attacks Aimed at Its Destruction}, 
  year={2025},
  volume={},
  number={},
  pages={906-911},
  abstract={This paper examines the development of a methodology for embedding a digital watermark in class files by replacing bytecode opcodes and adding certain Java programming language constructs that hinder decompilation. The possible actions of an intruder are considered, along with the methods by which an attacker might obtain a class file or a Java application. An analysis of existing decompilers is conducted, with examples of programming language constructs that lead to decompilation errors or incorrect decompilation results. The principle of the methodology is demonstrated, and experiments are conducted to confirm that after embedding the digital watermark using the developed approach, it remains resilient to decompilation attacks aimed at its removal. The compiled class file containing the modified bytecode is analyzed. It is proven that the logic of the Java application remains unchanged after editing the bytecode of the class file and recompiling it. Certain constructs and code design patterns are examined, demonstrating that specific constructs in the source code increases the likelihood of incorrect decompilation of Java application class files, the inability to recompile the decompiled code.},
  keywords={Resistance;Java;Computer languages;Codes;Source coding;Watermarking;Software;Logic;Usability;Resilience;Java;bytecode;digital watermark;decompilation attack;class file;obfuscation},
  doi={10.1109/SmartIndustryCon65166.2025.10986108},
  ISSN={},
  month={March},}@INPROCEEDINGS{8952404,
  author={Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={DIRE: A Neural Approach to Decompiled Identifier Naming}, 
  year={2019},
  volume={},
  number={},
  pages={628-639},
  abstract={The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.},
  keywords={Tools;Recurrent neural networks;Reverse engineering;Training;Software;Analytical models;Decompilation;Deep learning},
  doi={10.1109/ASE.2019.00064},
  ISSN={2643-1572},
  month={Nov},}@INPROCEEDINGS{1374303,
  author={Emmerik, M.V. and Waddington, T.},
  booktitle={11th Working Conference on Reverse Engineering}, 
  title={Using a decompiler for real-world source recovery}, 
  year={2004},
  volume={},
  number={},
  pages={27-36},
  abstract={Despite their 40 year history, native executable decompilers have found very limited practical application in commercial projects. The success of Java decompilers is well known, and a few decompilers perform well by recognising patterns from specific compilers. This work describes the experience gained from applying a native executable decompiler, assisted by a commercial disassembler and hand editing, to a real-world Windows-based application. The clients had source code for a prototype version of the program, and an executable that performed better, for which the source code was not available. The project was to recover the algorithm at the core of the program, and if time permitted, the recovery of other pieces of source code. Despite the difficulties, the core algorithm was successfully decompiled, and a portion of the rest of the program as well. There were surprises, including the ability to recover almost all original class names, and the complete class hierarchy.},
  keywords={Prototypes;Java;Reverse engineering;History;Software maintenance;Australia;Pattern recognition;Speech analysis;Mathematics;User interfaces;Reverse engineering;decompilation;source code recovery;native executable file;experience},
  doi={10.1109/WCRE.2004.42},
  ISSN={1095-1350},
  month={Nov},}@INPROCEEDINGS{10123452,
  author={Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie},
  booktitle={2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, 
  title={Extending Source Code Pre-Trained Language Models to Summarise Decompiled Binaries}, 
  year={2023},
  volume={},
  number={},
  pages={260-271},
  abstract={Binary reverse engineering is used to understand and analyse programs for which the source code is unavailable. Decompilers can help, transforming opaque binaries into a more readable source code-like representation. Still, reverse engineering is difficult and costly, involving considering effort in labelling code with helpful summaries. While the automated summarisation of decompiled code can help reverse engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise de-compiled binary functions. Further-more, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by removing identifiers, and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82 and, 44.21 for summarising source, decompiled, and obfuscated decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
  keywords={Source coding;Reverse engineering;Binary codes;Transformers;Data models;Software;Labeling;Task analysis;Optimization;Synthetic data;Decompilation;Binary;Reverse Engineering;Summarization;Deep Learning;Pre-trained Language Models;CodeT5;Transformers},
  doi={10.1109/SANER56733.2023.00033},
  ISSN={2640-7574},
  month={March},}@INPROCEEDINGS{9589721,
  author={Alsabbagh, Wael and Langendörfer, Peter},
  booktitle={IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society}, 
  title={A Control Injection Attack against S7 PLCs -Manipulating the Decompiled Code}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={In this paper, we discuss an approach which allows an attacker to modify the control logic program that runs in S7 PLCs in its high-level decompiled format. Our full attack-chain compromises the security measures of PLCs, retrieves the machine bytecode of the target device, and employs a decompiler to convert the stolen compiled bytecode (low-level) to its decompiled version (high-level) e.g. Ladder Diagram LAD. As the LAD code exposes the structure and semantics of the control logic, our attack also manipulates the LAD code based on the attacker’s understanding to the physical process causing abnormal behaviors of the system that we target. Finally, it converts the infected LAD code to its executable version i.e. machine bytecode that can run on the PLC using a compiler before pushing the malicious code back to the PLC. For a real scenario, we implemented our full attack-chain on a small industrial setting using real S7-300 PLCs, and built the database (for our decompiler and compiler) using 108 different control logic programs of varying complexity, ranging from simple programs consisting of a few instructions to more complex ones including multi functions, sub-functions and data blocks. We tested and evaluated the accuracy of our decompiler and compiler on 5 random programs written for real industrial applications. Our experimental results showed that an external adversary is able to infect S7 PLCs successfully. We eventually suggest some potential mitigation approaches to secure systems against such a threat.},
  keywords={Industrial electronics;Electric potential;Codes;Program processors;Semantics;Programmable logic devices;Process control;Programmable Logic Controllers (PLCs);Control Injection Attack;Decompiler;Compiler;Ladder Diagram},
  doi={10.1109/IECON48115.2021.9589721},
  ISSN={2577-1647},
  month={Oct},}@ARTICLE{11207559,
  author={Liao, Zeqin and Nan, Yuhong and Gao, Zixu and Liang, Henglong and Hao, Sicheng and Ren, Peifan and Zheng, Zibin},
  journal={IEEE Transactions on Software Engineering}, 
  title={Augmenting Smart Contract Decompiler Output Through Fine-Grained Dependency Analysis and LLM-Facilitated Semantic Recovery}, 
  year={2025},
  volume={51},
  number={12},
  pages={3574-3590},
  abstract={Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate function identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA’s accuracy in control and data flow analysis and LLM’s capability in semantic prediction. More specifically, SmartHalo constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract functions shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o mini with SmartHalo further enhances its performance, achieving a precision of 91.32% and a recall of 87.38% for function boundaries, a precision of 90.40% and a recall of 88.82% for variable types, and a precision of 80.66% and a recall of 91.78% for contract attributes.},
  keywords={Smart contracts;Codes;Optimization;Source coding;Static analysis;Large language models;Accuracy;Training;Semantics;Annotations;Smart contract;decompilation;static analysis;large language model},
  doi={10.1109/TSE.2025.3623325},
  ISSN={1939-3520},
  month={Dec},}@INPROCEEDINGS{10764949,
  author={She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
  booktitle={2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={WaDec: Decompiling WebAssembly Using Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={481-492},
  abstract={WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm’s binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art’s 116.94%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. In summary, WaDec enhances understanding of the code’s structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},
  keywords={Training;Measurement;Codes;Large language models;Source coding;Self-supervised learning;Security;Optimization;Faces;Software engineering;wasm;readability;binary;webassembly;llm;large language model;decompilation;finetune},
  doi={},
  ISSN={2643-1572},
  month={Oct},}@INPROCEEDINGS{11334528,
  author={Liu, Jiayang and Zhao, Yanjie and Xia, Pengcheng and Wang, Haoyu},
  booktitle={2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={APKARMOR: Low-Cost Lightweight Anti-Decompilation Techniques for Android Apps}, 
  year={2025},
  volume={},
  number={},
  pages={3180-3191},
  abstract={Android app security is a critical concern for the software industry, with companies investing significantly in protecting their intellectual property from reverse engineering attacks. While commercial protection tools exist to prevent decompilation and unauthorized code access, they pose substantial challenges for businesses: high licensing costs ranging from thousands to tens of thousands of dollars annually, significant performance overhead that impacts user experience and app ratings, and increased app size that affects download rates. These limitations particularly burden small to medium-sized enterprises and independent developers, creating an urgent industry need for cost-effective protection solutions.To address these challenges, we propose a novel file format-based anti-decompilation strategy that systematically exploits structural vulnerabilities in APK files. Building upon this strategy, we have developed APKARMOR, a lightweight and cost-effective anti-decompilation framework that exploits inherent vulnerabilities in popular reverse engineering tools. Through systematic analysis, we first identified critical weaknesses in common decompilation tools’ parsing mechanisms and structural assumptions. Based on these findings, we developed seven mutation-based protection strategies that deliberately trigger these vulnerabilities by introducing specific structural anomalies into APK files and the AndroidManifest.xml. These methods include Countermeasures against Dirty Code and Corrupted Payloads (CACoP), Pseudo-Encryption (PE), Using Unknown Compression Method (UUCM), Unavailable Magic Value (UMA), Modify the Offset Field in stringChunk (MOFS), and Dirty Bytecode Replacement of "Android" (DRA). We evaluated our exploitation strategies through extensive experiments on 100 randomly selected Android apps, testing against the latest versions of three widely used decompilation tools: JADX (v1.5.1), APKTool (v2.11.0), and Androguard (v4.1.2). Our results demonstrate that PE and DRA achieved complete protection by successfully exploiting vulnerabilities present in all tested tools. MOFS, UUCM, and UNV effectively exploited weaknesses in APKTool and Androguard’s parsing mechanisms.},
  keywords={Industries;Codes;Systematics;Reverse engineering;User experience;Software;Security;Protection;Testing;Software engineering},
  doi={10.1109/ASE63991.2025.00262},
  ISSN={2643-1572},
  month={Nov},}@INPROCEEDINGS{5714442,
  author={Fokin, A. and Troshina, K. and Chernov, A.},
  booktitle={2010 14th European Conference on Software Maintenance and Reengineering}, 
  title={Reconstruction of Class Hierarchies for Decompilation of C++ Programs}, 
  year={2010},
  volume={},
  number={},
  pages={240-243},
  abstract={This paper presents a method for automatic reconstruction of polymorphic class hierarchies from the assembly code obtained by compiling a C++ program. If the program is compiled with run-time type information (RTTI), class hierarchy is reconstructed via analysis of RTTI structures. In case RTTI structures are missing in the assembly, a technique based on the analysis of virtual function tables, constructors and destructors is used. A tool for automatic reconstruction of polymorphic class hierarchies that implements the described technique is presented. This tool is implemented as a plug in for IDA Pro Interactive Disassembler. Experimental study of the tool is provided.},
  keywords={Assembly;Computer languages;Open source software;Electronic mail;Optimization;History;decompilation;reverse engineering;C++;class hierarchy reconstruction},
  doi={10.1109/CSMR.2010.43},
  ISSN={1534-5351},
  month={March},}@INPROCEEDINGS{957846,
  author={Cifuentes, C. and Waddington, T. and Van Emmerik, M.},
  booktitle={Proceedings Eighth Working Conference on Reverse Engineering}, 
  title={Computer security analysis through decompilation and high-level debugging}, 
  year={2001},
  volume={},
  number={},
  pages={375-380},
  abstract={The extensive use of computers and networks worldwide has raised the awareness of the need for tools and techniques to aid in computer security analysis of binary code, such as the understanding of viruses, trojans, worms, backdoors and general security flaws, in order to provide immediate solutions with or without the aid of software vendors. The paper is a proposal for a high-level debugging tool to be used by computer security experts, which will reduce the amount of time needed to solve security-related problems in executable programs. The current state of the art involves the tracing of thousands of lines of assembly code using a standard debugger. A high-level debugger would be capable of displaying a high-level representation of an executable program in the C language, hence reducing the number of lines that need to be inspected by an order of magnitude (i.e. hundreds instead of thousands of lines). Effectively, these techniques will help in reducing the amount of time needed to trace a security flaw in an executable program, as well as reducing the costs of acquiring or training skilled assembler engineers.},
  keywords={Computer security;Assembly;Computer networks;Binary codes;Computer viruses;Computer worms;Software tools;Proposals;Debugging;Code standards},
  doi={10.1109/WCRE.2001.957846},
  ISSN={1095-1350},
  month={Oct},}@INPROCEEDINGS{7550827,
  author={Sateanpattanakul, Siwadol},
  booktitle={2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)}, 
  title={Comments recovery approach for Java decompiler}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={Java bytecode contains many data that relative to represent the program's behavior. It consists of many crucial to information such as offsets and sets of instruction. Those information are used to driven a program. All-important materials in those document can be used to organize in the reverse engineering process. It changes all sets of bytecode instruction to earlier source code. Although many sets of byte code instructions can be restored in source code, but its comments cannot be recovered. Because Java bytecode is not contains its comments. Supposing those comments are collected and provided in the proper format. It is a possibility for recovering original source code. This paper purposes the methodology for transforming Java bytecode and its comments from the AOCF to the original source code. The result represents all line of comments can be recovered by an extension. And the correctness of recovery is 90.9 % for control-flow test cases.},
  keywords={Decision support systems;Hafnium compounds;bytecode reverse engieering;Java decompiler;software reuseable;software engineering},
  doi={10.1109/ICIS.2016.7550827},
  ISSN={},
  month={June},}@ARTICLE{6894210,
  author={Cen, Lei and Gates, Christoher S. and Si, Luo and Li, Ninghui},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={A Probabilistic Discriminative Model for Android Malware Detection with Decompiled Source Code}, 
  year={2015},
  volume={12},
  number={4},
  pages={400-412},
  abstract={Mobile devices are an important part of our everyday lives, and the Android platform has become a market leader. In recent years a number of approaches for Android malware detection have been proposed, using permissions, source code analysis, or dynamic analysis. In this paper, we propose to use a probabilistic discriminative model based on regularized logistic regression for Android malware detection. Through extensive experimental evaluation, we demonstrate that it can generate probabilistic outputs with highly accurate classification results. In particular, we propose to use Android API calls as features extracted from decompiled source code, and analyze and explore issues in feature granularity, feature representation, feature selection, and regularization. We show that the probabilistic discriminative model also works well with permissions, and substantially outperforms the state-of-the-art methods for Android malware detection with application permissions. Furthermore, the discriminative learning model achieves the best detection results by combining both decompiled source code and application permissions. To the best of our knowledge, this is the first research that proposes probabilistic discriminative model for Android malware detection with a thorough study of desired representation of decompiled source code and is the first research work for Android malware detection task that combines both analysis of decompiled source code and application permissions.},
  keywords={Feature extraction;Malware;Androids;Humanoid robots;Smart phones;Probabilistic logic;Measurement;Android;malicious application;machine learning;discriminative model},
  doi={10.1109/TDSC.2014.2355839},
  ISSN={1941-0018},
  month={July},}@INPROCEEDINGS{5601851,
  author={Troshina, Katerina and Derevenets, Yegor and Chernov, Alexander},
  booktitle={2010 10th IEEE Working Conference on Source Code Analysis and Manipulation}, 
  title={Reconstruction of Composite Types for Decompilation}, 
  year={2010},
  volume={},
  number={},
  pages={179-188},
  abstract={Decompilation is reconstruction of a program in a high-level language from a program in a low-level language. This paper presents a method for automatic reconstruction of composite types (structures, arrays and combinations of them)in a high-level program during decompilation. Assembly code is obtained by disassembling a binary code or traces collected by a simulator. The proposed method is based on expressing memory access operations as pairs base offset, then building equivalence classes for the bases used in the program and accumulating offsets for each equivalence class. For Strictly conforming C programs our approach is substantiated by the C language semantics as defined in the international standard. However, experimental results have revealed that it is applicable for real-world programs also. Experimental results are obtained for a number of open-source programs as well as for traces collected from them. The method is an essential part of the tool for program decompilation TyDec being developed by the authors. Decompiler TyDec can be used as a standalone tool or as a plug-in for Interactive Trace Explorer TrEx being developed in Institute for System Programming, Russian Academy of Sciences.},
  keywords={Registers;Assembly;Indexes;Programming;Algorithm design and analysis;Arrays;reverse engineering;decompilation;data type reconstruction},
  doi={10.1109/SCAM.2010.24},
  ISSN={},
  month={Sep.},}@ARTICLE{10035436,
  author={Park, Jihee and Lee, Sungho and Hong, Jaemin and Ryu, Sukyoung},
  journal={IEEE Transactions on Software Engineering}, 
  title={Static Analysis of JNI Programs via Binary Decompilation}, 
  year={2023},
  volume={49},
  number={5},
  pages={3089-3105},
  abstract={JNI programs are widely used thanks to the combined benefits of C and Java programs. However, because understanding the interaction behaviors between two different programming languages is challenging, JNI program development is difficult to get right and vulnerable to security attacks. Thus, researchers have proposed static analysis of JNI program source code to detect bugs and security vulnerabilities in JNI programs. Unfortunately, such source code analysis is not applicable to compiled JNI programs that are not open-sourced or open-source JNI programs containing third-party binary libraries. While JN-SAF, the state-of-the-art analyzer for compiled JNI programs, can analyze binary code, it has several limitations due to its symbolic execution and summary-based bottom-up analysis. In this paper, we propose a novel approach to statically analyze compiled JNI programs without their source code using binary decompilation. Unlike JN-SAF that analyzes binaries directly, our approach decompiles binaries and analyzes JNI programs with the decompiled binaries using an existing JNI program analyzer for source code. To decompile binaries to compilable C source code with precise JNI-interoperation-related types, we improve an existing decompilation tool by leveraging the characteristics of JNI programs. Our evaluation shows that the approach is precise as almost the same as the state-of-the-art JNI program analyzer for source code, and more precise than JN-SAF.},
  keywords={Java;Codes;Source coding;Static analysis;Libraries;Computer architecture;Security;Java native interface;binary decompilation;static analysis},
  doi={10.1109/TSE.2023.3241639},
  ISSN={1939-3520},
  month={May},}@INPROCEEDINGS{6703690,
  author={Yakdan, Khaled and Eschweiler, Sebastian and Gerhards-Padilla, Elmar},
  booktitle={2013 8th International Conference on Malicious and Unwanted Software: "The Americas" (MALWARE)}, 
  title={REcompile: A decompilation framework for static analysis of binaries}, 
  year={2013},
  volume={},
  number={},
  pages={95-102},
  abstract={Reverse engineering of binary code is an essential step for malware analysis. However, it is a tedious and time-consuming task. Decompilation facilitates this process by transforming machine code into a high-level representation that is more concise and easier to understand. This paper describes REcompile, an efficient and extensible decompilation framework. REcompile uses the static single assignment form (SSA) as its intermediate representation and performs three main classes of analysis. Data flow analysis removes machine-specific details from code and transforms it into a concise high-level form. Type analysis finds variable types based on how those variables are used in code. Control flow analysis identifies high-level control structures such as conditionals, loops, and switch statements. These steps enable REcompile to produce well-readable decompiled code. The overall evaluation, using real programs and malware samples, shows that REcompile achieves a comparable and in many cases better performance than state-of-the-art decompilers.},
  keywords={Malware;Algorithm design and analysis;Semantics;Software;Transforms;Standards;Control systems},
  doi={10.1109/MALWARE.2013.6703690},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{11068876,
  author={Yang, Yuwei and Grandel, Skyler and Lacomis, Jeremy and Schwartz, Edward and Vasilescu, Bogdan and Le Goues, Claire and Leach, Kevin},
  booktitle={2025 55th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)}, 
  title={A Human Study of Automatically Generated Decompiler Annotations}, 
  year={2025},
  volume={},
  number={},
  pages={129-142},
  abstract={Reverse engineering is a crucial technique in software security, enabling professionals to analyze malware, identify vulnerabilities, and patch legacy software without access to source code. Although decompilers attempt to reconstruct high-level code from binaries, essential information, such as variable names and types, is often dissimilar from the original version, hindering readability and comprehension.Recent advancements have employed AI to enhance decompiler output by recovering original variable names and types. Traditional evaluation of recovery techniques relies on measuring similarity between original and recovered names, assuming that higher similarity enhances readability. However, studies suggest that these "intrinsic" metrics may not accurately predict "extrinsic" outcomes like user comprehension or task performance, revealing a gap in understanding readability and cognitive load in reverse engineering.This paper presents an extrinsic evaluation of machine-generated variable and type names, focusing on their impact on reverse engineers’ comprehension of decompiled code. We conducted a user study with 40 participants—including students and professionals—to assess code comprehension both with and without AI-generated variable and type name assistance. Our findings indicate a lack of correlation between traditional machine learning metrics and actual comprehension gains, highlighting limitations in current evaluation techniques. Despite this, participants showed a preference for AI-augmented decompiler outputs. These insights contribute to understanding the effectiveness of automatic recovery techniques in enhancing reverse engineering tasks and underscore the need for comprehensive, user-centered evaluation frameworks.},
  keywords={Measurement;Codes;Correlation;Source coding;Reverse engineering;Focusing;Machine learning;Cognitive load;Malware;Security;Decompilation;Binary Reverse Engineering;Human Study},
  doi={10.1109/DSN64029.2025.00026},
  ISSN={2158-3927},
  month={June},}@INPROCEEDINGS{10795101,
  author={Zhang, Runze and Cao, Ying and Liang, Ruigang and Hu, Peiwei and Chen, Kai},
  booktitle={2024 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Optimizing Decompiler Output by Eliminating Redundant Data Flow in Self-Recursive Inlining}, 
  year={2024},
  volume={},
  number={},
  pages={38-49},
  abstract={Decompilation, which aims to lift a binary to a high-level language such as C, is one of the most common approaches software security analysts use for analyzing binary code. Recovering decompiled code with high readability is essential, as humans must understand the code's functionality correctly. However, some compilation optimization strategies will introduce obfuscation into the binary code, thereby reducing the readability of decompiled code. Among them, the function inlining related optimization strategies combine functions, causing the original function's code volume and complexity to multiply. Especially with self-recursive inlining optimization, it transforms initially simple functions into ones with significantly increased code volume and complex logic, greatly hindering the understanding of security engineers. In this paper, we present Erase, the first approach to reverse the self-recursive inlining optimization technique. We compare Erase with state-of-the-art decompilers Ghidra and Hex-Rays to evaluate ERASE's improvement for the functions affected by self-recursive inlining. Experimental results show that Erase's output is 78.4% and 88.9% more compact (fewer lines of code) than Ghidra and Hex-Rays, respectively. Moreover, reverse engineers spend 88.5% less time analyzing ERASE's output than analyzing Ghidra and 90.4% less time than analyzing Hex-Rays, and the accuracy of analyzing Erase's output is 2.75 times higher than both Ghidra and Hex-Rays.},
  keywords={Software maintenance;Accuracy;Systematics;Binary codes;Transforms;Complexity theory;Security;Logic;High level languages;Optimization;Reverse Engineering;Decompilation;Compiler Optimization;Self-Recursive Inlining;Program Analysis},
  doi={10.1109/ICSME58944.2024.00015},
  ISSN={2576-3148},
  month={Oct},}@INPROCEEDINGS{5630895,
  author={Troshina, K. and Derevenets, Y. and Chernov, A.},
  booktitle={2010 International Conference on Multimedia Technology}, 
  title={Reconstruction of composite types for Decompilation}, 
  year={2010},
  volume={},
  number={},
  pages={1-4},
  abstract={Decompilation is reconstruction of a program in a highlevel language from a program in a low-level language. This paper presents a method for automatic reconstruction of composite types (structures, arrays and combinations of them) in a high-level program during decompilation. Assembly code is obtained by disassembling a binary code or traces collected by a simulator. The proposed method is based on expressing memory access operations as pairs (base+offset), then building equivalence classes for the bases used in the program and accumulating offsets for each equivalence class. For Strictly conforming C programs our approach is substantiated by the C language semantics as defined in the international standard. However, experimental results have revealed that it is applicable for real-world programs also. Experimental results are obtained for a number of opensource programs as well as for traces collected from them. The method is an essential part of the tool for program decompilation TyDec being developed by the authors. Decompiler TyDec can be used as a standalone tool or as a plug-in for Interactive Trace Explorer TrEx being developed in Institute for System Programming, Russian Academy of Sciences.},
  keywords={Frequency modulation;Time frequency analysis;Feature extraction;Filter bank;Signal resolution;Signal to noise ratio;Parameter extraction},
  doi={10.1109/ICMULT.2010.5630895},
  ISSN={},
  month={Oct},}@ARTICLE{11367734,
  author={Liu, Li and Sun, Fanghui and Wang, Shen and Jiang, Xunzhi},
  journal={IEEE Internet of Things Journal}, 
  title={DECodeT5: A Lightweight and Efficient Neural Decompiler with Assembly Semantic Assistance}, 
  year={2026},
  volume={},
  number={},
  pages={1-1},
  abstract={Decompilation plays a critical role in firmware analysis and reverse engineering by enabling the recovery of high-level source code from binary executables. However, existing neural decompilation models often face challenges due to the semantic gap between assembly code and high-level languages, and they typically require large-scale models that impose significant computational demands. In this paper, we propose DECodeT5, a lightweight and efficient neural decompilation method for the C language. DECodeT5 builds upon the CodeT5 code generation model by integrating a pre-trained assembly encoder in place of its original embedding layer. This modification allows for more effective semantic learning from assembly code, improving the model’s ability to capture the intricacies of assembly-to-source code mapping. The architecture of DECodeT5 not only accelerates convergence during end-to-end decompilation tasks but also supports rapid adaptation across different compilers and instruction set architectures (ISAs) by swapping out the encoder module as needed. Our experimental evaluation on the HumanEval and Exebench benchmarks reveals that DECodeT5 significantly improves semantic recovery accuracy, outperforming Ghidra by over 83% and LLM4Decompile by more than 43% in terms of execution recovery. Furthermore, DECodeT5 maintains a compact model size of only 360M parameters, providing inference speeds that are twice as fast as LLM4Decompile. These results underscore DECodeT5’s suitability for deployment in resource-constrained environments and its flexibility in adapting to real-world reverse engineering scenarios, offering a practical solution for modern firmware analysis and security tasks.},
  keywords={Codes;Semantics;Assembly;Source coding;Optimization;Instruction sets;Computer architecture;Adaptation models;Microprogramming;Internet of Things;Decompile;reverse engineering;firmware security;deep neural network},
  doi={10.1109/JIOT.2026.3659328},
  ISSN={2327-4662},
  month={},}@INPROCEEDINGS{10444788,
  author={Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F.P.},
  booktitle={2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, 
  title={SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly}, 
  year={2024},
  volume={},
  number={},
  pages={67-80},
  abstract={Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
  keywords={Codes;Transformers;Security;Task analysis;Optimization;Standards;Engines;decompilation;neural decompilation;Transformer;language models;type inference},
  doi={10.1109/CGO57630.2024.10444788},
  ISSN={2643-2838},
  month={March},}@INPROCEEDINGS{10646727,
  author={Pal, Kuntal Kumar and Bajaj, Ati Priya and Banerjee, Pratyay and Dutcher, Audrey and Nakamura, Mutsumi and Basque, Zion Leonahenahe and Gupta, Himanshu and Sawant, Saurabh Arjun and Anantheswaran, Ujjwala and Shoshitaishvili, Yan and Doupé, Adam and Baral, Chitta and Wang, Ruoyu},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={"Len or index or count, anything but v1": Predicting Variable Names in Decompilation Output with Transfer Learning}, 
  year={2024},
  volume={},
  number={},
  pages={4069-4087},
  abstract={Binary reverse engineering is an arduous and tedious task performed by skilled and expensive human analysts. Information about the source code is irrevocably lost in the compilation process. While modern decompilers attempt to generate C-style source code from a binary, they cannot recover lost variable names. Prior works have explored machine learning techniques for predicting variable names in decompiled code. However, the state-of-the-art systems, DIRE and DIRTY, generalize poorly to functions in the testing set that are not included in the training set—31.8% for DIRE on DIRTY’s data set and 36.9% for DIRTY on DIRTY’s data set.In this paper, we present VarBERT, a Bidirectional Encoder Representations from Transformers (BERT) to predict meaningful variable names in decompilation output. An advantage of VarBERT is that we can pre-train on human source code and then fine-tune the model to the task of predicting variable names. We also create a new data set VarCorpus, which significantly expands the size and variety of the data set. Our evaluation of VarBERT on VarCorpus, demonstrates a significant improvement in predicting the developer’s original variable names for O2 optimized binaries achieving accuracies of 54.43% for IDA and 54.49% for Ghidra. VarBERT is strictly better than state-of-the-art techniques: On a subset of VarCorpus, VarBERT could predict the developer’s original variable names 50.70% of the time, while DIRE and DIRTY predicted original variable names 35.94% and 38.00% of the time, respectively.},
  keywords={Training;Privacy;Codes;Source coding;Transfer learning;Reverse engineering;Transformers;Program and binary analysis;Machine learning and computer security;Decompilation},
  doi={10.1109/SP54263.2024.00152},
  ISSN={2375-1207},
  month={May},}@INPROCEEDINGS{8973072,
  author={Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan},
  booktitle={2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)}, 
  title={Meaningful Variable Names for Decompiled Code: A Machine Translation Approach}, 
  year={2018},
  volume={},
  number={},
  pages={20-30},
  abstract={When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2% of the time, which represents a lower bound for performance.},
  keywords={Lower bound;Codes;Translation;Source coding;Distortion;Noise measurement;Machine translation;Channel models;Standards;Software development management;Decompilation;Understandability;Statistical Machine Translation;Renaming Identifiers},
  doi={},
  ISSN={2643-7171},
  month={May},}@INPROCEEDINGS{6385101,
  author={Durfina, Luká and Kroustek, Jakub and Zemek, Petr and Kábele, Bretislav},
  booktitle={2012 19th Working Conference on Reverse Engineering}, 
  title={Detection and Recovery of Functions and their Arguments in a Retargetable Decompiler}, 
  year={2012},
  volume={},
  number={},
  pages={51-60},
  abstract={Detection and recovery of high-level control structures, such as functions and their arguments, plays an important role in decompilation. It has a direct impact on the quality of the generated code because it is needed for generating functionally equivalent and highly readable code. In this paper, we present an innovative, platform-independent method of detection and recovery of functions and their arguments. This method is based on static code interpretation and iterative bidirectional search over reconstructed basic blocks. This approach has been adopted and tested in an existing retarget able decompiler. According to our experimental results, the proposed retarget able solution is fully competitive with existing hand-coded decompilers.},
  keywords={Semantics;Detectors;Registers;Abstracts;Encoding;Debugging;Reverse engineering;decompilation;reverse engineering;control-flow analysis;function detection;Lissom},
  doi={10.1109/WCRE.2012.15},
  ISSN={2375-5369},
  month={Oct},}@ARTICLE{10740475,
  author={Reiter, Pemma and Tay, Hui Jun and Weimer, Westley and Doupé, Adam and Wang, Ruoyu and Forrest, Stephanie},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Automatically Mitigating Vulnerabilities in Binary Programs via Partially Recompilable Decompilation}, 
  year={2025},
  volume={22},
  number={3},
  pages={2270-2282},
  abstract={Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD) to help automate the process. PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically succeed on an entire binary, our approach does because it is limited to a few functions, such as those identified by our binary fault localization. We evaluate the assumptions underlying our approach and find that, without any grammar or compilation restrictions, up to 79% of individual functions are successfully decompiled and recompiled. In comparison, only 1.7% of the full C-binaries succeed. When recompilation succeeds, PRD produces test-equivalent binaries 93.0% of the time. We evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; and human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, perform as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with the same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, the evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries.},
  keywords={Maintenance engineering;Source coding;Codes;Location awareness;Software;Computer bugs;Prototypes;Measurement;Grammar;C++ languages;Software engineering;software maintenance},
  doi={10.1109/TDSC.2024.3482413},
  ISSN={1941-0018},
  month={May},}@INPROCEEDINGS{11334294,
  author={Sun, Xinyu and Tang, Fugen and Zhang, Yu and Shen, Han and Song, Chengru and Zhang, Di},
  booktitle={2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Enhancing LLM to Decompile Optimized PTX to Readable CUDA for Tensor Programs}, 
  year={2025},
  volume={},
  number={},
  pages={2235-2247},
  abstract={The growing demand for high-performance tensor programs on GPUs, especially for large language models (LLMs), necessitates advanced compilation and optimization techniques. However, the critical task of analyzing optimized, low-level PTX code for performance tuning or understanding poses significant challenges. While LLMs hold promise for PTX-to-CUDA de-compilation to improve code intelligibility, their effectiveness is severely limited by the scarcity of aligned training data and the inherent complexity of highly optimized, unrolled PTX code.In this work, we explore methodologies to significantly enhance LLM capabilities for accurate and readable PTX-to-CUDA decompilation and present PtxDec, a decompilation prototype implementing our approach. To overcome the critical barrier of data scarcity, we develop a compiler-based data augmentation framework coupled with rigorous post-processing, enabling the creation of a large-scale, high-quality dataset of 400K aligned CUDA-PTX kernel pairs for effective LLM training. Furthermore, to empower LLMs to handle the complexity of optimized PTX, we introduce Rolled-PTX—an intermediate representation generated through heuristic loop rerolling during preprocessing. Rolled-PTX condenses unrolled patterns, drastically simplifying the input structure presented to the LLM and aligning it better with higher-level loop constructs.Comprehensive evaluation demonstrates that PtxDec achieves substantial performance gains: our approach yields a 2.3×–3.1× improvement in functional accuracy over baseline methods, alongside significant enhancements in generated code readability and scheduling consistency with the original optimized kernels. Ablation studies further validate the contribution of each proposed component to the overall performance.To the best of our knowledge, this is the first work tackling PTX-to-CUDA decompilation, specifically focusing on and demonstrating effective strategies for augmenting LLMs to overcome the key challenges in this domain.},
  keywords={Codes;Tensors;Accuracy;Graphics processing units;Training data;Data augmentation;Complexity theory;Kernel;Optimization;Tuning;LLM;Decompilation;Deep learning compiler;GPU Programming},
  doi={10.1109/ASE63991.2025.00185},
  ISSN={2643-1572},
  month={Nov},}@INPROCEEDINGS{1560127,
  author={Stiff, G. and Vahid, F.},
  booktitle={ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.}, 
  title={New decompilation techniques for binary-level co-processor generation}, 
  year={2005},
  volume={},
  number={},
  pages={547-554},
  abstract={Existing ASIPs (application-specific instruction-set processors) and compiler-based co-processor synthesis approaches meet the increasing performance requirements of embedded applications while consuming less power than high-performance gigahertz microprocessors. However, existing approaches place restrictions on software languages and compilers. Binary-level co-processor generation has previously been proposed as a complementary approach to reduce impact on tool restrictions, supporting all languages and compilers, at the cost of some decrease in performance. In a binary-level approach, decompilation recovers much of the high-level information, like loops and arrays, needed for effective synthesis, and in many cases yields hardware similar to that of a compiler-based approach. However, previous binary-level approaches have not considered the effects of software compiler optimizations on the resulting hardware. In this paper, we introduce two new decompilation techniques, strength promotion and loop rerolling, and show that they are necessary to synthesize an efficient custom hardware coprocessor from a binary in the presence of software compiler optimizations. In addition, unlike previous approaches, we show the robustness of binary-level co-processor generation by achieving order of magnitude speedups for binaries generated for three different instruction sets, MIPS, ARM, and MicroBlaze, using two different levels of compiler optimizations.},
  keywords={Coprocessors;Hardware;Application software;Application specific processors;Microprocessors;Optimizing compilers;Software performance;Field programmable gate arrays;Costs;Fabrics},
  doi={10.1109/ICCAD.2005.1560127},
  ISSN={1558-2434},
  month={Nov},}@ARTICLE{10531777,
  author={Sang, Chao and Wu, Jun and Li, Jianhua and Guizani, Mohsen},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={From Control Application to Control Logic: PLC Decompile Framework for Industrial Control System}, 
  year={2024},
  volume={19},
  number={},
  pages={8685-8700},
  abstract={Industrial Control System (ICS) depends on the underlying Programmable Logical Controllers (PLCs) to run. As such, the security of the internal control logic of the PLCs is the top concern of ICS. Reversing analysis and forensic work against PLC require extracting control logic from the control application running inside PLC, which is still an unresolved problem. To address the challenge, we propose a PLC decompile framework named CLEVER, which can analyze the control application and extract the control logic. First, we propose a simulation execution based code extraction method, which is utilized to filter the control logic related data. Then, to normalize the control application decompile process, an intermediate representation (IR) is designed, which can simplify the analysis process and enhance the extensibility of CLEVER. Finally, a heuristic data flow analysis algorithm is proposed to find variable dependency, and a sequential parsing method is utilized to reconstruct the source code from the control application. To evaluate our work, real world PLC hardware and programming software are used for the experiment. We use 22 real-world, 58 hand-written, and 150 auto-generated control applications to demonstrate the usability, correctness, and operational efficiency of CLEVER.},
  keywords={Codes;Software;Registers;Data mining;Assembly;Process control;Feature extraction;Reverse engineering;program analysis;network forensics;industrial control system;PLC},
  doi={10.1109/TIFS.2024.3402117},
  ISSN={1556-6021},
  month={},}@INPROCEEDINGS{11334626,
  author={Li, Gangyang and Shang, Xiuwei and Cheng, Shaoyin and Zhang, Junqi and Hu, Li and Zhu, Xu and Zhang, Weiming and Yu, Nenghai},
  booktitle={2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={PseudoFix: Refactoring Distorted Structures in Decompiled C Pseudocode}, 
  year={2025},
  volume={},
  number={},
  pages={841-853},
  abstract={Decompilation can convert binary programs into clear C-style pseudocode, which is of great value in a wide range of security applications. Existing research primarily focuses on recovering symbolic information in pseudocode, such as function names, variable names, and data types, but neglecting structural information. We observe that even when symbolic information is fully preserved, severe and complex structure distortions remain in the pseudocode, greatly impairing code readability and comprehension. In this work, we first systematically investigate structure distortions in decompiled pseudocode, revealing their variation patterns through quantitative analysis. Using open coding, we derive a taxonomy comprising six top-level categories of structure distortions. Building upon this taxonomy, we propose PseudoFix, a novel framework that combines large language models (LLMs) with retrieval-based in-context learning. PseudoFix employs semantic retrieval to select the most relevant few-shot examples that provide structure distortion knowledge, and combines this with the well-structured coding patterns learned by LLMs from vast source code repositories, to efficiently refactor distorted pseudocode. Comprehensive evaluations demonstrate that PseudoFix significantly improves pseudocode readability, achieving up to a 34% reduction in Halstead Complexity Effort and a 105% increase in BLEU-4 score. Notably, it significantly outperforms state-of-the-art approaches in both temporary variable elimination and goto statement removal tasks. Additionally, human evaluations yield consistently positive feedback from users across readability, consistency, and reasonability.},
  keywords={Systematics;Statistical analysis;Large language models;Source coding;Taxonomy;Semantics;Software algorithms;Distortion;Security;Software engineering;Decompilation;Refactor Structure Distortion;Taxonomy;In-context Learning;Large Language Models},
  doi={10.1109/ASE63991.2025.00075},
  ISSN={2643-1572},
  month={Nov},}@INPROCEEDINGS{4689183,
  author={Myreen, Magnus O. and Gordon, Michael J. C. and Slind, Konrad},
  booktitle={2008 Formal Methods in Computer-Aided Design}, 
  title={Machine-Code Verification for Multiple Architectures - An Application of Decompilation into Logic}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  abstract={Realistic formal specifications of machine languages for commercial processors consist of thousands of lines of definitions. Current methods support trustworthy proofs of the correctness of programs for one such specification. However, these methods provide little or no support for reusing proofs of the same algorithm implemented in different machine languages. We describe an approach, based on proof-producing decompilation, which both makes machine-code verification tractable and supports proof reuse between different languages. We briefly present examples based on detailed models of machine code for ARM, PowerPC and x86. The theories and tools have been implemented in the HOL4 system.},
  keywords={Logic;Registers;Computer architecture;Application software;Laboratories;Buildings;Drives;Cities and towns;Formal specifications;Power system modeling},
  doi={10.1109/FMCAD.2008.ECP.24},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{7314672,
  author={Xu, Junfeng and Zhang, Li and Lin, Dong and Mao, Ye},
  booktitle={2015 Ninth International Conference on Frontier of Computer Science and Technology}, 
  title={Recommendable Schemes of Anti-decompilation for Android Applications}, 
  year={2015},
  volume={},
  number={},
  pages={184-190},
  abstract={Currently, Regular Android software, injected in malicious code, is one of the important factors of that Android virus run rampant. Protecting the Android software has become a focus of attention in academia and industry. Addressing the safety protection issues of the Android software, this paper will present some new schemes for Android software security technology. On the basis of existing research results, we will propose some recommendable solutions to prevent android applications being decompiled. Take advantage of these methods, we will build the Android software protection system, which will mostly eliminate the feasibility of the secondary packaging for Android software, and extend the Android software safety lifecycle.},
  keywords={Androids;Humanoid robots;Software;Smart phones;Encryption},
  doi={10.1109/FCST.2015.76},
  ISSN={2159-631X},
  month={Aug},}@INPROCEEDINGS{9282790,
  author={Korenčik, Lukáš and Ročkai, Petr and Lauko, Henrich and Barnat, Jiří},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={On Symbolic Execution of Decompiled Programs}, 
  year={2020},
  volume={},
  number={},
  pages={265-272},
  abstract={In this paper, we present a combination of existing and new tools that together make it possible to apply formal verification methods to programs in the form of ×86_64 machine code. Our approach first uses a decompilation tool (remill) to extract low-level intermediate representation (LLVM) from the machine code. This step consists of instruction translation (i.e. recovery of operation semantics), control flow extraction and address identification.The main contribution of this paper is the second step, which builds on data flow analysis and refinement of indirect (i.e. data-dependent) control flow. This step makes the processed bitcode much more amenable to formal analysis.To demonstrate the viability of our approach, we have compiled a set of benchmark programs into native executables and analysed them using two LLVM-based tools: DIVINE, a software model checker and KLEE, a symbolic execution engine. We have compared the outcomes to direct analysis of the same programs.},
  keywords={Semantics;Software quality;Tools;Software reliability;Security;Engines;Formal verification;decompilation;llvm;symbolic execution;program analysis;binary analysis},
  doi={10.1109/QRS51102.2020.00044},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{11050729,
  author={Magin, Florian and Patat, Gwendal and Scherf, Fabian},
  booktitle={2025 IEEE/ACM 1st International Workshop on Advancing Static Analysis for Researchers and Industry Practitioners in Software Engineering (STATIC)}, 
  title={Heros in Action: Analyzing Objective-C Binaries through Decompilation and IFDS}, 
  year={2025},
  volume={},
  number={},
  pages={1-6},
  abstract={This paper demonstrates static taint analysis on Objective-C binaries through the integration of the Ghidra framework with the Heros IFDS solver, achieving an inter-procedural, field-sensitive and flow-sensitive analysis. Our contributions include two plugins: one extending Ghidra Objective-C capabilities to improve decompilation accuracy, and another integrating the Heros framework for inter-procedural taint analysis on Ghidra Intermediate Representation (IR).To assess our approach, we introduce a new benchmark suite tailored to Objective-C, covering diverse dataflow challenges and promoting further community-driven research. By leveraging existing frameworks, this work demonstrates how established static analysis techniques can be adapted to binary targets, laying a groundwork for advancements in Objective-C binary analysis.},
  keywords={Industries;Accuracy;Conferences;Static analysis;Benchmark testing;Software engineering;Heros;IFDS;Objective-C;Decompilation},
  doi={10.1109/STATIC66697.2025.00005},
  ISSN={},
  month={April},}@INPROCEEDINGS{496990,
  author={Cifuentes, C.},
  booktitle={Proceedings 1995 Asia Pacific Software Engineering Conference}, 
  title={An environment for the reverse engineering of executable programs}, 
  year={1995},
  volume={},
  number={},
  pages={410-419},
  abstract={Reverse engineering of software systems has traditionally centered upon the generation of high level abstractions or specifications from high level code or databases. We report on a reverse engineering environment for low level executable code: a reverse compilation or decompilation environment that aids in the understanding of the underlying executable program. The reverse compilation process recovers high level code from executable programs at a higher representation level than that produced by disassemblers; in fact, disassembly is part of the first stage in this process. Several tools aid in the process of reverse compilation, these are: loaders, signature generators, library prototype generators, disassemblers, library bindings, and language to language translators. The integration of these tools in the whole process is presented in this paper. The results obtained by the prototype reverse compilation system dcc are encouraging: high level code is regenerated with correct use of expressions and control structures, and the complete elimination of registers and condition codes. An elimination rate of low level instructions of over 75% was reached, representing the overall improvement this decompiler system has made over previous decompilers and disassemblers (where the rate tends to be nil). A sample decompilation program is given.},
  keywords={Reverse engineering;Software systems;Databases;Libraries;Prototypes;Computer aided software engineering;High level languages;Computer science;Control systems;Australia Council},
  doi={10.1109/APSEC.1995.496990},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{1287233,
  author={Vinciguerra, L. and Wills, L. and Kejriwal, N. and Martino, P. and Vinciguerra, R.},
  booktitle={10th Working Conference on Reverse Engineering, 2003. WCRE 2003. Proceedings.}, 
  title={An experimentation framework for evaluating disassembly and decompilation tools for C++ and java}, 
  year={2003},
  volume={},
  number={},
  pages={14-23},
  abstract={},
  keywords={Java;Reverse engineering;Software tools;Application software;Data mining;Software testing;Computer architecture;Software protection;Research and development;History},
  doi={10.1109/WCRE.2003.1287233},
  ISSN={1095-1350},
  month={Nov},}@INPROCEEDINGS{10515515,
  author={Izrailov, Konstantin},
  booktitle={2024 International Russian Smart Industry Conference (SmartIndustryCon)}, 
  title={GREMC: Genetic Reverse-Engineering of Machine Code to Search Vulnerabilities in Software for Industry 4.0. Predicting the Size of the Decompiling Source Code}, 
  year={2024},
  volume={},
  number={},
  pages={622-628},
  abstract={The article is devoted to the problem of security of cyber-physical systems as part of production according to the Industry 4.0 concept. For this purpose, the author's approach of “Genetic Reverse Engineering of Machine Code” (GREMC) is proposed. The essence of this approach lies in the use of artificial intelligence in the field of genetic algorithms to restore the source code of software executed in the form of machine code on cyber-physical devices for Industry 4.0. The resulting code can then be analysed for vulnerabilities by an expert. One issue that arises during genetic reverse engineering is predicting the size of the source code based on its machine representation (i.e., in an object or executable file). The article is devoted to solving this problem for functions in the C programming language. To do this, a method for obtaining a relationship between the sizes of the source and machine code of individual functions is described, which consists of steps such as loading a dataset with C-functions, isolating the function code in it, preprocessing them, compiling them into machine code, calculating the required sizes, building dependencies between each source code size and the corresponding machine code sizes, generating the final table and determining the dependency formula. An experiment is carried out using a software prototype that implements the method; ExeBench with 83 thousand C-functions is taken as a dataset. Justifications are given regarding the appearance of abnormal machine code sizes and their impact on the dependence formula.},
  keywords={Codes;Source coding;Reverse engineering;Prototypes;Production;Genetics;Software;machine code;source code;reverse-engineering;decompilation;size dependence},
  doi={10.1109/SmartIndustryCon61328.2024.10515515},
  ISSN={},
  month={March},}@INPROCEEDINGS{10174218,
  author={Cao, Kevin and Leach, Kevin},
  booktitle={2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC)}, 
  title={Revisiting Deep Learning for Variable Type Recovery}, 
  year={2023},
  volume={},
  number={},
  pages={275-279},
  abstract={Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.},
  keywords={Training;Source coding;Semantics;Reverse engineering;Symbols;Computer architecture;Static analysis;Ghidra;Hex-Rays;Machine Learning;Transformers},
  doi={10.1109/ICPC58990.2023.00042},
  ISSN={2643-7171},
  month={May},}@ARTICLE{9520296,
  author={Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},
  journal={IEEE Transactions on Software Engineering}, 
  title={Learning to Find Usages of Library Functions in Optimized Binaries}, 
  year={2022},
  volume={48},
  number={10},
  pages={3862-3876},
  abstract={Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries’ behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, “natural” source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to “decompile” binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
  keywords={Libraries;Tools;Optimization;Databases;Reverse engineering;Training;Malware;Reverse engineering;software modeling;deep learning},
  doi={10.1109/TSE.2021.3106572},
  ISSN={1939-3520},
  month={Oct},}@INPROCEEDINGS{5609916,
  author={Liu, Xuying and Yin, Wenjian and Yin, Qing and Jiang, Liehui},
  booktitle={2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering}, 
  title={A SSA-based intermediate representation technique}, 
  year={2010},
  volume={6},
  number={},
  pages={98-101},
  abstract={Intermediate representation techniques are positive for resolving generalness of decompilation. Focus on non-generalness of intermediate language, a binary reverse intermediate language BRIL which is independent of machine instructions and contains code dataflow information is proposed. It has high level language characteristics and eliminates the side effects of complex addressing manner and instruction semantics, besides introduces φ-function denotation in SSA. It can reflect behaviors of object code and express semantics of code completely, and can be used in decompilation of object code on different platforms. So it makes it easy to process code transformation, and helps to develop general decompiler greatly.},
  keywords={Registers;BRIL;Binary Reverse Intermediate Language;SSA;Decompilation;Intermediate Language},
  doi={10.1109/CMCE.2010.5609916},
  ISSN={2159-6034},
  month={Aug},}@INPROCEEDINGS{7781805,
  author={Ragkhitwetsagul, Chaiyong and Krinke, Jens and Clark, David},
  booktitle={2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)}, 
  title={Similarity of Source Code in the Presence of Pervasive Modifications}, 
  year={2016},
  volume={},
  number={},
  pages={117-126},
  abstract={Source code analysis to detect code cloning, code plagiarism, and code reuse suffers from the problem of pervasive code modifications, i.e. transformations that may have a global effect. We compare 30 similarity detection techniques and tools against pervasive code modifications. We evaluate the tools using two experimental scenarios for Java source code. These are (1) pervasive modifications created with tools for source code and bytecode obfuscation and (2) source code normalisation through compilation and decompilation using different decompilers. Our experimental results show that highly specialised source code similarity detection techniques and tools can perform better than more general, textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for six of the tools. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code.},
  keywords={Cloning;Plagiarism;Detectors;Java;Optimization;Open source software;source code similarity;decompilation;code normalisation;code cloning;code reuse;code plagiarism},
  doi={10.1109/SCAM.2016.13},
  ISSN={2470-6892},
  month={Oct},}@INPROCEEDINGS{9833799,
  author={Liu, Zhibo and Yuan, Yuanyuan and Wang, Shuai and Bao, Yuyan},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
  title={SoK: Demystifying Binary Lifters Through the Lens of Downstream Applications}, 
  year={2022},
  volume={},
  number={},
  pages={1100-1119},
  abstract={Binary lifters convert executables into an intermediate representation (IR) of a compiler framework. The recovered IR code is generally deemed “analysis friendly,” bridging low-level code analysis with well-established compiler infrastructures. With years of development, binary lifters are becoming increasingly popular for use in various security, systems, and software (re)-engineering applications. Recent studies have also reported highly promising results that suggest binary lifters can generate LLVM IR code with correct functionality, even for complex cases.This paper conducts an in-depth study of binary lifters from an orthogonal and highly demanding perspective. We demystify the “expressiveness” of binary lifters, and reveal how well the lifted LLVM IR code can support critical downstream applications in security analysis scenarios. To do so, we generate two pieces of LLVM IR code by compiling C/C++ programs or by lifting the corresponding executables. We then feed these two pieces of LLVM IR code to three keystone downstream applications (pointer analysis, discriminability analysis, and decompilation) and determine whether inconsistent analysis results are generated. We study four popular static and dynamic LLVM IR lifters that were developed by the industry or academia from a total of 252,063 executables generated by various compilers and optimizations and on different architectures. Our findings show that modern binary lifters afford IR code that is highly suitable for discriminability analysis and decompilation, and suggest that such binary lifters can be applied in common similarity- or code comprehension-based security analysis (e.g., binary diffing). However, the lifted IR code appears unsuited to rigorous static analysis (e.g., pointer analysis). To obtain a more comprehensive view of the utility of binary lifters, we also compare the performance of lifter-enabled approaches with that of binary-only tools in three security tasks, i.e., sanitization, binary diffing, and C decompilation. We summarize our findings and make suggestions for the correct use and further enhancement of binary lifters. We also explored practical ways to enhance the accuracy of pointer analysis using lifted IR code, by using and augmenting Debin, a tool for predicting debug information.},
  keywords={Industries;Privacy;Codes;Static analysis;Software;Security;Feeds;reverse-engineering;software-security},
  doi={10.1109/SP46214.2022.9833799},
  ISSN={2375-1207},
  month={May},}@INPROCEEDINGS{8987703,
  author={Slawinski, Michael and Wortman, Andy},
  booktitle={2019 4th International Conference on System Reliability and Safety (ICSRS)}, 
  title={Applications of Graph Integration to Function Comparison and Malware Classification}, 
  year={2019},
  volume={},
  number={},
  pages={16-24},
  abstract={We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled. NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms. 11Code available at https://github.com/gtownrocks/grafuple},
  keywords={Training;Robust control;Taxonomy;Aerospace electronics;Syntactics;Vectors;Malware;Safety;Reliability;Glass box;graph integration;pagerank;machine learning;classification;malware;NET;decompilation;abstract syntax tree},
  doi={10.1109/ICSRS48664.2019.8987703},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10773695,
  author={Smith, Ian and Bertolaccini, Francesco and Tan, William and Brown, Michael D.},
  booktitle={MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM)}, 
  title={IRENE: A Toolchain for High-level Micropatching through Recompilable Sub-function Regions}, 
  year={2024},
  volume={},
  number={},
  pages={1064-1069},
  abstract={Devices with long life-cycles extend the duration of a software product’s exposure. Vulnerabilities in the device’s software may be discovered after the toolchain for building software for that device has been deprecated, the source-code for the program that is running on the device is unavailable, or both. Current solutions for patching these latent vulnerabilities are insufficient, either requiring deep technical expertise and manual effort or producing a large set of changes to the target binary. The large number of changes in the target program makes validating the patch difficult.We present IRENE, a decompiler that produces recompilable decompilation for patching. IRENE decompiles sub-function regions of a binary program to separate recompilable regions of code. The IRENE compiler replaces the original binary region with recompiled assembly, representing the user’s patch. IRENE allows developers to compose patches by modifying a high-level representation of the target program, but produces targeted micropatches. We evaluate the size of IRENE produced micropatches against patches produced both by recompilation and manual assembly patching in a case study. This evaluation shows a patch size reduction of 4x compared with recompilation and a 21x size reduction when compared with recompiling the patch with a newer toolchain version.},
  keywords={Military communication;Codes;Buildings;Manuals;Software;Assembly;binary patching;recompilation;binary rewriting;binary analysis},
  doi={10.1109/MILCOM61039.2024.10773695},
  ISSN={2155-7586},
  month={Oct},}@INPROCEEDINGS{10581196,
  author={Ahmad Ali Qureshi, Muhammad and Munawar Gill, Arsham and Sadaf, Memoona},
  booktitle={2024 International Conference on Engineering & Computing Technologies (ICECT)}, 
  title={APK Insight: Revolutionizing Forensic Analysis with a User-Friendly Approach}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This research paper introduces a novel forensic tool named ‘A2forensicsKit’, developed for non-technical users to proficiently analyze and decompile Android Application Package (APK) files [7]. With the increasing prevalence of mobile applications, the need for accessible forensic solutions has become imperative. Our tool focuses on simplifying the complex process of APK analysis, enabling investigators, law enforcement, and other non-technical users to extract valuable insights from mobile applications. The tool employs an intuitive user interface, providing a seamless experience for users with limited technical expertise. Through a combination of automated processes and user-friendly features, our tool allows for the swift identification of malicious code, potential vulnerabilities, and unauthorized activities within APK files. Its effectiveness lies in its ability to bridge the gap between intricate forensic procedures and the diverse user base involved in digital investigations. Key functionalities include static analysis and decompilationof APK files, all seamlessly integrated into a unified platform. The tool's efficiency is underscored by its capacity to generate comprehensive reports, helping investigators in presenting findings in a clear and understandable manner. In conclusion, our research presents a user-friendly forensic tool that empowers non-technical users to conduct thorough analysis and decompilation of APK files. By addressing the accessibility gap in digital forensics, this tool contributes to enhancing the efficiency and inclusivity of mobile application investigations.},
  keywords={Training;Technological innovation;Law enforcement;Operating systems;Digital forensics;Transforms;Static analysis;Digital Forensics;APK Analysis;Mobile Application Security;Cybersecurity;Android Application Package;Forensic Tools},
  doi={10.1109/ICECT61618.2024.10581196},
  ISSN={},
  month={May},}@INPROCEEDINGS{11185876,
  author={Enders, Steffen and Behner, Eva-Maria C. and Padilla, Elmar},
  booktitle={2025 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={A Jump-Table-Agnostic Switch Recovery on ASTs}, 
  year={2025},
  volume={},
  number={},
  pages={1-12},
  abstract={Recovering high-level control-flow structures is a crucial part of modern reverse engineering, especially in fields like binary analysis. Here, analysts often use decompilers to convert functions of binary programs into a more humanreadable C -like representation. Among these control-flow structures, switch statements have unique significance because of their ability to represent complex decision-making and branching behavior in a concise and readable manner. Consequently, the successful recovery of switch statements during decompilation can greatly enhance the readability of the resulting output, making it a highly desired goal in the field of reverse engineering. In this paper, we present a new technique for identifying abstract syntax tree components that can be transformed into semantically equivalent switches, thus improving code readability. In contrast to other approaches, we do not rely on jump tables that have or have not been emitted during compilation. Instead, we identify clusters of comparisons involving the same expression but with varying constant values within the abstract syntax tree to be transformed into switch constructs. Because this approach is inherently linked to the semantic definition of a switch statements, it only generates meaningful switches by design. We evaluated our approach on the coreutils-9.3 dataset and compared it to the leading decompilers Ghidra and Hex-Rays, both of which attempt to recover switch statements as well. Our evaluation results indicate that our approach outperforms both Ghidra and Hex-Rays by successfully recovering more than twice as many switch constructs in the given dataset.},
  keywords={Software maintenance;Source coding;Semantics;Reverse engineering;Decision making;Switches;Transforms;Syntactics;Control systems;Complexity theory;switch recovery;decompilation;reverse engineering;binary analysis;jump tables;control-flow structuring},
  doi={10.1109/ICSME64153.2025.00028},
  ISSN={2576-3148},
  month={Sep.},}@INPROCEEDINGS{11352404,
  author={Zhao, Shiwu and Zheng, Ningjun and Li, Haoyu and Feng, Ruizhi and Chen, Xingchen and Tan, Ru and Liu, Qixu},
  booktitle={2025 28th International Symposium on Research in Attacks, Intrusions and Defenses (RAID)}, 
  title={DEPHP: A Source Code Recovery Method for PHP Bytecode with Improved Structural Analysis}, 
  year={2025},
  volume={},
  number={},
  pages={77-91},
  abstract={Over the past decade, PHP has consistently been one of the most popular server-side programming languages among developers for web development. To protect intellectual property, various PHP source code obfuscation and encryption methods have been developed, which has led to difficulties in performing security analysis on PHP source code. Previous work has demonstrated the feasibility of recovering source code by extracting bytecode from PHP during dynamic execution. However, there is still a lack of a universal decompilation method for this kind of bytecode, tailored to PHP’s unique syntax. Thus, we propose a systematic decompilation framework for PHP bytecode. First, we design a unified intermediate representation that eliminates the differences between bytecodes from different PHP versions. Then, we introduce a structural analysis algorithm specifically for PHP syntax, improving upon existing methods to better accommodate PHP’s unique syntax. We use over 3 million lines of PHP code as a dataset and compiled it into PHP bytecode. After decompiling it with our method, we successfully recovered 92% of the classes and 85% of the methods. Furthermore, from the encrypted dataset containing 37 SQL injection and 31 XSS vulnerability patterns, we fully restored the original vulnerability patterns and reconstructed the exploitation chains. Furthermore, we identified a series of vulnerabilities in real-world projects and were assigned 6 new CVE IDs1, demonstrating the correctness of our method and its ability to assist in static analysis for vulnerability discovery.1CVE-2025-45046, CVE-2025-45047, CVE-2025-45048, CVE-2025-45049, CVE-2025-45050, CVE-2025-45052},
  keywords={Codes;Systematics;Source coding;Trees (botanical);Process control;Static analysis;Intellectual property;Syntactics;SQL injection;Protection;decompilation;structural analysis;code protection;PHP bytecode;vulnerability detection},
  doi={10.1109/RAID67961.2025.00032},
  ISSN={},
  month={Oct},}@ARTICLE{6601601,
  author={Cesare, Silvio and Xiang, Yang and Zhou, Wanlei},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Control Flow-Based Malware VariantDetection}, 
  year={2014},
  volume={11},
  number={4},
  pages={307-317},
  abstract={Static detection of malware variants plays an important role in system security and control flow has been shown as an effective characteristic that represents polymorphic malware. In our research, we propose a similarity search of malware to detect these variants using novel distance metrics. We describe a malware signature by the set of control flowgraphs the malware contains. We use a distance metric based on the distance between feature vectors of string-based signatures. The feature vector is a decomposition of the set of graphs into either fixed size k-subgraphs, or q-gram strings of the high-level source after decompilation. We use this distance metric to perform pre-filtering. We also propose a more effective but less computationally efficient distance metric based on the minimum matching distance. The minimum matching distance uses the string edit distances between programs' decompiled flowgraphs, and the linear sum assignment problem to construct a minimum sum weight matching between two sets of graphs. We implement the distance metrics in a complete malware variant detection system. The evaluation shows that our approach is highly effective in terms of a limited false positive rate and our system detects more malware variants when compared to the detection rates of other algorithms.},
  keywords={Malware;Flow graphs;Feature extraction;Measurement;Vectors;Software;Databases;Computer security;malware classification;static analysis;control flow;structuring;decompilation},
  doi={10.1109/TDSC.2013.40},
  ISSN={1941-0018},
  month={July},}@INPROCEEDINGS{884716,
  author={Monden, A. and Iida, H. and Matsumoto, K. and Inoue, K. and Torii, K.},
  booktitle={Proceedings 24th Annual International Computer Software and Applications Conference. COMPSAC2000}, 
  title={A practical method for watermarking Java programs}, 
  year={2000},
  volume={},
  number={},
  pages={191-197},
  abstract={Java programs distributed through the Internet are now suffering from program theft. This is because Java programs can be easily decomposed into reusable class files and even decompiled into source code by program users. We propose a practical method that discourages program theft by embedding Java programs with a digital watermark. Embedding a program developer's copyright notation as a watermark in Java class files will ensure the legal ownership of class files. Our embedding method is indiscernible by program users, yet enables us to identify an illegal program that contains stolen class files. The result of the experiment to evaluate our method showed most of the watermarks (20 out of 23) embedded in class files survived two kinds of attacks that attempt to erase watermarks: an obfuscactor attack, and a decompile-recompile attack.},
  keywords={Watermarking;Java;Law;Information science;Internet;Legal factors;Programming profession;Decoding;Information technology;Application software},
  doi={10.1109/CMPSAC.2000.884716},
  ISSN={0730-3157},
  month={Oct},}@INPROCEEDINGS{6120818,
  author={Cesare, Silvio and Xiang, Yang},
  booktitle={2011IEEE 10th International Conference on Trust, Security and Privacy in Computing and Communications}, 
  title={Malware Variant Detection Using Similarity Search over Sets of Control Flow Graphs}, 
  year={2011},
  volume={},
  number={},
  pages={181-189},
  abstract={Static detection of polymorphic malware variants plays an important role to improve system security. Control flow has shown to be an effective characteristic that represents polymorphic malware instances. In our research, we propose a similarity search of malware using novel distance metrics of malware signatures. We describe a malware signature by the set of control flow graphs the malware contains. We propose two approaches and use the first to perform pre-filtering. Firstly, we use a distance metric based on the distance between feature vectors. The feature vector is a decomposition of the set of graphs into either fixed size k-subgraphs, or q-gram strings of the high-level source after decompilation. We also propose a more effective but less computationally efficient distance metric based on the minimum matching distance. The minimum matching distance uses the string edit distances between programs' decompiled flow graphs, and the linear sum assignment problem to construct a minimum sum weight matching between two sets of graphs. We implement the distance metrics in a complete malware variant detection system. The evaluation shows that our approach is highly effective in terms of a limited false positive rate and our system detects more malware variants when compared to the detection rates of other algorithms.},
  keywords={Malware;Flow graphs;Vectors;Feature extraction;Measurement;Software;Support vector machine classification;computer security;malware classification;static analysi;control flow;structuring;decompilation},
  doi={10.1109/TrustCom.2011.26},
  ISSN={2324-9013},
  month={Nov},}@INPROCEEDINGS{9282282,
  author={Li, Xiaoqi and Chen, Ting and Luo, Xiapu and Zhang, Tao and Yu, Le and Xu, Zhou},
  booktitle={2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)}, 
  title={STAN: Towards Describing Bytecodes of Smart Contract}, 
  year={2020},
  volume={},
  number={},
  pages={273-284},
  abstract={More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named Stan to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, Stan can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that Stan can generate adequate, accurate and readable descriptions for contract’s bytecodes, which have practical value for users.},
  keywords={Smart contracts;Semantics;Software quality;Tools;Natural language processing;Software reliability;Security;Smart contract;Ethereum;Program comprehension},
  doi={10.1109/QRS51102.2020.00045},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8406593,
  author={Bodei, Chiara and Degano, Pierpaolo and Galletta, Letterio and Focardi, Riccardo and Tempesta, Mauro and Veronese, Lorenzo},
  booktitle={2018 IEEE European Symposium on Security and Privacy (EuroS&P)}, 
  title={Language-Independent Synthesis of Firewall Policies}, 
  year={2018},
  volume={},
  number={},
  pages={92-106},
  abstract={Configuring and maintaining a firewall configuration is notoriously hard. Policies are written in low-level, platform-specific languages where firewall rules are inspected and enforced along non trivial control flow paths. Further difficulties arise from Network Address Translation (NAT), since filters must be implemented with addresses translations in mind. In this work, we study the problem of decompiling a real firewall configuration into an abstract specification. This abstract version throws the low-level details away by exposing the meaning of the configuration, i.e., the allowed connections with possible address translations. The generated specification makes it easier for system administrators to check if: (i) the intended security policy is actually implemented; (ii) two configurations are equivalent; (iii) updates have the desired effect on the firewall behavior. The peculiarity of our approach is that is independent of the specific target firewall system and language. This independence is obtained through a generic intermediate language that provides the typical features of real configuration languages and that separates the specification of the rulesets, determining the destiny of packets, from the specification of the platform-dependent steps needed to elaborate packets. We present a tool that decompiles real firewall configurations from different systems into this intermediate language and uses the Z3 solver to synthesize the abstract specification that succinctly represents the firewall behavior and the NAT. Tests on real configurations show that the tool is effective: it synthesizes complex policies in a matter of minutes and, and it answers to specific queries in just a few seconds. The tool can also point out policy differences before and after configuration updates in a simple, tabular form.},
  keywords={Tools;Firewalls (computing);Proposals;Network address translation;Standards;Operating systems;Network Security;Firewall configuration;Policy Synthesis},
  doi={10.1109/EuroSP.2018.00015},
  ISSN={},
  month={April},}@INPROCEEDINGS{9850326,
  author={Zubair, Nauman and Ayub, Adeen and Yoo, Hyunguk and Ahmed, Irfan},
  booktitle={2022 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Control Logic Obfuscation Attack in Industrial Control Systems}, 
  year={2022},
  volume={},
  number={},
  pages={227-232},
  abstract={Industrial control systems (ICS) are essential for safe and efficient operations of critical infrastructures such as power grids, pipelines, and water treatment facilities. Attackers target ICS, mainly programmable logic controllers (PLC), to sabotage underlying infrastructure. A PLC controls a physical process through connected sensors and actuators. It runs a control-logic program that specifies monitoring and controlling a physical process and is a common target of cyberattacks. A vendor-provided proprietary engineering software is typically used to investigate the infected control logic. This paper shows that an attacker can use control-logic obfuscation as an anti-forensics technique to hinder the investigations and incident response. The control-logic obfuscation subverts the engineering software’s decompilation function; therefore, we call it a denial-of-decompilation attack. The attack exploits a fundamental design principle of creating compiled control logic in engineering software, thereby affecting the engineering software of multiple vendors in the industry.},
  keywords={Integrated circuits;Programmable logic devices;Pipelines;Process control;Control systems;Software;Power grids;Control-logic attacks;digital forensics;industrial control system (ICS);programmable logic controller (PLC)},
  doi={10.1109/CSR54599.2022.9850326},
  ISSN={},
  month={July},}@INPROCEEDINGS{972665,
  author={Deruelle, L. and Melab, N. and Bouneffa, M. and Basson, H.},
  booktitle={Proceedings First IEEE International Workshop on Source Code Analysis and Manipulation}, 
  title={Analysis and manipulation of distributed multi-language software code}, 
  year={2001},
  volume={},
  number={},
  pages={43-54},
  abstract={The authors propose a formal model and a platform to deal with distributed multi-language software analysis. These provide a graph representation of the software codes (source codes and byte-codes), a change propagation process based on graph rewriting, and an automatic profiling tool to measure the contribution of any component to the global performance of the software. The program codes are structured by a multi-graph in which the nodes represent the software components linked by edges representing the meaningful relationships. The software components and their relationships are extracted from the byte-code files, using the mocha decompiler tool, and from the source codes files, using the Javacc tool. Javacc allows one to generate parsers, based on grammar specification files, which include features to produce an XML (eXtensible Markup Language) representation of the software components. Furthermore, a graph of the software components is constructed on the top of the XML files, providing program analysis. This is implemented by an integrated platform including the mocha decompiler, a multi-language parsing tool, a software change management module, and a profiling tool.},
  keywords={Prototypes},
  doi={10.1109/SCAM.2001.972665},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{914984,
  author={Moretti, E. and Chanteperdrix, G. and Osorio, A.},
  booktitle={Proceedings Fifth European Conference on Software Maintenance and Reengineering}, 
  title={New algorithms for control-flow graph structuring}, 
  year={2001},
  volume={},
  number={},
  pages={184-187},
  abstract={New algorithms for the structuring of arbitrary control-flow graphs are presented. As they minimize the use of Gotos, these algorithms are adequate for the control-flow analysis needed in the process of decompilation: loops are properly identified even when nested loops share the some header node, and complex compound Boolean conditions are completely handled. With the described algorithms, the generated high level language code results are easier to understand, which eases the maintenance of legacy code. The presented techniques have been implemented and have proved successful in their aim of structuring decompiled graphs from a variety of industrial real time embedded software on diverse architectures.},
  keywords={High level languages;Algorithm design and analysis;Computer industry;Embedded software;Computer architecture;Graph theory;Optimizing compilers;Performance evaluation;Binary codes;Assembly},
  doi={10.1109/CSMR.2001.914984},
  ISSN={},
  month={March},}
