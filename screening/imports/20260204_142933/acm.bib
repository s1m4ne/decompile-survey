@inproceedings{10.1145/3719027.3765040,
author = {Wiedemeier, Joshua and Klancher, Simon and Flores, Joel and Zheng, Max and Park, Jaehyun and Cha, Sang Kil and Jee, Kangkook},
title = {Walking The Last Mile: Studying Decompiler Output Correction in Practice},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3765040},
doi = {10.1145/3719027.3765040},
abstract = {The increasing prevalence of Python has spurred interest in decompiling Python PYC bytecode. This work presents the first large-scale study on human-assisted Python decompilation in the wild, leveraging extensive data from pylingual.io, spanning 181,646 PYC binaries, 9,003 user-submitted patches, and 393 accuracy-verified patches. We investigate how reverse engineers respond to inaccurate decompilation and identify factors influencing their efforts to achieve accurate decompilation. We complement this unprecedented observational data with a controlled user study that isolates the technical difficulty of patching imperfect Python decompilations.By contrasting real-world patching behavior with that of the controlled setting, we discover that reversers' decision to repair a decompilation result is more strongly driven by the semantic content of the program (e.g., malware binaries or malicious tools) than by the technical difficulty of the patch. That is, a reverser's motivation is more important than their expertise.Our study reveals common patterns observed in the patching process, including how users approached the patching task, the types of errors they encountered, and the strategies they employed to resolve them. We also examine the strengths and limitations of assistive tools in the pursuit of perfect decompilation. Our findings offer unique insights into the practical dynamics of human-decompiler interaction, providing actionable recommendations for integrating human intelligence into the decompilation workflow and demonstrating the research potential of reliable decompilation accuracy verification.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2489–2503},
numpages = {15},
keywords = {decompilation, malware, reverse engineering, user study},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3719027.3760714,
author = {Giesen, Jens-Rene and Scholz, Christian and Davi, Lucas},
title = {Poster: Code HarvETHter: Corpus-Driven Decompilation of Ethereum Smart Contracts},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3760714},
doi = {10.1145/3719027.3760714},
abstract = {This poster introduces HarvETHter, a smart contract decompiler for EVM-based platforms such as Ethereum, Binance, and Polygon. We present the corpus completeness hypothesis, which we investigate through HarvETHter. Relying on our hypothesis, HarvETHter sources knowledge of the Ethereum blockchain and leverages it to decompile smart contracts to Solidity source code.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {4752–4754},
numpages = {3},
keywords = {decompilation, provenance &amp; security, smart contracts},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@article{10.1145/3772368,
author = {Dramko, Luke and Le Goues, Claire and Schwartz, Edward J.},
title = {Fast, Fine-Grained Equivalence Checking for Neural Decompilers},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3772368},
doi = {10.1145/3772368},
abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models are generally inadequate, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign,1 a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides—which parts of the functions are equivalent and how well the variable names match—is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
keywords = {Program Equivalence, Alignment, Program Analysis}
}

@inproceedings{10.1145/3733822.3764675,
author = {Soni, Vedant and Dutcher, Audrey and Bao, Tiffany and Wang, Ruoyu},
title = {Benchmarking Binary Type Inference Techniques in Decompilers},
year = {2025},
isbn = {9798400719103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733822.3764675},
doi = {10.1145/3733822.3764675},
abstract = {Decompilation is the process of translating low-level, machine-executable code back into a high-level representation. Decompilers–tools that perform this translation–are essential for reverse engineers and security professionals, supporting critical tasks within their workflows. However, due to the inherent loss of information during compilation as a result of optimizations, inlining, and other compiler-specific transformations, decompiled output is often incomplete or inaccurate.A central challenge in decompilation is accurate type inference: the reconstruction of high-level type information for variables based on low-level code patterns and memory access behaviors. Despite ongoing advancements in decompilation research, there is a notable lack of comprehensive comparative studies evaluating the type inference capabilities of existing decompilers.This paper presents a benchmark study of five decompilers, focusing on their ability to infer types at both the function and variable levels. We conduct the evaluation on a dataset of binaries compiled from the Nixpkgs collection at both -O0 and -O2 optimization levels, allowing us to assess decompiler performance across unoptimized and optimized executables. The results highlight the relative strengths and weaknesses of each decompiler and identify recurring scenarios in which incorrect type information is produced.},
booktitle = {Proceedings of the 2025 Workshop on Software Understanding and Reverse Engineering},
pages = {48–60},
numpages = {13},
keywords = {Decompilation, Type Inference, Benchmark},
location = {
},
series = {SURE '25}
}

@inproceedings{10.1145/3733822.3764668,
author = {Lerner, Sam},
title = {ideco: A Framework for Improving Non-C Decompilation},
year = {2025},
isbn = {9798400719103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733822.3764668},
doi = {10.1145/3733822.3764668},
abstract = {We introduce the ideco framework for improving the decompilation of non-C programming languages. ideco provides users with the ability to create rules which rewrite parts of the decompilation.We show that by using a small set of rules, the number of lines of decompiled code for binaries written in C++, Swift, Go, and Rust can be decreased by 5% to 10%. In addition, by using GPT-4o and GPT-4.1-mini as test subjects, we show that a reverse engineering task is easier to solve when its decompilation is processed by ideco.},
booktitle = {Proceedings of the 2025 Workshop on Software Understanding and Reverse Engineering},
pages = {17–40},
numpages = {24},
keywords = {Decompilation, Software Understanding, Reverse Engineering},
location = {
},
series = {SURE '25}
}

@inproceedings{10.1145/3708821.3733877,
author = {Sirlanci, Melih and Yagemann, Carter and Lin, Zhiqiang},
title = {An Empirical Study of C Decompilers: Performance Metrics and Error Taxonomy},
year = {2025},
isbn = {9798400714108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708821.3733877},
doi = {10.1145/3708821.3733877},
abstract = {Decompilation aims to simplify reverse engineering by transforming binary code into a high-level representation, such as C-like code. To determine the current progress towards perfect decompilation, and to identify and quantify open problems for future work, we perform the first comprehensive empirical study on state-of-the-art C decompilers using our framework, DiscScope, which employs symbolic execution and differential analysis to spot discrepancies in decompilation at intermediate program states, pinpointing the exact location where the decompiler makes errors. Using DiscScope and a dataset we built containing benign and malicious real-world programs, we measure the current performance of decompilers in a realistic setting. Our dataset contains programs compiled with different compilers (GCC and Clang) and with 7 different optimization levels. Manual validation of DiscScope shows that it is 96.3% accurate in identifying diverged and equivalent decompilation, which we then use to analyze 1,081,413 total decompiler outputs to build and quantify a taxonomy of open problems.},
booktitle = {Proceedings of the 20th ACM Asia Conference on Computer and Communications Security},
pages = {1707–1723},
numpages = {17},
keywords = {Binary Analysis, Decompilation, Symbolic Execution},
location = {
},
series = {ASIA CCS '25}
}

@article{10.1145/3749988,
author = {Udeshi, Meet and Krishnamurthy, Prashanth and Karri, Ramesh and Khorrami, Farshad},
title = {REMEND: Neural Decompilation for Reverse Engineering Math Equations from Binary Executables},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3749988},
doi = {10.1145/3749988},
abstract = {Analysis of binary executables implementing mathematical equations can benefit from the reverse engineering of semantic information about the implementation. Traditional algorithmic reverse engineering tools either do not recover semantic information or rely on dynamic analysis and symbolic execution with high reverse engineering time. Algorithmic tools also require significant re-engineering effort to target new platforms and languages. Recently, neural methods for decompilation have been developed to recover human-like source code, but they do not extract semantic information explicitly. We develop REMEND, a neural decompilation framework to reverse engineer math equations from binaries to explicitly recover program semantics like data flow and order of operations. REMEND combines a transformer encoder-decoder model for neural decompilation with algorithmic processing for enhanced symbolic reasoning necessary for math equations. REMEND is the first work to demonstrate that transformers for neural decompilation go beyond source code and reason about program semantics in the form of math equations. We train on a synthetically generated dataset containing multiple implementations and compilations of math equations to produce a robust neural decompilation model and demonstrate retargettability. REMEND obtains an accuracy of 89.8% to 92.4% across three Instruction Set Architectures (ISAs), three optimization levels, and two programming languages with a single trained model, extending the capability of state-of-the-art neural decompilers. We achieve high accuracy with a small model of upto 12 million parameters and an average execution time of 0.132 seconds per function. On a real-world dataset collected from open-source programs, REMEND generalizes better than state-of-the-art neural decompilers despite being trained with synthetic data, achieving 8% higher accuracy. The synthetic and real-world datasets are provided at .},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
keywords = {reverse engineering, neural decompilation, math equations}
}

@article{10.1145/3728958,
author = {Wong, Wai Kin and Wu, Daoyuan and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
title = {DecLLM: LLM-Augmented Recompilable Decompilation for Enabling Programmatic Use of Decompiled Code},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728958},
doi = {10.1145/3728958},
abstract = {Decompilers are widely used in reverse engineering (RE) to convert compiled executables into human-readable pseudocode and support various security analysis tasks. Existing decompilers, such as IDA Pro and Ghidra, focus on enhancing the readability of decompiled code rather than its recompilability, which limits further programmatic use, such as for CodeQL-based vulnerability analysis that requires compilable versions of the decompiled code. Recent LLM-based approaches for enhancing decompilation results, while useful for human RE analysts, unfortunately also follow the same path.    In this paper, we explore, for the first time, how off-the-shelf large language models (LLMs) can be used to enable recompilable decompilation—automatically correcting decompiler outputs into compilable versions. We first show that this is non-trivial through a pilot study examining existing rule-based and LLM-based approaches. Based on the lessons learned, we design DecLLM, an iterative LLM-based repair loop that utilizes both static recompilation and dynamic runtime feedback as oracles to iteratively fix decompiler outputs. We test DecLLM on popular C benchmarks and real-world binaries using two mainstream LLMs, GPT-3.5 and GPT-4, and show that off-the-shelf LLMs can achieve an upper bound of around 70% recompilation success rate, i.e., 70 out of 100 originally non-recompilable decompiler outputs are now recompilable. We also demonstrate the practical applicability of the recompilable code for CodeQL-based vulnerability analysis, which is impossible to perform directly on binaries. For the remaining 30% of hard cases, we further delve into their errors to gain insights for future improvements in decompilation-oriented LLM design.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA081},
numpages = {24},
keywords = {Large Language Model, Recompilable Decompilation, Reverse Engineering}
}

@article{10.1145/3728935,
author = {Lagouvardos, Sifis and Bollanos, Yannis and Grech, Neville and Smaragdakis, Yannis},
title = {The Incredible Shrinking Context... in a Decompiler Near You},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728935},
doi = {10.1145/3728935},
abstract = {Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrnkr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrnkr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly “forgetting” control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5% of contracts (compared to ∼95% for Elipmoc), covers (i.e., reaches and manages to decompile) 67% more code than Heimdall-rs, and reduces key imprecision metrics by over 65%, compared again to Elipmoc.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA060},
numpages = {24},
keywords = {Datalog, Decompilation, Ethereum, Program Analysis, Smart Contracts}
}

@article{10.1145/3729373,
author = {Su, Xing and Liang, Hanzhong and Wu, Hao and Niu, Ben and Xu, Fengyuan and Zhong, Sheng},
title = {DiSCo: Towards Decompiling EVM Bytecode to Source Code using Large Language Models},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3729373},
doi = {10.1145/3729373},
abstract = {Understanding the Ethereum smart contract bytecode is essential for ensuring cryptoeconomics security. However, existing decompilers primarily convert bytecode into pseudocode, which is not easily comprehensible for general users, potentially leading to misunderstanding of contract behavior and increased vulnerability to scams or exploits. In this paper, we propose DiSCo, the first LLMs-based EVM decompilation pipeline, which aims to enable LLMs to understand the opaque bytecode and lift it into smart contract code. DiSCo introduces three core technologies. First, a logic-invariant intermediate representation is proposed to reproject the low-level bytecode into high-level abstracted units. The second technique involves semantic enhancement based on a novel type-aware graph model to infer stripped variables during compilation, enhancing the lifting effect. The third technology is a flexible method incorporating code specifications to construct LLM-comprehensible prompts for source code generation. Extensive experiments illustrate that our generated code guarantees a high compilability rate at 75%, with differential fuzzing pass rate averaging at 50%. Manual validation results further indicate that the generated solidity contracts significantly outperforms baseline methods in tasks such as code comprehension and attack reproduction.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE103},
numpages = {24},
keywords = {Decompilation, EVM bytecode, Large Language Models, Smart Contract, Source Code Generation}
}

@inproceedings{10.1145/3696410.3714790,
author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Wu, Tingguan and Wang, Siyun and Chalkias, Kostas Kryptos},
title = {SuiGPT MAD: Move AI Decompiler to Improve Transparency and Auditability on Non-Open-Source Blockchain Smart Contract},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714790},
doi = {10.1145/3696410.3714790},
abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization. Moreover, MAD's methodology could potentially extend to other smart contract languages, like Solidity, further enhancing Web3 transparency.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {1567–1576},
numpages = {10},
keywords = {auditing tools, large language models, move, prompt engineering, smart contract, sui, transparency, web applications, web3},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3722041.3723097,
author = {Cotroneo, Domenico and Grasso, Francesco C. and Natella, Roberto and Orbinato, Vittorio},
title = {Can Neural Decompilation Assist Vulnerability Prediction on Binary Code?},
year = {2025},
isbn = {9798400715631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722041.3723097},
doi = {10.1145/3722041.3723097},
abstract = {Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.},
booktitle = {Proceedings of the 18th European Workshop on Systems Security},
pages = {26–32},
numpages = {7},
keywords = {Binary Analysis, Deep Learning, Neural Decompilation, Security, Vulnerability Prediction},
location = {Rotterdam, Netherlands},
series = {EuroSec'25}
}

@inproceedings{10.1145/3691620.3695020,
author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
title = {WaDec: Decompiling WebAssembly Using Large Language Model},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695020},
doi = {10.1145/3691620.3695020},
abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {481–492},
numpages = {12},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3650212.3652144,
author = {Cao, Ying and Zhang, Runze and Liang, Ruigang and Chen, Kai},
title = {Evaluating the Effectiveness of Decompilers},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652144},
doi = {10.1145/3650212.3652144},
abstract = {In software security tasks like malware analysis and vulnerability mining, reverse engineering is pivotal, with C decompilers playing a crucial role in understanding program semantics. However, reverse engineers still predominantly rely on assembly code rather than decompiled code when analyzing complex binaries. This practice underlines the limitations of current decompiled code, which hinders its effectiveness in reverse engineering. Identifying and analyzing the problems of existing decompilers and making targeted improvements can effectively enhance the efficiency of software analysis. In this study, we systematically evaluate current mainstream decompilers’ semantic consistency and readability. Semantic evaluation results show that the state-of-the-art decompiler Hex-Rays has about 55% accuracy at almost all optimization, which contradicts the common belief among many reverse engineers that decompilers are usually accurate. Readability evaluation indicates that despite years of efforts to improve the readability of the decompiled code, decompilers’ template-based approach still predominantly yields code akin to binary structures rather than human coding patterns. Additionally, our human study indicates that to enhance decompilers’ accuracy and readability, introducing human or compiler-aware strategies like a speculate-verify-correct approach to obtain recompilable decompiled code and iteratively refine it to more closely resemble the original binary, potentially offers a more effective optimization method than relying on static analysis and rule expansion.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {491–502},
numpages = {12},
keywords = {Decompiler, Reverse Engineering, Software Testing},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@article{10.1145/3643744,
author = {Eom, Haeun and Kim, Dohee and Lim, Sori and Koo, Hyungjoon and Hwang, Sungjae},
title = {R2I: A Relative Readability Metric for Decompiled Code},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3643744},
doi = {10.1145/3643744},
abstract = {Decompilation is a process of converting a low-level machine code snippet back into a high-level programming language such as C. It serves as a basis to aid reverse engineers in comprehending the contextual semantics of the code. In this respect, commercial decompilers like Hex-Rays have made significant strides in improving the readability of decompiled code over time. While previous work has proposed the metrics for assessing the readability of source code, including identifiers, variable names, function names, and comments, those metrics are unsuitable for measuring the readability of decompiled code primarily due to i) the lack of rich semantic information in the source and ii) the presence of erroneous syntax or inappropriate expressions. In response, to the best of our knowledge, this work first introduces R2I, the Relative Readability Index, a specialized metric tailored to evaluate decompiled code in a relative context quantitatively. In essence, R2I can be computed by i) taking code snippets across different decompilers as input and ii) extracting pre-defined features from an abstract syntax tree. For the robustness of R2I, we thoroughly investigate the enhancement efforts made by (non-)commercial decompilers and academic research to promote code readability, identifying 31 features to yield a reliable index collectively. Besides, we conducted a user survey to capture subjective factors such as one’s coding styles and preferences. Our empirical experiments demonstrate that R2I is a versatile metric capable of representing the relative quality of decompiled code (e.g., obfuscation, decompiler updates) and being well aligned with human perception in our survey.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {18},
numpages = {23},
keywords = {Code Metric, Code Readability, Decompiled Code, Decompiler}
}

@article{10.1145/3649860,
author = {Lu, Yifei and Hou, Weidong and Pan, Minxue and Li, Xuandong and Su, Zhendong},
title = {Understanding and Finding Java Decompiler Bugs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649860},
doi = {10.1145/3649860},
abstract = {Java decompilers are programs that perform the reverse process of Java compilers, i.e., they translate Java bytecode to Java source code. They are essential for reverse engineering purposes and have become more sophisticated and reliable over the years. However, it remains challenging for modern Java decompilers to reliably perform correct decompilation on real-world programs. To shed light on the key challenges of Java decompilation, this paper provides the first systematic study on the characteristics and causes of bugs in mature, widely-used Java decompilers. We conduct the study by investigating 333 unique bugs from three popular Java decompilers. Our key findings and observations include: (1) Although most of the reported bugs were found when decompiling large, real-world code, 40.2% of them have small test cases for bug reproduction; (2) Over 80% of the bugs manifest as exceptions, syntactic errors, or semantic errors, and bugs with source code artifacts are very likely semantic errors; (3) 57.7%,39.0%, and 41.1% of the bugs respectively are attributed to three stages of decompilers—loading structure entities from bytecode, optimizing these entities, and generating source code from these entities; (4) Bugs in decompilers’ type inference are the most complex to fix; and (5) Region restoration for structures like loop, sugaring for special structures like switch, and type inference of variables of generic types or indistinguishable types are the three most significant challenges in Java decompilation, which to some extent explains our findings in (3) and (4).Based on these findings, we present JD-Tester, a differential testing framework for Java decompilers, and our experience of using it in testing the three popular Java decompilers. JD-Tester utilizes different Java program generators to construct executable Java tests and finds exceptions, syntactic, and semantic inconsistencies (i.e. bugs) between a generated test and its compiled-decompiled version (through compilation and execution). In total, we have found 62 bugs in the three decompilers, demonstrating both the effectiveness of JD-Tester, and the importance of testing and validating Java decompilers.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {143},
numpages = {27},
keywords = {Reverse Engineering, Decompiler, Differential Testing}
}

@inproceedings{10.1109/CGO57630.2024.10444788,
author = {Armengol-Estap\'{e}, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F. P.},
title = {SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly},
year = {2024},
isbn = {9798350395099},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO57630.2024.10444788},
doi = {10.1109/CGO57630.2024.10444788},
abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect.This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code.We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6\texttimes{} more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4\texttimes{} more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
booktitle = {Proceedings of the 2024 IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {67–80},
numpages = {14},
keywords = {decompilation, neural decompilation, transformer, language models, type inference},
location = {Edinburgh, United Kingdom},
series = {CGO '24}
}

@article{10.1145/3591237,
author = {Sisco, Zachary D. and Balkind, Jonathan and Sherwood, Timothy and Hardekopf, Ben},
title = {Loop Rerolling for Hardware Decompilation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591237},
doi = {10.1145/3591237},
abstract = {We introduce the new problem of hardware decompilation. Analogous to software decompilation, hardware decompilation is about analyzing a low-level artifact—in this case a netlist, i.e., a graph of wires and logical gates representing a digital circuit—in order to recover higher-level programming abstractions, and using those abstractions to generate code written in a hardware description language (HDL). The overall problem of hardware decompilation requires a number of pieces. In this paper we focus on one specific piece of the puzzle: a technique we call hardware loop rerolling. Hardware loop rerolling leverages clone detection and program synthesis techniques to identify repeated logic in netlists (such as would be synthesized from loops in the original HDL code) and reroll them into syntactic loops in the recovered HDL code. We evaluate hardware loop rerolling for hardware decompilation over a set of hardware design benchmarks written in the PyRTL HDL and industry standard SystemVerilog. Our implementation identifies and rerolls loops in 52 out of 53 of the netlists in our benchmark suite, and we show three examples of how hardware decompilation can provide concrete benefits: transpilation between HDLs, faster simulation times over netlists (with mean speedup of 6x), and artifact compaction (39% smaller on average).},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {123},
numpages = {23},
keywords = {program synthesis, loop rerolling, hardware decompilation}
}

@article{10.1145/3546946,
author = {Dramko, Luke and Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Ed and Allamanis, Miltiadis and Neubig, Graham and Vasilescu, Bogdan and Le Goues, Claire},
title = {DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3546946},
doi = {10.1145/3546946},
abstract = {The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE’s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {39},
numpages = {34},
keywords = {data provenance, decompilation, Machine learning}
}

@inproceedings{10.1145/3582016.3582058,
author = {Tan, Zujun and Chon, Yebin and Kruse, Michael and Doerfert, Johannes and Xu, Ziyang and Homerding, Brian and Campanoni, Simone and August, David I.},
title = {SPLENDID: Supporting Parallel LLVM-IR Enhanced Natural Decompilation for Interactive Development},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582058},
doi = {10.1145/3582016.3582058},
abstract = {Manually writing parallel programs is difficult and error-prone.   Automatic parallelization could address this issue, but profitability can be limited by not having facts known only to the programmer.   A parallelizing compiler that collaborates with the programmer can increase the coverage and performance of parallelization while reducing the errors and overhead associated with manual parallelization.   Unlike collaboration involving analysis tools that report program properties or make parallelization suggestions to the programmer, decompiler-based collaboration could leverage the strength of existing parallelizing compilers to provide programmers with a natural compiler-parallelized starting point for further parallelization or refinement.  Despite this potential, existing decompilers fail to do this because they do not generate portable parallel source code compatible with any compiler of the source language.  This paper presents SPLENDID, an LLVM-IR to C/OpenMP decompiler that enables collaborative parallelization by producing standard parallel OpenMP code.   Using published manual parallelization of the PolyBench benchmark suite as a reference, SPLENDID's collaborative approach produces programs twice as fast as either Polly-based automatic parallelization or manual parallelization alone.   SPLENDID's portable parallel code is also more natural than that from existing decompilers, obtaining a 39x higher average BLEU score.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {679–693},
numpages = {15},
keywords = {decompilation, collaborative parallelization},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3564625.3567998,
author = {Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei},
title = {Boosting Neural Networks to Decompile Optimized Binaries},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564625.3567998},
doi = {10.1145/3564625.3567998},
abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21% higher accuracy than state-of-the-art neural decompilation frameworks.},
booktitle = {Proceedings of the 38th Annual Computer Security Applications Conference},
pages = {508–518},
numpages = {11},
location = {Austin, TX, USA},
series = {ACSAC '22}
}

@inproceedings{10.1145/3561320.3561328,
author = {Greg\'{o}rio, Nelson and Fernandes, Jo\~{a}o Paulo and Bispo, Jo\~{a}o and Medeiros, S\'{e}rgio},
title = {E-APK: Energy Pattern Detection in Decompiled Android Applications},
year = {2022},
isbn = {9781450397445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561320.3561328},
doi = {10.1145/3561320.3561328},
abstract = {Energy efficiency is a non-functional requirement that developers must consider. This requirement is particularly relevant when building software for battery-operated devices like mobile ones: a long-lasting battery is an essential requirement for an enjoyable user experience. It has been shown that many mobile applications include inefficiencies that cause battery to be drained faster than necessary. Some of these inefficiencies result from software patterns that have been catalogued in the literature. The catalogues often provide more energy-efficient alternatives. While the related literature is vast, most approaches so far assume as a fundamental requirement that one has access to the source code of an application in order to be able to analyse it. This requirement makes independent energy analysis challenging, or even impossible, e.g. for a mobile user or, most significantly, an App Store trying to provide information on how efficient an application being submitted for publication is. Our work studies the viability of looking for known energy patterns in applications by decompiling them and analysing the resulting code. For this, we decompiled and analysed 236 open-source applications. We extended an existing tool to aid in this process, making it capable of seamlessly decompiling and analysing android applications. With the collected data, we performed a comparative analysis of the presence of energy patterns between the source code and the decompiled code. While further research is required to more assertively say if this type of static analysis is viable, our results point in a promising direction with 163 applications, approximately 69%, containing the same number of detected patterns in both source code and the release APK.},
booktitle = {Proceedings of the XXVI Brazilian Symposium on Programming Languages},
pages = {50–58},
numpages = {9},
keywords = {android, code patterns, compilers, decompiler, energy efficiency, metaprogramming, mobile, static analysis},
location = {Virtual Event, Brazil},
series = {SBLP '22}
}

@article{10.1145/3527321,
author = {Grech, Neville and Lagouvardos, Sifis and Tsatiris, Ilias and Smaragdakis, Yannis},
title = {Elipmoc: advanced decompilation of Ethereum smart contracts},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3527321},
doi = {10.1145/3527321},
abstract = {Smart contracts on the Ethereum blockchain greatly benefit from cutting-edge analysis techniques and pose significant challenges. A primary challenge is the extremely low-level representation of deployed contracts. We present Elipmoc, a decompiler for the next generation of smart contract analyses. Elipmoc is an evolution of Gigahorse, the top research decompiler, dramatically improving over it and over other state-of-the-art tools, by employing several high-precision techniques and making them scalable. Among these techniques are a new kind of context sensitivity (termed “transactional sensitivity”) that provides a more effective static abstraction of distinct dynamic executions; a path-sensitive (yet scalable, through path merging) algorithm for inference of function arguments and returns; and a fully context sensitive private function reconstruction process. As a result, smart contract security analyses and reverse-engineering tools built on top of Elipmoc achieve high scalability, precision and completeness. Elipmoc improves over all notable past decompilers, including its predecessor, Gigahorse, and the state-of-the-art industrial tool, Panoramix, integrated into the primary Ethereum blockchain explorer, Etherscan. Elipmoc produces decompiled contracts with fully resolved operands at a rate of 99.5% (compared to 62.8% for Gigahorse), and achieves much higher completeness in code decompilation than Panoramix—e.g., up to 67% more coverage of external call statements—while being over 5x faster. Elipmoc has been the enabler for recent (independent) discoveries of several exploitable vulnerabilities on popular protocols, over funds in the many millions of dollars.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {77},
numpages = {27},
keywords = {Smart Contracts, Security, Program Analysis, Ethereum, Decompilation, Datalog, Blockchain}
}

@inproceedings{10.1145/3320269.3384766,
author = {Gussoni, Andrea and Di Federico, Alessandro and Fezzardi, Pietro and Agosta, Giovanni},
title = {A Comb for Decompiled C Code},
year = {2020},
isbn = {9781450367509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320269.3384766},
doi = {10.1145/3320269.3384766},
abstract = {Decompilers are fundamental tools to perform security assessments of third-party software. The quality of decompiled code can be a game changer in order to reduce the time and effort required for analysis. This paper proposes a novel approach to restructure the control flow graph recovered from binary programs in a semantics-preserving fashion. The algorithm is designed from the ground up with the goal of producing C code that is both goto-free and drastically reducing the mental load required for an analyst to understand it. As a result, the code generated with this technique is well-structured, idiomatic, readable, easy to understand and fully exploits the expressiveness of C language. The algorithm has been implemented on top of the revng static binary analysis framework. The resulting decompiler, revngc, is compared on real-world binaries with state-of-the-art commercial and open source tools. The results show that our decompilation process introduces between 40% and 50% less extra cyclomatic complexity.},
booktitle = {Proceedings of the 15th ACM Asia Conference on Computer and Communications Security},
pages = {637–651},
numpages = {15},
keywords = {reverse engineering, goto, decompilation, control flow restructuring},
location = {Taipei, Taiwan},
series = {ASIA CCS '20}
}

@inproceedings{10.1145/3395363.3397370,
author = {Liu, Zhibo and Wang, Shuai},
title = {How far we have come: testing decompilation correctness of C decompilers},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397370},
doi = {10.1145/3395363.3397370},
abstract = {A C decompiler converts an executable (the output from a C compiler) into source code. The recovered C source code, once recompiled, will produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications, including legacy software migration, security retrofitting, software comprehension, and to act as the first step in launching adversarial software exploitations. As the paramount component and the trust base in numerous cybersecurity tasks, C decompilers have enabled the analysis of malware, ransomware, and promoted cybersecurity professionals’ understanding of vulnerabilities in real-world systems.  In contrast to this flourishing market, our observation is that in academia, outputs of C decompilers (i.e., recovered C source code) are still not extensively used. Instead, the intermediate representations are often more desired for usage when developing applications such as binary security retrofitting. We acknowledge that such conservative approaches in academia are a result of widespread and pessimistic views on the decompilation correctness. However, in conventional software engineering and security research, how much of a problem is, for instance, reusing a piece of simple legacy code by taking the output of modern C decompilers?  In this work, we test decompilation correctness to present an up-to-date understanding regarding modern C decompilers. We detected a total of 1,423 inputs that can trigger decompilation errors from four popular decompilers, and with extensive manual effort, we identified 13 bugs in two open-source decompilers. Our findings show that the overly pessimistic view of decompilation correctness leads researchers to underestimate the potential of modern decompilers; the state-of-the-art decompilers certainly care about the functional correctness, and they are making promising progress. However, some tasks that have been studied for years in academia, such as type inference and optimization, still impede C decompilers from generating quality outputs more than is reflected in the literature. These issues rarely receive enough attention and can lead to great confusion that misleads users.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {475–487},
numpages = {13},
keywords = {Software Testing, Reverse Engineering, Decompiler},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3196321.3196330,
author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan},
title = {Meaningful variable names for decompiled code: a machine translation approach},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196330},
doi = {10.1145/3196321.3196330},
abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2% of the time, which represents a lower bound for performance.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {20–30},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3106237.3121274,
author = {Jaffe, Alan},
title = {Suggesting meaningful variable names for decompiled code: a machine translation approach},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3121274},
doi = {10.1145/3106237.3121274},
abstract = {Decompiled code lacks meaningful variable names. We used statistical machine translation to suggest variable names that are natural given the context. This technique has previously been successfully applied to obfuscated JavaScript code, but decompiled C code poses unique challenges in constructing an aligned corpus and selecting the best translation from among several candidates.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {1050–1052},
numpages = {3},
keywords = {Statistical machine translation, Reverse engineering, Decompilation},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1145/3236794,
author = {Nandi, Chandrakana and Wilcox, James R. and Panchekha, Pavel and Blau, Taylor and Grossman, Dan and Tatlock, Zachary},
title = {Functional programming for compiling and decompiling computer-aided design},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ICFP},
url = {https://doi.org/10.1145/3236794},
doi = {10.1145/3236794},
abstract = {Desktop-manufacturing techniques like 3D printing are increasingly popular because they reduce the cost and complexity of producing customized objects on demand. Unfortunately, the vibrant communities of early adopters, often referred to as "makers," are not well-served by currently available software pipelines. Users today must compose idiosyncratic sequences of tools which are typically repurposed variants of proprietary software originally designed for expert specialists.  This paper proposes fundamental programming-languages techniques to bring improved rigor, reduced complexity, and new functionality to the computer-aided design (CAD) software pipeline for applications like 3D-printing. Compositionality, denotational semantics, compiler correctness, and program synthesis all play key roles in our approach, starting from the perspective that solid geometry is a programming language.  Specifically, we define a purely functional language for CAD called LambdaCAD and a polygon surface-mesh intermediate representation. We then define denotational semantics of both languages to 3D solids and a compiler from CAD to mesh accompanied by a proof of semantics preservation. We illustrate the utility of this foundation by developing a novel synthesis algorithm based on evaluation contexts to "reverse compile" difficult-to-edit meshes downloaded from online maker communities back to more-editable CAD programs. All our prototypes have been implemented in OCaml to enable further exploration of functional programming for desktop manufacturing.},
journal = {Proc. ACM Program. Lang.},
month = jul,
articleno = {99},
numpages = {31},
keywords = {program synthesis, language design, denotational semantics, 3D printing}
}

@inproceedings{10.1145/3375894.3375895,
author = {Botacin, Marcus and Galante, Lucas and de Geus, Paulo and Gr\'{e}gio, Andr\'{e}},
title = {RevEngE is a dish served cold: Debug-Oriented Malware Decompilation and Reassembly},
year = {2020},
isbn = {9781450377751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375894.3375895},
doi = {10.1145/3375894.3375895},
abstract = {Malware analysis is key for cybersecurity overall improvement. Analysis tools have been evolving from complete static analyzers to decompilers. Malware decompilation allows for code inspection at higher abstraction levels, easing incident response. However, the decompilation procedure has many challenges, such as opaque constructions, irreversible mappings, semantic gap bridging, among others. In this paper, we propose a new approach that leverages the human analyst expertise to overcome decompilation challenges. We name this approach "DoD---debug-oriented decompilation", in which the analyst is able to reverse engineer the malware sample on his own and to instruct the decompiler to translate selected code portions (e.g., decision branches, fingerprinting functions, payloads etc.) into high level code. With DoD, the analyst might group all decompiled pieces into new code to be analyzed by other tool, or to develop a novel malware sample from previous pieces of code and thus exercise a Proof-of-Concept (PoC). To validate our approach, we propose RevEngE, the Reverse Engineering Engine for malware decompilation and reassembly, a set of GDB extensions that intercept and introspect into executed functions to build an Intermediate Representation (IR) in real-time, enabling any-time decompilation. We evaluate RevEngE with x86 ELF binaries collected from VirusShare, and show that a new malware sample created from the decompilation of independent functions of five known malware samples is considered "clean" by all VirusTotal's AVs.},
booktitle = {Proceedings of the 3rd Reversing and Offensive-Oriented Trends Symposium},
articleno = {1},
numpages = {12},
location = {Vienna, Austria},
series = {ROOTS'19}
}

@inproceedings{10.1109/ASE.2019.00064,
author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
title = {DIRE: a neural approach to decompiled identifier naming},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00064},
doi = {10.1109/ASE.2019.00064},
abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {628–639},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3319535.3363255,
author = {Jang, Heejun and Jin, Beomjin and Hyun, Sangwon and Kim, Hyoungshick},
title = {Kerberoid: A Practical Android App Decompilation System with Multiple Decompilers},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363255},
doi = {10.1145/3319535.3363255},
abstract = {Decompilation is frequently used to analyze binary programs. In Android, however, decompilers all perform differently with varying apps due to their own characteristics. Obviously, there is no universal solution in all conditions. Based on this observation, we present a practical Android app decompilation system (called Kerberoid) that automatically stitches the results from multiple decompilers together to maximize the coverage and the accuracy of decompiled codes. We evaluate the performance of Kerberoid with 151 Android apps in which their corresponding source codes are publicly available. Kerberoid fully recovered all functions for 17% of the apps tested and gained a similarity score over 50% for 40% of the apps tested, increased by 7% and 9%, respectively, compared with the best existing decompiler.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2557–2559},
numpages = {3},
keywords = {reverse engineering, mobile security, decompilation, android apps},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1109/ICSE.2019.00120,
author = {Grech, Neville and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
title = {Gigahorse: thorough, declarative decompilation of smart contracts},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00120},
doi = {10.1109/ICSE.2019.00120},
abstract = {The rise of smart contracts---autonomous applications running on blockchains---has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract.We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a high-level 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts.Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts---e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a "batteries included" approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1176–1186},
numpages = {11},
keywords = {program analysis, ethereum, decompilation, blockchain},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3735971,
author = {Chen, Jinze and Liu, Jieli and Wu, Jianlin and Lin, Dan and Wu, Jiajing and Zheng, Zibin},
title = {PonziHunter: Hunting Ethereum Ponzi Contract via Static Analysis and Contrastive Learning on the Bytecode Level},
year = {2026},
issue_date = {February 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3735971},
doi = {10.1145/3735971},
abstract = {In recent years, blockchain technology has developed rapidly and received widespread attention. However, its pseudonymous and decentralized nature has also attracted many criminal activities. Ponzi schemes, a kind of classic financial scam, also hide their true face in smart contracts, causing massive financial losses to blockchain users. Although several methods have been proposed to detect Ponzi contracts, there are still limitations in broad applicability, semantics understanding, and adversarial robustness. In this article, we propose PonziHunter, an intelligent framework for hunting Ponzi contracts on Ethereum. To tackle the problem of broad applicability, we train a detection model that does not require expert experience based on publicly available on-chain bytecode and off-chain contract labels. To tackle the problem of semantics understanding, we employ cross-function control flows and state variable dependencies to understand the logic of Ponzi contracts. Specifically, we decompile bytecodes into higher-order representations to analyze control flows and state variable dependencies and model the information as graph data. By combining the idea of code slicing, we identify the basic blocks related to Ponzi contract recognition. To tackle the problem of adversarial robustness, we model Ponzi contract recognition as a graph classification problem based on contrastive pre-training. We propose a data augmentation method for control flow graphs (CFGs), which preserves the basic blocks related to Ponzi contract recognition as much as possible during data perturbation. Experimental results show that PonziHunter outperforms state-of-the-art tools with average improvements of at least 4.77% on real-world ground-truth data and can newly discover 85 Ponzi contracts in the wild. More importantly, PonziHunter is robust against adversarial examples and can locate the critical basic blocks for smart Ponzi detection.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {37},
numpages = {21},
keywords = {Smart contract, Ponzi scheme, static analysis, contrastive learning, blockchain safety}
}

@inproceedings{10.1145/3719027.3765144,
author = {Zhong, Zheng and Wu, Ruoyu and Wan, Junpeng and Zou, Muqi and Tian, Dave (Jing)},
title = {Hardening Deep Neural Network Binaries against Reverse Engineering Attacks},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3765144},
doi = {10.1145/3719027.3765144},
abstract = {Deep Neural Networks (DNNs) are proprietary assets due to the expertise, confidential data, and high development costs involved in model training. Well-trained DNN models are compiled into DNN binaries to be efficiently executed on various platforms, such as edge devices and cloud infrastructures. Recent research on DNN binary decompilation shows the potential of stealing DNN models via binary reverse engineering techniques. While obfuscation is a well-studied technique to hamper binary reverse engineering, general obfuscation schemes are not designed for this new type of binary and have limitations in concealing information within DNN binaries due to the unique characteristics of DNN binaries. In this paper, we show that existing reverse engineering attacks on DNN binaries can recover 98.5% of DNN operators from DNN binaries that have been obfuscated using general obfuscators. We then propose new obfuscation schemes tailored for DNN binaries, namely, (1) Flexible Operator Fusion; (2) Fake Operator Insertion; and (3) Operator Computation Reordering. We implement our dedicated obfuscation schemes as an end-to-end obfuscation toolchain called NeuroShield. Experiments show that NeuroShield is resilient to existing model reverse engineering attacks while introducing a reasonable overhead. Specifically, NeuroShield reduces the operator recovery rate to 3.03% for CV models and 47.18% for NLP models. Moreover, it has comparable binary size overhead and significantly lower execution time overhead (7.8% - 36.1%) compared to OLLVM, one of the commonly used general obfuscators.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {201–215},
numpages = {15},
keywords = {binary analysis, deep neural network, reverse engineering},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3719027.3765089,
author = {Huang, Haohui and Liu, Yue and Cheng, Yuxi and Wei, Haiyang and Liu, Jiamu and Wang, Yu and Wang, Linzhang},
title = {Recover Function Signature from Combined Constraints},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3765089},
doi = {10.1145/3719027.3765089},
abstract = {Recovering function signatures is a cornerstone of binary program analysis, yet it remains a challenging task. Existing methods either rely on disassembly-based constraints, which struggle with cross-architecture compatibility and scalability, or adopt learning-based approaches that are resource-intensive and often inaccurate. In this paper, we present CDA, a novel decompilation-based method for recovering function signatures that combines the strengths of multiple decompilers while mitigating their limitations. The core idea behind CDA is leveraging probabilistic constraints to estimate the likelihood of each function signature recovery result produced by decompilers, guided by inference rules specifically designed to address the limitations of decompilers. Based on these probabilities, CDA selects the recovery results with the highest likelihood as the final outcomes. We extensively evaluate CDA across five tasks --- variadic function/position detection, parameter identification, return value detection, and parameter type recovery --- comparing it against state-of-the-art tools, including IDA, Ghidra, Binary Ninja, and TYGR. Experimental results show that CDA outperforms baseline tools across multiple architectures (x64, x86, AArch64, Arm, and Mips) and optimization levels (O0-O3), highlighting its robustness and reliability in diverse compilation environments.},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3386–3400},
numpages = {15},
keywords = {function signature recovery, probabilistic constraint, reverse engineering},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3719027.3762169,
author = {Pilgun, Aleksandr},
title = {Demo: Reverse Engineering Android Apps with Code Coverage},
year = {2025},
isbn = {9798400715259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719027.3762169},
doi = {10.1145/3719027.3762169},
abstract = {Reverse engineering Android apps remains a critical and labor-intensive task, particularly for analyzing novel malware. Analysts typically begin with decompiled Java code using tools like JaDX and often must correlate it with runtime information gathered from dynamic analysis. In this work, we present JaDX-ACVTool, a plugin that bridges this gap by integrating code coverage information from ACVTool directly into JaDX-GUI. Our approach highlights Java methods executed during analysis, enabling security analysts to quickly identify and navigate runtime-relevant code paths. Plugin repository: https://github.com/pilgun/jadx-acvtool},
booktitle = {Proceedings of the 2025 ACM SIGSAC Conference on Computer and Communications Security},
pages = {4725–4727},
numpages = {3},
keywords = {android, code coverage, malware analysis, reverse engineering},
location = {Taipei, Taiwan},
series = {CCS '25}
}

@inproceedings{10.1145/3746252.3761266,
author = {Wang, Yufeng and Feng, Yuhong and Cao, Yixuan and Li, Haoran and Feng, Haiyue and Wang, Yifeng},
title = {ORCAS: Obfuscation-Resilient Binary Code Similarity Analysis using Dominance Enhanced Semantic Graph},
year = {2025},
isbn = {9798400720406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746252.3761266},
doi = {10.1145/3746252.3761266},
abstract = {Binary code similarity analysis (BCSA) serves as a foundational technique for binary analysis tasks such as vulnerability detection and malware identification. Existing graph based BCSA approaches capture more binary code semantics and demonstrate remarkable performance. However, when code obfuscation is applied, the unstable control flow structure degrades their performance. To address this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based on Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary code representation, capturing more binaries' implicit semantics without control flow structure, including inter-instruction relations (e.g., def-use), inter-basic block relations (i.e., dominance and post-dominance), and instruction-basic block relations. ORCAS takes binary functions from different obfuscation options, optimization levels, and instruction set architectures as input and scores their semantic similarity more robustly. Extensive experiments have been conducted on ORCAS against eight baseline approaches over the BinKit dataset. For example, ORCAS achieves an average 12.1% PR-AUC improvement when using combined three obfuscation options compared to the state-of-the-art approaches. In addition, an original obfuscated real-world vulnerability dataset has been constructed and released to facilitate a more comprehensive research on obfuscated binary code analysis. ORCAS outperforms the state-of-the-art approaches over this newly released real-world vulnerability dataset by up to a recall improvement of 43%.},
booktitle = {Proceedings of the 34th ACM International Conference on Information and Knowledge Management},
pages = {3198–3208},
numpages = {11},
keywords = {binary code similarity analysis, dominator tree, obfuscation-resilient},
location = {Seoul, Republic of Korea},
series = {CIKM '25}
}

@inproceedings{10.1145/3733817.3762702,
author = {Altamura, Nicol\`{o} and Bragastini, Enrico and Campion, Marco and Dalla Preda, Mila},
title = {Assessing the Effectiveness of the Tigress Obfuscator Against MOPSA and BinaryNinja},
year = {2025},
isbn = {9798400719066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733817.3762702},
doi = {10.1145/3733817.3762702},
abstract = {We present an empirical evaluation of the Tigress obfuscator, focusing on its ability to degrade the precision of static analyses performed by two state-of-the-art tools: mopsa, a source-level static analyzer, and BinaryNinja&nbsp; a binary-level decompiler and analysis platform. By applying a variety of lightweight yet diverse obfuscation strategies—such as control flow flattening, opaque predicates, and data encoding—we systematically assess how these transformations affect the analyzability of C programs. Our findings highlight the scenarios in which obfuscation successfully confuses analysis tools and those where it fails to do so.},
booktitle = {Proceedings of the 2025 Workshop on Research on Offensive and Defensive Techniques in the Context of Man At The End (MATE) Attacks},
pages = {29–37},
numpages = {9},
keywords = {Software Protection, Static Analysis, Code Obfuscation},
location = {
},
series = {CheckMATE '25}
}

@inproceedings{10.1145/3733822.3764673,
author = {Sakamoto, Noriki and Takeuchi, Kazuhiro},
title = {Toward Inferring Structural Semantics from Binary Code Using Graph Neural Networks},
year = {2025},
isbn = {9798400719103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733822.3764673},
doi = {10.1145/3733822.3764673},
abstract = {Recovering semantic information from binary code is a fundamental challenge in reverse engineering, especially when source-level information is unavailable. We aim to analyze the types and roles of structural elements from the binary observed in the compiled program, focusing on their contextual usage patterns and associations to other members.We refer to such semantic aspects as structural semantics , meaning that cooccurring patterns of jointly updated structure members reveal the functional roles that can be inferred from their coupling, throughout this paper. Recent approaches have applied graph neural networks (GNNs) to data-flow graphs (DFGs) for variable type inference, but most rely on a single model architecture, such as the relational graph convolutional network (R-GCN). While effective, such models may overlook alternative patterns of structure member behavior. In this paper, we investigate the effectiveness of three alternative GNN architectures gated graph neural networks (GGNN), graph attention networks (GAT), and standard graph convolutional networks (GCN) in capturing structural semantics from binary-level data-flow graphs. We evaluate these models on real-world binaries compiled at multiple optimization levels, measuring their ability to infer semantic properties of structure members. Our results show that these architectures capture complementary aspects of structural semantics. GGNN is effective at modeling long-range dependencies, GAT suppresses irrelevant connections, and GCN offers computational simplicity. Different model architectures emphasize distinct aspects of structural semantics, capturing complementary patterns of how structure members are accessed together in memory. This demonstrates that architectural diversity provides richer perspectives for semantic inference in binary analysis.},
booktitle = {Proceedings of the 2025 Workshop on Software Understanding and Reverse Engineering},
pages = {102–113},
numpages = {12},
keywords = {Decompiler, Reverse Engineering},
location = {
},
series = {SURE '25}
}

@inproceedings{10.1145/3733822.3764672,
author = {Magin, Florian and Wache, Magdalena and Scherf, Fabian and Fischer, Cl\'{e}o and Zabel, Jonas},
title = {Towards Scalable Evaluation of Software Understanding: A Methodology Proposal},
year = {2025},
isbn = {9798400719103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733822.3764672},
doi = {10.1145/3733822.3764672},
abstract = {In reverse engineering our goal is to build systems that help people to understand software. However, the field has not converged on a way to measure software understanding. In this paper, we make the case that understanding should be measured via performance on understanding-questions. We propose a method for constructing understanding-questions and evaluating answers at scale. We conduct a case study in which we apply our method and compare Ghidra’s default auto analysis with an analysis that supports binary constructs that are specific to Objective-C.},
booktitle = {Proceedings of the 2025 Workshop on Software Understanding and Reverse Engineering},
pages = {61–67},
numpages = {7},
keywords = {Decompilation, Evaluation, Understanding, Large Language Models},
location = {
},
series = {SURE '25}
}

@article{10.1145/3765521,
author = {Dall'Aglio, Lorenzo and Binosi, Lorenzo and Carminati, Michele and Zanero, Stefano and Polino, Mario},
title = {Highliner: Enhancing Binary Analysis through NLP-Based Instruction-Level Detection of C++ Inline Functions},
year = {2025},
issue_date = {November 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3765521},
doi = {10.1145/3765521},
abstract = {The complexities introduced by compiler optimization have long stood as a significant obstacle in binary analysis and reverse engineering. Function inlining, in particular, complicates function recognition by replacing function calls with the entire body of the callee, mixing code from multiple functions. State-of-the-art approaches can identify inlined functions at basic block granularity, but cannot determine which instructions belong to each function and precisely deduce inlined boundaries. Without this information, further analyses such as decompilation cannot be performed effectively. This article presents Highliner, a novel approach that improves state-of-the-art approaches by identifying inline instances at instruction-level granularity. Highliner operates downstream of block-level detectors: given basic blocks reported by state-of-the-art approaches as belonging to a specific inlined function, it labels each instruction as Inlined or Not inlined and recovers the inlined-function boundaries. We treat the problem as a sequence tagging task typical of NLP and implement a learning-based technique involving instruction embedding and recurrent neural networks. We compile a dataset of open-source projects with different optimizations and use the DWARF debug information standard to construct labeled sequences of inline instructions. We use this dataset to train, validate, and test a sequence labeling architecture in which instructions are encoded via the pre-trained assembly language transformer PalmTree and then processed by an RNN-based classifier to produce binary predictions. When evaluated as a binary classifier, Highliner achieves an F1-score of 0.94 overall. In addition, when specifically tested on recognizing function boundaries, Highliner achieves an Accuracy of 0.82 on initial boundaries and 0.83 on final boundaries.},
journal = {ACM Trans. Priv. Secur.},
month = oct,
articleno = {51},
numpages = {22},
keywords = {Binary analysis, reverse engineering, function inlining, inline function recognition, natural language processing (NLP)}
}

@inproceedings{10.1145/3759425.3763397,
author = {Chen, Xiang and Zhou, Anshunkang and Ye, Chengfeng and Zhang, Charles},
title = {ClearAgent: Agentic Binary Analysis for Effective Vulnerability Detection},
year = {2025},
isbn = {9798400721489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759425.3763397},
doi = {10.1145/3759425.3763397},
abstract = {Statically detecting vulnerabilities at the binary level is crucial for the security of Commercial-Off-The-Shelf (COTS) software when source code is not available. However, traditional methods suffer from the inherent limitations of binary translation and static analysis, which hinder their scalability for complex real-world binaries. Recent efforts that leverage Large Language Models (LLMs) for vulnerability detection are still limited by possible hallucination, inaccurate code property retrieval, and insufficient guidance.    In this paper, we propose a new agentic binary analysis framework ClearAgent, which features a novel binary interface that provides both LLM-friendly and analyzer-friendly tools to facilitate effective understanding of binary code semantics with rich context. ClearAgent works by automatically interacting with the interface and iteratively exploring for buggy binary code. For candidate bug reports, ClearAgent further tries to verify the existence of the vulnerability by constructing concrete inputs that can trigger the buggy locations.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages},
pages = {130–137},
numpages = {8},
keywords = {Agent, Binary Analysis, Vulnerability Detection},
location = {Singapore, Singapore},
series = {LMPL '25}
}

@inproceedings{10.1145/3759425.3763387,
author = {Liu, Puzhuo and Di, Peng and Jiang, Yu},
title = {Function Renaming in Reverse Engineering of Embedded Device Firmware with ChatGPT},
year = {2025},
isbn = {9798400721489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3759425.3763387},
doi = {10.1145/3759425.3763387},
abstract = {Firmware reverse engineering is crucial for exposing internal mechanisms and identifying security vulnerabilities in embedded systems. While reconstructing the structural components of code is   generally feasible, the absence of function names greatly complicates efforts to analyze and comprehend firmware logic. Motivated by the demonstrated code generation capabilities of large   language models (LLMs), this paper investigates their potential to automate function renaming. We introduce FirmNamer, a prototype system designed to streamline the labor-intensive process of ana-   lyzing decompiled code and assigning meaningful function names. FirmNamer accomplishes this by dynamically constructing LLM prompts based on extracted function code and contextual informa-   tion. Extensive evaluation shows that FirmNamer achieves superior performance in function renaming, obtaining a functional precision of 86.6% and a semantic precision of 49%, thereby surpassing   existing state-of-the-art approaches such as DeGPT, DEBIN, NFRE, NERO, and SYMLM.},
booktitle = {Proceedings of the 1st ACM SIGPLAN International Workshop on Language Models and Programming Languages},
pages = {57–65},
numpages = {9},
keywords = {Firmware, Function Summary, Large Language Model},
location = {Singapore, Singapore},
series = {LMPL '25}
}

@article{10.1145/3757735,
author = {Blazquez, Eduardo and Tapiador, Juan},
title = {Practical Android Software Protection in the Wild},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3757735},
doi = {10.1145/3757735},
abstract = {Software protection refers to a range of methods used to protect applications against reverse engineering. Although this term is commonly used, distinctions arise in the specific tools and techniques utilized, such as packers, protectors, and obfuscators, as each category employs different strategies to defend applications against analysis. Given the growing importance of protecting intellectual property and sensitive user information stored in mobile applications, these protective measures have become indispensable. This article presents a taxonomy categorizing and describing the main techniques used to secure Android applications. Additionally, we analyze the available software tools designed to aid developers in protecting their applications, as well as their prevalence in the wild using a longitudinal dataset comprising nearly 2.5 million apps, including malicious software, pre-installed applications, and regular market application. Our key findings show that, although the use of software protection techniques has been steadily increasing over the last decade, they are still used only by a small fraction of applications in the Android ecosystem. Games and financial applications are by far the ones that most commonly use some form of protection, and we also observe noticeable differences between marketplaces.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {36},
numpages = {32},
keywords = {Android software protection, anti-analysis techniques, program obfuscation, Man-At-The-End (MATE) attacks}
}

@inbook{10.1145/3696630.3731468,
author = {Nguyen, Guillaume},
title = {Automating the conformity assessment of Cyber-Physical Systems software},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3731468},
abstract = {Cyber-physical systems (CPS) are tools used by humans to enhance the way they perform tasks. CPSs make tasks more efficient, more precise, and safer. Those systems are omnipresent in human lives, e.g., in cars with Advanced Driver Assistance Systems (ADAS), in Unmanned Aerial Vehicles (UAV) for self-balancing or even in medical devices. CPSs can read information from the real world, process it, and affect the real world back, considering constraints such as real-time processing. Furthermore, the safety and security of the software controlling the CPS are directly linked with the safety and security of human bystanders. The European Union (EU) has a process to assess the conformity of specific products exchanged within the EU to ensure the safety of its citizens. Recently, regulations and directives such as the Cyber Resilience Act (CRA) pressed European actors to provide compliant software products. Requirements on software started with the Medical Device Regulation (MDR) in 2017. However, technical requirements are challenging to understand from legal texts, and certification processes rely solely on manufacturer documentation. On the one hand, the EU has difficulty monitoring and opening the European market to products deemed compliant. On the other hand, manufacturers have difficulty understanding what is technically required of them when introducing products. This thesis aims to reconcile both parties.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {1281–1284},
numpages = {4}
}

@inbook{10.1145/3696630.3728508,
author = {Alecci, Marco and Sannier, Nicolas and Ceci, Marcello and Abualhaija, Sallam and Samhi, Jordan and Bianculli, Domenico and Bissyand\'{e}, Tegawend\'{e} and Klein, Jacques},
title = {Toward LLM-Driven GDPR Compliance Checking for Android Apps},
year = {2025},
isbn = {9798400712760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696630.3728508},
abstract = {Android apps extensively collect sensitive personal data from our devices daily. Despite stringent regulations like the European Union's General Data Protection Regulation (GDPR), many applications (apps) fail to comply with these legal requirements. While previous studies have focused on the compliance of privacy policies, checking how these policies are implemented in the actual code has not yet been extensively investigated. Moreover, previous efforts have often been limited in scope.This paper explores the potential of Large Language Models (LLMs) to address the challenge of verifying privacy regulation compliance in Android apps. Specifically, we address scenarios where source code is unavailable by investigating whether LLM can work with Smali code—a human-readable representation of Android byte-code extracted from APK files. Through this exploratory investigation, we aim to uncover if LLMs can bridge the gap between legal privacy requirements and their technical implementation in mobile apps. Through initial experiments, we assess the feasibility and effectiveness of a straightforward LLM-driven method for identifying compliance issues and provide directions for our future research efforts to improve our approach and perform large-scale experiments.},
booktitle = {Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
pages = {606–610},
numpages = {5}
}

@article{10.1145/3728911,
author = {Liu, Zhijie and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Liang Feng and Tang, Yutian},
title = {KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {ISSTA},
url = {https://doi.org/10.1145/3728911},
doi = {10.1145/3728911},
abstract = {Binary code similarity analysis (BCSA) is a crucial research area in many fields such as cybersecurity. Specifically, function-level diffing tools are the most widely used in BCSA: they perform function matching one by one for evaluating the similarity between binary programs. However, such methods need a high time complexity, making them unscalable in large-scale scenarios (e.g., 1/n-to-n search). Towards effective and efficient program-level BCSA, we propose KEENHash, a novel hashing approach that hashes binaries into program-level representations through large language model (LLM)-generated function embeddings. KEENHash condenses a binary into one compact and fixed-length program embedding using K-Means and Feature Hashing, allowing us to do effective and efficient large-scale program-level BCSA, surpassing the previous state-of-the-art methods. The experimental results show that KEENHash is at least 215 times faster than the state-of-the-art function matching tools while maintaining effectiveness. Furthermore, in a large-scale scenario with 5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while these tools will cost at least 56 days. We also evaluate KEENHash on the program clone search of large-scale BCSA across extensive datasets in 202,305 binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of them by at least 23.16%, and displays remarkable superiority over them in the large-scale BCSA security scenario of malware detection.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {ISSTA036},
numpages = {24},
keywords = {BCSA, Clone Search, LLM, Program}
}

@inproceedings{10.1145/3755881.3755883,
author = {Sun, Yu and Bao, Lingfeng and Yang, Xiaohu},
title = {FIRE: Smart Contract Bytecode Function Identification via Graph-Refined Hybrid Feature Encoding},
year = {2025},
isbn = {9798400719264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3755881.3755883},
doi = {10.1145/3755881.3755883},
abstract = {The growing popularity of smart contracts has spurred an increasing demand for efficient analysis of their bytecode. Reverse engineering plays a critical role in understanding and auditing smart contracts, with function identification being a key aspect. However, existing function identification techniques often struggle with scalability, accuracy, and adaptability across different contract versions. This paper presents FIRE (Smart Contract Bytecode Function Identification via Graph-Refined Hybrid Encoding), a novel approach to function identification in Ethereum smart contract bytecode. By leveraging hybrid encoding of basic blocks and incorporating a graph neural network (GNN) based on control flow graph (CFG), our method improves the effectiveness of function identification. The approach demonstrates strong generalization across contract versions and significantly reduces runtime. We evaluate FIRE on multiple datasets and show its superior performance compared to existing techniques, highlighting its potential for efficient smart contract bytecode analysis.},
booktitle = {Proceedings of the 16th International Conference on Internetware},
pages = {378–388},
numpages = {11},
keywords = {Smart Contract, Reverse Engineering, Function Identification, Machine Learning, Graph Neural Network},
location = {
},
series = {Internetware '25}
}

@article{10.1145/3715780,
author = {Zhu, Kangchen and Tian, Zhiliang and Wang, Shangwen and Chen, Weiguo and Dong, Zixuan and Leng, Mingyue and Mao, Xiaoguang},
title = {MiSum: Multi-modality Heterogeneous Code Graph Learning for Multi-intent Binary Code Summarization},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715780},
doi = {10.1145/3715780},
abstract = {The current landscape of binary code summarization predominantly focuses on generating a single summary, which limits its utility and understanding for reverse engineers. Existing approaches often fail to meet the diverse needs of users, such as providing detailed insights into usage patterns, implementation nuances, and design rationale, as observed in the field of source code summarization. This highlights the need for multi-intent binary code summarization to enhance the effectiveness of reverse engineering processes. To address this gap, we propose MiSum, a novel method that leverages multi-modality heterogeneous code graph alignment and learning to integrate both assembly code and pseudo-code. MiSum introduces a unified multi-modality heterogeneous code graph (MM-HCG) that aligns assembly code graphs with pseudo-code graphs, capturing both low-level execution details and high-level structural information. We further propose multi-modality heterogeneous graph learning with heterogeneous mutual attention and message passing, which highlights important code blocks and discovers inter-dependencies across different code forms. Additionally, an intent-aware summary generator with an intent-aware attention mechanism is introduced to produce customized summaries tailored to multiple intents. Extensive experiments, including evaluations across various architectures and optimization levels, demonstrate that MiSum outperforms state-of-the-art baselines in BLEU, METEOR, and ROUGE-L metrics. Human evaluations validate its capability to effectively support reverse engineers in understanding diverse binary code intents, marking a significant advancement in binary code analysis.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE061},
numpages = {24},
keywords = {Binary code understanding, Large language models, Multi-intent code summarization, Multi-modality fusion, Reverse engineering}
}

@inproceedings{10.1145/3756681.3756954,
author = {Jin, Hangzhan and Hamdaqa, Mohammad},
title = {CCCI: Code Completion with Contextual Information for Complex Data Transfer Tasks Using Large Language Models},
year = {2025},
isbn = {9798400713859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3756681.3756954},
doi = {10.1145/3756681.3756954},
abstract = {Unlike code generation, which involves creating code from scratch, code completion focuses on integrating new lines or blocks of code into an existing codebase. This process requires a deep understanding of the surrounding context, such as variable scope, object models, API calls, and database relations, to produce accurate results. These complex contextual dependencies make code completion a particularly challenging problem. Current models and approaches often fail to effectively incorporate such context, leading to inaccurate completions with low acceptance rates (around 30%). For tasks like data transfer, which rely heavily on specific relationships and data structures, acceptance rates drop even further. This study introduces CCCI, a novel method for generating context-aware code completions specifically designed to address data transfer tasks. By integrating contextual information, such as database table relationships, object models, and library details into Large Language Models (LLMs), CCCI improves the accuracy of code completions. We evaluate CCCI using 289 Java snippets, extracted from over 819 operational scripts in an industrial setting. The results demonstrate that CCCI achieved a 49.1% Build Pass rate and a 41.0% CodeBLEU score, comparable to state-of-the-art methods that often struggle with complex task completion.},
booktitle = {Proceedings of the 29th International Conference on Evaluation and Assessment in Software Engineering},
pages = {126–135},
numpages = {10},
keywords = {Code Completion, Large Language Models (LLMs), Data Transfer, Contextual Information},
location = {
},
series = {EASE '25}
}

@inproceedings{10.1145/3732771.3742724,
author = {Santos, Gustavo and Bispo, Jo\~{a}o and Mendes, Alexandra},
title = {Detecting Resource Leaks on Android with Alpakka},
year = {2025},
isbn = {9798400718847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732771.3742724},
doi = {10.1145/3732771.3742724},
abstract = {Mobile devices have become integral to our everyday lives, yet their utility hinges on their battery life. In Android apps, resource leaks caused by inefficient resource management are a significant contributor to battery drain and poor user experience. Our work introduces Alpakka, a source-to-source compiler for Android's Smali syntax. To showcase Alpakka's capabilities, we developed an Alpakka library capable of detecting and automatically correcting resource leaks in Android APK files. We demonstrate Alpakka's effectiveness through empirical testing on 124 APK files from 31 real-world Android apps in the DroidLeaks [12] dataset. In our analysis, Alpakka identified 93 unique resource leaks, of which we estimate 15% are false positives. From these, we successfully applied automatic corrections to 45 of the detected resource leaks.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {199–211},
numpages = {13},
keywords = {Android, Decompiler, Energy Efficiency, Green Software, Mobile Applications, Resource Leaks, Smali},
location = {Koblenz, Germany},
series = {SLE '25}
}

@inproceedings{10.1145/3732771.3742714,
author = {Ferreira, Jos\'{e} Pedro and Bispo, Jo\~{a}o and Lima, Susana},
title = {TranspileJS, an Intelligent Framework for Transpiling JavaScript to WebAssembly},
year = {2025},
isbn = {9798400718847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732771.3742714},
doi = {10.1145/3732771.3742714},
abstract = {WebAssembly (Wasm) has emerged as a powerful binary format, enabling the seamless integration of languages like C and Rust into web applications. JavaScript (JS), the dominant language for client-side web development, has its code susceptible to tampering and intellectual property theft due to its transparency in browser environments. We introduce TranspileJS, a novel tool designed to enhance code security by automatically selecting and translating JS snippets into Wasm. TranspileJS leverages a multi-stage architecture that converts JS to TypeScript, which is compiled into Wasm using the AssemblyScript compiler. TranspileJS addresses the challenges posed by the fundamental differences between JS and Wasm, including dynamic typing, runtime behaviour mismatches, and standard library discrepancies, ensuring that the original behaviour of the code is preserved while maximising the amount of code transpiled. Our experiments show that TranspileJS successfully transpiles approximately one-third of the code in our dataset, with a performance impact of up to a 12.3% increase in execution time. The transpilation process inherently obfuscates code, creating effects similar to standard obfuscation techniques, and generates a stealthy and resilient output. Furthermore, combining transpilation with WebAssembly-specific obfuscation techniques opens new possibilities for code protection and resistance against reverse engineering.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {71–83},
numpages = {13},
keywords = {AssemblyScript, Compilation, Cybersecurity, JavaScript, Obfuscation, Web Privacy, WebAssembly},
location = {Koblenz, Germany},
series = {SLE '25}
}

@inproceedings{10.1145/3713081.3731745,
author = {He, Yiling and She, Hongyu and Qian, Xingzhi and Zheng, Xinran and Chen, Zhuo and Qin, Zhan and Cavallaro, Lorenzo},
title = {On Benchmarking Code LLMs for Android Malware Analysis},
year = {2025},
isbn = {9798400714740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713081.3731745},
doi = {10.1145/3713081.3731745},
abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names.This paper presents Cama, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. Cama specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics—consistency, fidelity, and semantic relevance—enabling rigorous stability and effectiveness assessment and cross-model comparison.We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use Cama to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.},
booktitle = {Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–160},
numpages = {8},
keywords = {code LLM, malware analysis},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {ISSTA Companion '25}
}

@inproceedings{10.1145/3713081.3731728,
author = {Zhou, Li and Dacier, Marc and Konstantinou, Charalambos},
title = {ReGraph: A Tool for Binary Similarity Identification},
year = {2025},
isbn = {9798400714740},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713081.3731728},
doi = {10.1145/3713081.3731728},
abstract = {Binary Code Similarity Detection (BCSD) is not only essential for security tasks such as vulnerability identification but also for code copying detection, yet it remains challenging due to binary stripping and diverse compilation environments. Existing methods tend to adopt increasingly complex neural networks for better accuracy performance. The computation time increases with the complexity. Even with powerful GPUs, the treatment of large-scale software becomes time-consuming. To address these issues, we present a framework called ReGraph to efficiently compare binary code functions across architectures and optimization levels. Our evaluation with public datasets highlights that ReGraph exhibits a significant speed advantage, performing 700 times faster than Natural Language Processing (NLP)-based methods while maintaining comparable accuracy results with respect to the state-of-the-art models.},
booktitle = {Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {6–10},
numpages = {5},
keywords = {binary code similarity detection, code property graph, graph neural network, code lifting, binary code re-optimization},
location = {Clarion Hotel Trondheim, Trondheim, Norway},
series = {ISSTA Companion '25}
}

@inproceedings{10.1145/3722572.3727926,
author = {Mudraje, Ishwar and Vogelgesang, Kai and Devreker, Jasper and Gerhorst, Luis and Raffeck, Phillip and W\"{a}gemann, Peter and Herfet, Thorsten},
title = {Reverse Engineering the ESP32-C3 Wi-Fi Drivers for Static Worst-Case Analysis of Intermittently-Powered Systems},
year = {2025},
isbn = {9798400716065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722572.3727926},
doi = {10.1145/3722572.3727926},
abstract = {The Internet of Batteryless Things revolutionizes sustainable communication as it operates on harvested energy. This harvested energy is dependent on unpredictable environmental conditions; therefore, device operations, including those of its networking stack, must be resilient to power failures. Reactive intermittent computing provides an approach for solving this by notifications of impending power failures, which is implemented by monitoring the harvested energy buffered in a capacitor. However, to use this power-failure notification and guarantee forward progress, systems must break down tasks into atomic transactions that can be predictably finished before the energy runs out. Thus, static program-code analysis must determine the worst-case energy consumption (WCEC) of all transactions. In Wi-Fi--capable devices, drivers are often closed-source, which avoids the determination of WCEC bounds for transactions since static analysis requires all code along with its semantics.In this work, we integrate an energy-aware networking stack with reverse-engineered Wi-Fi drivers to enable full-stack WCEC analysis for physical transmission and reception of packets. Further, we extended a static worst-case analysis tool with a resource-consumption model of our Wi-Fi driver. Our evaluation with the RISC-V--based ESP32-C3 platform gives worst-case bounds with our static analysis approach for the transactions of the full communication stack, therefore showing that Wi-Fi--based reactive intermittent computing is feasible.},
booktitle = {Proceedings of the 13th International Workshop on Energy Harvesting and Energy-Neutral Sensing Systems},
pages = {1–7},
numpages = {7},
keywords = {IoT, Wi-Fi, batteryless systems, intermittently-powered devices, reverse engineering, static analysis, worst-case energy consumption},
location = {Irvine, CA, USA},
series = {ENSsys '25}
}

@article{10.1145/3702977,
author = {Liu, Yonghui and Chen, Xiao and Liu, Pei and Samhi, Jordan and Grundy, John and Chen, Chunyang and Li, Li},
title = {Demystifying React Native Android Apps for Static Analysis},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3702977},
doi = {10.1145/3702977},
abstract = {React Native, an open source framework, simplifies cross-platform app development by allowing JavaScript-side code to interact with native-side code. Previous studies disregarded React Native, resulting in insufficient static analysis of React Native app code. This study initiates the investigation of challenges when statically analyzing React Native apps. We propose ReuNify to improve Soot-based static analysis coverage for JavaScript-side and native-side code. ReuNify converts Hermes bytecode to Soot’s intermediate representation. Hermes bytecode, compiled from JavaScript code and integrated into React Native apps, possesses a unique syntax that eludes current JavaScript analyzers. Additionally, we investigate opcode distribution and conduct in-depth analyses of the usage of opcode between popular apps and malware. We also propose a benchmark consisting of 97 control flow-related cases to validate the control flow recovery of the generated intermediate representation. Furthermore, we model the cross-language communication mechanisms of React Native to expand the static analysis coverage for native-side code. Our evaluation demonstrates that ReuNify enables an average increase of 84% in reached nodes within the callgraph and further identifies an average of two additional privacy leaks in taint analysis. In summary, this article demonstrates that ReuNify significantly improves the static analysis for the React Native Android apps.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {107},
numpages = {33},
keywords = {Android, React Native, Mobile App, Static Analysis}
}

@inproceedings{10.1109/ICSE55347.2025.00231,
author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
title = {Formally Verified Binary-Level Pointer Analysis},
year = {2025},
isbn = {9798331505691},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE55347.2025.00231},
doi = {10.1109/ICSE55347.2025.00231},
abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the tradeoff between scalability and precision, such customization allows "meaningful" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
booktitle = {Proceedings of the IEEE/ACM 47th International Conference on Software Engineering},
pages = {42–53},
numpages = {12},
keywords = {binary analysis, pointer analysis, formal methods},
location = {Ottawa, Ontario, Canada},
series = {ICSE '25}
}

@inproceedings{10.1145/3723498.3723739,
author = {Berge, PB},
title = {Anti-Games, Fantasy Consoles, and the Rise of Speculative Game Development on Itch.io},
year = {2025},
isbn = {9798400718564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723498.3723739},
doi = {10.1145/3723498.3723739},
abstract = {Amid the ongoing escalation of anti-worker and anti-player practices across the game industry, the creative practices of developers are shifting: jammers, students, hobbyists, fangamers, and other digital game “zinesters” [3] seek to reclaim game design as a personal art form. This paper examines the emergence and rise of speculative game design communities on itch.io—sites of ongoing creative resistance to corporate encroachment and exclusionary game culture. Specifically, the study documents the activities of “anti-game” jammers and fantasy console development communities on itch.io—two groups that create speculative games (that is, projects that are conceptual, arbitrary, and materially or procedurally unrealizable) to make space for personal play that is divested from mainstream engines, markets, and pretenses of game commodity: 1) Firstly, “anti-game” designers use game jams, zines, and Discord servers to develop and circulate not-games, games that refuse play, and games that cannot be played. These makers create provocative thought experiments, poems, manifestos, and other “unplayable” works that resist hyper-capitalist investment in bigger, longer, more graphically precise, live-service, metaversal, forever-games. 2) Simultaneously, developers on itch build projects for fantasy consoles—such as PICO-8 and Bitsy—and imagine hardware that is mutually-constituted, rooted in community, and decommodified. Ultimately, this paper argues that speculative design practices divest from concerns of “playability,” center unencumbered participation, and allow developers to envision and participate in a post-capitalist imaginary of hardware, software, and play.},
booktitle = {Proceedings of the 20th International Conference on the Foundations of Digital Games},
articleno = {36},
numpages = {12},
keywords = {Speculative game design, anti-games, experimental game design, fantasy consoles, itch.io, unplayable games},
location = {
},
series = {FDG '25}
}

@inbook{10.1145/3672608.3707995,
author = {Brossard, Jonathan},
title = {Automatic Functions Annotations through Concrete Procedural Debugging and ELF Libification},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707995},
abstract = {In this article, we present a novel approach to program analysis through selective concrete execution. While static analysis of ELF binaries is necessarily limited by the theoretical undecidability of control-flow and data-flow analysis algorithms, we detail a new approach to reverse engineering through selective concrete execution of arbitrary functions within a x86_64 GNU/Linux binary by transforming ELF applications into shared libraries. This approach, named "procedural debugging", allows us to empirically recover information about function parameters and return values without resorting to any disassembly or decompilation, which are undecidable in general. In turn, this dynamic approach may be used as a feedback loop into existing program analyzers, being them static, fuzzing, symbolic, or concolic, to enrich their understanding of application interfaces. We publish an open-source framework, named the Witchcraft Compiler Collection, under a permissive MIT/BSD license, implementing binary libification, procedural debugging, and automatic function prototype annotations with the hope of benefiting the security community.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1894–1898},
numpages = {5}
}

@inproceedings{10.1145/3732365.3732428,
author = {Yan, Huaishuo and Cui, Baojiang},
title = {UEFUZZER: Enabling Struct-Aware Fuzzing on UEFI with Static Analysis},
year = {2025},
isbn = {9798400713613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732365.3732428},
doi = {10.1145/3732365.3732428},
abstract = {Since its extensive implementation in 2006, the Unified Extensible Firmware Interface (UEFI) has supplanted traditional BIOS as the industry standard, serving as an essential link between computer hardware and operating systems. UEFI's advantageous role in system design provides it with comprehensive access to system resources, beyond those of the operating system kernel. Consequently, detecting and thoroughly characterizing memory corruption vulnerabilities in UEFI firmware is essential for preserving the integrity and security of computer systems. The techniques for finding UEFI firmware vulnerabilities nowadays mostly face two main difficulties: First of all, the special character of UEFI firmware makes direct dynamic examination inside the operating system especially difficult; Second, conventional fuzzing techniques lead to ineffective testing with their shallow knowledge of UEFI input structures. We propose a novel approach combining static analysis with fuzzing techniques to help to reduce these limits. Our approach starts with static reverse engineering to fully understand the structural characteristics of inputs throughout several UEFI interfaces, followed by cross-validation against open-source firmware implementations. We then use this structured knowledge to guide the seed file mutation technique, hence improving the accuracy and efficiency of fuzzing activities.},
booktitle = {Proceedings of the 2025 5th International Conference on Computer Network Security and Software Engineering},
pages = {350–355},
numpages = {6},
keywords = {Fuzzing, Intelligent Mutation, Security Analysis, UEFI Firmware},
location = {
},
series = {CNSSE '25}
}

@inproceedings{10.1145/3726101.3726103,
author = {M S, Abhijith and Druva, K R and K, Harish and D, Geetha and Rapate, Gauri Sameer},
title = {Detection of Mobile Malware (ANDROID) using ML and Hybrid Analysis},
year = {2025},
isbn = {9798400707285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726101.3726103},
doi = {10.1145/3726101.3726103},
abstract = {In recent times, malware has increasingly become a significant concern in information and technology security, as evidenced by the substantial rise in attacks on various devices, including computers, the internet, and mobile devices. Detecting zero-day malware has become a primary focus for security researchers. Given that Google’s Android is one of the most widely used mobile operating systems, attackers have shifted their attention to creating malware specifically targeting Android. Numerous security researchers have employed various machine learning algorithms to detect these new Android and other types of malware. In this paper, we propose a system which employs a hybrid approach of analysis to better detect malware.The proposed model relies on a two-way approach in order to detect malwares in android application. This two-way approach first involves running the target APK through a static analytical model, If the result from the first stage comes out to be malicious, then the APK does not go to the second stage for further analysis. But, if the result from the first stage comes out to be benign, it is sent for further analysis by involving hybrid analysis (static + dynamic approach). The results given by the second stage will be ultimately determining whether the APK given is malicious or benign. The source code is available at: https://github.com/Hurry-sh/CCNCS-Project},
booktitle = {Proceedings of the 2025 7th Asia Pacific Information Technology Conference},
pages = {10–15},
numpages = {6},
keywords = {Malware Detection, Network Security, Machine Learning, Static Analysis, Dynamic Analysis},
location = {
},
series = {APIT '25}
}

@article{10.1145/3704857,
author = {Zhang, Cheng and Kapp\'{e}, Tobias and Narv\'{a}ez, David E. and Naus, Nico},
title = {CF-GKAT: Efficient Validation of Control-Flow Transformations},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {POPL},
url = {https://doi.org/10.1145/3704857},
doi = {10.1145/3704857},
abstract = {Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT’s abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren’s goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {21},
numpages = {27},
keywords = {Kleene algebra, Program equivalence, control flow recovery}
}

@inproceedings{10.1145/3658644.3691386,
author = {Naus, Nico and Verbeek, Freek and Atla, Sagar and Ravindran, Binoy},
title = {Poster: Formally Verified Binary Lifting to P-Code},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3691386},
doi = {10.1145/3658644.3691386},
abstract = {Analysis of binary software plays a critical role in software security. Reverse engineers analyze binaries to discover vulnerabilities, patch legacy software, and detect malware. Most of the reverse engineering tools have been developed from a practical point of view, and do not provide any guarantees with their results. Recently, formally verified reverse engineering and decompilation have gained traction. These formal tools are for the most part proof-of-concept systems not yet suitable for real-world reverse-engineering tasks. In this poster, we explore the idea of formalizing part of an existing decompilation tool instead. We focus on the lifting from assembly to the IR P-Code in one of the most popular decompilers, Ghidra. This step occurs immediately after disassembly. We are developing a proof system inside the Isabelle theorem prover, to automatically prove semantical equivalence between the assembly and P-Code instructions. We leverage machine-learned x86-64 semantics, to stay as close as possible to actual CPU behavior. This approach has uncovered several shortcomings in Ghidra's P-Code and the lifting it performs. By using a theorem prover, we obtain guarantees that our system of formal semantics and lifting is internally consistent. This work brings the powerful guarantees that formal methods provide in reverse engineering research to the real world.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {4973–4975},
numpages = {3},
keywords = {binary lifting, decompilation, formal verification, proof automation, reverse engineering, theorem provers},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3658644.3690244,
author = {Verbeek, Freek and Naus, Nico and Ravindran, Binoy},
title = {Verifiably Correct Lifting of Position-Independent x86-64 Binaries to Symbolized Assembly},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690244},
doi = {10.1145/3658644.3690244},
abstract = {We present an approach to lift position-independent x86-64 binaries to symbolized NASM. Symbolization is a decompilation step that enables binary patching: functions can be modified, and instructions can be interspersed. Moreover, it is the first abstraction step in a larger decompilation chain. The produced NASM is recompilable, and we extensively test the recompiled binaries to see if they exhibit the same behavior as the original ones. In addition to testing, the produced NASM is accompanied with a certificate, constructed in such a way that if all theorems in the certificate hold, symbolization has occurred correctly. The original and recompiled binary are lifted again with a third-party decompiler (Ghidra). These representations, as well as the certificate, are loaded into the Isabelle/HOL theorem prover, where proof scripts ensure that correctness can be proven automatically. We have applied symbolization to various stripped binaries from various sources, from various compilers, and ranging over various optimization levels. We show how symbolization enables binary-level patching, by tackling challenges originating from industry.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2786–2798},
numpages = {13},
keywords = {binary analysis, disassembly, formal methods},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3658644.3670340,
author = {Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu},
title = {ReSym: Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670340},
doi = {10.1145/3658644.3670340},
abstract = {Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, e.g., recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the token limitations inherent in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {4554–4568},
numpages = {15},
keywords = {large language models, program analysis, reverse engineering},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@article{10.1145/3699674,
author = {Udeshi, Meet and Krishnamurthy, Prashanth and Pearce, Hammond and Karri, Ramesh and Khorrami, Farshad},
title = {REMaQE: Reverse Engineering Math Equations from Executables},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3699674},
doi = {10.1145/3699674},
abstract = {Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This article proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer equations from object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool “tmon.” To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = nov,
articleno = {43},
numpages = {25},
keywords = {binary reverse engineering, embedded systems, symbolic execution, mathematical equations}
}

@inproceedings{10.1145/3686215.3690147,
author = {Yong Wong, Miuyin and Valakuzhy, Kevin and Ahamad, Mustaque and Blough, Doug and Monrose, Fabian},
title = {Understanding LLMs Ability to Aid Malware Analysts in Bypassing Evasion Techniques},
year = {2024},
isbn = {9798400704635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686215.3690147},
doi = {10.1145/3686215.3690147},
abstract = {Over the past few years, the threat of malware has become increasingly evident, posing a significant risk to cybersecurity worldwide and driving extensive research efforts to prevent and mitigate these attacks. Despite numerous efforts to automate malware analysis, these systems are constantly thwarted by evasive techniques developed by malware authors. As a result, the analysis of sophisticated evasive malware falls into the hands of human malware analysts, who must undertake the time-consuming process of overcoming each evasive technique to uncover malware’s malicious behaviors. This highlights the need for approaches that aid malware analysts in this process. Although active measures, such as forced execution and symbolic analysis, can automatically circumvent some evasive checks, they suffer from limitations like path explosion and fail to provide useful insights that analysts can use in their workflow. To fill this gap, we investigate how large language models (LLMs) can address shortcomings of symbolic analysis through the first comparative analysis between the two in bypassing evasion techniques. Our study leads to three key findings: (i) we find that LLMs outperform symbolic analysis in bypassing evasive code, especially in the presence of common code patterns, such as loops, which have historically posed a challenge for symbolic analysis, (ii) we show that LLMs correctly identify methods of bypassing evasive techniques in real-world malware, and (iii) we highlight how even in LLMs failure modes, human malware analysts can benefit from the step-by-step reasoning provided by the model.},
booktitle = {Companion Proceedings of the 26th International Conference on Multimodal Interaction},
pages = {36–40},
numpages = {5},
keywords = {Large Language Model, Malware Analysis, Symbolic Analysis},
location = {San Jose, Costa Rica},
series = {ICMI '24 Companion}
}

@inproceedings{10.1145/3646547.3688433,
author = {Yang, Han and Kuzniar, Carson and Jiang, Chengyan and Nikolaidis, Ioanis and Haque, Israat},
title = {Characterizing the Security Facets of IoT Device Setup},
year = {2024},
isbn = {9798400705922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646547.3688433},
doi = {10.1145/3646547.3688433},
abstract = {In this work, we characterize the potential information leakage from IoT platforms during their setup phase. Setup involves an IoT device, its ''app'', and a cloud-based service. We assume that the on-device firmware is inaccessible, e.g., read-protected. We focus on the combination of information that can be extracted from analyzing the app and the local communication between the app and the IoT device. An attacker can trivially obtain the app, analyze its operation, and potentially eavesdrop on the wireless communication occurring during the setup phase. We develop a semi-automated general methodology involving off-the-shelf tools to examine information disclosure during the setup phase. We tested our methodology on twenty commodity-grade IoT devices. The outcome reveals a wide range of device-dependent choices for encryption at various layers and the potential for exposure of, among other things, device-identifying information and local networking (WiFi) credentials. Our methodology contributes towards a means to assess and ''certify'' IoT devices.},
booktitle = {Proceedings of the 2024 ACM on Internet Measurement Conference},
pages = {612–621},
numpages = {10},
keywords = {information leakage, iot, setup security, smart home},
location = {Madrid, Spain},
series = {IMC '24}
}

@inproceedings{10.1145/3691620.3695502,
author = {Song, Zirui and Zhou, YuTong and Dong, Shuaike and Zhang, Ke and Zhang, Kehuan},
title = {TypeFSL: Type Prediction from Binaries via Inter-procedural Data-flow Analysis and Few-shot Learning},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695502},
doi = {10.1145/3691620.3695502},
abstract = {Type recovery in stripped binaries is a critical and challenging task in reverse engineering, as it is the basis for many security applications (e.g., vulnerability detection). Traditional analysis methods are limited by software complexity and emerging types in real-world projects. To address these limitations, machine learning methods have been explored. However, the existing supervised learning approaches struggle with analyzing complicated and uncommon types due to the limited availability of samples. Additionally, none of the existing works can capture fine-grained and inter-procedural features in the binaries. In this paper, we present TypeFSL, a framework that addresses the challenge of imbalanced type distributions by incorporating few-shot learning and captures inter-procedural semantics through program slicing. Moreover, based on a dataset with 3,003,117 functions, TypeFSL achieves an average of 77.9% and 84.6% accuracy across all architecture and optimizations in 20-way 5-shot and 10-shot classification tasks. Our prototype outperforms existing techniques in prediction accuracy and obfuscation resistance. Finally, the case studies demonstrate how TypeFSL predicts uncommon and complicated types in practical analysis.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1269–1281},
numpages = {13},
keywords = {reverse engineering, type recovery, few-shot learning},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1145/3691620.3695271,
author = {Zhao, Jian and Wang, Shenao and Zhao, Yanjie and Hou, Xinyi and Wang, Kailong and Gao, Peiming and Zhang, Yuanchao and Wei, Chen and Wang, Haoyu},
title = {Models Are Codes: Towards Measuring Malicious Code Poisoning Attacks on Pre-trained Model Hubs},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695271},
doi = {10.1145/3691620.3695271},
abstract = {The proliferation of pre-trained models (PTMs) and datasets has led to the emergence of centralized model hubs like Hugging Face, which facilitate collaborative development and reuse. However, recent security reports have uncovered vulnerabilities and instances of malicious attacks within these platforms, highlighting growing security concerns. This paper presents the first systematic study of malicious code poisoning attacks on pre-trained model hubs, focusing on the Hugging Face platform. We conduct a comprehensive threat analysis, develop a taxonomy of model formats, and perform root cause analysis of vulnerable formats. While existing tools like Fickling and ModelScan offer some protection, they face limitations in semantic-level analysis and comprehensive threat detection. To address these challenges, we propose MalHug, an end-to-end pipeline tailored for Hugging Face that combines dataset loading script extraction, model deserialization, in-depth taint analysis, and heuristic pattern matching to detect and classify malicious code poisoning attacks in datasets and models. In collaboration with Ant Group, a leading financial technology company, we have implemented and deployed MalHug on a mirrored Hugging Face instance within their infrastructure, where it has been operational for over three months. During this period, MalHug has monitored more than 705K models and 176K datasets, uncovering 91 malicious models and 9 malicious dataset loading scripts. These findings reveal a range of security threats, including reverse shell, browser credential theft, and system reconnaissance. This work not only bridges a critical gap in understanding the security of the PTM supply chain but also provides a practical, industry-tested solution for enhancing the security of pre-trained model hubs.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2087–2098},
numpages = {12},
keywords = {pre-trained model hub, code poisoning attacks, LLM supply chain},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3689711,
author = {Li, Yichuan and Song, Wei and Huang, Jeff},
title = {VarLifter: Recovering Variables and Types from Bytecode of Solidity Smart Contracts},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689711},
doi = {10.1145/3689711},
abstract = {Since funds or tokens in smart contracts are maintained through specific state variables, contract audit, an effective means for security assurance, particularly focuses on these variables and their related operations. However, the absence of publicly accessible source code for numerous contracts, with only bytecode exposed, hinders audit efforts. Recovering variables and their types from Solidity bytecode is thus a critical task in smart contract analysis and audit, yet this is a challenging task because the bytecode loses variable and type information, only with low-level data operated by stack manipulations and untyped memory/storage accesses. The state-of-the-art smart contract decompilers miss identifying many variables and incorrectly infer the types for many identified variables. To this end, we propose VarLifter, a lifter dedicated to the precise and efficient recovery of typed variables. VarLifter interprets every read or written field of a data region as at least one potential variable, and after discarding falsely identified variables, it progressively refines the variable types based on the variable behaviors in the form of operation sequences. We evaluate VarLifter on 34,832 real-world Solidity smart contracts. VarLifter attains a precision of 97.48% and a recall of 91.84% for typed variable recovery. Moreover, VarLifter finishes analyzing 77% of smart contracts in around 10 seconds per contract. If VarLifter is used to replace the variable recovery modules of the two state-of-the-art Solidity bytecode decompilers, 52.4%, and 74.6% more typed variables will be correctly recovered, respectively. The applications of VarLifter to contract decompilation, contract audit, and contract bytecode fuzzing illustrate that the recovered variable information improves many contract analysis tasks.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {271},
numpages = {29},
keywords = {Blockchain, EVM, Solidity bytecode, smart contract, variable recovery}
}

@inproceedings{10.1145/3678890.3678892,
author = {Botacin, Marcus},
title = {What do malware analysts want from academia? A survey on the state-of-the-practice to guide research developments},
year = {2024},
isbn = {9798400709593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678890.3678892},
doi = {10.1145/3678890.3678892},
abstract = {Malware analysis tasks are as fundamental for modern cybersecurity as they are challenging to perform. More than depending on any tool capability, malware analysis tasks depend on human analysts’ abilities, experiences, and practices when using the tools. Academic research has traditionally been focused on producing solutions to overcome malware analysis technical challenges, but are these solutions adopted in practice by malware analysts? Are these solutions useful? If not, how can the academic community improve its practices to foster adoption and cause a greater impact? To answer these questions, we surveyed 21 professional malware analysts working in different companies, from CSIRTs to AV companies, to hear their opinions about existing tools, practices, and the challenges they face in their daily tasks. In 31 questions, we cover a broad range of aspects, from the number of observed malware variants to the use of public sandboxes and the tools the analysts would like to exist to make their lives easier. We aim to bridge the gap between academic developments and malware practices. To do so, on the one hand, we suggest to the analysts the solutions proposed in the literature that could be integrated into their practices. On the other hand, we also point out to the academic community possible future directions to bridge existing development gaps that significantly affect malware analysis practices.},
booktitle = {Proceedings of the 27th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {77–96},
numpages = {20},
keywords = {Analysis Tools, Malware, Malware Analysis, Reverse Engineering},
location = {Padua, Italy},
series = {RAID '24}
}

@inproceedings{10.1145/3698062.3698068,
author = {Fu, Hao and Wu, Shaofei},
title = {An Android Malware Detection Method Based on Native Code and LSTM},
year = {2024},
isbn = {9798400717086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698062.3698068},
doi = {10.1145/3698062.3698068},
abstract = {The ubiquity of mobile Internet has made smartphones an essential component of modern life, yet they remain prime targets for information security threats. Despite a multitude of malware detection systems, skilled developers of malicious software adeptly conceal harmful code within applications, thus eluding standard detection techniques. A significant challenge in Android application reverse engineering is the extraction of critical code from ".so" files, known as native code, which is often viewed as a secure and clandestine strategy in Android development. Addressing this challenge, our research introduces a novel Android malware detection approach that leverages Long Short-Term Memory (LSTM) networks to scrutinize native code opcodes. By decompressing Android Application Packages (APKs) and extracting opcodes from shared libraries, we utilize the tf-idf algorithm for feature selection, facilitating the detection of obscured malicious code in a static analysis context. This methodology, bolstered by LSTM training, constructs a formidable framework for Android malware detection. Our empirical assessments confirm the method's effectiveness and superiority in detecting Android malware.},
booktitle = {Proceedings of the 2024 The 6th World Symposium on Software Engineering (WSSE)},
pages = {38–44},
numpages = {7},
keywords = {Android applications, long short-term memory (LSTM), malware detection, native code, term frequency-inverse document frequency (TF-IDF)},
location = {
},
series = {WSSE '24}
}

@inproceedings{10.1145/3650212.3680301,
author = {Nguyen, Huan and Priyadarshan, Soumyakant and Sekar, R.},
title = {Scalable, Sound, and Accurate Jump Table Analysis},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680301},
doi = {10.1145/3650212.3680301},
abstract = {Jump tables are a common source of indirect jumps in binary code. Resolving these indirect jumps is critical for constructing a complete control-flow graph, which is an essential first step for most applications involving binaries, including binary hardening and instrumentation, binary analysis and fuzzing for vulnerability discovery, malware analysis and reverse engineering. Existing techniques for jump table analysis generally prioritize performance over soundness. While lack of soundness may be acceptable for applications such as decompilation, it can cause unpredictable runtime failures in binary instrumentation applications. We therefore present SJA, a new jump table analysis technique in this paper that is sound and scalable. Our analysis uses a novel abstract domain to systematically track the "structure" of computed code pointers without relying on syntactic pattern-matching that is common in previous works. In addition, we present a bounds analysis that efficiently and losslessly reasons about equality and inequality relations that arise in the context of jump tables. As a result, our system reduces miss rate by 35\texttimes{} over the next best technique. When evaluated on error rate based on F1-score, our technique outperforms the best previous techniques by 3\texttimes{}.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {541–552},
numpages = {12},
keywords = {reverse engineering, static analysis},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3650212.3652133,
author = {Xiong, Hao and Dai, Qinming and Chang, Rui and Qiu, Mingran and Wang, Renxiang and Shen, Wenbo and Zhou, Yajin},
title = {Atlas: Automating Cross-Language Fuzzing on Android Closed-Source Libraries},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3652133},
doi = {10.1145/3650212.3652133},
abstract = {Fuzzing is an effective method for detecting security bugs in software, and there have been quite a few effective works on fuzzing Android. Researchers have developed methods for fuzzing open-source native APIs and Java interfaces on actual Android devices. However, the realm of automatically fuzzing Android closed-source native libraries, particularly on emulators, remains insufficiently explored. There are two key challenges: firstly, the multi-language programming model inherent to Android; and secondly, the absence of a Java runtime environment within the emulator.                 To address these challenges, we propose Atlas, a practical automated fuzz framework for Android closed-source native libraries. Atlas consists of an automatic harness generator and a fuzzer containing the necessary runtime environment. The generator uses static analysis techniques to deduce the correct calling sequences and parameters of the native API according to the information from the "native world" and the "Java world". To maximize the practicality of the generated harness, Atlas heuristically optimizes the generated harness. The Fuzzer provides the essential Java runtime environment in the emulator, making it possible to fuzz the Android closed-source native libraries on a multi-core server. We have tested Atlas on 17 pre-installed apps from four Android vendors. Atlas generates 820 harnesses containing 767 native APIs, of which 78% is practical. Meanwhile, Atlas has discovered 74 new security bugs with 16 CVEs assigned. The experiments show that Atlas can efficiently generate high-quality harnesses and find security bugs.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {350–362},
numpages = {13},
keywords = {Android, fuzzing, static analysis, vulnerability},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3678232.3678233,
author = {Fissore, Davide and Tassi, Enrico},
title = {Higher-Order unification for free!: Reusing the meta-language unification for the object language},
year = {2024},
isbn = {9798400709692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678232.3678233},
doi = {10.1145/3678232.3678233},
abstract = {Specifying and implementing a proof system from scratch requires significant effort. Logical Frameworks and Higher Order Logic Programming Languages provide dedicated, high-level meta languages to facilitate this task in two ways: 1) variable binding and substitution are for free when meta language binders represent object logic ones; 2) proof construction, and proof search, are greatly simplified by leveraging the unification procedure provided by the meta language. Notable examples of meta languages are Elf&nbsp;[21], Twelf&nbsp;[23], λ Prolog&nbsp;[16], Beluga&nbsp;[24], Abella&nbsp;[8] and Isabelle&nbsp;[31] which have been used to implement or specify many formal systems such as First Order Logic&nbsp;[5], Set Theory&nbsp;[20], Higher Order Logic&nbsp;[19], and the Calculus of Constructions&nbsp;[4]. The object logic we are interested in is Coq’s type theory&nbsp;[28]. We aim to develop a higher-order unification-based proof search procedure using the meta language Elpi&nbsp;[3], a dialect of λ Prolog. Elpi’s equational theory includes βη -equivalence and features a higher-order unification procedure ≃ m for the pattern fragment&nbsp;[15]. Elpi offers an encoding of Coq terms that is suitable for meta programming&nbsp;[6, 9, 26, 27] but that restricts ≃ m to first-order unification problems only. We refer to this basic encoding as &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $mathcal {O}$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name="ppdp2024-1-inline1" type="svg"/&gt;&lt;/Formula&gt;. In this paper we translate unification problems in &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $mathcal {O}$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 2&lt;/AltText&gt;&lt;File name="ppdp2024-1-inline2" type="svg"/&gt;&lt;/Formula&gt; to an alternative encoding called &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $mathcal {M}$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 3&lt;/AltText&gt;&lt;File name="ppdp2024-1-inline3" type="svg"/&gt;&lt;/Formula&gt;, from which we derive ≃ o, the higher-order unification procedure of &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $mathcal {O}$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 4&lt;/AltText&gt;&lt;File name="ppdp2024-1-inline4" type="svg"/&gt;&lt;/Formula&gt;. ≃ o honours βη -equivalence for terms within the pattern fragment, and allows for the use of heuristics when the terms fall outside the pattern fragment. Moreover, as ≃ o delegates most of the work to ≃ m, it can be used to efficiently simulate a logic program in &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $mathcal {O}$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 5&lt;/AltText&gt;&lt;File name="ppdp2024-1-inline5" type="svg"/&gt;&lt;/Formula&gt; by taking advantage of unification-related optimizations of the meta language, such as clause indexing.},
booktitle = {Proceedings of the 26th International Symposium on Principles and Practice of Declarative Programming},
articleno = {3},
numpages = {13},
keywords = {Higher-Order Unification, Logic Programming, Meta-Programming},
location = {Milano, Italy},
series = {PPDP '24}
}

@inproceedings{10.1145/3677999.3678276,
author = {Krook, Robert and Hammersberg, Samuel},
title = {Welcome to the Parti(tioning) (Functional Pearl): Using Rewrite Rules and Specialisation to Partition Haskell Programs},
year = {2024},
isbn = {9798400711022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677999.3678276},
doi = {10.1145/3677999.3678276},
abstract = {Writing distributed applications is hard, as the programmer needs to describe the communication protocol between the different endpoints. If this is not done correctly, we can introduce bugs such as deadlocks and data races.  Tierless and choreographic programming models aim to make this easier by describing the interactions of every endpoint in a single compilation unit. When such a program is compiled, ideally, a single endpoint is projected and the code for the other endpoints is removed. This leads to smaller binaries with fewer dependencies, and is called program partitioning. In this pearl, we show how we can use rewrite rules and specialisation to get GHC to partition our Haskell programs (almost) for free, if they are written using the Haste App or HasChor framework.  As an example of why partitioning is useful, we show how an example application can be more easily built and deployed after being partitioned.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Haskell Symposium},
pages = {27–40},
numpages = {14},
keywords = {Choreographic Programming, Haskell, Program Partitioning, Rewrite Rules, Specialisation, Tierless Programming},
location = {Milan, Italy},
series = {Haskell 2024}
}

@inproceedings{10.1145/3700058.3700123,
author = {Liao, Xue},
title = {Smart contract vulnerability detection based on dynamic and static combination},
year = {2024},
isbn = {9798400710261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700058.3700123},
doi = {10.1145/3700058.3700123},
abstract = {In the field of blockchain technology, smart contracts play a core role, but programming oversights may cause serious security risks. This study provides a comprehensive review of the types of smart contract security vulnerabilities and the development of detection techniques. This article conducts a comprehensive review of the current various detection methods, including static and dynamic analysis, and proposes a combined dynamic and static detection method for integer overflow vulnerabilities at the solidity code level and timestamp vulnerabilities at the blockchain system layer. It also analyzes a The performance of a series of mainstream detection tools in terms of detection accuracy and efficiency is compared in depth, and their advantages and limitations are analyzed.},
booktitle = {Proceedings of the International Conference on Digital Economy, Blockchain and Artificial Intelligence},
pages = {412–416},
numpages = {5},
location = {
},
series = {DEBAI '24}
}

@inproceedings{10.1145/3664476.3664486,
author = {Bove, Davide},
title = {A Large-Scale Study on the Prevalence and Usage of TEE-based Features on Android},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664486},
doi = {10.1145/3664476.3664486},
abstract = {In the realm of mobile security, where OS-based protections have proven insufficient against robust attackers, Trusted Execution Environments (TEEs) have emerged as a hardware-based security technology. Despite the industry’s persistence in advancing TEE technology, the impact on end users and developers remains largely unexplored. This study addresses this gap by conducting a large-scale analysis of TEE utilization in Android applications, focusing on the key areas of cryptography, digital rights management, biometric authentication, and secure dialogs. To facilitate our extensive analysis, we introduce Mobsec Analytika, a framework tailored for large-scale app examinations, which we make available to the research community. Through 333,475 popular Android apps, our analysis illuminates the implementation of TEE-related features and their contextual usage. Our findings reveal that TEE features are predominantly utilized indirectly through third-party libraries, with only 6.2% of apps directly invoking the APIs. Moreover, the study reveals the underutilization of the recent TEE-based UI feature Protected Confirmation.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {29},
numpages = {11},
keywords = {android, api, mobile security, tee, usage},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3663530.3665020,
author = {Dong, Liming and Lu, Qinghua and Zhu, Liming},
title = {A Pilot Study in Surveying Data Challenges of Automatic Software Engineering Tasks},
year = {2024},
isbn = {9798400706721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663530.3665020},
doi = {10.1145/3663530.3665020},
abstract = {The surge in automatic SE research aims to boost development efficiency and quality while reducing costs. However, challenges such as limited real-world project data and inadequate data conditions constrain the effectiveness of these methods. To systematically understand these challenges, our pilot study reviews prevalent data challenges across various SE tasks. Despite these challenges, thanks to the advances of large language model offers promising performance on SE tasks.   Overall, this pilot survey focused on provide a quick retrospective review on SE data challenges and introduce practical LLM solutions from the SE community to mitigate these challenges.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things},
pages = {6–11},
numpages = {6},
keywords = {Automatic Software Engineering, Data Challenge, LLM, Pilot Survey},
location = {Porto de Galinhas, Brazil},
series = {SEA4DQ 2024}
}

@inproceedings{10.1145/3652588.3663324,
author = {Brain, Martin and Malkawi, Mahdi},
title = {Misconceptions about Loops in C},
year = {2024},
isbn = {9798400706219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652588.3663324},
doi = {10.1145/3652588.3663324},
abstract = {Loop analysis is a key component of static analysis tools.         Unfortunately, there are several rare edge cases.         As a tool moves from academic prototype to production-ready,         obscure cases can and do occur.         This results in loop analysis being a key source of         late-discovered but significant algorithmic bugs.         To avoid these, this paper presents a collection of examples         and "folklore" challenges in loop analysis.},
booktitle = {Proceedings of the 13th ACM SIGPLAN International Workshop on the State Of the Art in Program Analysis},
pages = {60–66},
numpages = {7},
keywords = {Loop Analysis, Software Verification, Static Analysis},
location = {Copenhagen, Denmark},
series = {SOAP 2024}
}

@inproceedings{10.1145/3714393.3726486,
author = {Braconaro, Elisa and Losiouk, Eleonora},
title = {A Dataset for Evaluating LLMs Vulnerability Repair Performance in Android Applications: Data/Toolset paper},
year = {2025},
isbn = {9798400714764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3714393.3726486},
doi = {10.1145/3714393.3726486},
abstract = {Automated Program Repair (APR) is a well-established research area that enhances software reliability and security by automatically fixing bugs, reducing manual effort, and accelerating debugging. Despite progress in publishing benchmarks to evaluate APR tools, datasets specifically targeting Android are lacking.To address this gap, we introduce a dataset of 272 real-world violations of Google's Android Security Best Practices, identified by statically analyzing 113 real-world Android apps. In addition to the faulty code, we manually crafted repairs based on Google's guidelines, covering 176 Java-based and 96 XML-based violations from Android Java classes and Manifest files, respectively. Additionally, we leveraged our novel dataset to evaluate Large Language Models (LLMs) as they are the latest promising APR tools. In particular, we evaluated GPT-4o, Gemini 1.5 Flash and Gemini in Android Studio and we found that GPT-4o outperforms Google's models, demonstrating higher accuracy and robustness across a range of violations types. Hence, with this dataset, we aim to provide valuable insights for advancing APR research and improving tools for Android security.},
booktitle = {Proceedings of the Fifteenth ACM Conference on Data and Application Security and Privacy},
pages = {353–358},
numpages = {6},
keywords = {android vulnerabilities, automated program repair, large language models},
location = {Pittsburgh, PA, USA},
series = {CODASPY '25}
}

@article{10.1145/3631971,
author = {Pizzolotto, Davide and Berlato, Stefano and Ceccato, Mariano},
title = {Mitigating Debugger-based Attacks to Java Applications with Self-debugging},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631971},
doi = {10.1145/3631971},
abstract = {Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode.In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them.We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {107},
numpages = {38},
keywords = {Anti-debugging, maliciuos reverse engineering, tampering attacks, man at the end attacks}
}

@inproceedings{10.1145/3597503.3639153,
author = {Yang, Shuo and Chen, Jiachi and Huang, Mingyuan and Zheng, Zibin and Huang, Yuan},
title = {Uncover the Premeditated Attacks: Detecting Exploitable Reentrancy Vulnerabilities by Identifying Attacker Contracts},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639153},
doi = {10.1145/3597503.3639153},
abstract = {Reentrancy, a notorious vulnerability in smart contracts, has led to millions of dollars in financial loss. However, current smart contract vulnerability detection tools suffer from a high false positive rate in identifying contracts with reentrancy vulnerabilities. Moreover, only a small portion of the detected reentrant contracts can actually be exploited by hackers, making these tools less effective in securing the Ethereum ecosystem in practice.In this paper, we propose BlockWatchdog, a tool that focuses on detecting reentrancy vulnerabilities by identifying attacker contracts. These attacker contracts are deployed by hackers to exploit vulnerable contracts automatically. By focusing on attacker contracts, BlockWatchdog effectively detects truly exploitable reentrancy vulnerabilities by identifying reentrant call flow. Additionally, BlockWatchdog is capable of detecting new types of reentrancy vulnerabilities caused by poor designs when using ERC tokens or user-defined interfaces, which cannot be detected by current rule-based tools. We implement BlockWatchdog using cross-contract static dataflow techniques based on attack logic obtained from an empirical study that analyzes attacker contracts from 281 attack incidents. BlockWatchdog is evaluated on 421,889 Ethereum contract bytecodes and identifies 113 attacker contracts that target 159 victim contracts, leading to the theft of Ether and tokens valued at approximately 908.6 million USD. Notably, only 18 of the identified 159 victim contracts can be reported by current reentrancy detection tools.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {128},
numpages = {12},
keywords = {smart contract, dataflow analysis, reentrancy, attacker identification, ethereum},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3597503.3639140,
author = {Zhong, Zhijie and Zheng, Zibin and Dai, Hong-Ning and Xue, Qing and Chen, Junjia and Nan, Yuhong},
title = {PrettySmart: Detecting Permission Re-delegation Vulnerability for Token Behaviors in Smart Contracts},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639140},
doi = {10.1145/3597503.3639140},
abstract = {As an essential component in Ethereum and other blockchains, token assets have been interacted with by diverse smart contracts. Effective permission policies of smart contracts must prevent token assets from being manipulated by unauthorized adversaries. Recent efforts have studied the accessibility of privileged functions or state variables to unauthorized users. However, little attention is paid to how publicly accessible functions of smart contracts can be manipulated by adversaries to steal users' digital assets. This attack is mainly caused by the permission re-delegation (PRD) vulnerability. In this work, we propose PrettySmart, a bytecode-level Permission re-delegation vulnerability detector for Smart contracts. Our study begins with an empirical study on 0.43 million open-source smart contracts, revealing that five types of widely-used permission constraints dominate 98% of the studied contracts. Accordingly, we propose a mechanism to infer these permission constraints, as well as an algorithm to identify constraints that can be bypassed by unauthorized adversaries. Based on the identification of permission constraints, we propose to detect whether adversaries could manipulate the privileged token management functionalities of smart contracts. The experimental results on real-world datasets demonstrate the effectiveness of the proposed PrettySmart, which achieves the highest precision score and detects 118 new PRD vulnerabilities.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {168},
numpages = {12},
keywords = {smart contract, permission control, vulnerability detection},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3659211.3659272,
author = {Xiao, Yuxuan and Fei, Jinlong},
title = {Research on Improved OLLVM Based on Code Rearrangement Architecture},
year = {2024},
isbn = {9798400716669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659211.3659272},
doi = {10.1145/3659211.3659272},
abstract = {Code obfuscation increases the difficulty of reverse engineering software and devices, improves the security of software and devices, and also prevents governments, enterprises, and socially important groups from the loss of information leakage. In recent years, with the continuous rise of LLVM architecture, OLLVM obfuscation solves cross-platform code obfuscation while increasing the difficulty of reverse engineering. for OLLVM obfuscation, we propose an improvement scheme for OLLVM obfuscation, which improves the degree of obfuscation of OLLVM and increases the difficulty of reverse engineering of software and device source code. Firstly, we propose an obfuscation scheme for code rearrangement; secondly, for control flow obfuscation, we propose an improvement scheme for NOLLVM control flow obfuscation and add an enhancement obfuscation module to increase the obfuscation capability of OLLVM, and lastly, the obfuscation effect of the code rearrangement scheme and the enhancement effect of the control flow obfuscation improvement module are verified through experiments and verified to improve the software and device source codes' Security.},
booktitle = {Proceedings of the 2023 4th International Conference on Big Data Economy and Information Management},
pages = {350–358},
numpages = {9},
location = {Zhengzhou, China},
series = {BDEIM '23}
}

@inproceedings{10.1145/3611643.3617852,
author = {Spiess, Claudio},
title = {STraceBERT: Source Code Retrieval using Semantic Application Traces},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3617852},
doi = {10.1145/3611643.3617852},
abstract = {Software reverse engineering is an essential task in software engineering and security, but it can be a challenging process, especially for adversarial artifacts. To address this challenge, we present STraceBERT, a novel approach that utilizes a Java dynamic analysis tool to record calls to core Java libraries, and pretrain a BERT-style model on the recorded application traces for effective method source code retrieval from a candidate set. Our experiments demonstrate the effectiveness of STraceBERT in retrieving the source code compared to existing approaches. Our proposed approach offers a promising solution to the problem of code retrieval in software reverse engineering and opens up new avenues for further research in this area.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2207–2209},
numpages = {3},
keywords = {neural information retrieval, reverse engineering, tracing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3611643.3616343,
author = {Zhao, Kunsong and Li, Zihao and Li, Jianfeng and Ye, He and Luo, Xiapu and Chen, Ting},
title = {DeepInfer: Deep Type Inference from Smart Contract Bytecode},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616343},
doi = {10.1145/3611643.3616343},
abstract = {Smart contracts play an increasingly important role in Ethereum platform. It provides various functions implementing numerous services, whose bytecode runs on Ethereum Virtual Machine. To use services by invoking corresponding functions, the callers need to know the function signatures. Moreover, such signatures provide crucial information for many downstream applications, e.g., identifying smart contracts, fuzzing, detecting vulnerabilities, etc. However, it is challenging to infer function signatures from the bytecode due to a lack of type information. Existing work solving this problem depended heavily on limited databases or hard-coded heuristic patterns. However, these approaches are hard to be adapted to semantic differences in distinct languages and various compiler versions when developing smart contracts. In this paper, we propose a novel framework DeepInfer that first leverages deep learning techniques to automatically infer function signatures and returns. The novelties of DeepInfer are: 1) DeepInfer lifts the bytecode into the Intermediate Representation (IR) to preserve code semantics; 2) DeepInfer extracts the type-related knowledge (e.g., critical data flows, constant values, and control flow graphs) from the IR to recover function signatures and returns. We conduct experiments on Solidity and Vyper smart contracts and the results show that DeepInfer performs faster and more accurate than existing tools, while being immune to changes in different languages and various compiler versions.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {745–757},
numpages = {13},
keywords = {Deep Learning, Smart Contract, Type Inference},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3605762.3624432,
author = {Han, Yuyang and Ji, Xu and Wang, Zhiqiang and Zhang, Jianyi},
title = {Systematic Analysis of Security and Vulnerabilities in Miniapps},
year = {2023},
isbn = {9798400702587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605762.3624432},
doi = {10.1145/3605762.3624432},
abstract = {The past few years have witnessed a boom of miniapps, as lightweight applications, miniapps are of great importance in the mobile internet sector. Consequently, the security of miniapps can directly impact compromising the integrity of sensitive data, posing a potential threat to user privacy. However, after a thorough review of the various research efforts in miniapp security, we found that their actions in researching the safety of miniapp web interfaces are limited. This paper proposes a triad threat model focusing on users, servers and attackers to mitigate the security risk of miniapps. By following the principle of least privilege and the direction of permission consistency, we design a novel analysis framework for the security risk assessment of miniapps by this model. Then, we analyzed the correlation between the security risk assessment and the threat model associated with the miniapp. This analysis led to identifying potential scopes and categorisations with security risks. In the case study, we identify nine major categories of vulnerability issues, such as SQL injection, logical vulnerabilities and cross-site scripting. We also assessed a total of 50,628 security risk hazards and provide specific examples.},
booktitle = {Proceedings of the 2023 ACM Workshop on Secure and Trustworthy Superapps},
pages = {1–9},
numpages = {9},
keywords = {vulnerabilities, security risk, miniapps, least privilege},
location = {Copenhagen, Denmark},
series = {SaTS '23}
}

@inproceedings{10.1145/3605762.3624428,
author = {Tao, Junjie and Shi, Jifei and Fan, Ming and Wang, Yin and Liu, Junfeng and Liu, Ting},
title = {JSLibD: Reliable and Heuristic Detection of Third-party Libraries in Miniapps},
year = {2023},
isbn = {9798400702587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605762.3624428},
doi = {10.1145/3605762.3624428},
abstract = {Miniapps have become an indispensable part of people's lives. Meanwhile, the utilization of third-party libraries greatly streamlines, expedites, and enhances the development of miniapps. However, ensuring the security of these third-party libraries presents a challenge, as they may harbor security vulnerabilities, such as plaintext transmission. In this paper, we propose JSLibD, an automated extraction method for third-party libraries in miniapps. Unlike conventional extraction methods that heavily rely on prior knowledge, JSLibD introduces a heuristic prediction approach, comprising two integral components: a whitelist matching method to match the known libraries and a heuristic prediction method to extract the unknown libraries using function call relationships. The results demonstrate that JSLibD can efficiently match known libraries, and accurately predict unknown libraries, achieving an impressive precision rate of 85.9% and a high recall rate of 97.2%.},
booktitle = {Proceedings of the 2023 ACM Workshop on Secure and Trustworthy Superapps},
pages = {11–16},
numpages = {6},
keywords = {third-party library, mobile security, miniapp},
location = {Copenhagen, Denmark},
series = {SaTS '23}
}

@inproceedings{10.1145/3603273.3635055,
author = {Song, Jiahao and Li, Runzhi and Zhang, Zijiao},
title = {A Multi-modality Feature Fusion Method for Android Malware Detection},
year = {2024},
isbn = {9798400708268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603273.3635055},
doi = {10.1145/3603273.3635055},
abstract = {The high market share and open-source nature of the Android system led to a significant increase in the number of malicious Android applications. It poses a lot of threats for users, such as financial costs, privacy breaches, and remote control. It is more efficient to construct accurate models to detect Android malware. We propose a novel Android malware detection framework MGIDroid. It considers two modality feature representations at the same: the function call graph (FCG) and Dex bytecode image features of Android applications. First, we construct an FCG that describes the relations between function calls for an Android application. We use GraphSAGE with the SAGPool model to extract FCG features. Next, we convert Dalvik Executable files of Android applications to Dex bytecode image, Resnet model with Convolutional Block Attention Module (CBAM) is adopted to extract image features that represent the data section of an Android application. Then, we use soft attention to fuse two modalities features to finish classification. Lastly, extensive experiments were conducted to evaluate the effectiveness of our approach. The results show that our proposed method outperforms other methods and achieves a high f1-score of 98.60%.},
booktitle = {Proceedings of the 2023 International Conference on Advances in Artificial Intelligence and Applications},
pages = {380–384},
numpages = {5},
keywords = {Android malware, Multimodal Learning, deep learning, malware detection},
location = {Wuhan, China},
series = {AAIA '23}
}

@inproceedings{10.1145/3652628.3652786,
author = {Yan, Jinpei and Yang, Lu},
title = {Repackaged Android Apps Classification Based on Embedding Encoding and Decomposable Attention Neural Network},
year = {2024},
isbn = {9798400708831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652628.3652786},
doi = {10.1145/3652628.3652786},
abstract = {Repackaging is one of the potential threats to the Android ecosystem as it deprives app developers of their benefits. When it is obfuscated to avoid detection, making it harder to repack and classify. State-of-the-art Android repackaged app classification approaches heavily rely on manually extracting dependency features as fingerprints. In this paper, we present a novel Android app repackaging classification method that utilizes the proposed EDA model to learn the semantic and sequence context information from smali embedding. It is extracted automatically from decompiled android .smali files. We leverage the high-frequency attention of embedding encoding to quickly identify public library subsequences. And our model uses intra-sequence attention for encoding between opcode embeddings within each .smali file to capture richer embedding encoding vectors. We make experiments on 8,000 android app pairs downloaded by Androzoo dataset. The results demonstrate that our method achieves 87.53% accuracy for repackaged apps classification. It can effectively capture context between sequences, and exhibits better training and detection efficiency than time series-based deep neural network learning model.},
booktitle = {Proceedings of the 4th International Conference on Artificial Intelligence and Computer Engineering},
pages = {951–956},
numpages = {6},
location = {Dalian, China},
series = {ICAICE '23}
}

@inproceedings{10.1109/ASE56229.2023.00099,
author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
title = {HexT5: Unified Pre-Training for Stripped Binary Code Information Inference},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00099},
doi = {10.1109/ASE56229.2023.00099},
abstract = {Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pretraining objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {774–786},
numpages = {13},
keywords = {reverse engineering, deep learning, binary diffing, information inference, programming language model},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3650215.3650347,
author = {Xia, Bing and Yin, Jiabin and Ge, Yunxiang and Yang, Ruinan},
title = {A Binary Function Name Prediction Method Based on Variable Alignment and Translation Model},
year = {2024},
isbn = {9798400709449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650215.3650347},
doi = {10.1145/3650215.3650347},
abstract = {Binary function naming is a code analysis task that generates functional descriptions of functions, and its results can be applied in the fields of malicious code analysis, vulnerability causation analysis, and algorithm governance. Aiming at the shortcomings of the pseudocode abstract syntax tree being difficult to extract and the binary function naming scheme having low accuracy rate, a binary function naming prediction model A2N based on variable alignment and sequence translation model is proposed. First, A2N extracts the function variable features of binary files from debugging information and performs variable alignment with the pseudocode obtained from decompiling; then, it obtains the hierarchical structure of the binary functions and designs the node extraction rules to generate an abstract syntax tree AST for each function; then, extract the paths between the leaf nodes of the AST and serialize the tree structure to represent it; finally, with the help of the neural network translation model, establish a mapping between the AST and the binary function names to realize the prediction function. The experimental results show that compared with Dire, Nero and XFL models, the F1 value of A2N is improved by 84%, 44% and 14% on file-level isolation experiments respectively, and the F1 value reaches 80.94% on function-level isolation experiments.},
booktitle = {Proceedings of the 2023 4th International Conference on Machine Learning and Computer Application},
pages = {757–761},
numpages = {5},
location = {Hangzhou, China},
series = {ICMLCA '23}
}

@article{10.1145/3617686,
author = {Jin, Hai and Lei, Bo and Liu, Haikun and Liao, Xiaofei and Duan, Zhuohui and Ye, Chencheng and Zhang, Yu},
title = {A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3617686},
doi = {10.1145/3617686},
abstract = {Computing-in-Memory (CIM) architectures using Non-volatile Memories (NVMs) have emerged as a promising way to address the “memory wall” problem in traditional Von Neumann architectures. CIM accelerators can perform arithmetic or Boolean logic operations in NVMs by fully exploiting their high parallelism for bit-wise operations. These accelerators are often used in cooperation with general-purpose processors to speed up a wide variety of artificial neural network applications. In such a heterogeneous computing architecture, the legacy software should be redesigned and re-engineered to utilize new CIM accelerators. In this article, we propose a compilation tool to automatically migrate legacy programs to such heterogeneous architectures based on the low-level virtual machine (LLVM) compiler infrastructure. To accelerate some computations such as vector-matrix multiplication in CIM accelerators, we identify several typical computing patterns from LLVM intermediate representations, which are oblivious to high-level programming paradigms. Our compilation tool can modify accelerable LLVM IRs to offload them to CIM accelerators automatically, without re-engineering legacy software. Experimental results show that our compilation tool can translate many legacy programs to CIM-supported binary executables effectively, and improve application performance and energy efficiency by up to 51\texttimes{} and 309\texttimes{}, respectively, compared with general-purpose x86 processors.},
journal = {ACM Trans. Archit. Code Optim.},
month = oct,
articleno = {47},
numpages = {25},
keywords = {compilation, LLVM-IR, accelerator, ReRAM}
}

@inproceedings{10.1145/3623759.3624544,
author = {Pohjola, Johannes \r{A}man and Syeda, Hira Taqdees and Tanaka, Miki and Winter, Krishnan and Sau, Tsun Wang and Nott, Benjamin and Ung, Tiana Tsang and McLaughlin, Craig and Seassau, Remy and Myreen, Magnus O. and Norrish, Michael and Heiser, Gernot},
title = {Pancake: Verified Systems Programming Made Sweeter},
year = {2023},
isbn = {9798400704048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623759.3624544},
doi = {10.1145/3623759.3624544},
abstract = {We introduce Pancake, a new language for verifiable, low-level systems programming, especially device drivers. Pancake eschews complex type systems to make the language attractive to systems programmers, while at the same time aiming to ease the formal verification of code. We describe the design of the language and its verified compiler, and examine its usability, performance and current limitations through case studies of device drivers and related systems components for an seL4-based operating system.},
booktitle = {Proceedings of the 12th Workshop on Programming Languages and Operating Systems},
pages = {1–9},
numpages = {9},
location = {Koblenz, Germany},
series = {PLOS '23}
}

@inproceedings{10.1145/3617184.3630160,
author = {Wu, Qi and Wen, Shuo and Liu, Boliang},
title = {A Method for Identifying Encrypted Webshell Traffic},
year = {2023},
isbn = {9798400708800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617184.3630160},
doi = {10.1145/3617184.3630160},
abstract = {Abstract: With the widespread application of web service technologies, implanting Webshells on target servers has become one of the most common attack methods. To evade detection, many WebShells are encrypted, increasing the difficulty of Webshell detection. This paper presents a method for recognizing traffic from encrypted webshells, extracting and filtering encrypted request and response packets in traffic files, and identifying the encryptions and keys used in the Webshell files. Based on this method, this paper has implemented an encrypted traffic identification system, decrypting the ciphertext, keys, and passwords in malicious traffic. Experimental results show that the system can effectively identify a large variety of encrypted WebShells, ensuring concurrent stability while being functionally comprehensive.},
booktitle = {Proceedings of the 8th International Conference on Cyber Security and Information Engineering},
pages = {341–349},
numpages = {9},
keywords = {Godzilla encryption, Webshell, decrypt, traffic identification},
location = {Putrajaya, Malaysia},
series = {ICCSIE '23}
}

@inproceedings{10.5555/3615924.3615947,
author = {Shirani, Paria and Bhatt, Sagar and Hailane, Asmaa and Jourdan, Guy-Vincent},
title = {Towards Cross-Architecture Binary Code Vulnerability Detection},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Today’s Internet of Things (IoT) environments are heterogeneous as they are typically comprised of devices equipped with various CPU architectures and software platforms. Therefore, in defending IoT environments against security threats, the capability of crossarchitecture vulnerability detection is of paramount importance. In this paper, we propose BinX, a deep learning-based approach for code similarity detection in binaries that are obtained through different compilers and optimization levels for various architectures. Our research is guided by a key idea that involves leveraging the Ghidra decompiler to generate the decompiled C code and the high p-code intermediate representation and pre-train transformerbased model, specifically BERT and CodeBERT, to accurately generate semantic embeddings. These embeddings are then utilized as inputs to an RNN Siamese neural network, enhancing the learning process for code similarity detection. The effectiveness of our approach is demonstrated through several experiments and comparisons with existing methods. Our results showcase the potential of BinX in enabling cross-architecture vulnerability detection in cross-architecture cross-compiled binaries, contributing to the advancement of security in IoT environments.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {191–196},
numpages = {6},
keywords = {Binary code analysis, vulnerability detection, machine learning.},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3597926.3598124,
author = {Kong, Queping and Chen, Jiachi and Wang, Yanlin and Jiang, Zigui and Zheng, Zibin},
title = {DeFiTainter: Detecting Price Manipulation Vulnerabilities in DeFi Protocols},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598124},
doi = {10.1145/3597926.3598124},
abstract = {DeFi protocols are programs that manage high-value digital assets on blockchain. The price manipulation vulnerability is one of the common vulnerabilities in DeFi protocols, which allows attackers to gain excessive profits by manipulating token prices. In this paper, we propose DeFiTainter, an inter-contract taint analysis framework for detecting price manipulation vulnerabilities. DeFiTainter features two innovative mechanisms to ensure its effectiveness. The first mechanism is to construct a call graph for inter-contract taint analysis by restoring call information, not only from code constants but also from contract storage and function parameters. The second mechanism is a high-level semantic induction tailored for detecting price manipulation vulnerabilities, which accurately identifies taint sources and sinks and tracks taint data across contracts. Extensive evaluation of real-world incidents and high-value DeFi protocols shows that DeFiTainter outperforms existing approaches and achieves state-of-the-art performance with a precision of 96% and a recall of 91.3% in detecting price manipulation vulnerabilities. Furthermore, DeFiTainter uncovers three previously undisclosed price manipulation vulnerabilities.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1144–1156},
numpages = {13},
keywords = {vulnerability detection, taint analysis, smart contract},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3597926.3598068,
author = {Liu, Zhibo and Xiao, Dongwei and Li, Zongjie and Wang, Shuai and Meng, Wei},
title = {Exploring Missed Optimizations in WebAssembly Optimizers},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598068},
doi = {10.1145/3597926.3598068},
abstract = {The prosperous trend of deploying complex applications to web browsers has   boosted the development of WebAssembly (wasm) compilation toolchains. Software   written in different high-level programming languages are compiled into wasm   executables, which can be executed fast and safely in a virtual machine. The   performance of wasm executables depends highly on compiler optimizations.   Despite the prosperous use of wasm executables, recent research has indicated   that real-world wasm applications are slower than anticipated, suggesting   deficiencies in wasm optimizations.    This paper aims to present the first systematic and in-depth understanding of   the status quo of wasm optimizations. To do so, we present DITWO, a   differential testing framework to uncover missed optimizations   (MO) of wasm optimizers. DITWO compiles a C program into both   native x86 executable and wasm executable, and differentiates   optimization indication traces (OITraces) logged by running each   executable to uncover MO. Each OITrace is composed with global variable writes   and function calls, two performance indicators that practically and   systematically reflect the optimization degree across wasm and native   executables.   Our analysis of the official wasm optimizer, wasm-opt, successfully   identifies 1,293 inputs triggering MO of wasm-opt. With extensive manual effort, we  identify nine root causes for all MO, and we estimate that fixing discovered MO   can result in a performance improvement of at least 17.15%. We also summarize   four lessons from our findings to deliver better wasm optimizations.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {436–448},
numpages = {13},
keywords = {WebAssembly, Software Testing, Compiler Optimization},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3571786.3573015,
author = {Cong, Youyou and Asai, Kenichi},
title = {Towards a Reflection for Effect Handlers},
year = {2023},
isbn = {9798400700118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571786.3573015},
doi = {10.1145/3571786.3573015},
abstract = {A reflection is a relationship between compiling and decompiling functions.   This concept has been studied as a means to ensure correctness of compilers,   in particular, those for languages featuring control effects.   We aim to develop a reflection for algebraic effects and handlers.   As a first step towards this goal, we investigate what we obtain by following   the existing recipe for control operators.   We show that, if we use the simplest CPS translation as the compiling   function, we can prove most but not all theorems required of a reflection.   From this result, we identify two conditions of the CPS translation that   would lead to a reflection for effect handlers.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Workshop on Partial Evaluation and Program Manipulation},
pages = {55–65},
numpages = {11},
keywords = {reflection, direct style translation, algebraic effects and handlers, CPS translation},
location = {Boston, MA, USA},
series = {PEPM 2023}
}

@inproceedings{10.1145/3550355.3552396,
author = {Mittal, Rakshit and Blouin, Dominique and Bhobe, Anish and Bandyopadhyay, Soumyadip},
title = {Solving the instance model-view update problem in AADL},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552396},
doi = {10.1145/3550355.3552396},
abstract = {The Architecture Analysis and Design Language (AADL) is a rich language for modeling embedded systems through several constructs such as component extension and refinement to promote modularity of component declarations. To ease processing AADL models, OSATE, the reference tool for AADL, defines another model (namely 'instance' model) computed from a base 'declarative' model/s. An instance model is a simple object tree where all information from the declarative model is flattened so that tools can easily use this information to analyze the system. However for modifications, they have to make changes in the complex declarative model since there is no automated backward transformation (deinstantiation) from instance to declarative models. Since the instance model is a 'view' of the declarative model, this is a view-update problem. In this paper, we propose the OSATE Declarative-Instance Mapping Tool (OSATE-DIM1), an Eclipse plugin for deinstantiation of AADL models implementing a solution of this view-update problem. We evaluate OSATE-DIM with a benchmark of existing AADL model processing tools and verify the correctness of our deinstantiation transformations. We also discuss how our approach could be useful for decompilation of Object-Oriented languages' intermediate representations.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {55–65},
numpages = {11},
keywords = {view-update problem, model-driven engineering, embedded systems, cyber-physical systems, AADL},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3551349.3561339,
author = {Pauck, Felix},
title = {Scaling Arbitrary Android App Analyses},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3561339},
doi = {10.1145/3551349.3561339},
abstract = {More apps are published every day and the functionality of each app increases steadily as well. Consequently app analyses are often overwhelmed when confronted with up-to-date, real-world apps. One of the biggest issues originates from the scalability of analyses with respect to libraries. Analyses, more precisely the tools implementing them, cannot distinguish the app’s code from the code of a library. Always analyzing the whole code base is the result. However, this is usually not necessary, for example, when a security property is checked, trusted libraries must not be analyzed. We propose an approach to differentiate an app’s code from a library’s code. The approach is based on clone detection and implemented in our prototype APK-Simplifier. As the evaluation shows APK-Simplifier can be employed in a cooperative analysis to remove library code and to enhance arbitrary analysis tools’ scalability. In fact, five analysis tools have been enabled to analyze five up-to-date, real-world apps they could not analyze before. Still, it is alerting that the majority of such apps remains not analyzable as also shown during evaluation.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {204},
numpages = {7},
keywords = {Android, clone detection, cooperative analysis, security, software analysis, taint analysis},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3551349.3559505,
author = {Lv, Zhengwei and Peng, Chao and Zhang, Zhao and Su, Ting and Liu, Kai and Yang, Ping},
title = {Fastbot2: Reusable Automated Model-based GUI Testing for Android Enhanced by Reinforcement Learning},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559505},
doi = {10.1145/3551349.3559505},
abstract = {We introduce a reusable automated model-based GUI testing technique for Android apps to accelerate the testing cycle. Our key insight is that the knowledge of event-activity transitions from the previous testing runs, i.e., executing which events can reach which activities, is valuable for guiding the follow-up testing runs to quickly cover major app functionalities. To this end, we propose (1) a probabilistic model to memorize and leverage this knowledge during testing, and (2) design a model-based guided testing strategy (enhanced by a reinforcement learning algorithm). We implemented our technique as an automated testing tool named Fastbot2. The evaluation on two popular industrial apps (with billions of user installations), Douyin and Toutiao, shows that Fastbot2 outperforms the state-of-the-art testing tools (Monkey, Ape and Stoat) in both activity coverage and fault detection in the context of continuous testing. To date, Fastbot2 has been deployed in the CI pipeline at ByteDance for nearly two years, and 50.8% of the developer-fixed crash bugs were reported by Fastbot2, which significantly improves app quality. Fastbot2 has been made publicly available to benefit the community at: https://github.com/bytedance/Fastbot_Android.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {135},
numpages = {5},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3544902.3546240,
author = {Liu, Xin and Wu, Yixiong and Yu, Qingchen and Song, Shangru and Liu, Yue and Zhou, Qingguo and Zhuge, Jianwei},
title = {PG-VulNet: Detect Supply Chain Vulnerabilities in IoT Devices using Pseudo-code and Graphs},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546240},
doi = {10.1145/3544902.3546240},
abstract = {Background: With the boosting development of IoT technology, the supply chains of IoT devices become more powerful and sophisticated, and the security issues introduced by code reuse are becoming more prominent. Therefore, the detection and management of vulnerabilities through code similarity detection technology is of great significance for protecting the security of IoT devices. Aim: We aim to propose a more accurate, parallel-friendly, and realistic software supply chain vulnerability detection solution for IoT devices. Method: This paper presents PG-VulNet, standing for Vulnerability-detection Network based on Pseudo-code Graphs. It is a ”multi-model” cross-architecture vulnerability detection solution based on pseudo-code and Graph Matching Network (GMN). PG-VulNet extracts both behavioral and structural features of pseudo-code to build customized feature graphs and then uses GMN to detect supply chain vulnerabilities based on these graphs. Results: The experiments show that PG-VulNet achieves an average detection accuracy of 99.14%, significantly higher than existing approaches like Gemini, VulSeeker, FIT, and Asteria. In addition to this, PG-VulNet also excels in detection overhead and false alarms. In the real-world evaluation, PG-VulNet detected 690 known vulnerabilities in 1,611 firmwares. Conclusions: PG-VulNet can effectively detect the vulnerabilities introduced by software supply chain in IoT firmwares and is well suited for large-scale detection. Compared with existing approaches, PG-VulNet has significant advantages.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {205–215},
numpages = {11},
keywords = {Vulnerability Detection, IoT Software Supply Chain, Graph Neural Network, Binary Code Similarity},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1145/3556223.3556257,
author = {Jeng, Tzung-Han and Chang, Ying-Ching and Yang, Hui-Hsuan and Chen, Li-Kai and Chen, Yi-Ming},
title = {A Novel Deep Learning Based Attention Mechanism for Android Malware Detection and Explanation},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556223.3556257},
doi = {10.1145/3556223.3556257},
abstract = {With the popularity of Android mobile devices and the increase of related applications, hackers regard it as the primary attack target. Therefore, malware detection is essential nowadays, and many of these studies employ deep learning techniques. In recent years, the attention mechanism provides corresponding attention weights for different hidden states, and it is widely used in many fields, such as machine translation and image markup. However, no research has applied the attention mechanism to Android malware analysis. Hence, this paper completes the goal of malware family classification based on the static features of Android applications. We compare the difference between the original convolutional neural network (CNN) and the addition of the attention mechanism. The final experimental results show that the attention mechanism improves the accuracy of the existing CNN model by 1.99% in static opcode images. In addition, we further adopt the occlusion sensitivity method to try to explain the classification model proposed in this paper. Finally, the experimental results of model interpretation show that the classification model can effectively identify the threat behavior of malware.},
booktitle = {Proceedings of the 10th International Conference on Computer and Communications Management},
pages = {226–232},
numpages = {7},
keywords = {Model Interpretation, Deep Learning, CNN, Attention Mechanism, Android Malware Classification},
location = {Okayama, Japan},
series = {ICCCM '22}
}

@inproceedings{10.1145/3533767.3534222,
author = {Liao, Zeqin and Zheng, Zibin and Chen, Xiao and Nan, Yuhong},
title = {SmartDagger: a bytecode-based static analysis approach for detecting cross-contract vulnerability},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534222},
doi = {10.1145/3533767.3534222},
abstract = {With the increasing popularity of blockchain, automatically detecting vulnerabilities in smart contracts is becoming a significant problem. Prior research mainly identifies smart contract vulnerabilities without considering the interactions between multiple contracts. Due to the lack of analyzing the fine-grained contextual information during cross-contract invocations, existing approaches often produced a large number of false positives and false negatives. This paper proposes SmartDagger, a new framework for detecting cross-contract vulnerability through static analysis at the bytecode level. SmartDagger integrates a set of novel mechanisms to ensure its effectiveness and efficiency for cross-contract vulnerability detection. Particularly, SmartDagger effectively recovers the contract attribute information from the smart contract bytecode, which is critical for accurately identifying cross-contract vulnerabilities. Besides, instead of performing the typical whole-program analysis which is heavy-weight and time-consuming, SmartDagger selectively analyzes a subset of functions and reuses the data-flow results, which helps to improve its efficiency. Our further evaluation over a manually labelled dataset showed that SmartDagger significantly outperforms other state-of-the-art tools (i.e., Oyente, Slither, Osiris, and Mythril) for detecting cross-contract vulnerabilities. In addition, running SmartDagger over a randomly selected dataset of 250 smart contracts in the real-world, SmartDagger detects 11 cross-contract vulnerabilities, all of which are missed by prior tools.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {752–764},
numpages = {13},
keywords = {static analysis, smart contract, interprocedure analysis, bug finding},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3502718.3524744,
author = {OConnor, TJ and Mann, Carl and Petersen, Tiffanie and Thomas, Isaiah and Stricklan, Chris},
title = {Toward an Automatic Exploit Generation Competition for an Undergraduate Binary Reverse Engineering Course},
year = {2022},
isbn = {9781450392013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502718.3524744},
doi = {10.1145/3502718.3524744},
abstract = {Analyzing binary programs without source code is critical for cybersecurity professionals. This paper presents an undergraduate binary reverse engineering course design that culminates with a comprehensive binary exploitation competition. Our approach challenges students to develop tools that automatically detect and exploit program vulnerabilities. We hypothesize that this competition presents a unique opportunity to exercise the core competencies of binary reverse engineering. We share our detailed design, labs, experiences, and lessons learned from this course for others to build on our initial success.},
booktitle = {Proceedings of the 27th ACM Conference on on Innovation and Technology in Computer Science Education Vol. 1},
pages = {442–448},
numpages = {7},
keywords = {vulnerability research, reverse engineering, cybersecurity education},
location = {Dublin, Ireland},
series = {ITiCSE '22}
}

@article{10.1145/3524452,
author = {Xing, Tong and Barbalace, Antonio and Olivier, Pierre and Karaoui, Mohamed L. and Wang, Wei and Ravindran, Binoy},
title = {H-Container: Enabling Heterogeneous-ISA Container Migration in Edge Computing},
year = {2022},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1–4},
issn = {0734-2071},
url = {https://doi.org/10.1145/3524452},
doi = {10.1145/3524452},
abstract = {Edge computing is a recent computing paradigm that brings cloud services closer to the client. Among other features, edge computing offers extremely low client/server latencies. To consistently provide such low latencies, services should run on edge nodes that are physically as close as possible to their clients. Thus, when the physical location of a client changes, a service should migrate between edge nodes to maintain proximity. Differently from cloud nodes, edge nodes integrate CPUs of different Instruction Set Architectures (ISAs), hence a program natively compiled for a given ISA cannot migrate to a server equipped with a CPU of a different ISA. This hinders migration to the closest node.We introduce H-Container, a system that migrates natively compiled containerized applications across compute nodes featuring CPUs of different ISAs. H-Container advances over existing heterogeneous-ISA migration systems by being (a) highly compatible – no user’s source-code nor compiler toolchain modifications are needed; (b) easily deployable – fully implemented in user space, thus without any OS or hypervisor dependency, and (c) largely Linux-compliant – it can migrate most Linux software, including server applications and dynamically linked binaries. H-Container targets Linux and its already-compiled executables, adopts LLVM, extends CRIU, and integrates with Docker. Experiments demonstrate that H-Container adds no overheads during program execution, while 10–100&nbsp;ms are added during migration. Furthermore, we show the benefits of H-Container in real-world scenarios, demonstrating, for example, up to 94% increase in Redis throughput when client/server proximity is maintained through heterogeneous container migration.},
journal = {ACM Trans. Comput. Syst.},
month = jul,
articleno = {5},
numpages = {36},
keywords = {migration, containers, heterogeneous ISA, Edge}
}

@inproceedings{10.1145/3498361.3538938,
author = {Kim, Taegyu and Ding, Aolin and Etigowni, Sriharsha and Sun, Pengfei and Chen, Jizhou and Garcia, Luis and Zonouz, Saman and Xu, Dongyan and Tian, Dave (Jing)},
title = {Reverse engineering and retrofitting robotic aerial vehicle control firmware using dispatch},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538938},
doi = {10.1145/3498361.3538938},
abstract = {Unmanned Aerial Vehicles as a service (UAVaaS) has increased the field deployment of Robotic Aerial Vehicles (RAVs) for different services such as transportation and terrain exploration. These RAVs are controlled by firmware, which is often closed-source, developed by vendors, and flashed into the ROM. While these binary blobs enable off-the-shelf management of RAVs, end users (individuals or organizations) have no idea if the control firmware is designed and implemented correctly, and can only rely on firmware updates from vendors when any vulnerability is discovered. This paper proposes DisPatch, the first reverse engineering and patching framework for understanding and improving controller design and implementation within RAV firmware. DisPatch first decompiles binary instructions and recovers controller functions and core controller variables by combining control theory with program analysis using symbolic execution and data flow analysis. End users can then write a patch in a domain-specific language (DSL), which will be translated and injected into the binary firmware by DisPatch automatically. We have applied DisPatch to two instances of commodity firmware from3DR IRIS+ and MantisQ RAVs and demonstrated 100% and 80.7% accuracy respectively in the controller decompilation. We have also shown the ability to prevent severe controller performance degradation by patching two real-world bugs with in the firmware and without breaking other functionality. Finally, we show that DisPatch introduces less than 0.53% of space overhead and 1.48% of runtime overhead without violating the soft real-time deadlines. DisPatch provides the first step towards an RAV binary firmware reverse engineering and patching system to customize controller design and implementation.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {69–83},
numpages = {15},
keywords = {security, robotic aerial vehicle, firmware analysis, binary analysis},
location = {Portland, Oregon},
series = {MobiSys '22}
}

@inproceedings{10.1145/3548636.3548651,
author = {Sun, Min and Zhang, Danni},
title = {Two-path Android Malware Detection Based on N-gram Feature Weighting},
year = {2022},
isbn = {9781450396820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548636.3548651},
doi = {10.1145/3548636.3548651},
abstract = {In recent years, with the full popularity of Android system and applications, the types and number of Android malicious applications also show explosive growth, and more efficient detection technology is urgently needed to identify malicious software. In view of the current research on N-gram features is relatively single, in order to make more comprehensive use of N-gram features and explore the potential relationship between features and attributes of applications, this paper proposes a two-path Android malware detection model based on N-gram feature weighting, and achieves N-gram feature extraction in two different ways by setting an application file threshold. Finally, Neural network is used to classify the fused features. Testing results of 1205 malicious samples and 1084 benign samples shows that the detection accuracy of the model was up to 99.2%. At the same time, this experiment further verify the effectiveness of relevant improvements, and the results show that compared with traditional machine learning algorithms, this model has higher adaptability and accuracy.},
booktitle = {Proceedings of the 4th International Conference on Information Technology and Computer Communications},
pages = {99–104},
numpages = {6},
location = {Guangzhou, China},
series = {ITCC '22}
}

@inproceedings{10.1145/3520312.3534867,
author = {Armengol-Estap\'{e}, Jordi and Woodruff, Jackson and Brauckmann, Alexander and Magalh\~{a}es, Jos\'{e} Wesley de Souza and O'Boyle, Michael F. P.},
title = {ExeBench: an ML-scale dataset of executable C functions},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534867},
doi = {10.1145/3520312.3534867},
abstract = {Machine-learning promises to transform compilation and software engineering, yet is frequently limited by the scope of available datasets. In particular, there is a lack of runnable, real-world datasets required for a range of tasks ranging from neural program synthesis to machine learning-guided program optimization. We introduce a new dataset, ExeBench, which attempts to address this. It tackles two key issues with real-world code: references to external types and functions and scalable generation of IO examples. ExeBench is the first publicly available dataset that pairs real-world C code taken from GitHub with IO examples that allow these programs to be run. We develop a toolchain that scrapes GitHub, analyzes the code, and generates runnable snippets of code. We analyze our benchmark suite using several metrics, and show it is representative of real-world code. ExeBench contains 4.5M compilable and 700k executable C functions. This scale of executable, real functions will enable the next generation of machine learning-based programming tasks.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {50–59},
numpages = {10},
keywords = {Program Synthesis, Mining Software Repositories, Machine Learning for Code, Compilers, Code Dataset, C},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}

@inproceedings{10.1145/3519939.3523702,
author = {Verbeek, Freek and Bockenek, Joshua and Fu, Zhoulai and Ravindran, Binoy},
title = {Formally verified lifting of C-compiled x86-64 binaries},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523702},
doi = {10.1145/3519939.3523702},
abstract = {Lifting binaries to a higher-level representation is an essential step for decompilation, binary verification, patching and security analysis. In this paper, we present the first approach to provably overapproximative x86-64 binary lifting. A stripped binary is verified for certain sanity properties such as return address integrity and calling convention adherence. Establishing these properties allows the binary to be lifted to a representation that contains an overapproximation of all possible execution paths of the binary. The lifted representation contains disassembled instructions, reconstructed control flow, invariants and proof obligations that are sufficient to prove the sanity properties as well as correctness of the lifted representation. We apply this approach to Linux Foundation and Intel’s Xen Hypervisor covering about 400K instructions. This demonstrates our approach is the first approach to provably overapproximative binary lifting scalable to commercial off-the-shelf systems. The lifted representation is exportable to the Isabelle/HOL theorem prover, allowing formal verification of its correctness. If our technique succeeds and the proofs obligations are proven true, then – under the generated assumptions – the lifted representation is correct.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {934–949},
numpages = {16},
keywords = {Formal Verification, Disassembly, Binary Analysis},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{10.1145/3519939.3523449,
author = {Lehmann, Daniel and Pradel, Michael},
title = {Finding the Dwarf: Recovering Precise Types from WebAssembly Binaries},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523449},
doi = {10.1145/3519939.3523449},
abstract = {The increasing popularity of WebAssembly creates a demand for understanding and reverse engineering WebAssembly binaries. Recovering high-level function types is an important part of this process. One method to recover types is data-flow analysis, but it is complex to implement and may require manual heuristics when logical constraints fall short. In contrast, this paper presents SnowWhite, a learning-based approach for recovering precise, high-level parameter and return types for WebAssembly functions. It improves over prior work on learning-based type recovery by representing the types-to-predict in an expressive type language, which can describe a large number of complex types, instead of the fixed, and usually small type vocabulary used previously. Thus, recovery of a single type is no longer a classification task but sequence prediction, for which we build on the success of neural sequence-to-sequence models. We evaluate SnowWhite on a new, large-scale dataset of 6.3 million type samples extracted from 300,905 WebAssembly object files. The results show the type language is expressive, precisely describing 1,225 types instead the 7 to 35 types considered in previous learning-based approaches. Despite this expressiveness, our type recovery has high accuracy, exactly matching 44.5% (75.2%) of all parameter types and 57.7% (80.5%) of all return types within the top-1 (top-5) predictions.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {410–425},
numpages = {16},
keywords = {type recovery, type prediction, reverse engineering, neural networks, machine learning, debugging information, dataset, corpus, WebAssembly, DWARF},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{10.1145/3488932.3497764,
author = {Mantovani, Alessandro and Compagna, Luca and Shoshitaishvili, Yan and Balzarotti, Davide},
title = {The Convergence of Source Code and Binary Vulnerability Discovery -- A Case Study},
year = {2022},
isbn = {9781450391405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488932.3497764},
doi = {10.1145/3488932.3497764},
abstract = {Decompilers are tools designed to recover a high-level language representation (typically in C code) from program binaries. Over the past five years, decompilers have improved enormously, not only in terms of the readability of the produced pseudocode, but also in terms of similarity of the recovered representation to the original source code. Albeit decompilers are routinely used by reverse engineers in different disciplines (e.g., to support vulnerability discovery or malware analysis), they are not yet adopted to produce input for source-code static analysis tools. In particular, source code vulnerability discovery and binary vulnerability discovery remain today two very different areas of research, despite the fact that decompilers could potentially bridge this gap and enable source-code analysis on binary files.In this paper, we conducted a number of experiments on real world vulnerabilities to evaluate the feasibility of this approach. In particular, our measurements are intended to show how the differences between original and decompiled code impact the accuracy of static analysis tools.Remarkably, our results show that in 71% of the cases, the same vulnerabilities can be detected by running the static analyzers on the decompiled code, even though for several cases we observe a steep increment in the number of false positives. To understand the reasons behind these differences, we manually investigated all cases and we identified a number of root causes that affected the ability of static tools to 'understand' the generated code.},
booktitle = {Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security},
pages = {602–615},
numpages = {14},
keywords = {vulnerability, sast, reversing, decompiler},
location = {Nagasaki, Japan},
series = {ASIA CCS '22}
}

@inproceedings{10.1145/3510454.3516854,
author = {Mahmud, Tarek and Che, Meiru and Yang, Guowei},
title = {ACID: an API compatibility issue detector for Android apps},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516854},
doi = {10.1145/3510454.3516854},
abstract = {Android API is frequently updated, and compatibility issues may be induced when the API level supported by the device differs from the API level targeted by app developers. This paper presents ACID, an API compatibility issue detector for Android apps. ACID utilizes API differences and static analysis of Android apps to detect both API invocation compatibility issues and API callback compatibility issues. Our evaluation on 20 benchmark apps from previous studies shows that ACID is more accurate and faster in detecting compatibility issues than state-of-the-art techniques. We also ran ACID on 35 more real-world apps to demonstrate ACID's practical applicability. ACID is available at https://github.com/TSUMahmud/acid and the demonstration video of ACID is available at https://youtu.be/XUNBPMIx2q4.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {1–5},
numpages = {5},
keywords = {Android, API invocation compatibility issues, API evolution, API callback compatibility issues},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3507657.3528555,
author = {Zhang, Chennan and Li, Shuang and Diao, Wenrui and Guo, Shanqing},
title = {PITracker: Detecting Android PendingIntent Vulnerabilities through Intent Flow Analysis},
year = {2022},
isbn = {9781450392167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507657.3528555},
doi = {10.1145/3507657.3528555},
abstract = {Intent is an essential inter-component communication mechanism of Android OS, which can be used to request an action from another app component. The security of its design and implementation attracts lots of attention. However, the security of PendingIntent, a kind of delayed-triggered Intent, was neglected by most previous research, and the related analysis techniques are still imperfect. In this paper, we design a novel automated tool, PITracker, to detect the PendingIntent vulnerabilities in Android apps. It achieves the Intent flow tracking technique proposed by us, figuring out how an Intent is created and where it goes. In the real-world evaluations, PITracker discovered 2,939 potential threats in 10,000 third-party apps and 214 in 1,412 pre-installed apps. Among them, 11 exploitable vulnerabilities have been confirmed and acknowledged by the corresponding vendors.},
booktitle = {Proceedings of the 15th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {20–25},
numpages = {6},
keywords = {vulnerability detection, pendingintent, android},
location = {San Antonio, TX, USA},
series = {WiSec '22}
}

@inproceedings{10.1145/3520084.3520103,
author = {Otsuki, Naruaki and Tamada, Haruaki},
title = {Overcoming the obfuscation method of the dynamic name resolution},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520103},
doi = {10.1145/3520084.3520103},
abstract = {Using unevaluated obfuscation methods has a significant risk since the methods might have some vulnerabilities. One evaluation for obfuscation is de-obfuscation which discloses the hidden information by the obfuscation. This paper proposed the de-obfuscation method against for DNR (dynamic name resolution) obfuscation method. DNR hides system-defined names by encrypting them and resolves names dynamically during runtime. This paper clarifies the steps of de-obfuscation and proposes static and dynamic manners to de-obfuscate DNR. Through the case study, two ways both succeed in disclosing the hidden information of DNR.},
booktitle = {Proceedings of the 2022 5th International Conference on Software Engineering and Information Management},
pages = {118–124},
numpages = {7},
keywords = {reverse-transformation, resilience of the obfuscation method, obfuscation, dynamic name resolution, de-obfuscation},
location = {Yokohama, Japan},
series = {ICSIM '22}
}

@article{10.1145/3486860,
author = {Alrabaee, Saed and Debbabi, Mourad and Wang, Lingyu},
title = {A Survey of Binary Code Fingerprinting Approaches: Taxonomy, Methodologies, and Features},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3486860},
doi = {10.1145/3486860},
abstract = {Binary code fingerprinting is crucial in many security applications. Examples include malware detection, software infringement, vulnerability analysis, and digital forensics. It is also useful for security researchers and reverse engineers since it enables high fidelity reasoning about the binary code such as revealing the functionality, authorship, libraries used, and vulnerabilities. Numerous studies have investigated binary code with the goal of extracting fingerprints that can illuminate the semantics of a target application. However, extracting fingerprints is a challenging task since a substantial amount of significant information will be lost during compilation, notably, variable and function naming, the original data and control flow structures, comments, semantic information, and the code layout. This article provides the first systematic review of existing binary code fingerprinting approaches and the contexts in which they are used. In addition, it discusses the applications that rely on binary code fingerprints, the information that can be captured during the fingerprinting process, and the approaches used and their implementations. It also addresses limitations and open questions related to the fingerprinting process and proposes future directions.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {19},
numpages = {41},
keywords = {software security, reverse engineering, Binary code analysis}
}

@inproceedings{10.5555/3507788.3507824,
author = {Wood, Bradley and Azim, Akramul},
title = {A novel technique for control flow obfuscation in JVM applications using InvokeDynamic with native bootstrapping},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Protecting the intellectual property of end-user software is a challenging industry problem. Modern obfuscation techniques aim to prevent reverse engineering and unauthorized use or modification to software. Obfuscation algorithms are especially necessary in JVM applications due to the large amount of contextual information stored in JVM bytecode. However, many commercial-grade obfuscation methods can be easily be undone by deobfuscation software such that it can later be decompiled and refactored for illegitimate use. In this paper, we propose a method of control flow obfuscation using the InvokeDynamic instruction with native call site bootstrapping. The proposed method prevents JVM byte-code from leaking call site information in function invocations. We evaluate the proposed technique against a series of benchmarks comparing original software with its obfuscated form. To this end, we observe an insignificant difference in application running time.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {232–236},
numpages = {5},
keywords = {reverse engineering, obfuscation, control-flow, Java virtual machine},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1145/3418206,
author = {Ullah, Farhan and Naeem, Muhammad Rashid and Bajahzar, Abdullah S. and Al-Turjman, Fadi},
title = {IoT-based Cloud Service for Secured Android Markets using PDG-based Deep Learning Classification},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418206},
doi = {10.1145/3418206},
abstract = {Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones.Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48% among clones from different application stores.},
journal = {ACM Trans. Internet Technol.},
month = oct,
articleno = {40},
numpages = {17},
keywords = {Internet of Things, cloud services, program dependency graph, deep learning, Clone detection}
}

@inproceedings{10.1145/3490725.3490733,
author = {Wu, Qing and Sun, Peng and Hong, Xueshu and Zhu, Xueling and Liu, Bo},
title = {An Android Malware Detection and Malicious Code Location Method Based on Graph Neural Network},
year = {2022},
isbn = {9781450384247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490725.3490733},
doi = {10.1145/3490725.3490733},
abstract = {In recent years, enormously Android malware poses a significant threat to Android platform security. To detect malicious applications, researchers have done a lot of work, in which finding and locating malicious code segments is an important research content. In the previous research, most detection methods cannot directly locate malicious code, and some methods with the locate ability can only find some specific types of malicious operations. This paper proposed a graph convolution algorithm and weighted mechanism to find malicious nodes implied in the Android application function call graph and provided a general method for malicious code location. We analyzed the sub-graph structural differences between benign code and malicious payload in the function call graph, constructed graph convolution operation to make the nodes in the graph learn the surrounding sub-graph structure, designed the weighting mechanism to set the malicious score to every code node, and filtered out the nodes with the highest malicious score to locate the malicious code fragments. On the dataset composed of 2650 malicious and 2650 benign applications, the accuracy of malware detection is 92.6%, and the accuracy of malicious code location is between 72.6% and 88.1%, indicating that our method is accurate and efficient.},
booktitle = {Proceedings of the 2021 4th International Conference on Machine Learning and Machine Intelligence},
pages = {50–56},
numpages = {7},
keywords = {Malware detection, Malicious code localization, Machine learning, Graph Neural Network, Android},
location = {Hangzhou, China},
series = {MLMI '21}
}

@article{10.1145/3428151,
author = {Bibi, Iram and Akhunzada, Adnan and Malik, Jahanzaib and Khan, Muhammad Khurram and Dawood, Muhammad},
title = {Secure Distributed Mobile Volunteer Computing with Android},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3428151},
doi = {10.1145/3428151},
abstract = {Volunteer Computing provision of seamless connectivity that enables convenient and rapid deployment of greener and cheaper computing infrastructure is extremely promising to complement next-generation distributed computing systems. Undoubtedly, without tactile Internet and secure VC ecosystems, harnessing its full potentials and making it an alternative viable and reliable computing infrastructure is next to impossible. Android-enabled smart devices, applications, and services are inevitable for Volunteer computing. Contrarily, the progressive developments of sophisticated Android malware may reduce its exponential growth. Besides, Android malwares are considered the most potential and persistent cyber threat to mobile VC systems. To secure Android-based mobile volunteer computing, the authors proposed MulDroid, an efficient and self-learning autonomous hybrid (Long-Short-Term Memory, Convolutional Neural Network, Deep Neural Network) multi-vector Android malware threat detection framework. The proposed mechanism is highly scalable with well-coordinated infrastructure and self-optimizing capabilities to proficiently tackle fast-growing dynamic variants of sophisticated malware threats and attacks with 99.01% detection accuracy. For a comprehensive evaluation, the authors employed current state-of-the-art malware datasets (Android Malware Dataset, Androzoo) with standard performance evaluation metrics. Moreover, MulDroid is compared with our constructed contemporary hybrid DL-driven architectures and benchmark algorithms. Our proposed mechanism outperforms in terms of detection accuracy with a trivial tradeoff speed efficiency. Additionally, a 10-fold cross-validation is performed to explicitly show unbiased results.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {2},
numpages = {21},
keywords = {deep learning (DL), android malware, tactile internet, Volunteer computing (VC)}
}

@inproceedings{10.1145/3479394.3479399,
author = {Biernacki, Dariusz and Pyzik, Mateusz and Sieczkowski, Filip},
title = {Reflecting&nbsp;Stacked&nbsp;Continuations in&nbsp;a&nbsp;Fine-Grained&nbsp;Direct-Style&nbsp;Reduction&nbsp;Theory},
year = {2021},
isbn = {9781450386890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479394.3479399},
doi = {10.1145/3479394.3479399},
abstract = {The delimited-control operator shift0 has been formally shown to capture the operational semantics of deep handlers for algebraic effects. Its CPS translation generates λ-terms in which continuation composition is not expressed in terms of nested function calls, as is typical of other delimited-control operators, e.g. shift, but with function applications consuming a sequence of continuations one at a time, as if they formed a stack. We present a novel reduction theory for Moggi’s computational λ-calculus extended with shift0 and a control delimiter dollar, which models the capture of evaluation contexts in a fine-grained manner as an interaction between the let-expressions and the delimiter. We establish a connection between our reduction theory and the existing theories of shif0 and dollar. Moreover, we develop a CPS translation for our calculus along with a direct-style translation that together form a reflection, i.e. the translations preserve reductions and the direct-style translation is a right inverse of the CPS translation. This construction relies on the invariant that CPS root terms are in η-head-normal form. The results of this work could potentially be used for compiler optimisations and lead to a similar development for algebraic effects.},
booktitle = {Proceedings of the 23rd International Symposium on Principles and Practice of Declarative Programming},
articleno = {4},
numpages = {13},
keywords = {reflection, lambda calculus, delimited control, continuation-passing style},
location = {Tallinn, Estonia},
series = {PPDP '21}
}

@inproceedings{10.1145/3468264.3468607,
author = {Pei, Kexin and Guan, Jonas and Broughton, Matthew and Chen, Zhongtian and Yao, Songchen and Williams-King, David and Ummadisetty, Vikas and Yang, Junfeng and Ray, Baishakhi and Jana, Suman},
title = {StateFormer: fine-grained type recovery from binaries using generative state modeling},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468607},
doi = {10.1145/3468264.3468607},
abstract = {Binary type inference is a critical reverse engineering task supporting many security applications, including vulnerability analysis, binary hardening, forensics, and decompilation. It is a difficult task because source-level type information is often stripped during compilation, leaving only binaries with untyped memory and register accesses. Existing approaches rely on hand-coded type inference rules defined by domain experts, which are brittle and require nontrivial effort to maintain and update. Even though machine learning approaches have shown promise at automatically learning the inference rules, their accuracy is still low, especially for optimized binaries.  We present StateFormer, a new neural architecture that is adept at accurate and robust type inference. StateFormer follows a two-step transfer learning paradigm. In the pretraining step, the model is trained with Generative State Modeling (GSM), a novel task that we design to teach the model to statically approximate execution effects of assembly instructions in both forward and backward directions. In the finetuning step, the pretrained model learns to use its knowledge of operational semantics to infer types.  We evaluate StateFormer's performance on a corpus of 33 popular open-source software projects containing over 1.67 billion variables of different types. The programs are compiled with GCC and LLVM over 4 optimization levels O0-O3, and 3 obfuscation passes based on LLVM. Our model significantly outperforms state-of-the-art ML-based tools by 14.6% in recovering types for both function arguments and variables. Our ablation studies show that GSM improves type inference accuracy by 33%.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {690–702},
numpages = {13},
keywords = {Type Inference, Transfer Learning, Reverse Engineering, Machine Learning for Program Analysis},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3465481.3470077,
author = {Casolare, Rosangela and Ciaramella, Giovanni and Martinelli, Fabio and Mercaldo, Francesco and Santone, Antonella},
title = {Ste\ae{}lErgon: A Framework for Injecting Colluding Malicious Payload in Android Applications},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470077},
doi = {10.1145/3465481.3470077},
abstract = {Mobile malware is growing in number and its complexity is constantly increasing. Malware authors are continuously looking new ways to elude anti-malware controls. Anti-malware are not able to detect zero-day malware, because to detect malicious behaviour they need to know its signature, but to have this information the malware must already be widespread. Furthermore, anti-malware are able to scan one application at a time: for this reason a type of malware characterized by the colluding attack, where the malicious action is split in two (or more) applications, can not be recognised. To demonstrate the ineffectiveness of current anti-malware mechanisms in recognizing colluding attacks, in this paper we propose Ste\ae{}lErgon, a framework aimed to inject a malicious payload in two or more different Android applications. Clearly the malicious payload will be executed once all the applications composing the collusive attacks are installed into the infected device. In detail, Ste\ae{}lErgon is able to inject a collusive malicious payload attacking the external storage, allowing the attacker to catch sensitive and private information stored into the infected device. We perform an experimental analysis by submitting the generated colluding application to different 79 anti-malware, by showing that current detection mechanism are not able to detect this kind of threat. To boost research in focusing the attention in colluding attacks we freely release Ste\ae{}lErgon, is available for research purposes at the following url: https://github.com/vigimella/StealErgon.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {51},
numpages = {7},
keywords = {Android, colluding, covert channel, detection, malware, mobile, security},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/3465481.3470040,
author = {Singh, Shirish and Chaturvedy, Kushagra and Mishra, Bharavi},
title = {Multi-View Learning for Repackaged Malware Detection},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470040},
doi = {10.1145/3465481.3470040},
abstract = {Repackaging refers to the core process of unpacking a software package, then repackaging it after a probable modification to the decompiled code and/or to other resource files. In the case of repackaged malware, benign apps are injected with a malicious payload. Repackaged malware pose a serious threat to the Android ecosystem. Moreover, repackaged malware and benign apps share more than 80% of their features, which makes detection a challenging problem. This paper presents a novel technique based on multi-view learning to address this challenge of detecting repackaged malware. Multi-View Learning is a technique where data from multiple distinct feature groups, referred to as views, are fused to improve the model’s generalization performance. In the context of Android, we define views as different components of the app, such as permissions, APIs, sensor usage, etc. We analyzed 15,297 repackaged app pairs and extracted seven different views to represent an app. We perform an ablation study to identify which view(s) contribute more to the classification. The model was trained end-to-end to jointly learn appropriate features and to perform the classification. We show that our approach achieves accuracy and an F1-score of 97.46% and 0.98, respectively.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {53},
numpages = {9},
keywords = {Malware detection, Mobile apps, Multi-view learning, Repackaged malware},
location = {Vienna, Austria},
series = {ARES '21}
}

@article{10.1145/3461666,
author = {Abuhamad, Mohammed and Abuhmed, Tamer and Mohaisen, David and Nyang, Daehun},
title = {Large-scale and Robust Code Authorship Identification with Deep Feature Learning},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3461666},
doi = {10.1145/3461666},
abstract = {Successful software authorship de-anonymization has both software forensics applications and privacy implications. However, the process requires an efficient extraction of authorship attributes. The extraction of such attributes is very challenging, due to various software code formats from executable binaries with different toolchain provenance to source code with different programming languages. Moreover, the quality of attributes is bounded by the availability of software samples to a certain number of samples per author and a specific size for software samples. To this end, this work proposes a deep Learning-based approach for software authorship attribution, that facilitates large-scale, format-independent, language-oblivious, and obfuscation-resilient software authorship identification. This proposed approach incorporates the process of learning deep authorship attribution using a recurrent neural network, and ensemble random forest classifier for scalability to de-anonymize programmers. Comprehensive experiments are conducted to evaluate the proposed approach over the entire Google Code Jam (GCJ) dataset across all years (from 2008 to 2016) and over real-world code samples from 1,987 public repositories on GitHub. The results of our work show high accuracy despite requiring a smaller number of samples per author. Experimenting with source-code, our approach allows us to identify 8,903 GCJ authors, the largest-scale dataset used by far, with an accuracy of 92.3%. Using the real-world dataset, we achieved an identification accuracy of 94.38% for 745 C programmers on GitHub. Moreover, the proposed approach is resilient to language-specifics, and thus it can identify authors of four programming languages (e.g., C, C++, Java, and Python), and authors writing in mixed languages (e.g., Java/C++, Python/C++). Finally, our system is resistant to sophisticated obfuscation (e.g., using C Tigress) with an accuracy of 93.42% for a set of 120 authors. Experimenting with executable binaries, our approach achieves 95.74% for identifying 1,500 programmers of software binaries. Similar results were obtained when software binaries are generated with different compilation options, optimization levels, and removing of symbol information. Moreover, our approach achieves 93.86% for identifying 1,500 programmers of obfuscated binaries using all features adopted in Obfuscator-LLVM tool.},
journal = {ACM Trans. Priv. Secur.},
month = jul,
articleno = {23},
numpages = {35},
keywords = {software forensics, program features, deep learning identification, Software authorship identification}
}

@inproceedings{10.1145/3453483.3454091,
author = {Kalhauge, Christian Gram and Palsberg, Jens},
title = {Logical bytecode reduction},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454091},
doi = {10.1145/3453483.3454091},
abstract = {Reducing a failure-inducing input to a smaller one is challenging for input with internal dependencies because most sub-inputs are invalid. Kalhauge and Palsberg made progress on this problem by mapping the task to a reduction problem for dependency graphs that avoids invalid inputs entirely. Their tool J-Reduce efficiently reduces Java bytecode to 24 percent of its original size, which made it the most effective tool until now. However, the output from their tool is often too large to be helpful in a bug report. In this paper, we show that more fine-grained modeling of dependencies leads to much more reduction. Specifically, we use propositional logic for specifying dependencies and we show how this works for Java bytecode. Once we have a propositional formula that specifies all valid sub-inputs, we run an algorithm that finds a small, valid, failure-inducing input. Our algorithm interleaves runs of the buggy program and calls to a procedure that finds a minimal satisfying assignment. Our experiments show that we can reduce Java bytecode to 4.6 percent of its original size, which is 5.3 times better than the 24.3 percent achieved by J-Reduce. The much smaller output is more suitable for bug reports.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {1003–1016},
numpages = {14},
keywords = {type-safe code transformation, input reduction},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3453483.3454033,
author = {Ringer, Talia and Porter, RanDair and Yazdani, Nathaniel and Leo, John and Grossman, Dan},
title = {Proof repair across type equivalences},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454033},
doi = {10.1145/3453483.3454033},
abstract = {We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to suggested tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in Pumpkin Pi, an extension to the Pumpkin Patch Coq plugin suite for proof repair. We demonstrate Pumpkin Pi’s flexibility on eight case studies, including supporting a benchmark from a user study,easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {112–127},
numpages = {16},
keywords = {proof reuse, proof repair, proof engineering},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3411764.3445535,
author = {Liu, Richen and Gao, Min and Ye, Shunlong and Zhang, Jiang},
title = {IGScript: An Interaction Grammar for Scientific Data Presentation},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445535},
doi = {10.1145/3411764.3445535},
abstract = {Most of the existing scientific visualizations toward interpretive grammar aim to enhance customizability in either the computation stage or the rendering stage or both, while few approaches focus on the data presentation stage. Besides, most of these approaches leverage the existing components from the general-purpose programming languages (GPLs) instead of developing a standalone compiler, which pose a great challenge about learning curves for the domain experts who have limited knowledge about programming. In this paper, we propose IGScript, a novel script-based interaction grammar tool, to help build scientific data presentation animations for communication. We design a dual-space interface and a compiler which converts natural language-like grammar statements or scripts into a data story animation to make an interactive customization on script-driven data presentations, and then develop a code generator (decompiler) to translate the interactive data exploration animations back into script codes to achieve statement parameters. IGScript makes the presentation animations editable, e.g., it allows to cut, copy, paste, append, or even delete some animation clips. We demonstrate the usability, customizability, and flexibility of IGScript by a user study, four case studies conducted by using four types of commonly-used scientific data, and performance evaluations.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {26},
numpages = {13},
keywords = {scientific visualization, interaction grammar, data presentation},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3411764.3445249,
author = {Wang, Chenglong and Feng, Yu and Bodik, Rastislav and Dillig, Isil and Cheung, Alvin and Ko, Amy J},
title = {Falx: Synthesis-Powered Visualization Authoring},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445249},
doi = {10.1145/3411764.3445249},
abstract = {Modern visualization tools aim to allow data analysts to easily create exploratory visualizations. When the input data layout conforms to the visualization design, users can easily specify visualizations by mapping data columns to visual channels of the design. However, when there is a mismatch between data layout and the design, users need to spend significant effort on data transformation. We propose Falx, a synthesis-powered visualization tool that allows users to specify visualizations in a similarly simple way but without needing to worry about data layout. In Falx, users specify visualizations using examples of how concrete values in the input are mapped to visual channels, and Falx automatically infers the visualization specification and transforms the data to match the design. In a study with 33 data analysts on four visualization tasks involving data transformation, we found that users can effectively adopt Falx to create visualizations they otherwise cannot implement.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {106},
numpages = {15},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3422337.3450321,
author = {Kang, Seoyeon and Lee, Sujeong and Kim, Yumin and Mok, Seong-Kyun and Cho, Eun-Sun},
title = {OBFUS: An Obfuscation Tool for Software Copyright and Vulnerability Protection},
year = {2021},
isbn = {9781450381437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422337.3450321},
doi = {10.1145/3422337.3450321},
abstract = {In this paper, we propose OBFUS, a web-based tool that can easily apply obfuscation techniques to high-level and low-level programming languages. OBFUS's high-level obfuscator parses and obfuscates the source code, overlaying the obfuscation to produce more complex results. OBFUS's low-level obfuscator decompiles binary programs into LLVM IR. This LLVM IR pro-gram is obfuscated and the LLVM IR program is recompiled to become an obfuscated binary program.},
booktitle = {Proceedings of the Eleventh ACM Conference on Data and Application Security and Privacy},
pages = {309–311},
numpages = {3},
keywords = {software protection, obuscation},
location = {Virtual Event, USA},
series = {CODASPY '21}
}

@inproceedings{10.1145/3442381.3450138,
author = {Hilbig, Aaron and Lehmann, Daniel and Pradel, Michael},
title = {An Empirical Study of Real-World WebAssembly Binaries: Security, Languages, Use Cases},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450138},
doi = {10.1145/3442381.3450138},
abstract = {WebAssembly has emerged as a low-level language for the web and beyond. Despite its popularity in different domains, little is known about WebAssembly binaries that occur in the wild. This paper presents a comprehensive empirical study of 8,461 unique WebAssembly binaries gathered from a wide range of sources, including source code repositories, package managers, and live websites. We study the security properties, source languages, and use cases of the binaries and how they influence the security of the WebAssembly ecosystem. Our findings update some previously held assumptions about real-world WebAssembly and highlight problems that call for future research. For example, we show that vulnerabilities that propagate from insecure source languages potentially affect a wide range of binaries (e.g., two thirds of the binaries are compiled from memory unsafe languages, such as C and C++) and that 21% of all binaries import potentially dangerous APIs from their host environment. We also show that cryptomining, which once accounted for the majority of all WebAssembly code, has been marginalized (less than 1% of all binaries found on the web) and gives way to a diverse set of use cases. Finally, 29% of all binaries on the web are minified, calling for techniques to decompile and reverse engineer WebAssembly. Overall, our results show that WebAssembly has left its infancy and is growing up into a language that powers a diverse ecosystem, with new challenges and opportunities for security researchers and practitioners. Besides these insights, we also share the dataset underlying our study, which is 58 times larger than the largest previously reported benchmark.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2696–2708},
numpages = {13},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3441296,
title = {PEPM 2021: Proceedings of the 2021 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation},
year = {2021},
isbn = {9781450383059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to present the proceedings of the 2021 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation (PEPM 2021), held January 18–19th, 2021, in aliation with the annual Symposium on Principles of Programming Languages (POPL 2021). (Originally POPL 2021 was originally to be held in Copenhagen, Denmark, but due to the COVID-19 pandemic, it was moved entirely online.) PEPM has a history dating back to 1991, and originates in the discoveries of practically useful automated techniques for evaluating programs with only partial input. Over the years, the scope of PEPM has expanded to include a variety of research areas centered around the theme of semantics-based program manipulation — the systematic exploitation of treating programs not only as subject to black-box execution, but also as data structures that can be generated, analysed, and transformed whilst establishing or maintaining important semantic properties. Relevant topics range from refactoring, partial evaluation, supercompilation, staged programming, fusion, and other meta-programming to model-driven development, program analysis, inductive programming, decompilation, program generation, and abstract interpretation.},
location = {Virtual, Denmark}
}

@inproceedings{10.1145/3324884.3418905,
author = {Wang, Wei and Sun, Ruoxi and Xue, Minhui and Ranasinghe, Damith C.},
title = {An automated assessment of Android clipboards},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3418905},
doi = {10.1145/3324884.3418905},
abstract = {Since the new privacy feature in iOS enabling users to acknowledge which app is reading or writing to his or her clipboard through prompting notifications was updated, a plethora of top apps have been reported to frequently access the clipboard without user consent. However, the lack of monitoring and control of Android application's access to the clipboard data leave Android users blind to their potential to leak private information from Android clipboards, raising severe security and privacy concerns. In this preliminary work, we envisage and investigate an approach to (i) dynamically detect clipboard access behaviour, and (ii) determine privacy leaks via static data flow analysis, in which we enhance the results of taint analysis with call graph concatenation to enable leakage source backtracking. Our preliminary results indicate that the proposed method can expose clipboard data leakage as substantiated by our discovery of a popular app, i.e., Sogou Input, directly monitoring and transferring user data in a clipboard to backend servers.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1249–1251},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3428277,
author = {Albert, Elvira and Grossman, Shelly and Rinetzky, Noam and Rodr\'{\i}guez-N\'{u}\~{n}ez, Clara and Rubio, Albert and Sagiv, Mooly},
title = {Taming callbacks for smart contract modularity},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428277},
doi = {10.1145/3428277},
abstract = {Callbacks are an effective programming discipline for implementing event-driven programming, especially in environments like Ethereum which forbid shared global state and concurrency. Callbacks allow a callee to delegate the execution back to the caller. Though effective, they can lead to subtle mistakes principally in open environments where callbacks can be added in a new code. Indeed, several high profile bugs in smart contracts exploit callbacks. We present the first static technique ensuring modularity in the presence of callbacks and apply it to verify prominent smart contracts. Modularity ensures that external calls to other contracts cannot affect the behavior of the contract. Importantly, modularity is guaranteed without restricting programming. In general, checking modularity is undecidable—even for programs without loops. This paper describes an effective technique for soundly ensuring modularity harnessing SMT solvers. The main idea is to define a constructive version of modularity using commutativity and projection operations on program segments. We believe that this approach is also accessible to programmers, since counterexamples to modularity can be generated automatically by the SMT solvers, allowing programmers to understand and fix the error. We implemented our approach in order to demonstrate the precision of the modularity analysis and applied it to real smart contracts, including a subset of the 150 most active contracts in Ethereum. Our implementation decompiles bytecode programs into an intermediate representation and then implements the modularity checking using SMT queries. Overall, we argue that our experimental results indicate that the method can be applied to many realistic contracts, and that it is able to prove modularity where other methods fail.},
journal = {Proc. ACM Program. Lang.},
month = nov,
articleno = {209},
numpages = {30},
keywords = {smart contracts, program verification, program analysis, logic and verification, invariants, blockchain}
}

@inproceedings{10.1145/3372297.3417251,
author = {Erinfolami, Rukayat Ayomide and Prakash, Aravind},
title = {Devil is Virtual: Reversing Virtual Inheritance in C++ Binaries},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417251},
doi = {10.1145/3372297.3417251},
abstract = {The complexities that arise from the implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious of virtual inheritance. The conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10% on Linux and 12.5% on Windows) of real-world C++ programs including Mysql and Libstdc++. (2) We show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These falses either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95% and 95.5% (GCC -O0) and a minimum of 77.5% and 73.8% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {133–148},
numpages = {16},
keywords = {virtual inheritance recovery, software reverse engineering, class inheritance recovery},
location = {Virtual Event, USA},
series = {CCS '20}
}

@article{10.1145/3416262,
author = {Grech, Neville and Kong, Michael and Jurisevic, Anton and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
title = {MadMax: analyzing the out-of-gas world of smart contracts},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3416262},
doi = {10.1145/3416262},
abstract = {Ethereum is a distributed blockchain platform, serving as an ecosystem for smart contracts: full-fledged intercommunicating programs that capture the transaction logic of an account. A gas limit caps the execution of an Ethereum smart contract: instructions, when executed, consume gas, and the execution proceeds as long as gas is available.Gas-focused vulnerabilities permit an attacker to force key contract functionality to run out of gas---effectively performing a permanent denial-of-service attack on the contract. Such vulnerabilities are among the hardest for programmers to protect against, as out-of-gas behavior may be uncommon in nonattack scenarios and reasoning about these vulnerabilities is nontrivial.In this paper, we identify gas-focused vulnerabilities and present MadMax: a static program analysis technique that automatically detects gas-focused vulnerabilities with very high confidence. MadMax combines a smart contract decompiler and semantic queries in Datalog. Our approach captures high-level program modeling concepts (such as "dynamic data structure storage" and "safely resumable loops") and delivers high precision and scalability. MadMax analyzes the entirety of smart contracts in the Ethereum blockchain in just 10 hours and flags vulnerabilities in contracts with a monetary value in billions of dollars. Manual inspection of a sample of flagged contracts shows that 81% of the sampled warnings do indeed lead to vulnerabilities.},
journal = {Commun. ACM},
month = sep,
pages = {87–95},
numpages = {9}
}

@inproceedings{10.1145/3395363.3404365,
author = {Yan, Wentian and Gao, Jianbo and Wu, Zhenhao and Li, Yue and Guan, Zhi and Li, Qingshan and Chen, Zhong},
title = {EShield: protect smart contracts against reverse engineering},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404365},
doi = {10.1145/3395363.3404365},
abstract = {Smart contracts are the back-end programs of blockchain-based applications and the execution results are deterministic and publicly visible. Developers are unwilling to release source code of some smart contracts to generate randomness or for security reasons, however, attackers still can use reverse engineering tools to decompile and analyze the code. In this paper, we propose EShield, an automated security enhancement tool for protecting smart contracts against reverse engineering. EShield replaces original instructions of operating jump addresses with anti-patterns to interfere with control flow recovery from bytecode. We have implemented four methods in EShield and conducted an experiment on over 20k smart contracts. The evaluation results show that all the protected smart contracts are resistant to three different reverse engineering tools with little extra gas cost.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {553–556},
numpages = {4},
keywords = {Smart Contract, Reverse Engineering, Program Analysis, Ethereum, Blockchain},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3405962.3405980,
author = {Seraj, Saeed and Pavlidis, Michalis and Polatidis, Nikolaos},
title = {A Novel Dataset for Fake Android Anti-Malware Detection},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405980},
doi = {10.1145/3405962.3405980},
abstract = {Today in the world people are able to get all types of Android applications (apps) from the app store or various sources over the Internet. A large number of apps is being produced daily, some of which are infected with malware. Thus, the use of anti-malware identification tools is essential. At the same time, a number of attackers who exploit a number of anti-malwares have been doing obtaining information from mobile phones in various ways, such as decompiling or infecting anti-malware. Therefore, in this paper, we developed a classification dataset from collected anti-malware data looking for fraudulent anti-malware products. Additionally, we applied various machine learning algorithms and we propose a combination of algorithms which provides high accuracy over various evaluation tests, showing that our approach is both practical and effective.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {205–209},
numpages = {5},
keywords = {Malware, Machine learning, Fake anti-malware detection, Cyber security, Anti-malware, Android},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3385412.3386012,
author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
title = {Synthesizing structured CAD models with equality saturation and inverse transformations},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386012},
doi = {10.1145/3385412.3386012},
abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {31–44},
numpages = {14},
keywords = {Computer-Aided Design, Decompilation, Equality Saturation, Program Synthesis},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3371307.3371312,
author = {Vasileiadis, Leonidas and Ceccato, Mariano and Corradini, Davide},
title = {Revealing malicious remote engineering attempts on Android apps with magic numbers},
year = {2019},
isbn = {9781450377461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371307.3371312},
doi = {10.1145/3371307.3371312},
abstract = {Malicious reverse engineering is a prominent activity conducted by attackers to plan their code tampering attacks. Android apps are particularly exposed to malicious reverse engineering, because their code can be easily analyzed and decompiled, or monitored using debugging tools, that were originally meant to be used by developers.In this paper, we propose a solution to identify attempts of malicious reverse engineering on Android apps. Our approach is based on a series of periodic checks on the execution environment (i.e., Android components) and on the app itself. The check outcome is encoded into a Magic Number and send to a sever for validation. The owner of the app is then supposed to take countermeasures and react, by disconnecting or banning the apps under attack.Our empirical validation suggests that the execution overhead caused by our periodic checks is acceptable, because its resource consumption is compatible with the resources commonly available in smartphones.},
booktitle = {Proceedings of the 9th Workshop on Software Security, Protection, and Reverse Engineering},
articleno = {1},
numpages = {12},
keywords = {remote attestation, malicious reverse engineering, code tampering},
location = {San Juan, Puerto Rico, USA},
series = {SSPREW9 '19}
}

@inproceedings{10.1145/3338503.3357725,
author = {Rohleder, Roman},
title = {Hands-On Ghidra - A Tutorial about the Software Reverse Engineering Framework},
year = {2019},
isbn = {9781450368353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338503.3357725},
doi = {10.1145/3338503.3357725},
abstract = {In this tutorial, the Ghidra software reverse engineering framework will be presented, its characteristics highlighted and its features to the hitherto industry standard in reverse engineering tools, IDA Pro - the interactive disassembler, compared against. This framework was released on March the 5th 2019, by the National Security Agency under the Apache v2 license and brought with it a powerful decompiler for many different architectures (X86 16/32/64, ARM/AARCH64, Java/DEX bytecode, ...), which will be presented and its underlying intermediate language p-code and the corresponding SLEIGH-format explained. Further, hands-on demonstrations will follow, including the aforementioned SLEIGH-format, the plugin-system and the standalone-mode, showcased on different reverse engineering tasks like binary diffing, code lifting, deobfuscation and patching.},
booktitle = {Proceedings of the 3rd ACM Workshop on Software Protection},
pages = {77–78},
numpages = {2},
keywords = {reverse engineering, framework, disassembly, decompilation, code lifting},
location = {London, United Kingdom},
series = {SPRO'19}
}

@inproceedings{10.1145/3341161.3350841,
author = {Xylogiannopoulos, Konstantinos F. and Karampelas, Panagiotis and Alhajj, Reda},
title = {Text mining for malware classification using multivariate all repeated patterns detection},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3350841},
doi = {10.1145/3341161.3350841},
abstract = {Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {887–894},
numpages = {8},
keywords = {text mining, malware family classification, LERP-RSA, Android malware detection, ARPaD},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3338906.3338956,
author = {Kalhauge, Christian Gram and Palsberg, Jens},
title = {Binary reduction of dependency graphs},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338956},
doi = {10.1145/3338906.3338956},
abstract = {Delta debugging is a technique for reducing a failure-inducing input to a small input that reveals the cause of the failure. This has been successful for a wide variety of inputs including C programs, XML data, and thread schedules. However, for input that has many internal dependencies, delta debugging scales poorly. Such input includes C#, Java, and Java bytecode and they have presented a major challenge for input reduction until now. In this paper, we show that the core challenge is a reduction problem for dependency graphs, and we present a general strategy for reducing such graphs. We combine this with a novel algorithm for reduction called Binary Reduction in a tool called J-Reduce for Java bytecode. Our experiments show that our tool is 12x faster and achieves more reduction than delta debugging on average. This enabled us to create and submit short bug reports for three Java bytecode decompilers.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {556–566},
numpages = {11},
keywords = {reduction, dependencies, Debugging},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3340422.3343639,
author = {Kumar, Lov and Hota, Chinmay and Mahindru, Arvind and Neti, Lalita Bhanu Murthy},
title = {Android Malware Prediction Using Extreme Learning Machine with Different Kernel Functions},
year = {2019},
isbn = {9781450368490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340422.3343639},
doi = {10.1145/3340422.3343639},
abstract = {Android is currently the most popular smartphone platform which occupied 88% of global sale by the end of 2nd quarter 2018. With the popularity of these applications, it is also inviting cybercriminals to develop malware application for accessing important information from smartphones. The major objective of cybercriminals to develop Malware apps or Malicious apps to threaten the organization privacy data, user privacy data, and device integrity. Early identification of such malware apps can help the android user to save private data and device integrity. In this study, features extracted from intermediate code representations obtained using decompilation of APK file are used for providing requisite input data to develop the models for predicting android malware applications. These models are trained using extreme learning with multiple kernel functions ans also compared with the model trained using most frequently used classifiers like linear regression, decision tree, polynomial regression, and logistic regression. This paper also focuses on the effectiveness of data sampling techniques for balancing data and feature selection methods for selecting right sets of significant uncorrelated metrics. The high-value of accuracy and AUC confirm the predicting capability of data sampling, sets of metrics, and training algorithms to malware and normal applications.},
booktitle = {Proceedings of the 15th Asian Internet Engineering Conference},
pages = {33–40},
numpages = {8},
keywords = {Parallel Computing, Object-Oriented Metrics, Maintainability, Genetics algorithm, Artificial neural network},
location = {Phuket, Thailand},
series = {AINTEC '19}
}

@inproceedings{10.1145/3321705.3329833,
author = {Erinfolami, Rukayat Ayomide and Prakash, Aravind},
title = {DeClassifier: Class-Inheritance Inference Engine for Optimized C++ Binaries},
year = {2019},
isbn = {9781450367523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321705.3329833},
doi = {10.1145/3321705.3329833},
abstract = {Recovering class inheritance from C++ binaries has several security benefits including in solving problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining---or worse exclusion---due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5% and 71.4% true positive directed edges in the class hierarchy tree (CHT) under O0 and O2 optimizations respectively, (2) a combination of constructor-destructor (ctor-dtor) analysis provides a substantial improvement in inheritance inference than constructor-only (ctor-only) analysis.},
booktitle = {Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security},
pages = {28–40},
numpages = {13},
keywords = {software reverse engineering, class hierarchy recovery},
location = {Auckland, New Zealand},
series = {Asia CCS '19}
}

@article{10.1145/3306204,
author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
title = {Predicting program properties from 'big code'},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3306204},
doi = {10.1145/3306204},
abstract = {We present a new approach for predicting program properties from large codebases (aka "Big Code"). Our approach learns a probabilistic model from "Big Code" and uses this model to predict properties of new, unseen programs.The key idea of our work is to transform the program into a representation that allows us to formulate the problem of inferring program properties as structured prediction in machine learning. This enables us to leverage powerful probabilistic models such as Conditional Random Fields (CRFs) and perform joint prediction of program properties.As an example of our approach, we built a scalable prediction engine called JSNICE for solving two kinds of tasks in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63% of name identifiers and its type annotation predictions are correct in 81% of cases. Since its public release at http://jsnice.org, JSNice has become a popular system with hundreds of thousands of uses.By formulating the problem of inferring program properties as structured prediction, our work opens up the possibility for a range of new "Big Code" applications such as de-obfuscators, decompilers, invariant generators, and others.},
journal = {Commun. ACM},
month = feb,
pages = {99–107},
numpages = {9}
}

@inproceedings{10.1145/3293880.3294102,
author = {Roessle, Ian and Verbeek, Freek and Ravindran, Binoy},
title = {Formally verified big step semantics out of x86-64 binaries},
year = {2019},
isbn = {9781450362221},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293880.3294102},
doi = {10.1145/3293880.3294102},
abstract = {This paper presents a methodology for generating formally proven equivalence theorems between decompiled x86-64 machine code and big step semantics. These proofs are built on top of two additional contributions. First, a robust and tested formal x86-64 machine model containing small step semantics for 1625 instructions. Second, a decompilation-into-logic methodology supporting both x86-64 assembly and machine code at large scale. This work enables black-box binary verification, i.e., formal verification of a binary where source code is unavailable. As such, it can be applied to safety-critical systems that consist of legacy components, or components whose source code is unavailable due to proprietary reasons. The methodology minimizes the trusted code base by leveraging machine-learned semantics to build a formal machine model. We apply the methodology to several case studies, including binaries that heavily rely on the SSE2 floating-point instruction set, and binaries that are obtained by compiling code that is obtained by inlining assembly into C code.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {181–195},
numpages = {15},
keywords = {x86-64, theorem proving, semantics},
location = {Cascais, Portugal},
series = {CPP 2019}
}

@article{10.1145/3276486,
author = {Grech, Neville and Kong, Michael and Jurisevic, Anton and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
title = {MadMax: surviving out-of-gas conditions in Ethereum smart contracts},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276486},
doi = {10.1145/3276486},
abstract = {Ethereum is a distributed blockchain platform, serving as an ecosystem for smart contracts: full-fledged inter-communicating programs that capture the transaction logic of an account. Unlike programs in mainstream languages, a gas limit restricts the execution of an Ethereum smart contract: execution proceeds as long as gas is available. Thus, gas is a valuable resource that can be manipulated by an attacker to provoke unwanted behavior in a victim's smart contract (e.g., wasting or blocking funds of said victim). Gas-focused vulnerabilities exploit undesired behavior when a contract (directly or through other interacting contracts) runs out of gas. Such vulnerabilities are among the hardest for programmers to protect against, as out-of-gas behavior may be uncommon in non-attack scenarios and reasoning about it is far from trivial.  In this paper, we classify and identify gas-focused vulnerabilities, and present MadMax: a static program analysis technique to automatically detect gas-focused vulnerabilities with very high confidence. Our approach combines a control-flow-analysis-based decompiler and declarative program-structure queries. The combined analysis captures high-level domain-specific concepts (such as "dynamic data structure storage" and "safely resumable loops") and achieves high precision and scalability. MadMax analyzes the entirety of smart contracts in the Ethereum blockchain in just 10 hours (with decompilation timeouts in 8% of the cases) and flags contracts with a (highly volatile) monetary value of over $2.8B as vulnerable. Manual inspection of a sample of flagged contracts shows that 81% of the sampled warnings do indeed lead to vulnerabilities, which we report on in our experiment.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {116},
numpages = {27},
keywords = {Smart Contracts, Security, Program Analysis, Blockchain}
}

@inproceedings{10.1145/3212480.3212487,
author = {Feichtner, Johannes and Missmann, David and Spreitzer, Raphael},
title = {Automated Binary Analysis on iOS: A Case Study on Cryptographic Misuse in iOS Applications},
year = {2018},
isbn = {9781450357319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3212480.3212487},
doi = {10.1145/3212480.3212487},
abstract = {A wide range of mobile applications for Apple's iOS platform process sensitive data and, therefore, rely on protective mechanisms natively provided by the operating system. A wrong application of cryptography or security-critical APIs, however, exposes secrets to unrelated parties and undermines the overall security.We introduce an approach for uncovering cryptographic misuse in iOS applications. We present a way to decompile 64-bit ARM binaries to their LLVM intermediate representation (IR). Based on the reverse-engineered code, static program slicing is applied to determine the data flow in relevant code segments. For this analysis to be most accurate, we propose an adapted version of Andersen's pointer analysis, capable of handling decompiled LLVM IR code with type information recovered from the binary. To finally highlight the improper usage of cryptographic APIs, a set of predefined security rules is checked against the extracted execution paths. As a result, we are not only able to confirm the existence of problematic statements in iOS applications but can also pinpoint their origin.To evaluate the applicability of our solution and to disclose possible weaknesses, we conducted a manual and automated inspection on a set of iOS applications that include cryptographic functionality. We found that 343 out of 417 applications (82%) are subject to at least one security misconception. Among the most common flaws are the usage of non-random initialization vectors and constant encryption keys as input to cryptographic primitives.},
booktitle = {Proceedings of the 11th ACM Conference on Security &amp; Privacy in Wireless and Mobile Networks},
pages = {236–247},
numpages = {12},
keywords = {iOS, Reverse Engineering, Program Analysis, Cryptographic Misuse},
location = {Stockholm, Sweden},
series = {WiSec '18}
}

@article{10.1145/3191737,
author = {Classen, Jiska and Wegemer, Daniel and Patras, Paul and Spink, Tom and Hollick, Matthias},
title = {Anatomy of a Vulnerable Fitness Tracking System: Dissecting the Fitbit Cloud, App, and Firmware},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191737},
doi = {10.1145/3191737},
abstract = {Fitbit fitness trackers record sensitive personal information, including daily step counts, heart rate profiles, and locations visited. By design, these devices gather and upload activity data to a cloud service, which provides aggregate statistics to mobile app users. The same principles govern numerous other Internet-of-Things (IoT) services that target different applications. As a market leader, Fitbit has developed perhaps the most secure wearables architecture that guards communication with end-to-end encryption. In this article, we analyze the complete Fitbit ecosystem and, despite the brand's continuous efforts to harden its products, we demonstrate a series of vulnerabilities with potentially severe implications to user privacy and device security. We employ a range of techniques, such as protocol analysis, software decompiling, and both static and dynamic embedded code analysis, to reverse engineer previously undocumented communication semantics, the official smartphone app, and the tracker firmware. Through this interplay and in-depth analysis, we reveal how attackers can exploit the Fitbit protocol to extract private information from victims without leaving a trace, and wirelessly flash malware without user consent. We demonstrate that users can tamper with both the app and firmware to selfishly manipulate records or circumvent Fitbit's walled garden business model, making the case for an independent, user-controlled, and more secure ecosystem. Finally, based on the insights gained, we make specific design recommendations that can not only mitigate the identified vulnerabilities, but are also broadly applicable to securing future wearable system architectures.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {5},
numpages = {24},
keywords = {Nexmon, Wearables, firmware reverse engineering, health}
}

@inproceedings{10.1145/3176258.3176319,
author = {Senthivel, Saranyan and Dhungana, Shrey and Yoo, Hyunguk and Ahmed, Irfan and Roussev, Vassil},
title = {Denial of Engineering Operations Attacks in Industrial Control Systems},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176319},
doi = {10.1145/3176258.3176319},
abstract = {We present a new type of attack termed denial of engineering operations in which an attacker can interfere with the normal cycle of an engineering operation leading to a loss of situational awareness. Specifically, the attacker can deceive the engineering software during attempts to retrieve the ladder logic program from a programmable logic controller (PLC) by manipulating the ladder logic on the PLC, such that the software is unable to process it while the PLC continues to execute it successfully. This attack vector can provide sufficient cover for the attacker»s actual scenario to play out while the owner tries to understand the problem and reestablish positive operational control. To enable the forensic analysis and, eventually, eliminate the threat, we have developed the first decompiler for ladder logic programs.Ladder logic is a graphical programming language for PLCs that control physical processes such as power grid, pipelines, and chemical plants; PLCs are a common target of malicious modifications leading to the compromise of the control behavior (and potentially serious consequences). Our decompiler, Laddis, transforms a low-level representation to its corresponding high-level original representation comprising of graphical symbols and connections. The evaluation of the accuracy of the decompiler on the program of varying complexity demonstrates perfect reconstruction of the original program. We present three new attack scenarios on PLC-deployed ladder logic and demonstrate the effectiveness of the decompiler on these scenarios.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {319–329},
numpages = {11},
keywords = {scada, protocol reverse engineering, plc, ladder logic, industrial control system, forensics, disassembler},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.1145/3098954.3103152,
author = {Kirchner, Kevin and Rosenthaler, Stefan},
title = {bin2llvm: Analysis of Binary Programs Using LLVM Intermediate Representation},
year = {2017},
isbn = {9781450352574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098954.3103152},
doi = {10.1145/3098954.3103152},
abstract = {The analysis of programs is an important and well researched topic in information security, specifically for finding bugs in binary programs or analyzing malicious software. Many commonly used techniques rely on dynamic analysis by running samples and monitoring their behavior or are based on the cumbersome and time consuming inspection of plain assembly code.In this paper we present a novel approach for static analysis we are using in bin2llvm, a work-in-progress analysis framework, in order to find and identify cryptographic routines in binary programs. Our approach does not need to run the target of analysis in any way and is based on decompilation of binaries to an intermediate language similar to assembly code, the LLVM Intermediate Representation (IR), by using the open source decompiler Dagger. After decompilation we are able to apply various analysis techniques to the resulting code. These methods can be easily implemented and extended as optimization passes for the LLVM optimizer and can therefore benefit from its extensive API. Although we discovered certain drawbacks and issues with this approach, our results and proof of concept show that IR code is a very suitable target for analyses and it is well worth driving research further into this topic.},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security},
articleno = {45},
numpages = {7},
keywords = {LLVM, Intermediate Representation, Decompiler, Cryptography, Binary, Analysis},
location = {Reggio Calabria, Italy},
series = {ARES '17}
}

@inproceedings{10.1145/3019612.3019926,
author = {Kim, Gyoosik},
title = {On computing similarity of android executables using text mining: student research abstract},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019926},
doi = {10.1145/3019612.3019926},
abstract = {According to Comscore1, Android users in the U.S spend an average of 2.8 hours per day using mobile media. On the other hand, according to Statista reports2, Android users were able to choose between 2.2 million applications on June 2016. Among these applications, there are ones reported by Google Android Security Service3 as malware, virus, or illegal theft. Many tools such as Dex2Jar4, apktool5, and jd-gui6 analyze and reverse engineer Android applications and can be used to illegally copy or transform the applications as well. In order to protect applications from piracy or illegal theft, it is necessary to detect theft by measuring application similarity. In the literature, previous studies on theft detection have measured application similarity at two levels, source or executable code level, which have some limitations. Source codes are not available if the codes are legacy one or are developed by upstream suppliers. In the case of the executable codes, application similarity is measured 1) using the source codes decompiled from the executables, or 2) using the characteristics extracted from the executables (i.e., birthmark). For example, DroidMoss [5] applied a fuzzy hashing technique to effectively localize and detect the changes from app-repackaging behavior. Reference [4] proposed software birthmarks to show the unique characteristics of a program and detected software theft based on the birthmarks.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {1761–1762},
numpages = {2},
location = {Marrakech, Morocco},
series = {SAC '17}
}

