@article{arXiv:2306.14168,
  title = {FastBCSD: Fast and Efficient Neural Network for Binary Code Similarity Detection},
  author = {Chensen Huang and Guibo Zhu and Guojing Ge and Taihao Li and Jinqiao Wang},
  abstract = {Binary code similarity detection (BCSD) has various applications, including but not limited to vulnerability detection, plagiarism detection, and malware detection. Previous research efforts mainly focus on transforming binary code to assembly code strings using reverse compilation and then using pre-trained deep learning models with large parameters to obtain feature representation vector of binary code. While these models have proven to be effective in representing binary code, their large parameter size leads to considerable computational expenses during both training and inference. In this paper, we present a lightweight neural network, called FastBCSD, that employs a dynamic instruction vector encoding method and takes only assembly code as input feature to achieve comparable accuracy to the pre-training models while reducing the computational resources and time cost. On the BinaryCorp dataset, our method achieves a similar average MRR score to the state-of-the-art pre-training-based method (jTrans), while on the BinaryCorp 3M dataset, our method even outperforms the latest technology by 0.01. Notably, FastBCSD has a much smaller parameter size (13.4M) compared to jTrans (87.88M), and its latency time is 1/5 of jTrans on NVIDIA GTX 1080Ti.},
  journal = {arXiv preprint arXiv:2306.14168},
  year = {2023},
  month = {06},
  eprint = {2306.14168},
  archivePrefix = {arXiv},
  primaryClass = {cs.CR},
  categories = {cs.CR},
  updated = {2023-06-25T08:22:10Z},
  published = {2023-06-25T08:22:10Z},
  url = {https://arxiv.org/abs/2306.14168v1},
  pdf = {https://arxiv.org/pdf/2306.14168v1},
}

@article{arXiv:2304.04658,
  title = {GraphBinMatch: Graph-based Similarity Learning for Cross-Language Binary and Source Code Matching},
  author = {Ali TehraniJamsaz and Hanze Chen and Ali Jannesari},
  abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate our approach performance on single-language binary-to-source code matching. Experimental results show that GraphBinMatch outperforms state-of-the-art significantly, with improvements as high as 15\% over the F1 score.},
  journal = {arXiv preprint arXiv:2304.04658},
  year = {2023},
  month = {04},
  eprint = {2304.04658},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE},
  updated = {2023-04-10T15:36:31Z},
  published = {2023-04-10T15:36:31Z},
  url = {https://arxiv.org/abs/2304.04658v1},
  pdf = {https://arxiv.org/pdf/2304.04658v1},
}

@article{arXiv:2509.14646,
  title = {SALT4Decompile: Inferring Source-level Abstract Logic Tree for LLM-Based Binary Decompilation},
  author = {Yongpan Wang and Xin Xu and Xiaojie Zhu and Xiaodong Gu and Beijun Shen},
  abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose \\saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of \\saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, \\saltm constructs a Source-level Abstract Logic Tree (\\salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed \\salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare \\saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that \\saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\\\% TCP rate on Decompile-Eval with a 10.6\\\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
  journal = {arXiv preprint arXiv:2509.14646},
  year = {2025},
  month = {09},
  eprint = {2509.14646},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.PL},
  comment = {13 pages, 7 figures},
  updated = {2025-09-18T05:57:15Z},
  published = {2025-09-18T05:57:15Z},
  url = {https://arxiv.org/abs/2509.14646v1},
  pdf = {https://arxiv.org/pdf/2509.14646v1},
}

@article{arXiv:2201.07420,
  title = {Cross-Language Binary-Source Code Matching with Intermediate Representations},
  author = {Yi Gui and Yao Wan and Hongyu Zhang and Huifang Huang and Yulei Sui and Guandong Xu and Zhiyuan Shao and Hai Jin},
  abstract = {Binary-source code matching plays an important role in many security and software engineering related tasks such as malware detection, reverse engineering and vulnerability assessment. Currently, several approaches have been proposed for binary-source code matching by jointly learning the embeddings of binary code and source code in a common vector space. Despite much effort, existing approaches target on matching the binary code and source code written in a single programming language. However, in practice, software applications are often written in different programming languages to cater for different requirements and computing platforms. Matching binary and source code across programming languages introduces additional challenges when maintaining multi-language and multi-platform applications. To this end, this paper formulates the problem of cross-language binary-source code matching, and develops a new dataset for this new problem. We present a novel approach XLIR, which is a Transformer-based neural network by learning the intermediate representations for both binary and source code. To validate the effectiveness of XLIR, comprehensive experiments are conducted on two tasks of cross-language binary-source code matching, and cross-language source-source code matching, on top of our curated dataset. Experimental results and analysis show that our proposed XLIR with intermediate representations significantly outperforms other state-of-the-art models in both of the two tasks.},
  journal = {arXiv preprint arXiv:2201.07420},
  year = {2022},
  month = {01},
  eprint = {2201.07420},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI},
  comment = {SANER2022},
  updated = {2022-01-19T05:17:02Z},
  published = {2022-01-19T05:17:02Z},
  url = {https://arxiv.org/abs/2201.07420v1},
  pdf = {https://arxiv.org/pdf/2201.07420v1},
}

@article{arXiv:2405.19581,
  title = {Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases},
  author = {Zian Su and Xiangzhe Xu and Ziyang Huang and Kaiyuan Zhang and Xiangyu Zhang},
  abstract = {Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3\% relative gain in CHRF and a 16.7\% relative gain in a GPT4-based metric for summarization, as well as a 6.7\% and 7.4\% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.},
  journal = {arXiv preprint arXiv:2405.19581},
  year = {2024},
  month = {05},
  eprint = {2405.19581},
  archivePrefix = {arXiv},
  primaryClass = {cs.SE},
  categories = {cs.SE cs.AI cs.CL},
  updated = {2024-10-30T16:12:36Z},
  published = {2024-05-30T00:17:44Z},
  url = {https://arxiv.org/abs/2405.19581v2},
  pdf = {https://arxiv.org/pdf/2405.19581v2},
}

