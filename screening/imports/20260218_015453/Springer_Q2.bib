@incollection{springer_10_1007_978_3_032_15700_3_8,
  title = {Forward Symbolic Execution for Trustworthy Automation of Binary Code Verification},
  author = {Lindner, Andreas and Palmskog, Karl and Constable, Scott and Dam, Mads and Guanciale, Roberto and Nemati, Hamed},
  booktitle = {Lecture Notes in Computer Science},
  year = {2026},
  pages = {147-172},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-032-15700-3\_8},
  url = {https://doi.org/10.1007/978-3-032-15700-3\_8},
  abstract = {Control flow in unstructured programs can be complex and dynamic, which makes static analysis difficult. Yet, automated reasoning about unstructured control flow is important when certifying properties of binary (machine) code in trustworthy systems, e.g., cryptographic routines. We present a theory of forward symbolic execution for unstructured programs suitable for use in theorem provers that enables automated verification of both functional and non-functional program properties. The theory’s foundation is a set of inference rules where each member corresponds to an operation in a symbolic execution engine. The rules are designed to give control over the tradeoff between the preservation of precision and introduction of overapproximation. We instantiate our theory for BIR, a previously proposed intermediate language for binary analysis. We demonstrate how symbolic executors can be constructed for BIR with common optimizations such as pruning of infeasible symbolic states. We implemented our theory in the HOL4 theorem prover using the HolBA binary analysis library, obtaining machine-checked proofs of soundness of symbolic execution for BIR. We practically evaluated two applications of our theory: verification of functional properties of RISC-V binaries and verification of execution time bounds of programs running on the ARM Cortex-M0 processor. The evaluation shows that such verification can be automated with moderate overhead on medium-sized programs.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_981_96_4566_4_11,
  title = {Automatic Software Vulnerability Detection in Binary Code},
  author = {Liu, Shigang and Li, Lin and Ban, Xinbo and Chen, Chao and Zhang, Jun and Camtepe, Seyit and Xiang, Yang},
  booktitle = {Lecture Notes in Computer Science},
  year = {2025},
  pages = {148-166},
  publisher = {Springer Nature Singapore},
  doi = {10.1007/978-981-96-4566-4\_11},
  url = {https://doi.org/10.1007/978-981-96-4566-4\_11},
  abstract = {Cybersecurity is critical in today’s digital world, where the severity of threats from software vulnerabilities grows significantly each year. Many techniques have been developed to analyze vulnerabilities in source code. However, source code is not always available (for example, most industry software is closed-source). As a result, analyzing vulnerabilities in binary code becomes necessary and more challenging. This paper presents a novel approach called BiVulD for detecting vulnerabilities at the binary level. BiVulD has three phases: generating assembly language instructions, learning good embeddings, and building a prediction model. First, we create a database of vulnerable binaries using CVE and NVD. Next, we propose using codeBERT to obtain good embeddings. Finally, we apply a bidirectional LSTM on top of codeBERT to build the predictive model. To demonstrate BiVulD’s effectiveness, we compared it with several baselines, including source code-based, binary code-based, and machine learning-based techniques on real-world projects. The experimental results show that BiVulD outperforms the baselines and can detect more vulnerabilities. For instance, BiVulD achieves at least 20\% improvement in Precision, Recall, and F-measure. We believe this work will serve as a foundation for future research in vulnerability detection using only binary code.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_031_72322_3_4,
  title = {LEARNT: A Neural Machine Translation Framework for Accurate Binary Lifting to High-Level Representation},
  author = {Baasantogtokh, Duulga and Yoon, Yoseob and Batzorig, Munkhdelgerekh and Sahlabadi, Mahdi and Yim, Kangbin},
  booktitle = {Lecture Notes on Data Engineering and Communications Technologies},
  year = {2024},
  pages = {33-44},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-72322-3\_4},
  url = {https://doi.org/10.1007/978-3-031-72322-3\_4},
  abstract = {In binary analysis, performing static analyses on architecture-agnostic intermediate representation is efficient and strongly demanded. Sound and accurate Low-Level Virtual Machine Intermediate Representation (LLVM IR) lifted from binary could make the reuse of dozens of existing analysis programs of the LLVM ecosystem possible. However, current binary lifters lack the resources to improve manually developed lifting rules and develop more of them. This work aims to solve the problem of lifting low-level language to sound high-level Intermediate Representation (IR) as a formal language translation problem, enabling automatic learning of binary lifting. Therefore, we propose a neural machine translation-based binary lifting framework named LEARNT with a parallel corpus generation method leveraging a compiler. The evaluation results show that LEARNT’s average translation accuracy is 93\%, which proves that translation rules automatically learned by LEARNT are sound.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_031_20738_9_4,
  title = {Cross Architecture Function Similarity Detection with Binary Lifting and Neural Metric Learning},
  author = {Tian, Zhenzhou and Li, Chen and Qiu, Sihao},
  booktitle = {Lecture Notes on Data Engineering and Communications Technologies},
  year = {2023},
  pages = {27-34},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-031-20738-9\_4},
  url = {https://doi.org/10.1007/978-3-031-20738-9\_4},
  abstract = {Binary code similarity detection has extensive and important applications in IoT device security, yet which suffers the challenges from the differentiated underlying architectures of the diverse IoT devices. To this end, this paper presents XFSim ( Cross -architecture F unction-level binary code Sim ilarity detection), through binary lifting and neural similarity metric learning. Firstly, to make the detection method architecture agnostic, the binaries to be analyzed are lifted to an intermediate code called LLVM-IR and normalized for an uniform representation, so as to alleviate the discrepancies between the raw assemblies of different instruction set architectures (ISAs). Secondly, we utilize FastText, a widely used word embedding algorithm, that learns on the functions’ normalized intermediate codes to obtain high quality token embeddings. Then, an efficient CNN-based model is utilized to encode the semantics of each function into numerical vectors, meanwhile the siamese neural network structure is resorted to supervise the whole model training, with the goal of minimizing the contrastive loss. Finally, the similarity of two binary code snippets can measured by the cosine similarity of their encoded vectors. The experiments conducted on a public dataset show that, the strategy of lifting and normalizing the assemblies to uniform representations greatly alleviates the semantic-gaps between different ISAs, and XFSim outperforms two existing cross-architecture binary code similarity detectors.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s10207_025_01148_3,
  title = {A cross-language and cross-binary type approach to binary-source software composition analysis using BM25},
  author = {Kim, Jong-Wouk and Choi, Mi-Jung},
  journal = {International Journal of Information Security},
  year = {2025},
  volume = {24},
  number = {6},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10207-025-01148-3},
  url = {https://doi.org/10.1007/s10207-025-01148-3},
  abstract = {Software composition analysis (SCA) involves analyzing open-source software (OSS) components used in software development to identify license compliance issues and security vulnerabilities. It helps developers mitigate the legal and security risks associated with OSS, ensuring safer and more reliable software. Traditional SCA methods often require users to upload their source code to an SCA server for inspection, which then generates reports on component usage. However, vendors have significant concerns about this approach because source code often includes sensitive information like proprietary algorithms, core business logic, and confidential user data. To address these challenges, various SCA techniques, such as binary-binary SCA and binary-source SCA, have been proposed, though most are limited to specific programming languages. Existing SCA frameworks primarily focus on C/C + + and Java. This paper introduces the first binary-source SCA method capable of analyzing three binary types across five programming languages (C/C + + , Objective-C, Swift, Go). Our approach utilizes BM25-based text tokens, significantly reducing computational cost while maintaining detection performance. Empirical results demonstrate that our method achieves up to 81.4\% recall in identifying reused OSS components, providing an efficient and scalable solution for secure software development. Additionally, this study highlights the limitations of the proposed method and suggests future research directions to address these challenges.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_031_97620_9_6,
  title = {CodeGrafter: Unifying Source and Binary Graphs for Robust Vulnerability Detection},
  author = {Irtiza, Saquib and Zamani, Mahmoud and Wickramasuriya, Shamila and Hamlen, Kevin W. and Khan, Latifur},
  booktitle = {Lecture Notes in Computer Science},
  year = {2025},
  pages = {96-117},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-97620-9\_6},
  url = {https://doi.org/10.1007/978-3-031-97620-9\_6},
  abstract = {CodeGrafter is a novel framework for detecting security vulnerabilities in compiled C/C++ programs by integrating source- and binary-level code features into a unified Cross-Domain Code Property Graph (CDCPG). By combining the high-level semantic insights from source code with the detailed low-level information from compiled assembly, CodeGrafter uncovers vulnerabilities that are not detectable via source analysis or binary analysis alone. By combining both, it examines compiler decisions, such as dead code elimination, build-environment-dependent semantics (e.g., macros and pragmas), and compiler-generated interface code, to avoid false positives and false negatives in its analysis. For example, it can detect Points of Interests (POIs) where vulnerability severity is influenced by compilation-specific factors, such as stack layouts that place critical data near buffers. To streamline vulnerability detection, CodeGrafter represents these POIs as graphs and leverages Graph Neural Networks (GNNs) to significantly reduce manual auditing effort. Evaluations on six real-world applications demonstrate that CodeGrafter outperforms prior works that rely solely on source or binary-level representations alone, achieving an F1-score of 0.937 and a recall of 0.945 in identifying vulnerable functions.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s10836_025_06191_5,
  title = {PrecIRisc: A High-Precision and Low-Bloat Dynamic Binary Instrumentation Tailored for RISC Architectures},
  author = {Xie, Wenbing and Guan, Ruixue and Yu, Fanyue and Zhang, Yiming},
  journal = {Journal of Electronic Testing},
  year = {2025},
  volume = {41},
  number = {4},
  pages = {483-502},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10836-025-06191-5},
  url = {https://doi.org/10.1007/s10836-025-06191-5},
  abstract = {Dynamic binary instrumentation (DBI) tools are essential for program analysis and error detection. The Disassemble-and-Resynthesise (D\&R) method, a key DBI technique, translates binary code into an intermediate representation (IR) for consistent instrumentation, minimizing architecture-specific dependencies. However, D\&R results in significant code bloat, which reduces runtime efficiency compared to native code. Additionally, the complexity of D\&R requires developers to understand both the program’s execution and the DBI framework, complicating practical application. To address these challenges, we propose a D\&R-based DBI framework that optimizes VEX IR and ensures precise debugging. We present a low-bloat optimization mechanism, including redundant copy elimination, IR type extension, and symbolic self-extension. Additionally, we propose a precise debugging and diagnostic method based on memory value consistency comparison, ensuring that code behaves consistently before and after instrumentation. We implement our design in PrecIRisc, a Valgrind-based DBI system, and evaluate PrecIRisc on SW64 and MIPS architectures. Benchmarks show that PrecIRisc achieves a 14.7\% code bloat reduction on SW64 compared to Valgrind for x86. Meanwhile, results show a 3.88\% code bloat reduction on MIPS compared to the original Valgrind. Further testing with code mutations and real-world programs confirms PrecIRisc’s effectiveness in debugging and diagnosis.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_031_94448_2_6,
  title = {Is This the Same Code? A Comprehensive Study of Decompilation Techniques for WebAssembly Binaries},
  author = {Wu, Wei-Cheng and Yan, Yutian and Egilsson, Hallgrimur David and Park, David and Chan, Steven and Hauser, Christophe and Wang, Weihang},
  booktitle = {Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
  year = {2026},
  pages = {108-130},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-94448-2\_6},
  url = {https://doi.org/10.1007/978-3-031-94448-2\_6},
  abstract = {WebAssembly (abbreviated WASM) is a low-level bytecode language designed for client-side execution in web browsers. As WASM continues to gain widespread adoption and its security concerns, the need for decompilation techniques that recover high-level source code from WASM binaries has grown. However, little research has been done to assess the quality of decompiled code from WASM. This paper aims to fill this gap by conducting a comprehensive comparative analysis between decompiled C code from WASM binaries and state-of-the-art native binary decompilers. To achieve this goal, we presented a novel framework for empirically evaluating C-based decompilers from various aspects, thus assessing the proficiency of WASM decompilers in generating readable and correct code when compared to native binary decompilers. Specifically, we evaluated the decompiled code’s correctness , readability , and structural similarity with the original code from current WASM decompilers. We validated the proposed metrics’ practicality in decompiler assessment and provided insightful observations regarding the characteristics and constraints of existing decompiled code. By encouraging improvements in these tools, we seek to enhance their use in critical tasks such as auditing and sandboxing third-party libraries. This, in turn, contributes to bolstering the security and reliability of software systems that rely on WASM and native binaries.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_031_06975_8_22,
  title = {AndroClonium: Bytecode-Level Code Clone Detection for Obfuscated Android Apps},
  author = {Foroughipour, Ardalan and Stakhanova, Natalia and Abazari, Farzaneh and Sistany, Bahman},
  booktitle = {IFIP Advances in Information and Communication Technology},
  year = {2022},
  pages = {379-397},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-031-06975-8\_22},
  url = {https://doi.org/10.1007/978-3-031-06975-8\_22},
  abstract = {Detecting code clones is essential for many security tasks, e.g., vulnerability detection, malware analysis, legacy software patching. In many of these security scenarios, source code is not available, leaving binary code analysis as the only option. Yet, evaluation of binary code is often exacerbated by the wide use of obfuscation. In this work, we propose an approach for obfuscation-resistant fine-grained detection of code clones in Android apps at the bytecode level. To mitigate inherent constraints of static analysis and to achieve obfuscation resistance, we partially simulate the execution of Android code, and abstract the resulting execution traces. We validate our approach’s ability to detect different types of code clones on a set of 20 injected clones and explore its resistance against obfuscation on a set of 1085 obfuscated apps.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s10664_025_10671_9,
  title = {JNFuzz-Droid: a lightweight fuzzing and taint analysis framework for native code of Android applications},
  author = {Cao, Jianchao and Guo, Fan and Qu, Yanwen},
  journal = {Empirical Software Engineering},
  year = {2025},
  volume = {30},
  number = {5},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10664-025-10671-9},
  url = {https://doi.org/10.1007/s10664-025-10671-9},
  abstract = {The need to account for native code in Android apps is becoming urgent as the usage of native code is growing in both benign and malicious apps. However, most current state-of-the-art analysis tools cannot effectively analyze the data-flow behavior of native code. On the one hand, existing native dynamic analysis tools are primarily based on test input generation tools to analyze Android apps and are therefore unable to locate native code quickly. On the other hand, existing native static analysis tools are based on symbolic execution to analyze native code and are therefore limited by the path and state explosion issues. In order to effectively analyze the behavior of sensitive data in the native code, we first propose JNFuzz , a fuzzing module for Android native libraries based on Client/Server architecture. Then, we propose JNFuzz-Droid , a lightweight automated fuzzing and taint analysis framework for Android native code, based on this. JNFuzz-Droid first locates the Android native code to which sensitive data is passed and then uses JNFuzz to fuzz the native code to improve code coverage while analyzing the data flow in native code with a dynamic binary tool. Experimental results on benchmarks and real-world apps show that JNFuzz-Droid can effectively detect the leakage or transfer of sensitive data in app native code and outperforms the state-of-the-art native analysis tools.},
  content_type = {Article},
}

@article{springer_10_1007_s10009_021_00644_w,
  title = {SaBRe: load-time selective binary rewriting},
  author = {Arras, Paul-Antoine and Andronidis, Anastasios and Pina, Luís and Mituzas, Karolis and Shu, Qianyi and Grumberg, Daniel and Cadar, Cristian},
  journal = {International Journal on Software Tools for Technology Transfer},
  year = {2022},
  volume = {24},
  number = {2},
  pages = {205-223},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10009-021-00644-w},
  url = {https://doi.org/10.1007/s10009-021-00644-w},
  abstract = {Abstract Binary rewriting consists in disassembling a program to modify its instructions. However, existing solutions suffer from shortcomings in terms of soundness and performance. We present SaBRe , a load-time system for selective binary rewriting. SaBRe rewrites specific constructs—particularly system calls and functions—when the program is loaded into memory, and intercepts them using plugins through a simple API. We also discuss the theoretical underpinnings of disassembling and rewriting. We developed two backends—for and —which were used to implement three plugins: a fast system call tracer, a multi-version executor, and a fault injector. Our evaluation shows that SaBRe imposes little overhead, typically below 3\%.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_032_08124_7_27,
  title = {Ali2Vul: Binary Vulnerability Dataset Expansion via Cross-Modal Alignment},
  author = {Bai, Xinyu and Wang, Yisen and Du, Jiajun and Liang, Chen and Liang, Siyuan and Jiang, Zirui},
  booktitle = {Lecture Notes in Computer Science},
  year = {2026},
  pages = {474-493},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-032-08124-7\_27},
  url = {https://doi.org/10.1007/978-3-032-08124-7\_27},
  abstract = {In the context of software supply chain security and IoT device firmware analysis, binary vulnerability detection faces dual challenges of detection efficiency and coverage due to scarce annotated binary data. Although the open-source ecosystem has accumulated vast amounts of source-level vulnerability data, direct migration to binary vulnerability detection inevitably encounters a semantic gap caused by cross-modal representation differences such as compiler optimizations and symbol stripping. To address data scarcity in binary vulnerability detection and bridge the semantic gap in cross-modal matching with source code, this paper proposes a hierarchical semantic fusion framework for binary-source alignment. Through heterogeneous modal semantic bridging and hierarchical attention mechanisms, our approach significantly enhances cross-modal matching precision and scalability between binary and source code, achieving 94.3\% accuracy. Furthermore, we introduce a vulnerability detection task-driven transfer framework that maps source-level vulnerability patterns to binary code feature space via cross-modal alignment. Leveraging dimensional expansion within the model’s knowledge space enables exponential scaling of usable data for binary vulnerability detection, thereby transcending data scarcity constraints. We collected 400 CVEs from 8 real-world vulnerable projects, achieving 80.3\% detection accuracy. This research establishes an effective technical pathway for expanding usable data resources in automated binary vulnerability detection.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_030_34238_8_2,
  title = {Binary Analysis Overview},
  author = {Alrabaee, Saed and Debbabi, Mourad and Shirani, Paria and Wang, Lingyu and Youssef, Amr and Rahimian, Ashkan and Nouh, Lina and Mouheb, Djedjiga and Huang, He and Hanna, Aiman},
  booktitle = {Advances in Information Security},
  year = {2020},
  pages = {7-44},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-34238-8\_2},
  url = {https://doi.org/10.1007/978-3-030-34238-8\_2},
  abstract = {When the source code is unavailable, it is important for security applications, such as malware detection, software license infringement , vulnerability analysis , and digital forensics to be able to efficiently extract meaningful fingerprints from the binary code. Such fingerprints will enhance the effectiveness and efficiency of reverse engineering tasks as they can provide a range of insights into the program binaries. However, a great deal of important information will likely be lost during the compilation process, including variable and function names, the original control and data flow structures, comments, and layout. In this chapter, we provide a comprehensive review of existing binary code fingerprinting frameworks. As such, we systematize the study of binary code fingerprints based on the most important dimensions: the applications that motivate it, the approaches used and their implementations, the specific aspects of the fingerprinting framework, and how the results are evaluated.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_031_82349_7_32,
  title = {frameD: Toward Automated Identification of Embedded Frameworks in Firmware Images},
  author = {van Nielen, Jorik and Peter, Andreas and Continella, Andrea},
  booktitle = {Lecture Notes in Computer Science},
  year = {2025},
  pages = {514-533},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-82349-7\_32},
  url = {https://doi.org/10.1007/978-3-031-82349-7\_32},
  abstract = {In the era of the Internet of Things, firmware security analyses have become tremendously important to protect networks and guarantee safety-critical operations. Indeed, the firmware running on smart devices (which are increasingly adopted also in critical infrastructures) often contains security vulnerabilities, and delivering timely updates proved to be challenging, both from a technical perspective and due to a lack of support from device vendors. In particular, firmware images present difficulties that hinder automated analyses and patching, mostly because their code and data are opaquely intermixed and squashed together on top of embedded development frameworks. In this paper, we propose a new lightweight approach to automatically analyze firmware images and identify the embedded frameworks they are built upon. Our approach facilitates reverse engineering, reducing the scope for security analyses and assisting the vulnerability detection and patching process of embedded devices. We implement our approach in frameD , and we evaluate it on a dataset of 536 firmware images from different devices and vendors. Our system identifies embedded frameworks with an accuracy of 83\%, and we perform a case study to combine frameD with an existing patch injection framework, demonstrating to be a helpful and effective tool for security analysts and reverse engineers.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s43926_023_00045_2,
  title = {A survey on IoT \& embedded device firmware security: architecture, extraction techniques, and vulnerability analysis frameworks},
  author = {Ul Haq, Shahid and Singh, Yashwant and Sharma, Amit and Gupta, Rahul and Gupta, Dipak},
  journal = {Discover Internet of Things},
  year = {2023},
  volume = {3},
  number = {1},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s43926-023-00045-2},
  url = {https://doi.org/10.1007/s43926-023-00045-2},
  abstract = {Abstract IoT and Embedded devices grow at an exponential rate, however, without adequate security mechanisms in place. One of the key challenges in the cyber world is the security of these devices. One of the main reasons that these devices are active targets for large-scale cyber-attacks is a lack of security standards and thorough testing by manufacturers. Manufacturer-specific operating systems or firmware of various architectures and characteristics are typically included with these devices. However, due to a lack of security testing and/or late patching, the underlying firmware or operating systems are vulnerable to numerous types of vulnerabilities. Reverse engineering and in-depth research of the firmware is required to detect the vulnerabilities. In this paper, we've delved into various aspects of IoT and embedded devices. This includes a comprehensive survey on the architecture of firmware, techniques for firmware extraction, and state-of-the-art vulnerability analysis frameworks for the detection of vulnerabilities using various approaches like static, dynamic, and hybrid approaches. Furthermore, we’ve scrutinized the challenges of existing vulnerability analysis frameworks and proposed a novel framework to address these issues.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_031_45933_7_15,
  title = {Decompilation Based Deep Binary-Source Function Matching},
  author = {Wang, Xiaowei and Yuan, Zimu and Xiao, Yang and Wang, Liyan and Yao, Yican and Chen, Haiming and Huo, Wei},
  booktitle = {Lecture Notes in Computer Science},
  year = {2023},
  pages = {244-260},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-45933-7\_15},
  url = {https://doi.org/10.1007/978-3-031-45933-7\_15},
  abstract = {Binary and source matching is vital for vulnerability detection or program comprehension. Most existing works focus on library matching (coarse-grained) by utilizing some simple features. However, they are so coarse-grained that high false positives occur since developers tend to reuse source code library partly. These shortcomings drive us to perform fine-grained matching (i.e., binary and source function matching). At the same time, due to the enormous differences between the form of binary and source functions, function matching (fine-grained) meets huge challenges. In this work, inspired by the decompilation technique and advanced neural networks, we propose tool, a D ecompilation based deep B inary- S ource function M atching framework. Specifically, we take the triplet features from both binary pseudo-code and source code functions as input, which are extracted from code property graph and can represent both the syntactic and semantic information. In this way, the binary and source functions are represented in the same feature space so to ease the matching model to learn function similarity. For the matching model, we adopt a self-attention based siamese network with contrastive loss. Experiments on two datasets, R 0 and R 3, show that our tool achieves consistent improvements than other methods, which demonstrate the effectiveness of our self-attention based matching model, and our triplets features can well capture the two kinds of code functions. Our work improves the accuracy of binary and source code matching, which in turn enables us to better address security issues such as vulnerability detection and program comprehension.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s10207_021_00568_1,
  title = {Data space randomization for securing cyber-physical systems},
  author = {Potteiger, Bradley and Cai, Feiyang and Zhang, Zhenkai and Koutsoukos, Xenofon},
  journal = {International Journal of Information Security},
  year = {2022},
  volume = {21},
  number = {3},
  pages = {597-610},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10207-021-00568-1},
  url = {https://doi.org/10.1007/s10207-021-00568-1},
  abstract = {Non-control data attacks have become widely popular for circumventing authentication mechanisms in websites, servers, and personal computers. These attacks can be executed against cyber-physical systems (CPSs) in which not only authentication is an issue, but safety is at risk. Furthermore, any unauthorized change to safety-critical variables within the software may cause damage or even catastrophic consequences. Moving target defense techniques such as data space randomization (DSR) have become popular for protecting against memory corruption attacks such as non-control data attacks. However, current DSR implementations rely on source code transformations and do not stop critical variables from being overwritten, only that the new overwritten value will be vastly different than expected by the attacker. As such, these implementations are often ineffective for legacy CPS software in which only a binary is available. The problem addressed in this paper is how do we protect against non-control data attacks in legacy CPS software while ensuring that we can detect instances of variable integrity violations. We solve this problem by combining DSR at the binary level with variable comparison checks to ensure that we can detect and mitigate any attacker attempt to overwrite safety-critical variables. Our security approach is demonstrated utilizing an autonomous emergency braking system case study.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_030_92571_0_6,
  title = {Raising MIPS Binaries to LLVM IR},
  author = {Romana, Sandeep and Bandgar, Anil D. and Kumar, Mohit and Patil, Mahesh U. and Lakshmi Eswari, P. R.},
  booktitle = {Lecture Notes in Computer Science},
  year = {2021},
  pages = {94-108},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-92571-0\_6},
  url = {https://doi.org/10.1007/978-3-030-92571-0\_6},
  abstract = {The need for automated, scalable and machine speed analysis is significant with the ever-increasing quantity of code that requires security analysis. Recent advancements in technology demonstrate the possibility of automated analysis of binaries by raising/lifting/translating them to an intermediate representation. This paper describes the efforts towards developing utilities for raising MIPS binaries to an intermediate representation (IR) of LLVM. Using LLVM-IR, one can leverage the existing utilities built over LLVM for performing automated analysis of lifted code. The implemented utilities extend open-source tools McSema and Remill for MIPS ISA. The paper presents the methodology of raising the MIPS binaries as a systematically arranged step by step procedure. While presenting the procedure, the text highlights the challenges faced during each of these translation steps. The results from the two test suites demonstrate that the implemented static binary translation (SBT) utilities can produce the LLVM-IR that can be analysed or recompiled back to an executable form.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s10207_024_00953_6,
  title = {Speeding-up fuzzing through directional seeds},
  author = {Koffi, Koffi Anderson and Kampourakis, Vyron and Kolias, Constantinos and Song, Jia and Ivans, Robert C.},
  journal = {International Journal of Information Security},
  year = {2025},
  volume = {24},
  number = {2},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10207-024-00953-6},
  url = {https://doi.org/10.1007/s10207-024-00953-6},
  abstract = {Abstract Fuzzing is an automated process for discovering inputs in a program that may trigger unexpected behavior. Today, fuzzing has become a standard practice for the discovery of bugs and security vulnerabilities. However, the main issue with such practices is that the exploration of the input space of programs can often be prohibitively expensive. Therefore, several alternative fuzzing strategies have been introduced during the last few years. Some fuzzing techniques rely on human expertise to provide a plausible set of initial input examples, namely, seeds. However, the process of handcrafting seeds for fuzzing purposes often becomes strenuous for humans as it requires a deeper understanding of the Program-Under-Test (PUT). Also, the use of known inputs to programs often does not trigger vulnerable program behavior or may not reach potentially vulnerable code locations. To address those issues, we propose a seed generation framework that enables Human-In-The-Loop (HITL) directed fuzzing where the human assumes a more active role in the creation of seeds that can penetrate and assess desired locations of the PUT. Our proposed framework uses Symbolic Execution (SE) to generate seeds that exercise paths to target program locations. Moreover, our framework enables the visualization of the explored execution paths in the binary of the PUT for the generated seeds. We evaluated our approach on a set of 12 carefully designed C programs with diverse characteristics that mimic real-world programs. The experimental results show the effectiveness of the proposed approach in improving the performance of standard fuzzing tools such as the American Fuzzy Lop ("Image missing"). Specifically, our solution can generate seeds that substantially enhance the performance of the fuzzer, achieving speedups ranging from \$\$1.46\\times \$\$ 1.46 × to \$\$68.53\\times \$\$ 68.53 × for branch conditions, \$\$1.39\\times \$\$ 1.39 × to \$\$254.62\\times \$\$ 254.62 × for branch depths, \$\$14,879.59\\times \$\$ 14 , 879.59 × to \$\$30,295.88\\times \$\$ 30 , 295.88 × for branch widths over traditional seeds. Additionally, the speedup increases with the number of target function ranging from \$\$12,260\\times \$\$ 12 , 260 × to \$\$22,856.07\\times \$\$ 22 , 856.07 × over traditional seeds while only requiring less than 15 seconds on average for the seed generation step.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_031_22390_7_21,
  title = {Representing LLVM-IR in a Code Property Graph},
  author = {Küchler, Alexander and Banse, Christian},
  booktitle = {Lecture Notes in Computer Science},
  year = {2022},
  pages = {360-380},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-031-22390-7\_21},
  url = {https://doi.org/10.1007/978-3-031-22390-7\_21},
  abstract = {In the past years, a number of static application security testing tools have been proposed which use so-called code property graphs (CPGs), a graph model which keeps rich information about the source code while enabling its user to write language-agnostic analyses. However, they suffer from several shortcomings. They work mostly on source code and exclude the analysis of third-party dependencies if they are only available as compiled binaries. Furthermore, they are limited in their analysis to whether an individual programming language is supported or not. While often support for well-established languages such as C/C++ or Java is included, languages that are still heavily evolving, such as Rust, are not considered because of the constant changes in the language design. To overcome these limitations, we extend an open source implementation of a code property graph to support LLVM-IR which can be used as output by many compilers and binary lifters. In this paper, we discuss how we address challenges that arise when mapping concepts of an intermediate representation to a CPG. At the same time, we optimize the resulting graph to be minimal and close to the representation of equivalent source code. Our case-study on detecting cryptographic misuse indicates that existing analyses can be reused and that the analysis time is comparable to operating on source code. This makes the approach suitable for a security analysis of large-scale projects.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_031_94455_0_1,
  title = {Fast Firmware Fuzz with Input/Output Reposition},
  author = {Xin, Mingfeng and Deng, Liting and Wen, Hui and Fang, Dongliang and Lv, Shichao and Sun, Limin},
  booktitle = {Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
  year = {2026},
  pages = {3-27},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-94455-0\_1},
  url = {https://doi.org/10.1007/978-3-031-94455-0\_1},
  abstract = {Embedded devices are used ubiquitously for managing both personnel belongs and critical infrastructures. Researchers have adopted various fuzzing techniques to test firmware images of embedded devices for detecting security vulnerabilities. However, firmware images are different from traditional desktop applications, especially in the input/output system, which brings unique changes that hinder efficient testing. In this paper, we propose a novel method, input/output reposition (or I/O reposition ) to improve the performance of fuzzing firmware images. The core idea is to automatically replace the interrupt-based I/O method with polling to address two issues. First, interrupt introduces a lot of context switches, leading to heavy overhead; Second, interrupt is triggered in a low frequency and the system either starves on input or gets stuck on output. We implement Fire that utilizes probabilistic invocation and interrupt service slicing to achieve practical I/O reposition. We evaluate Fire on 30 popular and widely used firmware images. The results show that Fire brings 95.4\% higher throughput and achieves 30.4\% more code coverage. Fuzzing Fire -modified images can detect 12 known bugs with 63.6\% of the time required by fuzzing the original images. We detect three new vulnerabilities from tested firmware images and have reported to their developers.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_031_48044_7_5,
  title = {Decision-Making Framework to Support the End-of-Life Management of High-Activity Radioactive Sources},
  author = {Khadka, Rajiv and Yang, Xingyue and Koudelka, John and Walker, Victor and Kenney, Kevin},
  booktitle = {Lecture Notes in Computer Science},
  year = {2023},
  pages = {63-74},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-48044-7\_5},
  url = {https://doi.org/10.1007/978-3-031-48044-7\_5},
  abstract = {Sealed radioactive sources are used for an extremely wide range of applications, including at various nuclear facilities, universities, hospitals, and biomedical and industrial organizations, as well as for geological prospecting and exploration. However, proper management of these sealed radioactive sources after they have reached their end-of-life (EOL) remains a global challenge. Very few field-tested mobile hot cells (MHCs) exist to support the EOL management of disused high-activity radioactive sources. One critical challenge encountered by source recovery teams at each site is that of finding adequate space–as well as access to that space—to complete the staging and recapture process by using a MHC. This process is tedious, uncertain, and time-consuming, thus making it essential to properly understand how to plan and manage the setting up and taking down of the MHC. However, no tool currently exists that can serve as an effective medium for making such plans, or for informing, educating, and communicating to source recovery teams important knowledge about the environment in which the MHC will be deployed. In this report, we present a novel approach to planning and managing the deployment of MHCs, and apply it to several different sites that rely on such technology. Furthermore, this planning tool will support collaboration, promote information sharing, and foster better decision-making processes, enabling MHCs to be more efficiently and reliably set up and used for source recovery purposes.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_030_78120_0_5,
  title = {QuickBCC: Quick and Scalable Binary Vulnerable Code Clone Detection},
  author = {Jang, Hajin and Yang, Kyeongseok and Lee, Geonwoo and Na, Yoonjong and Seideman, Jeremy D. and Luo, Shoufu and Lee, Heejo and Dietrich, Sven},
  booktitle = {IFIP Advances in Information and Communication Technology},
  year = {2021},
  pages = {66-82},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-78120-0\_5},
  url = {https://doi.org/10.1007/978-3-030-78120-0\_5},
  abstract = {Due to code reuse among software packages, vulnerabilities can propagate from one software package to another. Current code clone detection techniques are useful for preventing and managing such vulnerability propagation. When the source code for a software package is not available, such as when working with proprietary or custom software distributions, binary code clone detection can be used to examine software for flaws. However, existing binary code clone detectors have scalability issues, or are limited in their accurate detection of vulnerable code clones . In this paper, we introduce QuickBCC, a scalable binary code clone detection framework designed for vulnerability scanning. The framework was built on the idea of extracting semantics from vulnerable binaries both before and after security patches, and comparing them to target binaries. In order to improve performance, we created a signature based on the changes between the pre- and post-patched binaries, and implemented a filtering process when comparing the signatures to the target binaries. In addition, we leverage the smallest semantic unit, a strand , to improve accuracy and robustness against compile environments. QuickBCC is highly optimized, capable of preprocessing 5,439 target binaries within 111 min, and is able to match those binaries against 6 signatures in 23 s when running as a multi-threaded application. QuickBCC takes, on average, 3 ms to match one target binary. Comparing performance to other approaches, we found that it outperformed other approaches in terms of performance when detecting well known vulnerabilities with acceptable level of accuracy.},
  content_type = {Conference paper},
}

@article{springer_10_1186_s42400_021_00085_7,
  title = {B2SMatcher: fine-Grained version identification of open-Source software in binary files},
  author = {Ban, Gu and Xu, Lili and Xiao, Yang and Li, Xinhua and Yuan, Zimu and Huo, Wei},
  journal = {Cybersecurity},
  year = {2021},
  volume = {4},
  number = {1},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1186/s42400-021-00085-7},
  url = {https://doi.org/10.1186/s42400-021-00085-7},
  abstract = {Abstract Codes of Open Source Software (OSS) are widely reused during software development nowadays. However, reusing some specific versions of OSS introduces 1-day vulnerabilities of which details are publicly available, which may be exploited and lead to serious security issues. Existing state-of-the-art OSS reuse detection work can not identify the specific versions of reused OSS well. The features they selected are not distinguishable enough for version detection and the matching scores are only based on similarity.This paper presents B2SMatcher, a fine-grained version identification tool for OSS in commercial off-the-shelf (COTS) software. We first discuss five kinds of version-sensitive code features that are trackable in both binary and source code. We categorize these features into program-level features and function-level features and propose a two-stage version identification approach based on the two levels of code features. B2SMatcher also identifies different types of OSS version reuse based on matching scores and matched feature instances. In order to extract source code features as accurately as possible, B2SMatcher innovatively uses machine learning methods to obtain the source files involved in the compilation and uses function abstraction and normalization methods to eliminate the comparison costs on redundant functions across versions. We have evaluated B2SMatcher using 6351 candidate OSS versions and 585 binaries. The result shows that B2SMatcher achieves a high precision up to 89.2\% and outperforms state-of-the-art tools. Finally, we show how B2SMatcher can be used to evaluate real-world software and find some security risks in practice.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_031_46077_7_13,
  title = {Reliable Basic Block Energy Accounting},
  author = {Lamprakos, Christos P. and Bouras, Dimitrios S. and Catthoor, Francky and Soudris, Dimitrios},
  booktitle = {Lecture Notes in Computer Science},
  year = {2023},
  pages = {193-208},
  publisher = {Springer Nature Switzerland},
  doi = {10.1007/978-3-031-46077-7\_13},
  url = {https://doi.org/10.1007/978-3-031-46077-7\_13},
  abstract = {Modeling the energy consumption of low-level code will enable (i) a better understanding of its relationship to execution time and (ii) compiler/runtime optimizations tailored for energy efficiency. But such models need reliable ground truth data to be trained on. We thus attack extracting machine-specific datasets for the energy consumption of basic blocks–a problem with surprisingly few solutions available. Given the impact of execution context on energy, we are interested in recording sequences of basic blocks coupled to corresponding energy measurements. Our design is lightweight and portable; no manual hardware/software instrumentation is required. Its main components are an energy estimation interface with sufficiently high refresh rate, access to an application’s complete execution trace, and LLVM pass-based instrumentation. We extract half a million basic block-energy mappings overall, and achieve a mean whole-program error of \\(\\sim \\) 3\% on two different machines. This paper demonstrates that commodity resources suffice to perform a very crucial task on the road to energy-optimal computing.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_030_88418_5_11,
  title = {Towards Automating Code-Reuse Attacks Using Synthesized Gadget Chains},
  author = {Schloegel, Moritz and Blazytko, Tim and Basler, Julius and Hemmer, Fabian and Holz, Thorsten},
  booktitle = {Lecture Notes in Computer Science},
  year = {2021},
  pages = {218-239},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-88418-5\_11},
  url = {https://doi.org/10.1007/978-3-030-88418-5\_11},
  abstract = {In the arms race between binary exploitation techniques and mitigation schemes, code-reuse attacks have been proven indispensable. Typically, one of the initial hurdles is that an attacker cannot execute their own code due to countermeasures such as data execution prevention (DEP, ). While this technique is powerful, the task of finding and correctly chaining gadgets remains cumbersome. Although various methods automating this task have been proposed, they either rely on hard-coded heuristics or make specific assumptions about the gadgets’ semantics. This not only drastically limits the search space but also sacrifices their capability to find valid chains unless specific gadgets can be located. As a result, they often produce no chain or an incorrect chain that crashes the program. In this paper, we present SGC , the first generic approach to identify gadget chains in an automated manner without imposing restrictions on the gadgets or limiting its applicability to specific exploitation scenarios. Instead of using heuristics to find a gadget chain, we offload this task to an SMT solver. More specifically, we build a logical formula that encodes the CPU and memory state at the time when the attacker can divert execution flow to the gadget chain, as well as the attacker’s desired program state that the gadget chain should construct. In combination with a logical encoding of the data flow between gadgets, we query an SMT solver whether a valid gadget chain exists. If successful, the solver provides a proof of existence in the form of a synthesized gadget chain. This way, we remain fully flexible w.r.t. to the gadgets. In empirical tests, we find that the solver often uses all types of control-flow transfer instructions and even gadgets with side effects. Our evaluation shows that SGC successfully finds working gadget chains for real-world exploitation scenarios within minutes, even when all state-of-the-art approaches fail.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_031_17510_7_21,
  title = {Building Deobfuscated Applications from Polymorphic Binaries},
  author = {Crăciun, Vlad Constantin and Mogage, Andrei-Cătălin},
  booktitle = {Lecture Notes in Computer Science},
  year = {2022},
  pages = {308-323},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-031-17510-7\_21},
  url = {https://doi.org/10.1007/978-3-031-17510-7\_21},
  abstract = {Along with the rise of the cyber threats industry, attackers have become more fluent in developing and integrating various obfuscation layers. This is mainly focused on impeding or at least slowing the analysis and the reverse engineering process, both manually and automatically, such that their threats will have more time to do damage. Our contribution comes two-fold: we propose a semi-formal description to reason with a certain class of obfuscators, while also presenting a concrete implementation proving our deobfuscation mechanisms. Our results are based on a set of case studies of both common threats and legitimate software, running on Windows operating systems. We evaluate our results comparing with PINDemonium, a tool built on top of PIN dynamic binary instrumentation tool. Our solution CFGDump attempts to brute-force and hash inter-procedural control flow graphs, opening the doors to future optimisations and possible other features.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_030_05487_8_1,
  title = {On Efficiency and Effectiveness of Linear Function Detection Approaches for Memory Carving},
  author = {Liebler, Lorenz and Baier, Harald},
  booktitle = {Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
  year = {2019},
  pages = {3-22},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-05487-8\_1},
  url = {https://doi.org/10.1007/978-3-030-05487-8\_1},
  abstract = {In the field of unstructured memory analysis, the context-unaware detection of function boundaries leads to meaningful insights. For instance, in the field of binary analysis, those structures yield further inference, e.g., identifying binaries known to be bad. However, recent publications discuss different strategies for the problem of function boundary detection and consider it to be a difficult problem. One of the reasons is that the detection process depends on a quantity of parameters including the used architecture, programming language and compiler parameters. Initially a typical memory carving approach transfers the paradigm of signature-based detection techniques from the mass storage analysis to memory analysis. To automate and generalise the signature matching, signature-based recognition approaches have been extended by machine learning algorithms. Recently a review of function detection approaches claims that the results are possibly biased by large portions of shared code between the used samples. In this work we reassess the application of recently discussed machine learning based function detection approaches. We analyse current approaches in the context of memory carving with respect to both their efficiency and their effectiveness. We show the capabilities of function start identification by reducing the features to vectorised mnemonics. In all this leads to a significant reduction of runtime by keeping a high value of accuracy and a good value of recall.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_030_65411_5_26,
  title = {Assembly or Optimized C for Lightweight Cryptography on RISC-V?},
  author = {Campos, Fabio and Jellema, Lars and Lemmen, Mauk and Müller, Lars and Sprenkels, Amber and Viguier, Benoit},
  booktitle = {Lecture Notes in Computer Science},
  year = {2020},
  pages = {526-545},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-65411-5\_26},
  url = {https://doi.org/10.1007/978-3-030-65411-5\_26},
  abstract = {A major challenge when applying cryptography on constrained environments is the trade-off between performance and security. In this work, we analyzed different strategies for the optimization of several candidates of NIST’s lightweight cryptography standardization project on a RISC-V architecture. In particular, we studied the general impact of optimizing symmetric-key algorithms in assembly and in plain C. Furthermore, we present optimized implementations, achieving a speed-up of up to 81\% over available implementations, and discuss general implementation strategies.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_030_00470_5_20,
  title = {\$\$\\tau \$\$ CFI: Type-Assisted Control Flow Integrity for x86-64 Binaries},
  author = {Muntean, Paul and Fischer, Matthias and Tan, Gang and Lin, Zhiqiang and Grossklags, Jens and Eckert, Claudia},
  booktitle = {Lecture Notes in Computer Science},
  year = {2018},
  pages = {423-444},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-030-00470-5\_20},
  url = {https://doi.org/10.1007/978-3-030-00470-5\_20},
  abstract = {Programs aiming for low runtime overhead and high availability draw on several object-oriented features available in the C/C++ programming language, such as dynamic object dispatch. However, there is an alarmingly high number of object dispatch ( i.e., forward-edge) corruption vulnerabilities, which undercut security in significant ways and are in need of a thorough solution. In this paper, we propose \\(\\tau \{\\textsc \{CFI\}\}\\) , an extended control flow integrity (CFI) model that uses both the types and numbers of function parameters to enforce forward- and backward-edge control flow transfers. At a high level, it improves the precision of existing forward-edge recognition approaches by considering the type information of function parameters, which are directly extracted from the application binaries. Therefore, \\(\\tau \{\\textsc \{CFI\}\}\\) can be used to harden legacy applications for which source code may not be available. We have evaluated \\(\\tau \{\\textsc \{CFI\}\}\\) on real-world binaries including Nginx, NodeJS, Lighttpd, MySql and the SPEC CPU2006 benchmark and demonstrate that \\(\\tau \{\\textsc \{CFI\}\}\\) is able to effectively protect these applications from forward- and backward-edge corruptions with low runtime overhead. In direct comparison with state-of-the-art tools, \\(\\tau \{\\textsc \{CFI\}\}\\) achieves higher forward-edge caller-callee matching precision.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_94_017_7267_9_18,
  title = {Host-Compiled Simulation},
  author = {Mueller-Gritschneder, Daniel and Gerstlauer, Andreas},
  booktitle = {Handbook of Hardware/Software Codesign},
  year = {2017},
  pages = {593-619},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-94-017-7267-9\_18},
  url = {https://doi.org/10.1007/978-94-017-7267-9\_18},
  abstract = {Virtual Prototypes (VPs), also known as virtual platforms, have been now widely adopted by industry as platforms for early software development, HW/SW coverification, performance analysis, and architecture exploration. Yet, rising design complexity, the need to test an increasing amount of software functionality as well as the verification of timing properties pose a growing challenge in the application of VPs. New approaches overcome the accuracy-speed bottleneck of today’s virtual prototyping methods. These next-generation VPs are centered around ultra-fast host-compiled software models. Accuracy is obtained by advanced methods, which reconstruct the execution times of the software and model the timing behavior of the operating system, target processor, and memory system. It is shown that simulation speed can further be increased by abstract TLM-based communication models. This support of ultra-fast and accurate HW/SW cosimulation will be a key enabler for successfully developing tomorrows Multi-Processor System-on-Chip (MPSoC) platforms.},
  content_type = {Reference work entry},
}

@incollection{springer_10_1007_978_94_017_7267_9_22,
  title = {Timing Models for Fast Embedded Software Performance Analysis},
  author = {Bringmann, Oliver and Gerum, Christoph and Ottlik, Sebastian},
  booktitle = {Handbook of Hardware/Software Codesign},
  year = {2017},
  pages = {655-682},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-94-017-7267-9\_22},
  url = {https://doi.org/10.1007/978-94-017-7267-9\_22},
  abstract = {In this chapter, we give an overview on timing models which provide an abstract representation of the timing behavior for a given software. These models can be driven by a functional simulation based on the simulated control flow. As the timing model itself can reach a level of accuracy that is comparable to a classic timing simulation of the represented software, these approaches enable a fast yet accurate software performance analysis. In this chapter, we focus on the generation and structure of various models but also provide a brief introduction into their integration with a functional simulation. The presented approaches are targeting software executing on current and future system-on-chips with a wide range of embedded processors – including Graphics Processing Units (GPUs).},
  content_type = {Reference work entry},
}

@incollection{springer_10_1007_978_3_662_46681_0_18,
  title = {Insight: An Open Binary Analysis Framework},
  author = {Fleury, Emmanuel and Ly, Olivier and Point, Gérald and Vincent, Aymeric},
  booktitle = {Lecture Notes in Computer Science},
  year = {2015},
  pages = {218-224},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-662-46681-0\_18},
  url = {https://doi.org/10.1007/978-3-662-46681-0\_18},
  abstract = {We present Insight , a framework for binary program analysis and two tools provided with it: CFGRecovery and iii. Insight is intended to be a full environment for analyzing, interacting and verifying executable programs. Insight is able to translate x86 , x86-64 and msp430 binary code to our intermediate representation and execute it symbolically in an abstract domain where each variable (register, memory cell) is substituted by a formula representing all its possible values along the current execution path. CFGRecovery aims at automatically rebuilding the program control flow based only on the executable file. It heavily relies on SMT solvers. iii provides an interactive and a (Python) programmable interface to a coherent set of features from the Insight framework. It behaves like a debugger except that the execution traces that are examined are symbolic and cover a collection of possible concrete executions at once. For example, iii allows to perform an interactive reconstruction of the CFG.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_94_017_7358_4_18_1,
  title = {Host-Compiled Simulation},
  author = {Mueller-Gritschneder, Daniel and Gerstlauer, Andreas},
  booktitle = {Handbook of Hardware/Software Codesign},
  year = {2016},
  pages = {1-27},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-94-017-7358-4\_18-1},
  url = {https://doi.org/10.1007/978-94-017-7358-4\_18-1},
  abstract = {Virtual Prototypes (VPs), also known as virtual platforms, have been now widely adopted by industry as platforms for early software development, HW/SW coverification, performance analysis, and architecture exploration. Yet, rising design complexity, the need to test an increasing amount of software functionality as well as the verification of timing properties pose a growing challenge in the application of VPs. New approaches overcome the accuracy-speed bottleneck of today’s virtual prototyping methods. These next-generation VPs are centered around ultra-fast host-compiled software models. Accuracy is obtained by advanced methods, which reconstruct the execution times of the software and model the timing behavior of the operating system, target processor, and memory system. It is shown that simulation speed can further be increased by abstract TLM-based communication models. This support of ultra-fast and accurate HW/SW cosimulation will be a key enabler for successfully developing tomorrows Multi-Processor System-on-Chip (MPSoC) platforms.},
  content_type = {Living reference work entry},
}

@incollection{springer_10_1007_978_3_319_30806_7_6,
  title = {Semantics-Based Repackaging Detection for Mobile Apps},
  author = {Guan, Quanlong and Huang, Heqing and Luo, Weiqi and Zhu, Sencun},
  booktitle = {Lecture Notes in Computer Science},
  year = {2016},
  pages = {89-105},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-30806-7\_6},
  url = {https://doi.org/10.1007/978-3-319-30806-7\_6},
  abstract = {While Android app stores keep growing in size and in number, app repackaging has become a major threat to the health of the mobile ecosystem. Different from many syntax-based repackaging detection techniques, in this work we propose a semantic-based approach, RepDetector, which is more robust against code obfuscation attacks. To capture an app’s semantics, our approach extracts input-output states of core functions in the app and then compare function and app similarity. We implement a prototype of RepDetector, and evaluate it against various obfuscation technologies. The results show that our approach can detect repackaged apps effectively. It is also at least a hundred times faster than Androguard.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_94_017_7358_4_22_2,
  title = {Timing Models for Fast Embedded Software Performance Analysis},
  author = {Bringmann, Oliver and Gerum, Christoph and Ottlik, Sebastian},
  booktitle = {Handbook of Hardware/Software Codesign},
  year = {2016},
  pages = {1-28},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-94-017-7358-4\_22-2},
  url = {https://doi.org/10.1007/978-94-017-7358-4\_22-2},
  abstract = {In this chapter, we give an overview on timing models which provide an abstract representation of the timing behavior for a given software. These models can be driven by a functional simulation based on the simulated control flow. As the timing model itself can reach a level of accuracy that is comparable to a classic timing simulation of the represented software, these approaches enable a fast yet accurate software performance analysis. In this chapter, we focus on the generation and structure of various models but also provide a brief introduction into their integration with a functional simulation. The presented approaches are targeting software executing on current and future system-on-chips with a wide range of embedded processors – including Graphics Processing Units (GPUs).},
  content_type = {Living reference work entry},
}

@incollection{springer_10_1007_978_3_319_26408_0_1,
  title = {FPGA Versus Software Programming: Why, When, and How?},
  author = {Koch, Dirk and Ziener, Daniel and Hannig, Frank},
  booktitle = {FPGAs for Software Programmers},
  year = {2016},
  pages = {1-21},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-26408-0\_1},
  url = {https://doi.org/10.1007/978-3-319-26408-0\_1},
  abstract = {This chapter provides background information for readers who are interested in the philosophy and technology behind FPGAs. We present this from a software engineer’s viewpoint without hiding the hardware specific characteristics of FPGAs. The chapter discusses the architecture and programming models as well as the pros and cons of CPUs, GPUs and FPGAs. The operation of FPGAs will be described as well as the major steps that are needed to map a circuit description on an FPGA. This will provide a deep enough understanding of the characteristics of an FPGA and how this helps in accelerating certain parts of an application.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_319_06865_7_5,
  title = {Embedded Development System and C Programming},
  author = {Di Paolo Emilio, Maurizio},
  booktitle = {Embedded Systems Design for High-Speed Data Acquisition and Control},
  year = {2015},
  pages = {93-100},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-06865-7\_5},
  url = {https://doi.org/10.1007/978-3-319-06865-7\_5},
  abstract = {An embedded system is identified as the electronic processing device designed for a certain function. The designers usually use compilers, assembler, debugger, and a whole range suites for the development of both software and hardware in it.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_642_03259_2_6,
  title = {Rover vision—fundamentals},
  author = {Ellery, Alex},
  booktitle = {Planetary Rovers},
  year = {2016},
  pages = {199-262},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-03259-2\_6},
  url = {https://doi.org/10.1007/978-3-642-03259-2\_6},
  abstract = {No single modality sensor can provide sufficient data to extract all the relevant features of the environment but vision is the most information-rich modality. It is the primary sensory modality for planetary rovers in providing distance observation for navigation and obstacle avoidance. Furthermore, it is the most information-rich form of sensory data—in autonomous rovers it is the primary means of generating maps of the locality representing distance information about the external world. Stereovision is required for the recognition of objects, the determination of their positions and orientations in space, and for visual servoing. Vision also provides the basis for scientific analysis by the science team and as the first step towards autonomous science. Natural environments such as those on planetary surfaces are unstructured making image processing more difficult. Furthermore, sensor data are always corrupted by noise and characterized by limited observability.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_642_36949_0_46,
  title = {Compiler Help for Binary Manipulation Tools},
  author = {Ince, Tugrul and Hollingsworth, Jeffrey K.},
  booktitle = {Lecture Notes in Computer Science},
  year = {2013},
  pages = {404-413},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-36949-0\_46},
  url = {https://doi.org/10.1007/978-3-642-36949-0\_46},
  abstract = {Parsing machine code is the first step for most analyses performed on binary files. These analyses build control flow graphs (CFGs). In this work we propose a compilation mechanism that augments binary files with information about where each basic block is located and how they are connected to each other. This information makes it unnecessary to analyze most instructions in a binary during the initial CFG build process. As a result, these binary analysis tools experience dramatically increased parsing speeds - 3.8x on average.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_642_41383_4_2,
  title = {Static Integer Overflow Vulnerability Detection in Windows Binary},
  author = {Deng, Yi and Zhang, Yang and Cheng, Liang and Sun, Xiaoshan},
  booktitle = {Lecture Notes in Computer Science},
  year = {2013},
  pages = {19-35},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-41383-4\_2},
  url = {https://doi.org/10.1007/978-3-642-41383-4\_2},
  abstract = {In this paper, we present a static binary analysis based approach to detect integer overflow vulnerabilities in windows binary. We first translate the binary to our intermediate representation and perform Sign type analysis to reconstruct sufficient type information, and then use dataflow analysis to collect suspicious integer overflow vulnerabilities. To alleviate the problem that static vulnerability detection has high false positive rate, we use the information how variables which may be affected by integer overflow are used in security sensitive operations to compute priority and rank the suspicious integer overflow vulnerabilities. Finally the weakest preconditions technique is used to validate the suspicious integer overflow vulnerabilities. Our approach is static so that it does not run the software directly in real environment. We implement a prototype called EIOD and use it to analyze real-world windows binaries. Experiments show that EIOD can effectively and efficiently detect integer overflow vulnerabilities.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_1_4471_2909_7_5,
  title = {Static Analysis of Binaries},
  author = {Cesare, Silvio and Xiang, Yang},
  booktitle = {SpringerBriefs in Computer Science},
  year = {2012},
  pages = {41-49},
  publisher = {Springer London},
  doi = {10.1007/978-1-4471-2909-7\_5},
  url = {https://doi.org/10.1007/978-1-4471-2909-7\_5},
  abstract = {Static binary analysis is more difficult than if source code is available. In many cases, the analyses are unsound and behaviours are omitted to make problems feasible. Heuristics may be required to separate code and data in a disassembly or pointer behaviour may be weakly modelled to make statically analysing programs feasible. Nevertheless, static analysis of binaries is an important area of research with a number of practical applications including the detection of software theft and the classification and detection of malware. This chapter examines static analysis of binaries with the intent that properties and features of binary programs can be extracted to create useful birthmarks for software similarity and classification.},
  content_type = {Chapter},
}

@article{springer_10_1007_s11241_012_9155_z,
  title = {Performance debugging of Esterel specifications},
  author = {Ju, Lei and Huynh, Bach Khoa and Roychoudhury, Abhik and Chakraborty, Samarjit},
  journal = {Real-Time Systems},
  year = {2012},
  volume = {48},
  number = {5},
  pages = {570-600},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s11241-012-9155-z},
  url = {https://doi.org/10.1007/s11241-012-9155-z},
  abstract = {Synchronous languages like Esterel have been widely adopted for designing reactive systems in safety-critical domains such as avionics. Specifications written in Esterel are based on the underlying “synchrony hypothesis”, which needs to be validated when Esterel specifications get compiled to real implementations (such as C code). In this work, we present a model-driven and architecture-aware timing analysis framework for C code generated from Esterel and executed on general-purpose processors. By integrating model-level information into the traditional timing analysis, we can efficiently compute accurate time estimates via systematically eliminating a large number of infeasible paths in the generated code. Experimental results show that with our proposed intermediate representation level infeasible path analysis in the model compilation, we obtain up to 16.1 \% tighter WCET estimates compared to the traditional assembly code level infeasible path detection with substantially less analysis time. Furthermore, by maintaining the traceability links between Esterel specifications and the generated C code, we are able to map the time-critical computations at the C-level back to the Esterel-level.},
  content_type = {Article},
}

@article{springer_10_1007_s10664_012_9199_7,
  title = {Software Bertillonage},
  author = {Davies, Julius and German, Daniel M. and Godfrey, Michael W. and Hindle, Abram},
  journal = {Empirical Software Engineering},
  year = {2013},
  volume = {18},
  number = {6},
  pages = {1195-1237},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10664-012-9199-7},
  url = {https://doi.org/10.1007/s10664-012-9199-7},
  abstract = {Deployed software systems are typically composed of many pieces, not all of which may have been created by the main development team. Often, the provenance of included components—such as external libraries or cloned source code—is not clearly stated, and this uncertainty can introduce technical and ethical concerns that make it difficult for system owners and other stakeholders to manage their software assets. In this work, we motivate the need for the recovery of the provenance of software entities by a broad set of techniques that could include signature matching, source code fact extraction, software clone detection, call flow graph matching, string matching, historical analyses, and other techniques. We liken our provenance goals to that of Bertillonage, a simple and approximate forensic analysis technique based on bio-metrics that was developed in 19th century France before the advent of fingerprints. As an example, we have developed a fast, simple, and approximate technique called anchored signature matching for identifying the source origin of binary libraries within a given Java application. This technique involves a type of structured signature matching performed against a database of candidates drawn from the Maven2 repository, a 275 GB collection of open source Java libraries. To show the approach is both valid and effective, we conducted an empirical study on 945 jars from the Debian GNU/Linux distribution, as well as an industrial case study on 81 jars from an e-commerce application.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_642_24349_3_7,
  title = {Reconciling Compilation and Timing Analysis},
  author = {Falk, Heiko and Marwedel, Peter and Lokuciejewski, Paul},
  booktitle = {Advances in Real-Time Systems},
  year = {2012},
  pages = {145-170},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-24349-3\_7},
  url = {https://doi.org/10.1007/978-3-642-24349-3\_7},
  abstract = {According to forecasts such as a report published by the National Research Council in the US [21], embedded devices will be a main application area of information technology in the future. Therefore, we can observe an increased interest into embedded systems. Funding of embedded systems research in Europe by the European Community (see “Objective ICT-2009.3.4 Embedded Systems Design” in [3]) is a clear indicator of this trend. Also, market statistics [12] demonstrate the increasing market for certain embedded devices.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_642_22110_1_37,
  title = {BAP: A Binary Analysis Platform},
  author = {Brumley, David and Jager, Ivan and Avgerinos, Thanassis and Schwartz, Edward J.},
  booktitle = {Lecture Notes in Computer Science},
  year = {2011},
  pages = {463-469},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-22110-1\_37},
  url = {https://doi.org/10.1007/978-3-642-22110-1\_37},
  abstract = {BAP is a publicly available infrastructure for performing program verification and analysis tasks on binary (i.e., executable) code. In this paper, we describe BAP as well as lessons learned from previous incarnations of binary analysis platforms. BAP explicitly represents all side effects of instructions in an intermediate language (IL), making syntaxdirected analysis possible. We have used BAP to routinely generate and solve verification conditions that are hundreds of megabytes in size and encompass 100,000’s of assembly instructions.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s11334_010_0125_0,
  title = {Software model checking without source code},
  author = {Chaki, Sagar and Ivers, James},
  journal = {Innovations in Systems and Software Engineering},
  year = {2010},
  volume = {6},
  number = {3},
  pages = {233-242},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s11334-010-0125-0},
  url = {https://doi.org/10.1007/s11334-010-0125-0},
  abstract = {We present a framework, called air , for verifying safety properties of assembly language programs via software model checking. air extends the applicability of predicate abstraction and counterexample guided abstraction refinement to the automated verification of low-level software. By working at the assembly level, air allows verification of programs for which source code is unavailable—such as legacy and COTS software—and programs that use features—such as pointers, structures, and object-orientation—that are problematic for source-level software verification tools. In addition, air makes no assumptions about the underlying compiler technology. We have implemented a prototype of air and present encouraging results on several non-trivial examples.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_3_540_70545_1_40,
  title = {Jakstab: A Static Analysis Platform for Binaries},
  author = {Kinder, Johannes and Veith, Helmut},
  booktitle = {Lecture Notes in Computer Science},
  year = {None},
  pages = {423-427},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-70545-1\_40},
  url = {https://doi.org/10.1007/978-3-540-70545-1\_40},
  abstract = {For processing compiled code, model checkers require accurate model extraction from binaries. We present our fully configurable binary analysis platform Jakstab , which resolves indirect branches by multiple rounds of disassembly interleaved with dataflow analysis. We demonstrate that this iterative disassembling strategy achieves better results than the state-of-the-art tool IDA Pro.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_540_93900_9_19,
  title = {An Abstract Interpretation-Based Framework for Control Flow Reconstruction from Binaries},
  author = {Kinder, Johannes and Zuleger, Florian and Veith, Helmut},
  booktitle = {Lecture Notes in Computer Science},
  year = {2008},
  pages = {214-228},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-93900-9\_19},
  url = {https://doi.org/10.1007/978-3-540-93900-9\_19},
  abstract = {Due to indirect branch instructions, analyses on executables commonly suffer from the problem that a complete control flow graph of the program is not available. Data flow analysis has been proposed before to statically determine branch targets in many cases, yet a generic strategy without assumptions on compiler idioms or debug information is lacking. We have devised an abstract interpretation-based framework for generic low level programs with indirect jumps which safely combines a pluggable abstract domain with the notion of partial control flow graphs. Using our framework, we are able to show that the control flow reconstruction algorithm of our disassembly tool Jakstab produces the most precise overapproximation of the control flow graph with respect to the used abstract domain.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_540_89862_7_1,
  title = {BitBlaze: A New Approach to Computer Security via Binary Analysis},
  author = {Song, Dawn and Brumley, David and Yin, Heng and Caballero, Juan and Jager, Ivan and Kang, Min Gyung and Liang, Zhenkai and Newsome, James and Poosankam, Pongsin and Saxena, Prateek},
  booktitle = {Lecture Notes in Computer Science},
  year = {2008},
  pages = {1-25},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-89862-7\_1},
  url = {https://doi.org/10.1007/978-3-540-89862-7\_1},
  abstract = {In this paper, we give an overview of the BitBlaze project, a new approach to computer security via binary analysis. In particular, BitBlaze focuses on building a unified binary analysis platform and using it to provide novel solutions to a broad spectrum of different security problems. The binary analysis platform is designed to enable accurate analysis, provide an extensible architecture, and combines static and dynamic analysis as well as program verification techniques to satisfy the common needs of security applications. By extracting security-related properties from binary programs directly, BitBlaze enables a principled, root-cause based approach to computer security, offering novel and effective solutions, as demonstrated with over a dozen different security applications.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_1_4302_1016_0_15,
  title = {Backing Up Databases},
  booktitle = {Expert Oracle Database 11g Administration},
  year = {None},
  pages = {725-800},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-1016-0\_15},
  url = {https://doi.org/10.1007/978-1-4302-1016-0\_15},
  abstract = {A s an Oracle DBA, one of your fundamental tasks is to regularly back up databases. Backups involve making copies of your database to re-create the database if necessary. They provide the basis of all database recoveries—no backup, no recovery. One of the best things you can do to help yourself as a DBA is to focus on a tried-and-tested strategy for backing up the database, because the more time you spend planning backups, the less time you’ll spend recovering the database from a mishap.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_1_4302_0200_4_15,
  title = {Introducing .NET Assemblies},
  booktitle = {Pro VB 2008 and the .NET 3.5 Platform},
  year = {None},
  pages = {437-481},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0200-4\_15},
  url = {https://doi.org/10.1007/978-1-4302-0200-4\_15},
  abstract = {Each of the applications developed in this book’s first 14 chapters were along the lines of traditional stand-alone applications, given that almost all of your custom programming logic was contained within a single executable file (*.exe). However, one major aspect of the .NET platform is the notion of binary reuse , where applications make use of the types contained within various external assemblies (aka code libraries). The point of this chapter is to examine the core details of creating, deploying, and configuring .NET assemblies.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_540_69170_9_27,
  title = {Decoding Code on a Sensor Node},
  author = {von Rickenbach, Pascal and Wattenhofer, Roger},
  booktitle = {Lecture Notes in Computer Science},
  year = {2008},
  pages = {400-414},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-69170-9\_27},
  url = {https://doi.org/10.1007/978-3-540-69170-9\_27},
  abstract = {Wireless sensor networks come of age and start moving out of the laboratory into the field. As the number of deployments is increasing the need for an efficient and reliable code update mechanism becomes pressing. Reasons for updates are manifold ranging from fixing software bugs to retasking the whole sensor network. The scale of deployments and the potential physical inaccessibility of individual nodes asks for a wireless software management scheme. In this paper we present an efficient code update strategy which utilizes the knowledge of former program versions to distribute mere incremental changes. Using a small set of instructions, a delta of minimal size is generated. This delta is then disseminated throughout the network allowing nodes to rebuild the new application based on their currently running code. The asymmetry of computational power available during the process of encoding (PC) and decoding (sensor node) necessitates a careful balancing of the decoder complexity to respect the limitations of today’s sensor network hardware. We provide a seamless integration of our work into Deluge, the standard TinyOS code dissemination protocol. The efficiency of our approach is evaluated by means of testbed experiments showing a significant reduction in message complexity and thus faster updates.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_1_4302_0201_1_11,
  title = {Introducing .NET Assemblies},
  booktitle = {Pro C\# with .NET 3.0},
  year = {None},
  pages = {347-389},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0201-1\_11},
  url = {https://doi.org/10.1007/978-1-4302-0201-1\_11},
  abstract = {Each of the applications developed in this book’s first ten chapters were along the lines of traditional “stand-alone” applications, given that all of your custom programming logic was contained within a single executable file (*.exe). However, one major aspect of the .NET platform is the notion of binary reuse , where applications make use of the types contained within various external assemblies (aka code libraries). The point of this chapter is to examine the core details of creating, deploying, and configuring .NET assemblies.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_1_4302_0422_0_15,
  title = {Introducing .NET Assemblies},
  author = {Troelsen, Andrew},
  booktitle = {Pro C\# 2008 and the .NET 3.5 Platform},
  year = {2007},
  pages = {475-522},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0422-0\_15},
  url = {https://doi.org/10.1007/978-1-4302-0422-0\_15},
  abstract = {E ach of the applications developed in this book’s first fourteen chapters were along the lines of traditional “stand-alone” applications, given that all of your custom programming logic was contained within a single executable file (*.exe). However, one major aspect of the .NET platform is the notion of binary reuse , where applications make use of the types contained within various external assemblies (aka code libraries). The point of this chapter is to examine the core details of creating, deploying, and configuring .NET assemblies.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_1_4302_0469_5_6,
  title = {Backup and Recovery},
  booktitle = {Oracle Database 11g},
  year = {2007},
  pages = {265-311},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0469-5\_6},
  url = {https://doi.org/10.1007/978-1-4302-0469-5\_6},
  abstract = {O racle Database 11 g provides several useful and powerful features relating to the management of backup and recovery and provides several enhancements to RMAN, which is the main Oracle backup and recovery tool. The following are the key new features and enhancements in the backup and recovery area: • Data Recovery Advisor • Enhanced block media recovery • Tighter integration of RMAN and Data Guard • Enhanced archived redo log management • Virtual private recovery catalogs • Network-enabled database duplication without the use of prior backups • Long-term backups • Multisection backups • New validate command},
  content_type = {Chapter},
}

@article{springer_10_1007_s10664_007_9037_5,
  title = {Empirical studies in reverse engineering: state of the art and future trends},
  author = {Tonella, Paolo and Torchiano, Marco and Du Bois, Bart and Systä, Tarja},
  journal = {Empirical Software Engineering},
  year = {2007},
  volume = {12},
  number = {5},
  pages = {551-571},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s10664-007-9037-5},
  url = {https://doi.org/10.1007/s10664-007-9037-5},
  abstract = {Starting with the aim of modernizing legacy systems, often written in old programming languages, reverse engineering has extended its applicability to virtually every kind of software system. Moreover, the methods originally designed to recover a diagrammatic, high-level view of the target system have been extended to address several other problems faced by programmers when they need to understand and modify existing software. The authors’ position is that the next stage of development for this discipline will necessarily be based on empirical evaluation of methods. In fact, this evaluation is required to gain knowledge about the actual effects of applying a given approach, as well as to convince the end users of the positive cost–benefit trade offs. The contribution of this paper to the state of the art is a roadmap for the future research in the field, which includes: clarifying the scope of investigation, defining a reference taxonomy, and adopting a common framework for the execution of the experiments.},
  content_type = {Article},
}

@incollection{springer_10_1007_11823230_21,
  title = {Analysis of Low-Level Code Using Cooperating Decompilers},
  author = {Chang, Bor-Yuh Evan and Harren, Matthew and Necula, George C.},
  booktitle = {Lecture Notes in Computer Science},
  year = {2006},
  pages = {318-335},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/11823230\_21},
  url = {https://doi.org/10.1007/11823230\_21},
  abstract = {Analysis or verification of low-level code is useful for minimizing the disconnect between what is verified and what is actually executed and is necessary when source code is unavailable or is, say, intermingled with inline assembly. We present a modular framework for building pipelines of cooperating decompilers that gradually lift the level of the language to something appropriate for source-level tools. Each decompilation stage contains an abstract interpreter that encapsulates its findings about the program by translating the program into a higher-level intermediate language. We provide evidence for the modularity of this framework through the implementation of multiple decompilation pipelines for both x86 and MIPS assembly produced by gcc , gcj , and coolc (a compiler for a pedagogical Java-like language) that share several low-level components. Finally, we discuss our experimental results that apply the BLAST model checker for C and the Cqual analyzer to decompiled assembly.},
  content_type = {Conference paper},
}

@article{springer_10_1007_s11219_005_1751_x,
  title = {Using Dynamic Information in the Interprocedural Static Slicing of Binary Executables},
  author = {Kiss, Ákos and Jász, Judit and Gyimóthy, Tibor},
  journal = {Software Quality Journal},
  year = {2005},
  volume = {13},
  number = {3},
  pages = {227-245},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1007/s11219-005-1751-x},
  url = {https://doi.org/10.1007/s11219-005-1751-x},
  abstract = {Although the slicing of programs written in a high-level language has been widely studied in the literature, relatively few papers have been published on the slicing of binary executable programs. The lack of existing solutions for the latter is really hard to understand since the application domain for slicing binaries is similar to that for slicing high-level languages. Furthermore, there are special applications of the slicing of programs without source code like source code recovery, code transformation and the detection of security critical code fragments. In this paper, in addition to describing the method of interprocedural static slicing of binaries, we discuss how the set of the possible targets of indirect call sites can be reduced by dynamically gathered information. Our evaluation of the slicing method shows that, if indirect function calls are extensively used, both the number of edges in the call graph and the size of the slices can be significantly reduced.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_1_4302_0160_1_13,
  title = {Introducing .NET Assemblies},
  booktitle = {Pro VB 2005 and the .NET 2.0 Platform},
  year = {None},
  pages = {363-406},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0160-1\_13},
  url = {https://doi.org/10.1007/978-1-4302-0160-1\_13},
  abstract = {This chapter drilled down into the details of how the CLR resolves the location of externally referenced assemblies. You began by examining the content within an assembly: headers, metadata, manifests, and CIL. Then you constructed single-file and multifile assemblies and a handful of client applications (written in a language-agonistic manner). As you have seen, assemblies may be private or shared. Private assemblies are copied to the client’s subdirectory, whereas shared assemblies are deployed to the Global Assembly Cache (GAC), provided they have been assigned a strong name. Finally, as you have seen, private and shared assemblies can be configured using a client-side XML configuration file or, alternatively, via a publisher policy assembly.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_1_4302_0060_4_11,
  title = {Introducing .NET Assemblies},
  booktitle = {Pro C\# 2005 and the .NET 2.0 Platform},
  year = {None},
  pages = {347-389},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0060-4\_11},
  url = {https://doi.org/10.1007/978-1-4302-0060-4\_11},
  abstract = {This chapter drilled down into the details of how the CLR resolves the location of externally referenced assemblies. You began by examining the content within an assembly: headers, metadata, manifests, and CIL. Then you constructed single-file and multifile assemblies and a handful of client applications (written in a language-agonistic manner). As you have seen, assemblies may be private or shared. Private assemblies are copied to the Client’s subdirectory, whereas shared assemblies are deployed to the Global Assembly Cache (GAC), provided they have been assigned a strong name. Finally, has you have seen, private and shared assemblies can be configured using a client-side XML configuration file or, alternatively, via a publisher policy assembly.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_3_540_24591_9_23,
  title = {A Software Fingerprinting Scheme for Java Using Classfiles Obfuscation},
  author = {Fukushima, Kazuhide and Sakurai, Kouichi},
  booktitle = {Lecture Notes in Computer Science},
  year = {2004},
  pages = {303-316},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-24591-9\_23},
  url = {https://doi.org/10.1007/978-3-540-24591-9\_23},
  abstract = {Embedding a personal identifier as a watermark to Java classfile is effective in order to protect copyrights of them. Monden et al. [1] proposed watermarking scheme that embeds arbitrary character sequence to the target method in a Java classfiles. But the scheme can be only used to embed the same watermark to each user’s classfiles. Therefore, if we apply this scheme for embedding each user’s personal identifier, the watermarks can be specified by comparing two or more users’ Java classfiles. In this paper solve the problem by using “Classfiles Obfuscation” which is our obfuscation scheme for Java sourcecodes. By the scheme, we distribute all the methods among the all the Java classfiles at random. Evrey user’s Java classfiles will have different structures respectively by appling “Clasfiles Obfuscation”. As the result, to specify watermark by compareing classfiles will be difficult.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_540_39920_9_7,
  title = {Reconstructing Control Flow from Predicated Assembly Code},
  author = {Decker, Björn and Kästner, Daniel},
  booktitle = {Lecture Notes in Computer Science},
  year = {2003},
  pages = {81-100},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-39920-9\_7},
  url = {https://doi.org/10.1007/978-3-540-39920-9\_7},
  abstract = {Predicated instructions are a feature more and more common in contemporary instruction set architectures. Machine instructions are only executed if an individual guard register associated with the instruction evaluates to true. This enhances execution efficiency, but comes at a price: the control flow of a program is not explicit any more. Instead instructions from the same basic block may belong to different execution paths if they are subject to disjoint guard predicates. Postpass tools processing machine code with the purpose of program analyses or optimizations require the control flow graph of the input program to be known. The effectiveness of postpass analyses and optimizations strongly depends on the precision of the control flow reconstruction. If traditional reconstruction techniques are applied for processors with predicated instructions, their precision is seriously deteriorated. In this paper a generic algorithm is presented that can precisely reconstruct control flow from predicated assembly code. The algorithm is incorporated in the Propan system that enables high-quality machine-dependent postpass optimizers to be generated from a concise hardware specification. The control flow reconstruction algorithm is machine-independent, and automatically derives the required hardware-specific knowledge from the machine specification. Experimental results obtained for the Philips TriMedia TM1000 processor show that the precision of the reconstructed control flow is significantly higher than with reconstruction algorithms that do not specifically take predicated instructions into account.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_1_4302_0778_8_11,
  title = {VB.Classic Debugging},
  author = {Pearce, Mark},
  booktitle = {Comprehensive VB .NET Debugging},
  year = {2003},
  pages = {321-338},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-0778-8\_11},
  url = {https://doi.org/10.1007/978-1-4302-0778-8\_11},
  abstract = {It’s clear that in spite of the arrival of .NET, VB.Classic applications are still going to be around for a long time. Moving a company’s business applications from VB.Classic to VB .NET is simply not a cost-effective proposition in most cases, not to mention the problem with re-skilling whole IT departments. Fortunately, Microsoft realizes that it isn’t feasible to ask companies to abandon or migrate all of their legacy code, so they’ve provided a COM Interoperability (usually shortened to “Interop”) layer to allow cooperation between the managed and unmanaged worlds. This chapter discusses the debugging of the COM Interop layer between VB .NET and VB.Classic.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_3_540_45603_1_84,
  title = {5dpo Team Description},
  author = {Costa, Paulo and Sousa, Armando and Marques, Paulo and Costa, Pedro and Gaio, Susana and Moreira, António},
  booktitle = {Lecture Notes in Computer Science},
  year = {2002},
  pages = {563-566},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/3-540-45603-1\_84},
  url = {https://doi.org/10.1007/3-540-45603-1\_84},
  abstract = {The 5dpo team presented a solid set of innovative solutions. The overall workings of the team are presented. Mechanical and electronic solutions are explained and closed loop working is discussed. Main innovative features include I-R communications link and circular bar code for robot tracking. Low level control now presents a dynamics prediction layer for enhanced motion control. Team strategy is also new and a multi-layered high level reasoning system based on state charts allows for cooperative game play.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_3_540_36377_7_14,
  title = {A Symmetric Approach to Compilation and Decompilation},
  author = {Sig Ager, Mads and Danvy, Olivier and Goldberg, Mayer},
  booktitle = {Lecture Notes in Computer Science},
  year = {2002},
  pages = {296-331},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/3-540-36377-7\_14},
  url = {https://doi.org/10.1007/3-540-36377-7\_14},
  abstract = {Just as an interpreter for a source language can be turned into a compiler from the source language to a target language, we observe that an interpreter for a target language can be turned into a compiler from the target language to a source language. In both cases, the key issue is the choice of whether to perform an evaluation or to emit code that represents this evaluation. We substantiate this observation with two source interpreters and two target interpreters. We first consider a source language of arithmetic expressions and a target language for a stack machine, and then the λ- calculus and the SECD-machine language. In each case, we prove that the target-to-source compiler is a left inverse of the source-to-target compiler, i.e., that it is a decompiler. In the context of partial evaluation, the binding-time shift of going from a source interpreter to a compiler is classically referred to as a Futamura projection. By symmetry, it seems logical to refer to the binding-time shift of going from a target interpreter to a compiler as a Futamura embedding.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_1_4302_1147_1_5,
  title = {Virtual Frameworks—Error Control},
  author = {Birmingham, David C. and Perry, Valerie Haynes},
  booktitle = {Software Development on a Leash},
  year = {2002},
  pages = {273-304},
  publisher = {Apress},
  doi = {10.1007/978-1-4302-1147-1\_5},
  url = {https://doi.org/10.1007/978-1-4302-1147-1\_5},
  abstract = {A virtual framework , like the one illustrated in Figure 5-1, is actually an assembly of frameworks, each with a role or scope of responsibility, and ideally integrated to the remainder with an interpreting behavioral controller. The application prototype and project exercises in Chapter 4 show how interpretation provides the virtual glue to bind these elements together.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_3_540_45309_1_22,
  title = {Typestate Checking of Machine Code},
  author = {Xu, Zhichen and Reps, Thomas and Miller, Barton P.},
  booktitle = {Lecture Notes in Computer Science},
  year = {2001},
  pages = {335-351},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/3-540-45309-1\_22},
  url = {https://doi.org/10.1007/3-540-45309-1\_22},
  abstract = {We check statically whether it is safe for untrusted foreign machine code to be loaded into a trusted host system. Our technique works on ordinary machine code, and mechanically synthesizes (and verifies) a safety proof. Our earlier work along these lines was based on a C-like type system, which does not suffice for machine code whose origin is C++ source code. In the present paper, we address this limitation with an improved typestate system and introduce several new techniques, including: summarizing the effects of function calls so that our analysis can stop at trusted boundaries, inferring information about the sizes and types of stack-allocated arrays, and a symbolic range analysis for propagating information about array bounds. These techniques make our approach to safety checking more precise, more efficient, and able to handle a larger collection of real-life code sequences than was previously the case.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_978_3_642_56691_2_3,
  title = {Structuring the Field},
  author = {Fleischmann, Moritz},
  booktitle = {Lecture Notes in Economics and Mathematical Systems},
  year = {2001},
  pages = {17-34},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-642-56691-2\_3},
  url = {https://doi.org/10.1007/978-3-642-56691-2\_3},
  abstract = {The objective of this chapter is to lay out a structure of Reverse Logistics serving as a point of reference in the remainder of our investigation. Section 3.1 discusses major dimensions of the Reverse Logistics context, namely drivers, actors, dispositioning options, and cycle times. Based on these aspects, Section 3.2 characterises different categories of Reverse Logistics flows. Section 3.3 concludes the chapter with a literature review complementing the field of our investigation.},
  content_type = {Chapter},
}

@article{springer_10_1023_a_1018993212326,
  title = {The legal status of reverse engineering of computer software},
  author = {Cifuentes, Cristina and Fitzgerald, Anne},
  journal = {Annals of Software Engineering},
  year = {2000},
  volume = {9},
  number = {1-4},
  pages = {337-351},
  publisher = {Springer Science and Business Media LLC},
  doi = {10.1023/a:1018993212326},
  url = {https://doi.org/10.1023/a:1018993212326},
  abstract = {Reverse engineering of computer software has assumed greater importance in recent years because of the need to examine legacy code to remove the year 2000 bug. There are different types of reverse engineering based on the level of abstraction of the code to be reengineered; machine code, assembly code, source code or even CASE code. We describe the different types of reverse engineering and the extent of copyright protection for software. The most common uses of reverse engineering are described. This provides for a comparative overview of the legal standing on reverse engineering at the international level. We propose challenges to the global electronic community in relation to existing and future legislation in the area of reverse engineering and protection of digital works.},
  content_type = {Article},
}

@incollection{springer_10_1007_978_1_4757_3169_9_9,
  title = {Conclusions},
  author = {Leupers, Rainer},
  booktitle = {Code Optimization Techniques for Embedded Processors},
  year = {2000},
  pages = {181-182},
  publisher = {Springer US},
  doi = {10.1007/978-1-4757-3169-9\_9},
  url = {https://doi.org/10.1007/978-1-4757-3169-9\_9},
  abstract = {The need for higher productivity in embedded system design demands for taking the step from assembly to C level programming of embedded processors. While compiler technology is relatively mature for general-purpose processors, this generally is not the case for embedded processors. The reason is that embedded systems require higher code efficiency than general-purpose systems. Simultaneously, recent embedded processors show specialized architectural features that cannot be well exploited with standard compiler techniques. However, C compilers for embedded processors will only gain more acceptance in industry, if their code quality overhead is low as compared to assembly code. The key concept to overcome these problems is the development of new compiler techniques that take the special constraints in embedded system design into account.},
  content_type = {Chapter},
}

@incollection{springer_10_1007_978_1_4757_3169_9_1,
  title = {Introduction},
  author = {Leupers, Rainer},
  booktitle = {Code Optimization Techniques for Embedded Processors},
  year = {2000},
  pages = {1-28},
  publisher = {Springer US},
  doi = {10.1007/978-1-4757-3169-9\_1},
  url = {https://doi.org/10.1007/978-1-4757-3169-9\_1},
  abstract = {According to the 1998 SIA roadmap [SIA98], integrated circuits (ICs) at the end of this decade will typically have more than 500 million transistors, an integration scale more than one order of magnitude higher than in today’s microelectronics. For the high performance processor segment, even larger integration scales have been predicted, with processor performance reaching 3 million MIPS by the year 2010 [Tech00].},
  content_type = {Chapter},
}

@incollection{springer_10_1007_3_540_49099_x_14,
  title = {Type-Based Decompilation (or Program Reconstruction via Type Reconstruction)},
  author = {Mycroft, Alan},
  booktitle = {Lecture Notes in Computer Science},
  year = {1999},
  pages = {208-223},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/3-540-49099-x\_14},
  url = {https://doi.org/10.1007/3-540-49099-x\_14},
  abstract = {We describe a system which decompiles (reverse engineers) C programs from target machine code by type-inference techniques. This extends recent trends in the converse process of compiling high-level languages whereby type information is preserved during compilation. The algorithms remain independent of the particular architecture by virtue of treating target instructions as register-transfer specifications. Target code expressed in such RTL form is then transformed into SSA form (undoing register colouring etc.); this then generates a set of type constraints. Iteration and recursion over data-structures causes synthesis of appropriate recursive C structs; this is triggered by and resolves occurs-check constraint violation. Other constraint violations are resolved by C’s casts and unions. In the limit we use heuristics to select between equally suitable C code — a good GUI would clearly facilitate its professional use.},
  content_type = {Conference paper},
}

@incollection{springer_10_1007_3_540_63531_9_27,
  title = {Cryptographic verification of test coverage claims},
  author = {Devanbu, Prem and Stubblebine, Stuart G.},
  booktitle = {Lecture Notes in Computer Science},
  year = {1997},
  pages = {395-413},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/3-540-63531-9\_27},
  url = {https://doi.org/10.1007/3-540-63531-9\_27},
  abstract = {The market for software components is growing, driven on the “demand side” by the need for rapid deployment of highly functional products, and on the “supply side” by distributed object standards. As components and component vendors proliferate, there is naturally a growing concern about quality, and the effectiveness of testing processes. White box testing, particularly the use of coverage criteria, is a widely used method for measuring the “thoroughness” of testing efforts. High levels of test coverage are used as indicators of good quality control procedures. Software vendors who can demonstrate high levels of test coverage have a credible claim to high quality. However, verifying such claims involves knowledge of the source code, test cases, build procedures etc. In applications where reliability and quality are critical, it would be desirable to verify test coverage claims without forcing vendors to give up valuable technical secrets. In this paper, we explore cryptographic techniques that can be used to verify such claims. Our techniques have some limitations; however, if such methods can be perfected and popularized, they can have an important “leveling” effect on the software market place: small, relatively unknown software vendors with limited resources can provide credible evidence of high-quality processes, and thus compete with much larger corporations.},
  content_type = {Conference paper},
}
