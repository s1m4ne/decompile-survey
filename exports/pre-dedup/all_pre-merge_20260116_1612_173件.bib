
@article{dramko_fast_2025,
	address = {New York, NY, USA},
	title = {Fast, {Fine}-{Grained} {Equivalence} {Checking} for {Neural} {Decompilers}},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3772368},
	doi = {10.1145/3772368},
	abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models are generally inadequate, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign,1 a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides—which parts of the functions are equivalent and how well the variable names match—is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Dramko, Luke and Le Goues, Claire and Schwartz, Edward J.},
	month = oct,
	year = {2025},
	keywords = {Program Analysis, Alignment, Program Equivalence},
	annote = {Just Accepted},
}

@article{lagouvardos_incredible_2025,
	address = {New York, NY, USA},
	title = {The {Incredible} {Shrinking} {Context}... in a {Decompiler} {Near} {You}},
	volume = {2},
	url = {https://doi.org/10.1145/3728935},
	doi = {10.1145/3728935},
	abstract = {Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrnkr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrnkr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly “forgetting” control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5\% of contracts (compared to ∼95\% for Elipmoc), covers (i.e., reaches and manages to decompile) 67\% more code than Heimdall-rs, and reduces key imprecision metrics by over 65\%, compared again to Elipmoc.},
	number = {ISSTA},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Lagouvardos, Sifis and Bollanos, Yannis and Grech, Neville and Smaragdakis, Yannis},
	month = jun,
	year = {2025},
	keywords = {Decompilation, Program Analysis, Datalog, Ethereum, Smart Contracts},
}

@article{su_disco_2025,
	address = {New York, NY, USA},
	title = {{DiSCo}: {Towards} {Decompiling} {EVM} {Bytecode} to {Source} {Code} using {Large} {Language} {Models}},
	volume = {2},
	url = {https://doi.org/10.1145/3729373},
	doi = {10.1145/3729373},
	abstract = {Understanding the Ethereum smart contract bytecode is essential for ensuring cryptoeconomics security. However, existing decompilers primarily convert bytecode into pseudocode, which is not easily comprehensible for general users, potentially leading to misunderstanding of contract behavior and increased vulnerability to scams or exploits. In this paper, we propose DiSCo, the first LLMs-based EVM decompilation pipeline, which aims to enable LLMs to understand the opaque bytecode and lift it into smart contract code. DiSCo introduces three core technologies. First, a logic-invariant intermediate representation is proposed to reproject the low-level bytecode into high-level abstracted units. The second technique involves semantic enhancement based on a novel type-aware graph model to infer stripped variables during compilation, enhancing the lifting effect. The third technology is a flexible method incorporating code specifications to construct LLM-comprehensible prompts for source code generation. Extensive experiments illustrate that our generated code guarantees a high compilability rate at 75\%, with differential fuzzing pass rate averaging at 50\%. Manual validation results further indicate that the generated solidity contracts significantly outperforms baseline methods in tasks such as code comprehension and attack reproduction.},
	number = {FSE},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Su, Xing and Liang, Hanzhong and Wu, Hao and Niu, Ben and Xu, Fengyuan and Zhong, Sheng},
	month = jun,
	year = {2025},
	keywords = {Decompilation, EVM bytecode, Large Language Models, Smart Contract, Source Code Generation},
}

@inproceedings{chen_suigpt_2025,
	address = {New York, NY, USA},
	series = {{WWW} '25},
	title = {{SuiGPT} {MAD}: {Move} {AI} {Decompiler} to {Improve} {Transparency} and {Auditability} on {Non}-{Open}-{Source} {Blockchain} {Smart} {Contract}},
	isbn = {979-8-4007-1274-6},
	url = {https://doi.org/10.1145/3696410.3714790},
	doi = {10.1145/3696410.3714790},
	abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33\% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization. Moreover, MAD's methodology could potentially extend to other smart contract languages, like Solidity, further enhancing Web3 transparency.},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2025},
	publisher = {Association for Computing Machinery},
	author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Wu, Tingguan and Wang, Siyun and Chalkias, Kostas Kryptos},
	year = {2025},
	keywords = {smart contract, auditing tools, large language models, move, prompt engineering, sui, transparency, web applications, web3},
	pages = {1567--1576},
}

@inproceedings{cotroneo_can_2025,
	address = {New York, NY, USA},
	series = {{EuroSec}'25},
	title = {Can {Neural} {Decompilation} {Assist} {Vulnerability} {Prediction} on {Binary} {Code}?},
	isbn = {979-8-4007-1563-1},
	url = {https://doi.org/10.1145/3722041.3723097},
	doi = {10.1145/3722041.3723097},
	abstract = {Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.},
	booktitle = {Proceedings of the 18th {European} {Workshop} on {Systems} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Cotroneo, Domenico and Grasso, Francesco C. and Natella, Roberto and Orbinato, Vittorio},
	year = {2025},
	keywords = {Security, Deep Learning, Binary Analysis, Neural Decompilation, Vulnerability Prediction},
	pages = {26--32},
}

@inproceedings{she_wadec_2024,
	address = {New York, NY, USA},
	series = {{ASE} '24},
	title = {{WaDec}: {Decompiling} {WebAssembly} {Using} {Large} {Language} {Model}},
	isbn = {979-8-4007-1248-7},
	url = {https://doi.org/10.1145/3691620.3695020},
	doi = {10.1145/3691620.3695020},
	abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art's 116.94\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%. In summary, WaDec enhances understanding of the code's structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
	year = {2024},
	pages = {481--492},
}

@article{lu_understanding_2024,
	address = {New York, NY, USA},
	title = {Understanding and {Finding} {Java} {Decompiler} {Bugs}},
	volume = {8},
	url = {https://doi.org/10.1145/3649860},
	doi = {10.1145/3649860},
	abstract = {Java decompilers are programs that perform the reverse process of Java compilers, i.e., they translate Java bytecode to Java source code. They are essential for reverse engineering purposes and have become more sophisticated and reliable over the years. However, it remains challenging for modern Java decompilers to reliably perform correct decompilation on real-world programs. To shed light on the key challenges of Java decompilation, this paper provides the first systematic study on the characteristics and causes of bugs in mature, widely-used Java decompilers. We conduct the study by investigating 333 unique bugs from three popular Java decompilers. Our key findings and observations include: (1) Although most of the reported bugs were found when decompiling large, real-world code, 40.2\% of them have small test cases for bug reproduction; (2) Over 80\% of the bugs manifest as exceptions, syntactic errors, or semantic errors, and bugs with source code artifacts are very likely semantic errors; (3) 57.7\%,39.0\%, and 41.1\% of the bugs respectively are attributed to three stages of decompilers—loading structure entities from bytecode, optimizing these entities, and generating source code from these entities; (4) Bugs in decompilers’ type inference are the most complex to fix; and (5) Region restoration for structures like loop, sugaring for special structures like switch, and type inference of variables of generic types or indistinguishable types are the three most significant challenges in Java decompilation, which to some extent explains our findings in (3) and (4).Based on these findings, we present JD-Tester, a differential testing framework for Java decompilers, and our experience of using it in testing the three popular Java decompilers. JD-Tester utilizes different Java program generators to construct executable Java tests and finds exceptions, syntactic, and semantic inconsistencies (i.e. bugs) between a generated test and its compiled-decompiled version (through compilation and execution). In total, we have found 62 bugs in the three decompilers, demonstrating both the effectiveness of JD-Tester, and the importance of testing and validating Java decompilers.},
	number = {OOPSLA1},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Lu, Yifei and Hou, Weidong and Pan, Minxue and Li, Xuandong and Su, Zhendong},
	month = apr,
	year = {2024},
	keywords = {Reverse Engineering, Decompiler, Differential Testing},
}

@inproceedings{armengol-estape_slade_2024,
	series = {{CGO} '24},
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	isbn = {979-8-3503-9509-9},
	url = {https://doi.org/10.1109/CGO57630.2024.10444788},
	doi = {10.1109/CGO57630.2024.10444788},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect.This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code.We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	booktitle = {Proceedings of the 2024 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization}},
	publisher = {IEEE Press},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F. P.},
	year = {2024},
	keywords = {decompilation, neural decompilation, language models, transformer, type inference},
	pages = {67--80},
}

@article{sisco_loop_2023,
	address = {New York, NY, USA},
	title = {Loop {Rerolling} for {Hardware} {Decompilation}},
	volume = {7},
	url = {https://doi.org/10.1145/3591237},
	doi = {10.1145/3591237},
	abstract = {We introduce the new problem of hardware decompilation. Analogous to software decompilation, hardware decompilation is about analyzing a low-level artifact—in this case a netlist, i.e., a graph of wires and logical gates representing a digital circuit—in order to recover higher-level programming abstractions, and using those abstractions to generate code written in a hardware description language (HDL). The overall problem of hardware decompilation requires a number of pieces. In this paper we focus on one specific piece of the puzzle: a technique we call hardware loop rerolling. Hardware loop rerolling leverages clone detection and program synthesis techniques to identify repeated logic in netlists (such as would be synthesized from loops in the original HDL code) and reroll them into syntactic loops in the recovered HDL code. We evaluate hardware loop rerolling for hardware decompilation over a set of hardware design benchmarks written in the PyRTL HDL and industry standard SystemVerilog. Our implementation identifies and rerolls loops in 52 out of 53 of the netlists in our benchmark suite, and we show three examples of how hardware decompilation can provide concrete benefits: transpilation between HDLs, faster simulation times over netlists (with mean speedup of 6x), and artifact compaction (39\% smaller on average).},
	number = {PLDI},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Sisco, Zachary D. and Balkind, Jonathan and Sherwood, Timothy and Hardekopf, Ben},
	month = jun,
	year = {2023},
	keywords = {hardware decompilation, loop rerolling, program synthesis},
}

@article{dramko_dire_2023,
	address = {New York, NY, USA},
	title = {{DIRE} and its {Data}: {Neural} {Decompiled} {Variable} {Renamings} with {Respect} to {Software} {Class}},
	volume = {32},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3546946},
	doi = {10.1145/3546946},
	abstract = {The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE’s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23\% more frequently than the original DIRE model.},
	number = {2},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Dramko, Luke and Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Ed and Allamanis, Miltiadis and Neubig, Graham and Vasilescu, Bogdan and Le Goues, Claire},
	month = mar,
	year = {2023},
	keywords = {decompilation, Machine learning, data provenance},
}

@inproceedings{cao_boosting_2022,
	address = {New York, NY, USA},
	series = {{ACSAC} '22},
	title = {Boosting {Neural} {Networks} to {Decompile} {Optimized} {Binaries}},
	isbn = {978-1-4503-9759-9},
	url = {https://doi.org/10.1145/3564625.3567998},
	doi = {10.1145/3564625.3567998},
	abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21\% higher accuracy than state-of-the-art neural decompilation frameworks.},
	booktitle = {Proceedings of the 38th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei},
	year = {2022},
	pages = {508--518},
}

@article{grech_elipmoc_2022,
	address = {New York, NY, USA},
	title = {Elipmoc: advanced decompilation of {Ethereum} smart contracts},
	volume = {6},
	url = {https://doi.org/10.1145/3527321},
	doi = {10.1145/3527321},
	abstract = {Smart contracts on the Ethereum blockchain greatly benefit from cutting-edge analysis techniques and pose significant challenges. A primary challenge is the extremely low-level representation of deployed contracts. We present Elipmoc, a decompiler for the next generation of smart contract analyses. Elipmoc is an evolution of Gigahorse, the top research decompiler, dramatically improving over it and over other state-of-the-art tools, by employing several high-precision techniques and making them scalable. Among these techniques are a new kind of context sensitivity (termed “transactional sensitivity”) that provides a more effective static abstraction of distinct dynamic executions; a path-sensitive (yet scalable, through path merging) algorithm for inference of function arguments and returns; and a fully context sensitive private function reconstruction process. As a result, smart contract security analyses and reverse-engineering tools built on top of Elipmoc achieve high scalability, precision and completeness. Elipmoc improves over all notable past decompilers, including its predecessor, Gigahorse, and the state-of-the-art industrial tool, Panoramix, integrated into the primary Ethereum blockchain explorer, Etherscan. Elipmoc produces decompiled contracts with fully resolved operands at a rate of 99.5\% (compared to 62.8\% for Gigahorse), and achieves much higher completeness in code decompilation than Panoramix—e.g., up to 67\% more coverage of external call statements—while being over 5x faster. Elipmoc has been the enabler for recent (independent) discoveries of several exploitable vulnerabilities on popular protocols, over funds in the many millions of dollars.},
	number = {OOPSLA1},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Grech, Neville and Lagouvardos, Sifis and Tsatiris, Ilias and Smaragdakis, Yannis},
	month = apr,
	year = {2022},
	keywords = {Security, Decompilation, Program Analysis, Datalog, Ethereum, Smart Contracts, Blockchain},
}

@inproceedings{jaffe_meaningful_2018,
	address = {New York, NY, USA},
	series = {{ICPC} '18},
	title = {Meaningful variable names for decompiled code: a machine translation approach},
	isbn = {978-1-4503-5714-2},
	url = {https://doi.org/10.1145/3196321.3196330},
	doi = {10.1145/3196321.3196330},
	abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2\% of the time, which represents a lower bound for performance.},
	booktitle = {Proceedings of the 26th {Conference} on {Program} {Comprehension}},
	publisher = {Association for Computing Machinery},
	author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan},
	year = {2018},
	pages = {20--30},
}

@inproceedings{lacomis_dire_2020,
	series = {{ASE} '19},
	title = {{DIRE}: a neural approach to decompiled identifier naming},
	isbn = {978-1-7281-2508-4},
	url = {https://doi.org/10.1109/ASE.2019.00064},
	doi = {10.1109/ASE.2019.00064},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	booktitle = {Proceedings of the 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	year = {2020},
	pages = {628--639},
}

@inproceedings{grech_gigahorse_2019,
	series = {{ICSE} '19},
	title = {Gigahorse: thorough, declarative decompilation of smart contracts},
	url = {https://doi.org/10.1109/ICSE.2019.00120},
	doi = {10.1109/ICSE.2019.00120},
	abstract = {The rise of smart contracts—autonomous applications running on blockchains—has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract.We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a high-level 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts.Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts—e.g., Gigahorse can decompile over 99.98\% of deployed contracts, compared to 88\% for the recently-published Vandal decompiler and under 50\% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a "batteries included" approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Grech, Neville and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	year = {2019},
	keywords = {decompilation, program analysis, blockchain, ethereum},
	pages = {1176--1186},
}

@article{dallaglio_highliner_2025,
	address = {New York, NY, USA},
	title = {Highliner: {Enhancing} {Binary} {Analysis} through {NLP}-{Based} {Instruction}-{Level} {Detection} of {C}++ {Inline} {Functions}},
	volume = {28},
	issn = {2471-2566},
	url = {https://doi.org/10.1145/3765521},
	doi = {10.1145/3765521},
	abstract = {The complexities introduced by compiler optimization have long stood as a significant obstacle in binary analysis and reverse engineering. Function inlining, in particular, complicates function recognition by replacing function calls with the entire body of the callee, mixing code from multiple functions. State-of-the-art approaches can identify inlined functions at basic block granularity, but cannot determine which instructions belong to each function and precisely deduce inlined boundaries. Without this information, further analyses such as decompilation cannot be performed effectively. This article presents Highliner, a novel approach that improves state-of-the-art approaches by identifying inline instances at instruction-level granularity. Highliner operates downstream of block-level detectors: given basic blocks reported by state-of-the-art approaches as belonging to a specific inlined function, it labels each instruction as Inlined or Not inlined and recovers the inlined-function boundaries. We treat the problem as a sequence tagging task typical of NLP and implement a learning-based technique involving instruction embedding and recurrent neural networks. We compile a dataset of open-source projects with different optimizations and use the DWARF debug information standard to construct labeled sequences of inline instructions. We use this dataset to train, validate, and test a sequence labeling architecture in which instructions are encoded via the pre-trained assembly language transformer PalmTree and then processed by an RNN-based classifier to produce binary predictions. When evaluated as a binary classifier, Highliner achieves an F1-score of 0.94 overall. In addition, when specifically tested on recognizing function boundaries, Highliner achieves an Accuracy of 0.82 on initial boundaries and 0.83 on final boundaries.},
	number = {4},
	journal = {ACM Trans. Priv. Secur.},
	publisher = {Association for Computing Machinery},
	author = {Dall'Aglio, Lorenzo and Binosi, Lorenzo and Carminati, Michele and Zanero, Stefano and Polino, Mario},
	month = oct,
	year = {2025},
	keywords = {reverse engineering, Binary analysis, function inlining, inline function recognition, natural language processing (NLP)},
}

@inproceedings{chen_clearagent_2025,
	address = {New York, NY, USA},
	series = {{LMPL} '25},
	title = {{ClearAgent}: {Agentic} {Binary} {Analysis} for {Effective} {Vulnerability} {Detection}},
	isbn = {979-8-4007-2148-9},
	url = {https://doi.org/10.1145/3759425.3763397},
	doi = {10.1145/3759425.3763397},
	abstract = {Statically detecting vulnerabilities at the binary level is crucial for the security of Commercial-Off-The-Shelf (COTS) software when source code is not available. However, traditional methods suffer from the inherent limitations of binary translation and static analysis, which hinder their scalability for complex real-world binaries. Recent efforts that leverage Large Language Models (LLMs) for vulnerability detection are still limited by possible hallucination, inaccurate code property retrieval, and insufficient guidance. In this paper, we propose a new agentic binary analysis framework ClearAgent, which features a novel binary interface that provides both LLM-friendly and analyzer-friendly tools to facilitate effective understanding of binary code semantics with rich context. ClearAgent works by automatically interacting with the interface and iteratively exploring for buggy binary code. For candidate bug reports, ClearAgent further tries to verify the existence of the vulnerability by constructing concrete inputs that can trigger the buggy locations.},
	booktitle = {Proceedings of the 1st {ACM} {SIGPLAN} {International} {Workshop} on {Language} {Models} and {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Xiang and Zhou, Anshunkang and Ye, Chengfeng and Zhang, Charles},
	year = {2025},
	keywords = {Binary Analysis, Agent, Vulnerability Detection},
	pages = {130--137},
}

@inproceedings{he_benchmarking_2025,
	address = {New York, NY, USA},
	series = {{ISSTA} {Companion} '25},
	title = {On {Benchmarking} {Code} {LLMs} for {Android} {Malware} {Analysis}},
	isbn = {979-8-4007-1474-0},
	url = {https://doi.org/10.1145/3713081.3731745},
	doi = {10.1145/3713081.3731745},
	abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names.This paper presents Cama, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. Cama specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics—consistency, fidelity, and semantic relevance—enabling rigorous stability and effectiveness assessment and cross-model comparison.We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use Cama to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.},
	booktitle = {Proceedings of the 34th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {He, Yiling and She, Hongyu and Qian, Xingzhi and Zheng, Xinran and Chen, Zhuo and Qin, Zhan and Cavallaro, Lorenzo},
	year = {2025},
	keywords = {malware analysis, code LLM},
	pages = {153--160},
}

@inproceedings{zhou_regraph_2025,
	address = {New York, NY, USA},
	series = {{ISSTA} {Companion} '25},
	title = {{ReGraph}: {A} {Tool} for {Binary} {Similarity} {Identification}},
	isbn = {979-8-4007-1474-0},
	url = {https://doi.org/10.1145/3713081.3731728},
	doi = {10.1145/3713081.3731728},
	abstract = {Binary Code Similarity Detection (BCSD) is not only essential for security tasks such as vulnerability identification but also for code copying detection, yet it remains challenging due to binary stripping and diverse compilation environments. Existing methods tend to adopt increasingly complex neural networks for better accuracy performance. The computation time increases with the complexity. Even with powerful GPUs, the treatment of large-scale software becomes time-consuming. To address these issues, we present a framework called ReGraph to efficiently compare binary code functions across architectures and optimization levels. Our evaluation with public datasets highlights that ReGraph exhibits a significant speed advantage, performing 700 times faster than Natural Language Processing (NLP)-based methods while maintaining comparable accuracy results with respect to the state-of-the-art models.},
	booktitle = {Proceedings of the 34th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Li and Dacier, Marc and Konstantinou, Charalambos},
	year = {2025},
	keywords = {binary code re-optimization, binary code similarity detection, code lifting, code property graph, graph neural network},
	pages = {6--10},
}

@inproceedings{verbeek_formally_2025,
	series = {{ICSE} '25},
	title = {Formally {Verified} {Binary}-{Level} {Pointer} {Analysis}},
	isbn = {979-8-3315-0569-1},
	url = {https://doi.org/10.1109/ICSE55347.2025.00231},
	doi = {10.1109/ICSE55347.2025.00231},
	abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the tradeoff between scalability and precision, such customization allows "meaningful" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
	booktitle = {Proceedings of the {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
	year = {2025},
	keywords = {binary analysis, formal methods, pointer analysis},
	pages = {42--53},
}

@article{zhang_cf-gkat_2025,
	address = {New York, NY, USA},
	title = {{CF}-{GKAT}: {Efficient} {Validation} of {Control}-{Flow} {Transformations}},
	volume = {9},
	url = {https://doi.org/10.1145/3704857},
	doi = {10.1145/3704857},
	abstract = {Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT’s abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren’s goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation.},
	number = {POPL},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Cheng and Kappé, Tobias and Narváez, David E. and Naus, Nico},
	month = jan,
	year = {2025},
	keywords = {control flow recovery, Kleene algebra, Program equivalence},
}

@article{udeshi_remaqe_2024,
	address = {New York, NY, USA},
	title = {{REMaQE}: {Reverse} {Engineering} {Math} {Equations} from {Executables}},
	volume = {8},
	issn = {2378-962X},
	url = {https://doi.org/10.1145/3699674},
	doi = {10.1145/3699674},
	abstract = {Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This article proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer equations from object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool “tmon.” To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow.},
	number = {4},
	journal = {ACM Trans. Cyber-Phys. Syst.},
	publisher = {Association for Computing Machinery},
	author = {Udeshi, Meet and Krishnamurthy, Prashanth and Pearce, Hammond and Karri, Ramesh and Khorrami, Farshad},
	month = jan,
	year = {2024},
	keywords = {symbolic execution, binary reverse engineering, embedded systems, mathematical equations},
}

@article{li_varlifter_2024,
	address = {New York, NY, USA},
	title = {{VarLifter}: {Recovering} {Variables} and {Types} from {Bytecode} of {Solidity} {Smart} {Contracts}},
	volume = {8},
	url = {https://doi.org/10.1145/3689711},
	doi = {10.1145/3689711},
	abstract = {Since funds or tokens in smart contracts are maintained through specific state variables, contract audit, an effective means for security assurance, particularly focuses on these variables and their related operations. However, the absence of publicly accessible source code for numerous contracts, with only bytecode exposed, hinders audit efforts. Recovering variables and their types from Solidity bytecode is thus a critical task in smart contract analysis and audit, yet this is a challenging task because the bytecode loses variable and type information, only with low-level data operated by stack manipulations and untyped memory/storage accesses. The state-of-the-art smart contract decompilers miss identifying many variables and incorrectly infer the types for many identified variables. To this end, we propose VarLifter, a lifter dedicated to the precise and efficient recovery of typed variables. VarLifter interprets every read or written field of a data region as at least one potential variable, and after discarding falsely identified variables, it progressively refines the variable types based on the variable behaviors in the form of operation sequences. We evaluate VarLifter on 34,832 real-world Solidity smart contracts. VarLifter attains a precision of 97.48\% and a recall of 91.84\% for typed variable recovery. Moreover, VarLifter finishes analyzing 77\% of smart contracts in around 10 seconds per contract. If VarLifter is used to replace the variable recovery modules of the two state-of-the-art Solidity bytecode decompilers, 52.4\%, and 74.6\% more typed variables will be correctly recovered, respectively. The applications of VarLifter to contract decompilation, contract audit, and contract bytecode fuzzing illustrate that the recovered variable information improves many contract analysis tasks.},
	number = {OOPSLA2},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Li, Yichuan and Song, Wei and Huang, Jeff},
	month = oct,
	year = {2024},
	keywords = {smart contract, Blockchain, EVM, Solidity bytecode, variable recovery},
}

@article{pizzolotto_mitigating_2024,
	address = {New York, NY, USA},
	title = {Mitigating {Debugger}-based {Attacks} to {Java} {Applications} with {Self}-debugging},
	volume = {33},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/3631971},
	doi = {10.1145/3631971},
	abstract = {Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode.In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them.We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.},
	number = {4},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	publisher = {Association for Computing Machinery},
	author = {Pizzolotto, Davide and Berlato, Stefano and Ceccato, Mariano},
	month = apr,
	year = {2024},
	keywords = {Anti-debugging, maliciuos reverse engineering, man at the end attacks, tampering attacks},
}

@inproceedings{zhao_deepinfer_2023,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {{DeepInfer}: {Deep} {Type} {Inference} from {Smart} {Contract} {Bytecode}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3616343},
	doi = {10.1145/3611643.3616343},
	abstract = {Smart contracts play an increasingly important role in Ethereum platform. It provides various functions implementing numerous services, whose bytecode runs on Ethereum Virtual Machine. To use services by invoking corresponding functions, the callers need to know the function signatures. Moreover, such signatures provide crucial information for many downstream applications, e.g., identifying smart contracts, fuzzing, detecting vulnerabilities, etc. However, it is challenging to infer function signatures from the bytecode due to a lack of type information. Existing work solving this problem depended heavily on limited databases or hard-coded heuristic patterns. However, these approaches are hard to be adapted to semantic differences in distinct languages and various compiler versions when developing smart contracts. In this paper, we propose a novel framework DeepInfer that first leverages deep learning techniques to automatically infer function signatures and returns. The novelties of DeepInfer are: 1) DeepInfer lifts the bytecode into the Intermediate Representation (IR) to preserve code semantics; 2) DeepInfer extracts the type-related knowledge (e.g., critical data flows, constant values, and control flow graphs) from the IR to recover function signatures and returns. We conduct experiments on Solidity and Vyper smart contracts and the results show that DeepInfer performs faster and more accurate than existing tools, while being immune to changes in different languages and various compiler versions.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Kunsong and Li, Zihao and Li, Jianfeng and Ye, He and Luo, Xiapu and Chen, Ting},
	year = {2023},
	keywords = {Deep Learning, Type Inference, Smart Contract},
	pages = {745--757},
}

@inproceedings{xiong_hext5_2024,
	series = {{ASE} '23},
	title = {{HexT5}: {Unified} {Pre}-{Training} for {Stripped} {Binary} {Code} {Information} {Inference}},
	isbn = {979-8-3503-2996-4},
	url = {https://doi.org/10.1109/ASE56229.2023.00099},
	doi = {10.1109/ASE56229.2023.00099},
	abstract = {Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pretraining objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
	booktitle = {Proceedings of the 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
	year = {2024},
	keywords = {deep learning, reverse engineering, binary diffing, information inference, programming language model},
	pages = {774--786},
}

@inproceedings{verbeek_formally_2022,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Formally verified lifting of {C}-compiled x86-64 binaries},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523702},
	doi = {10.1145/3519939.3523702},
	abstract = {Lifting binaries to a higher-level representation is an essential step for decompilation, binary verification, patching and security analysis. In this paper, we present the first approach to provably overapproximative x86-64 binary lifting. A stripped binary is verified for certain sanity properties such as return address integrity and calling convention adherence. Establishing these properties allows the binary to be lifted to a representation that contains an overapproximation of all possible execution paths of the binary. The lifted representation contains disassembled instructions, reconstructed control flow, invariants and proof obligations that are sufficient to prove the sanity properties as well as correctness of the lifted representation. We apply this approach to Linux Foundation and Intel’s Xen Hypervisor covering about 400K instructions. This demonstrates our approach is the first approach to provably overapproximative binary lifting scalable to commercial off-the-shelf systems. The lifted representation is exportable to the Isabelle/HOL theorem prover, allowing formal verification of its correctness. If our technique succeeds and the proofs obligations are proven true, then – under the generated assumptions – the lifted representation is correct.},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Verbeek, Freek and Bockenek, Joshua and Fu, Zhoulai and Ravindran, Binoy},
	year = {2022},
	keywords = {Binary Analysis, Disassembly, Formal Verification},
	pages = {934--949},
}

@article{alrabaee_survey_2022,
	address = {New York, NY, USA},
	title = {A {Survey} of {Binary} {Code} {Fingerprinting} {Approaches}: {Taxonomy}, {Methodologies}, and {Features}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3486860},
	doi = {10.1145/3486860},
	abstract = {Binary code fingerprinting is crucial in many security applications. Examples include malware detection, software infringement, vulnerability analysis, and digital forensics. It is also useful for security researchers and reverse engineers since it enables high fidelity reasoning about the binary code such as revealing the functionality, authorship, libraries used, and vulnerabilities. Numerous studies have investigated binary code with the goal of extracting fingerprints that can illuminate the semantics of a target application. However, extracting fingerprints is a challenging task since a substantial amount of significant information will be lost during compilation, notably, variable and function naming, the original data and control flow structures, comments, semantic information, and the code layout. This article provides the first systematic review of existing binary code fingerprinting approaches and the contexts in which they are used. In addition, it discusses the applications that rely on binary code fingerprints, the information that can be captured during the fingerprinting process, and the approaches used and their implementations. It also addresses limitations and open questions related to the fingerprinting process and proposes future directions.},
	number = {1},
	journal = {ACM Comput. Surv.},
	publisher = {Association for Computing Machinery},
	author = {Alrabaee, Saed and Debbabi, Mourad and Wang, Lingyu},
	month = jan,
	year = {2022},
	keywords = {reverse engineering, Binary code analysis, software security},
}

@article{ullah_iot-based_2021,
	address = {New York, NY, USA},
	title = {{IoT}-based {Cloud} {Service} for {Secured} {Android} {Markets} using {PDG}-based {Deep} {Learning} {Classification}},
	volume = {22},
	issn = {1533-5399},
	url = {https://doi.org/10.1145/3418206},
	doi = {10.1145/3418206},
	abstract = {Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones.Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48\% among clones from different application stores.},
	number = {2},
	journal = {ACM Trans. Internet Technol.},
	publisher = {Association for Computing Machinery},
	author = {Ullah, Farhan and Naeem, Muhammad Rashid and Bajahzar, Abdullah S. and Al-Turjman, Fadi},
	month = oct,
	year = {2021},
	keywords = {deep learning, Internet of Things, Clone detection, cloud services, program dependency graph},
}

@inproceedings{ringer_proof_2021,
	address = {New York, NY, USA},
	series = {{PLDI} 2021},
	title = {Proof repair across type equivalences},
	isbn = {978-1-4503-8391-2},
	url = {https://doi.org/10.1145/3453483.3454033},
	doi = {10.1145/3453483.3454033},
	abstract = {We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to suggested tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in Pumpkin Pi, an extension to the Pumpkin Patch Coq plugin suite for proof repair. We demonstrate Pumpkin Pi’s flexibility on eight case studies, including supporting a benchmark from a user study,easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.},
	booktitle = {Proceedings of the 42nd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Ringer, Talia and Porter, RanDair and Yazdani, Nathaniel and Leo, John and Grossman, Dan},
	year = {2021},
	keywords = {proof engineering, proof repair, proof reuse},
	pages = {112--127},
}

@article{albert_taming_2020,
	address = {New York, NY, USA},
	title = {Taming callbacks for smart contract modularity},
	volume = {4},
	url = {https://doi.org/10.1145/3428277},
	doi = {10.1145/3428277},
	abstract = {Callbacks are an effective programming discipline for implementing event-driven programming, especially in environments like Ethereum which forbid shared global state and concurrency. Callbacks allow a callee to delegate the execution back to the caller. Though effective, they can lead to subtle mistakes principally in open environments where callbacks can be added in a new code. Indeed, several high profile bugs in smart contracts exploit callbacks. We present the first static technique ensuring modularity in the presence of callbacks and apply it to verify prominent smart contracts. Modularity ensures that external calls to other contracts cannot affect the behavior of the contract. Importantly, modularity is guaranteed without restricting programming. In general, checking modularity is undecidable—even for programs without loops. This paper describes an effective technique for soundly ensuring modularity harnessing SMT solvers. The main idea is to define a constructive version of modularity using commutativity and projection operations on program segments. We believe that this approach is also accessible to programmers, since counterexamples to modularity can be generated automatically by the SMT solvers, allowing programmers to understand and fix the error. We implemented our approach in order to demonstrate the precision of the modularity analysis and applied it to real smart contracts, including a subset of the 150 most active contracts in Ethereum. Our implementation decompiles bytecode programs into an intermediate representation and then implements the modularity checking using SMT queries. Overall, we argue that our experimental results indicate that the method can be applied to many realistic contracts, and that it is able to prove modularity where other methods fail.},
	number = {OOPSLA},
	journal = {Proc. ACM Program. Lang.},
	publisher = {Association for Computing Machinery},
	author = {Albert, Elvira and Grossman, Shelly and Rinetzky, Noam and Rodríguez-Núñez, Clara and Rubio, Albert and Sagiv, Mooly},
	month = jan,
	year = {2020},
	keywords = {program analysis, smart contracts, blockchain, invariants, logic and verification, program verification},
}

@inproceedings{erinfolami_devil_2020,
	address = {New York, NY, USA},
	series = {{CCS} '20},
	title = {Devil is {Virtual}: {Reversing} {Virtual} {Inheritance} in {C}++ {Binaries}},
	isbn = {978-1-4503-7089-9},
	url = {https://doi.org/10.1145/3372297.3417251},
	doi = {10.1145/3372297.3417251},
	abstract = {The complexities that arise from the implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious of virtual inheritance. The conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10\% on Linux and 12.5\% on Windows) of real-world C++ programs including Mysql and Libstdc++. (2) We show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These falses either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95\% and 95.5\% (GCC -O0) and a minimum of 77.5\% and 73.8\% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.},
	booktitle = {Proceedings of the 2020 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Erinfolami, Rukayat Ayomide and Prakash, Aravind},
	year = {2020},
	keywords = {class inheritance recovery, software reverse engineering, virtual inheritance recovery},
	pages = {133--148},
}

@article{grech_madmax_2020,
	address = {New York, NY, USA},
	title = {{MadMax}: analyzing the out-of-gas world of smart contracts},
	volume = {63},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3416262},
	doi = {10.1145/3416262},
	abstract = {Ethereum is a distributed blockchain platform, serving as an ecosystem for smart contracts: full-fledged intercommunicating programs that capture the transaction logic of an account. A gas limit caps the execution of an Ethereum smart contract: instructions, when executed, consume gas, and the execution proceeds as long as gas is available.Gas-focused vulnerabilities permit an attacker to force key contract functionality to run out of gas—effectively performing a permanent denial-of-service attack on the contract. Such vulnerabilities are among the hardest for programmers to protect against, as out-of-gas behavior may be uncommon in nonattack scenarios and reasoning about these vulnerabilities is nontrivial.In this paper, we identify gas-focused vulnerabilities and present MadMax: a static program analysis technique that automatically detects gas-focused vulnerabilities with very high confidence. MadMax combines a smart contract decompiler and semantic queries in Datalog. Our approach captures high-level program modeling concepts (such as "dynamic data structure storage" and "safely resumable loops") and delivers high precision and scalability. MadMax analyzes the entirety of smart contracts in the Ethereum blockchain in just 10 hours and flags vulnerabilities in contracts with a monetary value in billions of dollars. Manual inspection of a sample of flagged contracts shows that 81\% of the sampled warnings do indeed lead to vulnerabilities.},
	number = {10},
	journal = {Commun. ACM},
	publisher = {Association for Computing Machinery},
	author = {Grech, Neville and Kong, Michael and Jurisevic, Anton and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	month = sep,
	year = {2020},
	pages = {87--95},
}

@inproceedings{nandi_synthesizing_2020,
	address = {New York, NY, USA},
	series = {{PLDI} 2020},
	title = {Synthesizing structured {CAD} models with equality saturation and inverse transformations},
	isbn = {978-1-4503-7613-6},
	url = {https://doi.org/10.1145/3385412.3386012},
	doi = {10.1145/3385412.3386012},
	abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
	year = {2020},
	keywords = {Decompilation, Program Synthesis, Computer-Aided Design, Equality Saturation},
	pages = {31--44},
}

@inproceedings{xylogiannopoulos_text_2020,
	address = {New York, NY, USA},
	series = {{ASONAM} '19},
	title = {Text mining for malware classification using multivariate all repeated patterns detection},
	isbn = {978-1-4503-6868-1},
	url = {https://doi.org/10.1145/3341161.3350841},
	doi = {10.1145/3341161.3350841},
	abstract = {Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Xylogiannopoulos, Konstantinos F. and Karampelas, Panagiotis and Alhajj, Reda},
	year = {2020},
	keywords = {Android malware detection, ARPaD, LERP-RSA, malware family classification, text mining},
	pages = {887--894},
}

@inproceedings{erinfolami_declassifier_2019,
	address = {New York, NY, USA},
	series = {Asia {CCS} '19},
	title = {{DeClassifier}: {Class}-{Inheritance} {Inference} {Engine} for {Optimized} {C}++ {Binaries}},
	isbn = {978-1-4503-6752-3},
	url = {https://doi.org/10.1145/3321705.3329833},
	doi = {10.1145/3321705.3329833},
	abstract = {Recovering class inheritance from C++ binaries has several security benefits including in solving problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining—or worse exclusion—due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5\% and 71.4\% true positive directed edges in the class hierarchy tree (CHT) under O0 and O2 optimizations respectively, (2) a combination of constructor-destructor (ctor-dtor) analysis provides a substantial improvement in inheritance inference than constructor-only (ctor-only) analysis.},
	booktitle = {Proceedings of the 2019 {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {Association for Computing Machinery},
	author = {Erinfolami, Rukayat Ayomide and Prakash, Aravind},
	year = {2019},
	keywords = {software reverse engineering, class hierarchy recovery},
	pages = {28--40},
}

@article{raychev_predicting_2019,
	address = {New York, NY, USA},
	title = {Predicting program properties from 'big code'},
	volume = {62},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3306204},
	doi = {10.1145/3306204},
	abstract = {We present a new approach for predicting program properties from large codebases (aka "Big Code"). Our approach learns a probabilistic model from "Big Code" and uses this model to predict properties of new, unseen programs.The key idea of our work is to transform the program into a representation that allows us to formulate the problem of inferring program properties as structured prediction in machine learning. This enables us to leverage powerful probabilistic models such as Conditional Random Fields (CRFs) and perform joint prediction of program properties.As an example of our approach, we built a scalable prediction engine called JSNICE for solving two kinds of tasks in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63\% of name identifiers and its type annotation predictions are correct in 81\% of cases. Since its public release at http://jsnice.org, JSNice has become a popular system with hundreds of thousands of uses.By formulating the problem of inferring program properties as structured prediction, our work opens up the possibility for a range of new "Big Code" applications such as de-obfuscators, decompilers, invariant generators, and others.},
	number = {3},
	journal = {Commun. ACM},
	publisher = {Association for Computing Machinery},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	month = feb,
	year = {2019},
	pages = {99--107},
}

@inproceedings{harrand_strengths_2019,
	title = {The {Strengths} and {Behavioral} {Quirks} of {Java} {Bytecode} {Decompilers}},
	issn = {2470-6892},
	doi = {10.1109/SCAM.2019.00019},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84\% of classes of our dataset and semantically equivalent code output for 78\% of classes.},
	booktitle = {2019 19th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation} ({SCAM})},
	author = {Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit},
	month = sep,
	year = {2019},
	keywords = {Program processors, Semantics, Distortion, decompilation, Measurement, Java, Java bytecode, reverse engineering, source code analysis, Syntactics, Uniform resource locators},
	pages = {92--102},
}

@inproceedings{grech_gigahorse_2019-1,
	title = {Gigahorse: {Thorough}, {Declarative} {Decompilation} of {Smart} {Contracts}},
	issn = {1558-1225},
	doi = {10.1109/ICSE.2019.00120},
	abstract = {The rise of smart contracts - autonomous applications running on blockchains - has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - e.g., Gigahorse can decompile over 99.98\% of deployed contracts, compared to 88\% for the recently-published Vandal decompiler and under 50\% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a “batteries included” approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Grech, Neville and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	month = may,
	year = {2019},
	keywords = {Security, Task analysis, Decompilation, Java, Program Analysis, Smart contracts, Ethereum, Blockchain, Virtual machining},
	pages = {1176--1186},
}

@inproceedings{lacomis_dire_2019,
	title = {{DIRE}: {A} {Neural} {Approach} to {Decompiled} {Identifier} {Naming}},
	issn = {2643-1572},
	doi = {10.1109/ASE.2019.00064},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = jan,
	year = {2019},
	keywords = {Software, Decompilation, Reverse engineering, Analytical models, Deep learning, Recurrent neural networks, Tools, Training},
	pages = {628--639},
}

@inproceedings{al-kaswan_extending_2023,
	title = {Extending {Source} {Code} {Pre}-{Trained} {Language} {Models} to {Summarise} {Decompiled} {Binaries}},
	issn = {2640-7574},
	doi = {10.1109/SANER56733.2023.00033},
	abstract = {Binary reverse engineering is used to understand and analyse programs for which the source code is unavailable. Decompilers can help, transforming opaque binaries into a more readable source code-like representation. Still, reverse engineering is difficult and costly, involving considering effort in labelling code with helpful summaries. While the automated summarisation of decompiled code can help reverse engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise de-compiled binary functions. Further-more, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by removing identifiers, and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82 and, 44.21 for summarising source, decompiled, and obfuscated decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
	booktitle = {2023 {IEEE} {International} {Conference} on {Software} {Analysis}, {Evolution} and {Reengineering} ({SANER})},
	author = {Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie},
	month = mar,
	year = {2023},
	keywords = {Binary codes, Source coding, Optimization, Software, Task analysis, Binary, CodeT5, Data models, Decompilation, Deep Learning, Labeling, Pre-trained Language Models, Reverse engineering, Reverse Engineering, Summarization, Synthetic data, Transformers},
	pages = {260--271},
}

@article{liao_augmenting_2025,
	title = {Augmenting {Smart} {Contract} {Decompiler} {Output} {Through} {Fine}-{Grained} {Dependency} {Analysis} and {LLM}-{Facilitated} {Semantic} {Recovery}},
	volume = {51},
	issn = {1939-3520},
	doi = {10.1109/TSE.2025.3623325},
	abstract = {Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate function identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA’s accuracy in control and data flow analysis and LLM’s capability in semantic prediction. More specifically, SmartHalo constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract functions shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o mini with SmartHalo further enhances its performance, achieving a precision of 91.32\% and a recall of 87.38\% for function boundaries, a precision of 90.40\% and a recall of 88.82\% for variable types, and a precision of 80.66\% and a recall of 91.78\% for contract attributes.},
	number = {12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Liao, Zeqin and Nan, Yuhong and Gao, Zixu and Liang, Henglong and Hao, Sicheng and Ren, Peifan and Zheng, Zibin},
	month = feb,
	year = {2025},
	keywords = {Codes, Source coding, Optimization, Training, Semantics, Accuracy, decompilation, large language model, Large language models, Static analysis, Annotations, static analysis, Smart contracts, Smart contract},
	pages = {3574--3590},
}

@inproceedings{she_wadec_2024-1,
	title = {{WaDec}: {Decompiling} {WebAssembly} {Using} {Large} {Language} {Model}},
	issn = {2643-1572},
	abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm’s binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm.In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art’s 116.94\%. Unlike the output of baselines that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%. In summary, WaDec enhances understanding of the code’s structure and execution flow, facilitating automated code analysis, optimization, and security auditing.},
	booktitle = {2024 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
	month = oct,
	year = {2024},
	keywords = {Codes, Security, Source coding, Optimization, Training, decompilation, binary, Faces, finetune, large language model, Large language models, llm, Measurement, readability, Self-supervised learning, Software engineering, wasm, webassembly},
	pages = {481--492},
}

@article{park_static_2023,
	title = {Static {Analysis} of {JNI} {Programs} via {Binary} {Decompilation}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2023.3241639},
	abstract = {JNI programs are widely used thanks to the combined benefits of C and Java programs. However, because understanding the interaction behaviors between two different programming languages is challenging, JNI program development is difficult to get right and vulnerable to security attacks. Thus, researchers have proposed static analysis of JNI program source code to detect bugs and security vulnerabilities in JNI programs. Unfortunately, such source code analysis is not applicable to compiled JNI programs that are not open-sourced or open-source JNI programs containing third-party binary libraries. While JN-SAF, the state-of-the-art analyzer for compiled JNI programs, can analyze binary code, it has several limitations due to its symbolic execution and summary-based bottom-up analysis. In this paper, we propose a novel approach to statically analyze compiled JNI programs without their source code using binary decompilation. Unlike JN-SAF that analyzes binaries directly, our approach decompiles binaries and analyzes JNI programs with the decompiled binaries using an existing JNI program analyzer for source code. To decompile binaries to compilable C source code with precise JNI-interoperation-related types, we improve an existing decompilation tool by leveraging the characteristics of JNI programs. Our evaluation shows that the approach is precise as almost the same as the state-of-the-art JNI program analyzer for source code, and more precise than JN-SAF.},
	number = {5},
	journal = {IEEE Transactions on Software Engineering},
	author = {Park, Jihee and Lee, Sungho and Hong, Jaemin and Ryu, Sukyoung},
	month = may,
	year = {2023},
	keywords = {Codes, Security, Source coding, Java, Static analysis, Computer architecture, static analysis, Libraries, binary decompilation, Java native interface},
	pages = {3089--3105},
}

@inproceedings{armengol-estape_slade_2024-1,
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	issn = {2643-2838},
	doi = {10.1109/CGO57630.2024.10444788},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. Nevertheless, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence Transformer trained over real-world code and augmented with a type inference engine. We utilize a novel tokenizer, dropout-free regularization, and type inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 ExeBench functions on two ISAs and at two optimization levels. SLaDe is up to 6× more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4× more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	booktitle = {2024 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F.P.},
	month = mar,
	year = {2024},
	keywords = {Codes, Security, Optimization, Task analysis, Transformers, Standards, decompilation, Engines, neural decompilation, language models, type inference, Transformer},
	pages = {67--80},
}

@inproceedings{jaffe_meaningful_2018-1,
	title = {Meaningful {Variable} {Names} for {Decompiled} {Code}: {A} {Machine} {Translation} {Approach}},
	issn = {2643-7171},
	abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2\% of the time, which represents a lower bound for performance.},
	booktitle = {2018 {IEEE}/{ACM} 26th {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Le Goues, Claire and Vasilescu, Bogdan},
	month = may,
	year = {2018},
	keywords = {Codes, Source coding, Decompilation, Channel models, Distortion, Lower bound, Machine translation, Noise measurement, Renaming Identifiers, Software development management, Standards, Statistical Machine Translation, Translation, Understandability},
	pages = {20--30},
}

@article{reiter_automatically_2025,
	title = {Automatically {Mitigating} {Vulnerabilities} in {Binary} {Programs} via {Partially} {Recompilable} {Decompilation}},
	volume = {22},
	issn = {1941-0018},
	doi = {10.1109/TDSC.2024.3482413},
	abstract = {Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD) to help automate the process. PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically succeed on an entire binary, our approach does because it is limited to a few functions, such as those identified by our binary fault localization. We evaluate the assumptions underlying our approach and find that, without any grammar or compilation restrictions, up to 79\% of individual functions are successfully decompiled and recompiled. In comparison, only 1.7\% of the full C-binaries succeed. When recompilation succeeds, PRD produces test-equivalent binaries 93.0\% of the time. We evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; and human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, perform as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with the same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, the evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries.},
	number = {3},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Reiter, Pemma and Tay, Hui Jun and Weimer, Westley and Doupé, Adam and Wang, Ruoyu and Forrest, Stephanie},
	month = may,
	year = {2025},
	keywords = {Codes, Computer bugs, Source coding, Software, Prototypes, Measurement, Software engineering, C++ languages, Grammar, Location awareness, Maintenance engineering, software maintenance},
	pages = {2270--2282},
}

@article{sang_control_2024,
	title = {From {Control} {Application} to {Control} {Logic}: {PLC} {Decompile} {Framework} for {Industrial} {Control} {System}},
	volume = {19},
	issn = {1556-6021},
	doi = {10.1109/TIFS.2024.3402117},
	abstract = {Industrial Control System (ICS) depends on the underlying Programmable Logical Controllers (PLCs) to run. As such, the security of the internal control logic of the PLCs is the top concern of ICS. Reversing analysis and forensic work against PLC require extracting control logic from the control application running inside PLC, which is still an unresolved problem. To address the challenge, we propose a PLC decompile framework named CLEVER, which can analyze the control application and extract the control logic. First, we propose a simulation execution based code extraction method, which is utilized to filter the control logic related data. Then, to normalize the control application decompile process, an intermediate representation (IR) is designed, which can simplify the analysis process and enhance the extensibility of CLEVER. Finally, a heuristic data flow analysis algorithm is proposed to find variable dependency, and a sequential parsing method is utilized to reconstruct the source code from the control application. To evaluate our work, real world PLC hardware and programming software are used for the experiment. We use 22 real-world, 58 hand-written, and 150 auto-generated control applications to demonstrate the usability, correctness, and operational efficiency of CLEVER.},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Sang, Chao and Wu, Jun and Li, Jianhua and Guizani, Mohsen},
	year = {2024},
	keywords = {Codes, Software, Reverse engineering, Process control, Feature extraction, program analysis, Data mining, industrial control system, Assembly, network forensics, PLC, Registers},
	pages = {8685--8700},
}

@inproceedings{cao_revisiting_2023,
	title = {Revisiting {Deep} {Learning} for {Variable} {Type} {Recovery}},
	issn = {2643-7171},
	doi = {10.1109/ICPC58990.2023.00042},
	abstract = {Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.},
	booktitle = {2023 {IEEE}/{ACM} 31st {International} {Conference} on {Program} {Comprehension} ({ICPC})},
	author = {Cao, Kevin and Leach, Kevin},
	month = may,
	year = {2023},
	keywords = {Source coding, Reverse engineering, Transformers, Training, Semantics, Static analysis, Machine Learning, Computer architecture, Ghidra, Hex-Rays, Symbols},
	pages = {275--279},
}

@article{ahmed_learning_2022,
	title = {Learning to {Find} {Usages} of {Library} {Functions} in {Optimized} {Binaries}},
	volume = {48},
	issn = {1939-3520},
	doi = {10.1109/TSE.2021.3106572},
	abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries’ behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, “natural” source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to “decompile” binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
	number = {10},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},
	month = oct,
	year = {2022},
	keywords = {Optimization, Reverse engineering, Tools, Training, Malware, deep learning, Databases, Libraries, software modeling},
	pages = {3862--3876},
}

@inproceedings{slawinski_applications_2019,
	title = {Applications of {Graph} {Integration} to {Function} {Comparison} and {Malware} {Classification}},
	doi = {10.1109/ICSRS48664.2019.8987703},
	abstract = {We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled. NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms. 11Code available at https://github.com/gtownrocks/grafuple},
	booktitle = {2019 4th {International} {Conference} on {System} {Reliability} and {Safety} ({ICSRS})},
	author = {Slawinski, Michael and Wortman, Andy},
	month = jan,
	year = {2019},
	keywords = {Training, Malware, decompilation, Syntactics, abstract syntax tree, Aerospace electronics, classification, Glass box, graph integration, machine learning, malware, NET, pagerank, Reliability, Robust control, Safety, Taxonomy, Vectors},
	pages = {16--24},
}

@inproceedings{li_stan_2020,
	title = {{STAN}: {Towards} {Describing} {Bytecodes} of {Smart} {Contract}},
	doi = {10.1109/QRS51102.2020.00045},
	abstract = {More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1\% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named Stan to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, Stan can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that Stan can generate adequate, accurate and readable descriptions for contract’s bytecodes, which have practical value for users.},
	booktitle = {2020 {IEEE} 20th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Li, Xiaoqi and Chen, Ting and Luo, Xiapu and Zhang, Tao and Yu, Le and Xu, Zhou},
	month = feb,
	year = {2020},
	keywords = {Security, Tools, Semantics, Software quality, Software reliability, Natural language processing, Smart contracts, Smart contract, Ethereum, Program comprehension},
	pages = {273--284},
}

@article{peng_bctd-ics_2025,
	title = {{BCTD}-{ICS}: {A} {Blockchain}-{Aided} {Framework} for {Trusted} {Detection} of {Industrial} {Control} {System} {Components}},
	volume = {12},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2025.3583304},
	abstract = {The cybersecurity threats targeting industrial control systems (ICSs) are evolving with increasing sophistication. Addressing the detection blind spots in existing source code analysis techniques, this study reveals a dual security paradox arising from code sensitivity: privacy leakage risks caused by decompilation techniques and integrity verification deficiencies in reverse engineering. This article investigates three critical challenges: 1) what are the component flow process and detection elements of ICS component source code? 2) how can high-performance and reliable tracing and traceability be provided for ICS component source code exceptions and routine detection? and 3) how can privacy enhancement and trusted detection of ICS component source code with high sensitivity be achieved? This article proposes a blockchain-integrated trusted detection framework for ICS (BCTD-ICS), delivering groundbreaking solutions: 1) establishing a lifecycle circulation model that systematically maps component types, stakeholders, and detection parameters; 2) developing a tripartite collaborative architecture [blockchain- identification resolution zero-knowledge proofs (ZKPs)], featuring a traceability mechanism with trusted identification codes (resolution efficiency: 40 ms/105 queries) to eliminate decompilation-induced privacy risks; and 3) creating an industrial-oriented privacy enhancement system utilizing DBSCAN clustering for intelligent sampling (26\% compression rate on BCN3D Moveo) and optimizing ZK-SNARK protocols through Shamir’s secret sharing, establishing a backdoor-resistant distributed parameter generation system (time delay increment {\textless} 100 ms). Experimentally verified, our solution enables ICS component code detection supply-chain-wise without sensitive data leakage in real-world industries. This work establishes a novel trusted detection paradigm for ICS, advancing detection efficiency and credibility under strict privacy preservation requirements, meeting Industry 4.0 security demands.},
	number = {18},
	journal = {IEEE Internet of Things Journal},
	author = {Peng, Xiangzhen and Ma, Tianyu and Zheng, Chengliang and Shen, Zhidong and Cui, Xiaohui},
	month = sep,
	year = {2025},
	keywords = {Codes, Privacy, Security, Source coding, Protocols, Supply chains, Internet of Things, blockchain, Artificial intelligence technology, Blockchains, identification resolution technology, industrial control systems (ICSs), Object recognition, source code detection, Telecommunication traffic, zero-knowledge proofs (ZKPs)},
	pages = {37552--37570},
}

@article{nghi_phu_efficient_2019,
	title = {An {Efficient} {Algorithm} to {Extract} {Control} {Flow}-{Based} {Features} for {IoT} {Malware} {Detection}},
	volume = {64},
	issn = {1460-2067},
	doi = {10.1093/comjnl/bxaa087},
	abstract = {Control flow-based feature extraction method has the ability to detect malicious code with higher accuracy than traditional text-based methods. Unfortunately, this method has been encountered with the NP-hard problem, which is infeasible for the large-sized and high-complexity programs. To tackle this, we propose a control flow-based feature extraction dynamic programming algorithm for fast extraction of control flow-based features with polynomial time O(N$^{\textrm{2}}$), where N is the number of basic blocks in decompiled executable codes. From the experimental results, it is demonstrated that the proposed algorithm is more efficient and effective in detecting malware than the existing ones. Applying our algorithm to an Internet of Things dataset gives better results on three measures: Accuracy = 99.05\%, False Positive Rate = 1.31\% and False Negative Rate = 0.66\%.},
	number = {1},
	journal = {The Computer Journal},
	author = {Nghi Phu, Tran and Dai Tho, Nguyen and Huy Hoang, Le and Ngoc Toan, Nguyen and Ngoc Binh, Nguyen},
	month = jan,
	year = {2019},
	keywords = {CFD, control flow-based features, dynamic programming, embedded malware, IoT malware detection},
	pages = {599--609},
}

@article{chu_security_2019,
	title = {Security and {Privacy} {Analyses} of {Internet} of {Things} {Children}’s {Toys}},
	volume = {6},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2018.2866423},
	abstract = {This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many Internet of Things toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks.},
	number = {1},
	journal = {IEEE Internet of Things Journal},
	author = {Chu, Gordon and Apthorpe, Noah and Feamster, Nick},
	month = feb,
	year = {2019},
	keywords = {Privacy, Mobile applications, Internet of Things, Servers, Authentication, Data security, Internet of Things (IoT), privacy, Toy manufacturing industry},
	pages = {978--985},
}

@article{gao_malware_2022,
	title = {Malware {Detection} by {Control}-{Flow} {Graph} {Level} {Representation} {Learning} {With} {Graph} {Isomorphism} {Network}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3215267},
	abstract = {With society’s increasing reliance on computer systems and network technology, the threat of malicious software grows more and more serious. In the field of information security, malware detection has been a key problem that academia and industry are committed to solving. Machine learning is an effective method for processing large-scale data, such as the Gradient Boosting Decision Tree (GBDT) and deep neural network technology. Although these types of detection methods can deal with cyber threats, most feature extraction methods are based on the statistical information features of portable executable (PE) files and thus lack the decompiled code and execution flow structure of the PE samples. Therefore, we propose a Control-Flow Graph (CFG)- and Graph Isomorphism Network (GIN)-based malware classification system. The feature vectors of CFG basic blocks are generated using the large-scale pre-trained language model MiniLM, which is beneficial for the GIN to further learn and compress the CFG-based representation, and classified with multi-layer perceptron. In addition, we evaluated the effectiveness of the representation under different dimensions and classifiers. To evaluate our method, we set up a CFG-based malware detection graph dataset from a PE file of the Blue Hexagon Open Dataset for Malware Analysis (BODMAS), which we call the Malware Geometric Binary Dataset (MGD-BINARY) and collected the experimental results of CFG representation in different dimensions and classifier settings. The evaluation results show that our proposal has proved an Accuracy metric of 0.99160 and achieved 0.99148 Area Under the Curve (AUC) results.},
	journal = {IEEE Access},
	author = {Gao, Yun and Hasegawa, Hirokazu and Yamaguchi, Yukiko and Shimada, Hajime},
	year = {2022},
	keywords = {Codes, Analytical models, Feature extraction, Malware, machine learning, Machine learning, Malware detection, Data mining, graph classification, Graphics, Natural language processing, static analysis},
	pages = {111830--111841},
}

@article{costa_lightweight_2023,
	title = {A {Lightweight} and {Multi}-{Stage} {Approach} for {Android} {Malware} {Detection} {Using} {Non}-{Invasive} {Machine} {Learning} {Techniques}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3296606},
	abstract = {Android has been a constant target of cybercriminals that try to attack one of the most used operating systems, commonly using malicious applications (denominated malware) that, once installed on a device, can harm users in several ways. Existing malware detection solutions are usually invasive as they obtain classification features by performing reverse engineering, decompilation, or disassembly of the analyzed application, which infringes licenses and terms of use of applications. In addition, these solutions often employ a single machine learning (ML) model to detect various types of malware, resulting in several false alarms. In this context, we propose an approach to detect Android malware consisting of a set of specific-type detectors in which each one performs a multi-stage analysis, based on rules and ML techniques, in different phases of the application cycle (before and after its installation). Our approach also differs from state-of-the-art solutions by being non-invasive, since it leverages a process to obtain application’s features that does not infringe licenses and terms of use of applications. In addition, according to experiments performed on a real Android smartphone, our proposal presents the following additional advantages over state-of-the-art solutions: a more efficient process to classify applications that is three times faster and requires ten times less CPU usage in some cases (saving device energy); and a better detection performance, with higher balanced accuracy, nine times less false positive cases, and ten times less false negative cases.},
	journal = {IEEE Access},
	author = {Costa, Leonardo da and Moia, Vitor},
	year = {2023},
	keywords = {Feature extraction, Malware, machine learning, Android, Operating systems, Smart phones, Proposals, malware detection, Androids, Behavioral sciences, Detectors, multi-stage analysis, non-invasive feature extraction, Noninvasive treatment},
	pages = {73127--73144},
}

@article{rodriguez-bazan_android_2023,
	title = {Android {Ransomware} {Analysis} {Using} {Convolutional} {Neural} {Network} and {Fuzzy} {Hashing} {Features}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3328314},
	abstract = {Most of the time, cybercriminals look for new ways to bypass security controls by improving their attacks. In the 1980s, attackers developed malware to kidnap user data by requesting payments. Malware is called a ransomware. Recently, they have demanded payment in Bitcoin or any other cryptocurrency. Ransomware is one of the most dangerous threats on the Internet, and this type of malware could affect almost all devices. Malware cipher device data, making them inaccessible to users. In this study, a new method for Android ransomware classification was proposed. This method implements a Convolutional Neural Network (CNN) for malware classification based on images. This paper presents a novel method for transforming an Android Application Package (APK) into a grayscale image. The image creation relies on using Natural Language Processing (NLP) techniques for text cleaning and Fuzzy Hashing to represent the decompiled code from the APK in a set of hashes after preprocessing using NLP techniques. The image is composed of n fuzzy hashes that represent the APK. The method was tested using a dataset of 7,765 Android ransomware samples obtained from external researchers and public sources. The accuracy of the proposed method was higher than that of other methods in the literature.},
	journal = {IEEE Access},
	author = {Rodriguez-Bazan, Horacio and Sidorov, Grigori and Escamilla-Ambrosio, Ponciano Jorge},
	year = {2023},
	keywords = {Deep learning, deep learning, Operating systems, Classification algorithms, Convolutional neural networks, Natural language processing, Android ransomware, Androids, convolutional neural network, fuzzy hashing, Fuzzy systems, malware classification, Matched filters, Metadata, ransomware, Ransomware, XML},
	pages = {121724--121738},
}

@article{nethala_deep_2025,
	title = {A {Deep} {Learning}-{Based} {Ensemble} {Framework} for {Robust} {Android} {Malware} {Detection}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3551152},
	abstract = {The exponential growth of Android applications has resulted in a surge of malware threats, posing severe risks to user privacy and data security. To address these challenges, this study introduces a novel malware detection approach utilizing an ensemble of Convolutional Neural Networks (CNNs) for enhanced classification accuracy. The methodology incorporates a multi-phase process, starting with the extraction and preprocessing of APK (Android app) files. The preprocessing phase involves decompressing, decompiling, and transforming the APK files into bytecode and Dex files. The extracted byte data is converted into 1D vectors and reshaped into 2D grayscale images, enabling efficient feature learning through CNNs. The proposed ensemble of CNN-based models undergoes comprehensive training, validation, and evaluation, demonstrating superior performance compared to existing approaches. We used two popular Android datasets to evaluate the performance of our proposed model. Specifically, the model achieves an accuracy of 98.65\%, F1-score of 96.43\% on the Drebin dataset and attains 97.91\% accuracy, 96.73\% of F1-score on the AMD dataset. These results confirm the mode’s ability to effectively identify Android malware with high precision and reliability, outperforming traditional techniques. This research not only underscores the potential of our proposed approach in malware detection but also sets a foundation for future advancements. Future efforts will focus on real-time malware detection, integration with mobile security frameworks, and evaluation across diverse datasets to ensure adaptability to emerging malware threats.},
	journal = {IEEE Access},
	author = {Nethala, Sainag and Chopra, Pronoy and Kamaluddin, Khaja and Alam, Shahid and Alharbi, Soltan and Alsaffar, Mohammad},
	year = {2025},
	keywords = {Deep learning, Accuracy, Feature extraction, Malware, deep learning, Static analysis, machine learning, Machine learning, Support vector machines, Computational modeling, malware classification, Android malware detection, attention mechanism, convolutional neural networks, ensemble learning, Meta-CNN, Random forests, Real-time systems},
	pages = {46673--46696},
}

@article{zhang_bian_2023,
	title = {{BiAn}: {Smart} {Contract} {Source} {Code} {Obfuscation}},
	volume = {49},
	issn = {1939-3520},
	doi = {10.1109/TSE.2023.3298609},
	abstract = {With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174\% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8\% and 40.5\% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50\% of cases for ten widely-used static analysis detection tools.},
	number = {9},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhang, Pengcheng and Yu, Qifan and Xiao, Yan and Dong, Hai and Luo, Xiapu and Wang, Xiao and Zhang, Meng},
	month = sep,
	year = {2023},
	keywords = {Codes, Security, Source coding, source code, obfuscation, Complexity theory, Intellectual property, smart contract, Smart contracts, Layout, Ethereum, Blockchain},
	pages = {4456--4476},
}

@article{zhang_fault_2022,
	title = {Fault {Diagnosis} of {Power} {Transformer} {Based} on {SSA}—{MDS} {Pretreatment}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3202982},
	abstract = {Aiming at the problems of coupling between transformer input characteristics and low accuracy of transformer fault diagnosis, SSA-MDS and other soft technologies are used to analyze the key characteristics of transformer faults, so as to improve the accuracy of transformer fault diagnosis. The SSA algorithm cascade MDS algorithm to process the DGA data is proposed. Subsequently, the TSSA-RF model is introduced to classify the DGA data. The DGA data is first mapped to a high-dimensional space. Next, the optimal feature subset is encoded using the SSA algorithm to reduce irrelevant and redundant features. In this study, the correlation between the optimal feature dimension and the transformer fault diagnosis accuracy is investigated. the expression of the optimal feature subset is obtained by decompiling the SSA operator. The pre-processed data are classified using the RF model, and the TSSA -RF model for classifying the DGA data is found with the highest accuracy through the comparison of different optimization algorithms. After the RF model is optimized using the TSSA algorithm, its accuracy increases by 7.89\%, and the accuracy of the TSSA -RF model is obtained as 92.11\%. The example results show that compared with the original data, the proposed data processing algorithm improves the diagnostic accuracy of transformer by 11.97 \% in the RF model. Compared with multiple preprocessing methods, SSA-MDS has the highest accuracy. Compared with the original data, the accuracy of TSSA-RF model increases by 11.64 \%.},
	journal = {IEEE Access},
	author = {Zhang, Mei and Chen, Wanli},
	year = {2022},
	keywords = {Optimization, Data models, feature extraction, Classification algorithms, Classification tree analysis, fault diagnosis, Fault diagnosis, Oil insulation, Power transformer, Power transformer insulation, TSSA algorithm, RF model},
	pages = {92505--92515},
}

@inproceedings{xiong_hext5_2023,
	title = {{HexT5}: {Unified} {Pre}-{Training} for {Stripped} {Binary} {Code} {Information} {Inference}},
	issn = {2643-1572},
	doi = {10.1109/ASE56229.2023.00099},
	abstract = {Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pre-training objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
	month = sep,
	year = {2023},
	keywords = {Binary codes, Task analysis, Deep Learning, Reverse Engineering, Semantics, Natural languages, Data mining, Computer languages, Object recognition, Binary Diffing, Information Inference, Programming Language Model},
	pages = {774--786},
}

@inproceedings{zhang_novel_2024,
	title = {A {Novel} {Approach} to {Malicious} {Code} {Detection} {Using} {CNN}-{BiLSTM} and {Feature} {Fusion}},
	doi = {10.1109/RICAI64321.2024.10911787},
	abstract = {With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness.This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware.In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability.The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.},
	booktitle = {2024 6th {International} {Conference} on {Robotics}, {Intelligent} {Control} and {Artificial} {Intelligence} ({RICAI})},
	author = {Zhang, Lixia and Liu, Tianxu and Shen, Kaihui and Chen, Cheng},
	month = feb,
	year = {2024},
	keywords = {Deep Learning, Deep learning, Training, Accuracy, Feature extraction, Malware, Malware Detection, Support vector machines, Transfer learning, Adaptation models, Technological innovation, Explainable AI, Feature Fusion},
	pages = {745--755},
}

@article{chen_android_2024,
	title = {Android {Malware} {Family} {Clustering} {Based} on {Multiple} {Features}},
	volume = {73},
	issn = {1558-1721},
	doi = {10.1109/TR.2023.3332090},
	abstract = {Familiar analysis for malware plays an important role in comprehending the diversity of malicious behaviors and identifying the emerging security threats. Existing studies mainly focus on classifying malware into known families by supervised learning. However, these methods face two main challenges, 1) the lack of a large amount of labeled data and 2) the poor effectiveness in identifying unknown families of malware. To overcome these challenges, we propose a new method called multiple features (MulFC) based on unsupervised learning. In the method, we first leverage a decompiling tool to extract multiple features, including manifest features, application programming interface (API) features, and opcode features. Then, the opcode features are preprocessed to filter out the redundant ones to reduce the calculation cost. After that, we adopt the Jaccard index to calculate the similarities between malware and construct a malware network. Finally, InfoMap is applied to perform the clustering on the basis of the malware network. Overall, MulFC does not require the use of labeled data and can identify unknown families of malware. Experiments are conducted on two datasets for the performance evaluation of MulFC. The experimental results show that MulFC achieves 0.810 in terms of normalized mutual information, 0.576 in terms of adjusted rand index, 0.620 in terms of the Fowlkes–Mallows index, and 0.805 in terms of V-measure on average, and outperforms the state-of-the-art baseline method by 0.060, 0.054, 0.038, and 0.065, respectively.},
	number = {2},
	journal = {IEEE Transactions on Reliability},
	author = {Chen, Xin and Yu, Dongjin and Cai, Xinxin and Jiang, He and Yu, Haihua},
	month = jun,
	year = {2024},
	keywords = {Feature extraction, Malware, Operating systems, Data mining, Androids, Android malware, Costs, Indexes, InfoMap, malware family clustering, multiple features, unsupervised learning, Unsupervised learning},
	pages = {1202--1215},
}

@inproceedings{xylogiannopoulos_text_2019,
	title = {Text {Mining} for {Malware} {Classification} {Using} {Multivariate} {All} {Repeated} {Patterns} {Detection}},
	issn = {2473-991X},
	doi = {10.1145/3341161.3350841},
	abstract = {Mobile phones have become nowadays a commodity to the majority of people. Using them, people are able to access the world of Internet and connect with their friends, their colleagues at work or even unknown people with common interests. This proliferation of the mobile devices has also been seen as an opportunity for the cyber criminals to deceive smartphone users and steel their money directly or indirectly, respectively, by accessing their bank accounts through the smartphones or by blackmailing them or selling their private data such as photos, credit card data, etc. to third parties. This is usually achieved by installing malware to smartphones masking their malevolent payload as a legitimate application and advertise it to the users with the hope that mobile users will install it in their devices. Thus, any existing application can easily be modified by integrating a malware and then presented it as a legitimate one. In response to this, scientists have proposed a number of malware detection and classification methods using a variety of techniques. Even though, several of them achieve relatively high precision in malware classification, there is still space for improvement. In this paper, we propose a text mining all repeated pattern detection method which uses the decompiled files of an application in order to classify a suspicious application into one of the known malware families. Based on the experimental results using a real malware dataset, the methodology tries to correctly classify (without any misclassification) all randomly selected malware applications of 3 categories with 3 different families each.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining} ({ASONAM})},
	author = {Xylogiannopoulos, Konstantinos F. and Karampelas, Panagiotis and Alhajj, Reda},
	month = aug,
	year = {2019},
	keywords = {Malware, Mobile handsets, Androids, Android malware detection, ARPaD, Humanoid robots, LERP-RSA, malware family classification, Payloads, Social network services, text mining, Text mining},
	pages = {887--894},
}

@inproceedings{verbeek_formally_2025-1,
	title = {Formally {Verified} {Binary}-{Level} {Pointer} {Analysis}},
	issn = {1558-1225},
	doi = {10.1109/ICSE55347.2025.00231},
	abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows “meaningful” precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
	booktitle = {2025 {IEEE}/{ACM} 47th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
	month = apr,
	year = {2025},
	keywords = {Scalability, Software, Accuracy, binary analysis, Software engineering, Testing, formal methods, pointer analysis},
	pages = {42--53},
}

@article{ghimire_survey_2025,
	title = {A {Survey} on {Application} of {AI} on {Reverse} {Engineering} for {Software} {Analysis} and {Security}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3593456},
	abstract = {Reverse engineering process serves essential functions in software analysis and security auditing and malware detection but requires significant time and effort. Researchers and practitioners now investigate how Artificial Intelligence (AI) technology can automate and improve different reverse engineering procedures. This survey provides an extensive evaluation of recent AI-based reverse engineering techniques which focus on software decompilation and function identification as well as control flow recovery and vulnerability analysis. The paper presents a classification system for existing methods while comparing them through an analysis of their development from traditional rule-based systems to contemporary deep learning frameworks. The research examines fundamental datasets together with field tools and evaluation metrics. This paper establishes a fundamental understanding of AI integration in reverse engineering for software security while discussing future development directions.},
	journal = {IEEE Access},
	author = {Ghimire, Ashutosh and Lingala, Sahasra Rao and Zhang, Junjie and Alsulami, Faris and Amsaad, Fathi},
	year = {2025},
	keywords = {Codes, Security, Source coding, Software, Reverse engineering, Malware, Static analysis, Logic, malware detection, Surveys, software security, anomalies, Artificial intelligence, threat analysis},
	pages = {152903--152913},
}

@article{priambodo_malqwen_2025,
	title = {{MalQwen}: {Fine} {Tuned} {LLM} for {Static} {Android} {Malware} {Analysis} {Report}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3637047},
	abstract = {The Android operating system continues to face escalating security challenges, primarily due to its open-source nature and the rapid proliferation of applications from untrusted sources. Traditional static analysis tools lack the flexibility to capture evolving malware behaviors, limiting their interpretability and scalability. Large Language Models (LLMs) are now applied in cybersecurity for malware detection, phishing classification, and cyber threat intelligence. However, their use has not been extended to producing detailed and interpretable Android malware analysis reports. This study integrates LLMs into Android malware analysis by creating a dataset for instruction tuning and fine-tuning the Qwen-7B model using the LoRA method. The model MalQwen is developed by fine-tuning Qwen 2.5-7B with 429 malware samples containing decompiled code and expert labeled security reports. MalQwen outperforms models like Gemini and LLaMA, achieving a BERTscore of 0.84 for SMS malware and a Perplexity score of 3.30 for Scareware. These findings confirm MalQwen’s superior performance in generating precise malware reports, validating LLMs as a powerful new method for Android malware analysis.},
	journal = {IEEE Access},
	author = {Priambodo, Tegar Ganang Satrio and Prabowo, Angela Oryza and Puspitarini, Annisa Dwi and Winarso, Raihan Adam Handoyo and Aisyah, Nur and Pratama, Mohammad Yoga and Purwitasari, Diana and Pratomo, Baskoro Adi},
	year = {2025},
	keywords = {Codes, Security, Training, Feature extraction, Malware, large language model, Static analysis, Operating systems, Data mining, static analysis, Adaptation models, Android malware analysis, Cyber threat intelligence, LoRA fine-tuning, report generation},
	pages = {208483--208497},
}

@article{hartman_cross-architecture_2025,
	title = {Cross-{Architecture} {Binary} {Function} {Fingerprinting}},
	volume = {23},
	issn = {1558-4046},
	doi = {10.1109/MSEC.2024.3468153},
	abstract = {By combining the SLEIGH decompiler in Ghidra with an machine learning-based technique we can fingerprint reused functions across processor architectures with high accuracy. This opens the door for reverse engineers and antivirus tools to more effectively identify vulnerable and malware code.},
	number = {2},
	journal = {IEEE Security \& Privacy},
	author = {Hartman, Corey M. and Rimal, Bhaskar P. and de Leon, Daniel Conte and Budhathoki, Nirmal},
	month = mar,
	year = {2025},
	keywords = {Codes, Source coding, Optimization, Accuracy, Malware, Machine learning, Libraries, Internet of Things, Object recognition, Fingerprint recognition, Systems architecture},
	pages = {71--80},
}

@article{liao_augmenting_2025-1,
	address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
	title = {Augmenting {Smart} {Contract} {Decompiler} {Output} {Through} {Fine}-{Grained} {Dependency} {Analysis} and {LLM}-{Facilitated} {Semantic} {Recovery}},
	volume = {51},
	issn = {0098-5589},
	doi = {10.1109/TSE.2025.3623325},
	abstract = {Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate function identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, SmartHalo constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract functions shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o mini with SmartHalo further enhances its performance, achieving a precision of 91.32\% and a recall of 87.38\% for function boundaries, a precision of 90.40\% and a recall of 88.82\% for variable types, and a precision of 80.66\% and a recall of 91.78\% for contract attributes.},
	language = {English},
	number = {12},
	journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
	publisher = {IEEE COMPUTER SOC},
	author = {Liao, Zeqin and Nan, Yuhong and Gao, Zixu and Liang, Henglong and Hao, Sicheng and Ren, Peifan and Zheng, Zibin},
	month = feb,
	year = {2025},
	note = {Type: Article},
	keywords = {Codes, Source coding, Optimization, Training, Semantics, Accuracy, decompilation, large language model, Large language models, Static analysis, Annotations, static analysis, Smart contracts, Smart contract},
	pages = {3574--3590},
}

@article{reiter_automatically_2025-1,
	address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
	title = {Automatically {Mitigating} {Vulnerabilities} in {Binary} {Programs} via {Partially} {Recompilable} {Decompilation}},
	volume = {22},
	issn = {1545-5971},
	doi = {10.1109/TDSC.2024.3482413},
	abstract = {Vulnerabilities are challenging to locate and repair, especially when source code is unavailable and binary patching is required. Manual methods are time-consuming, require significant expertise, and do not scale to the rate at which new vulnerabilities are discovered. Automated methods are an attractive alternative, and we propose Partially Recompilable Decompilation (PRD) to help automate the process. PRD lifts suspect binary functions to source, available for analysis, revision, or review, and creates a patched binary using source- and binary-level techniques. Although decompilation and recompilation do not typically succeed on an entire binary, our approach does because it is limited to a few functions, such as those identified by our binary fault localization. We evaluate the assumptions underlying our approach and find that, without any grammar or compilation restrictions, up to 79\% of individual functions are successfully decompiled and recompiled. In comparison, only 1.7\% of the full C-binaries succeed. When recompilation succeeds, PRD produces test-equivalent binaries 93.0\% of the time. We evaluate PRD in two contexts: a fully automated process incorporating source-level Automated Program Repair (APR) methods; and human-edited source-level repairs. When evaluated on DARPA Cyber Grand Challenge (CGC) binaries, we find that PRD-enabled APR tools, operating only on binaries, perform as well as, and sometimes better than full-source tools, collectively mitigating 85 of the 148 scenarios, a success rate consistent with the same tools operating with access to the entire source code. PRD achieves similar success rates as the winning CGC entries, sometimes finding higher-quality mitigations than those produced by top CGC teams. For generality, the evaluation includes two independently developed APR tools and C++, Rode0day, and real-world binaries.},
	language = {English},
	number = {3},
	journal = {IEEE TRANSACTIONS ON DEPENDABLE AND SECURE COMPUTING},
	publisher = {IEEE COMPUTER SOC},
	author = {Reiter, Pemma and Tay, Hui Jun and Weimer, Westley and Doupe, Adam and Wang, Ruoyu and Forrest, Stephanie},
	month = jun,
	year = {2025},
	note = {Type: Article},
	keywords = {Codes, Computer bugs, Source coding, Software, Prototypes, Measurement, Software engineering, Grammar, Location awareness, Maintenance engineering, software maintenance, C plus plus languages},
	pages = {2270--2282},
}

@article{lu_understanding_2024-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {Understanding and {Finding} {Java} {Decompiler} {Bugs}},
	volume = {8},
	doi = {10.1145/3649860},
	abstract = {Java decompilers are programs that perform the reverse process of Java compilers, i.e., they translate Java bytecode to Java source code. They are essential for reverse engineering purposes and have become more sophisticated and reliable over the years. However, it remains challenging for modern Java decompilers to reliably perform correct decompilation on real-world programs. To shed light on the key challenges of Java decompilation, this paper provides the first systematic study on the characteristics and causes of bugs in mature, widely-used Java decompilers. We conduct the study by investigating 333 unique bugs from three popular Java decompilers. Our key findings and observations include: (1) Although most of the reported bugs were found when decompiling large, real-world code, 40.2\% of them have small test cases for bug reproduction; (2) Over 80\% of the bugs manifest as exceptions, syntactic errors, or semantic errors, and bugs with source code artifacts are very likely semantic errors; (3) 57.7\%, 39.0\%, and 41.1\% of the bugs respectively are attributed to three stages of decompilers-loading structure entities from bytecode, optimizing these entities, and generating source code from these entities; (4) Bugs in decompilers' type inference are the most complex to fix; and (5) Region restoration for structures like loop, sugaring for special structures like switch, and type inference of variables of generic types or indistinguishable types are the three most significant challenges in Java decompilation, which to some extent explains our findings in (3) and (4). Based on these findings, we present JD-Tester, a differential testing framework for Java decompilers, and our experience of using it in testing the three popular Java decompilers. JD-Tester utilizes different Java program generators to construct executable Java tests and finds exceptions, syntactic, and semantic inconsistencies (i.e. bugs) between a generated test and its compiled-decompiled version (through compilation and execution). In total, we have found 62 bugs in the three decompilers, demonstrating both the effectiveness of JD-Tester, and the importance of testing and validating Java decompilers.},
	language = {English},
	number = {OOPSLA},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Lu, Yifei and Hou, Weidong and Pan, Minxue and Li, Xuandong and Su, Zhendong},
	month = apr,
	year = {2024},
	note = {Type: Article},
	keywords = {Reverse Engineering, Decompiler, Differential Testing},
}

@article{sang_control_2024-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {From {Control} {Application} to {Control} {Logic}: {PLC} {Decompile} {Framework} for {Industrial} {Control} {System}},
	volume = {19},
	issn = {1556-6013},
	doi = {10.1109/TIFS.2024.3402117},
	abstract = {Industrial Control System (ICS) depends on the underlying Programmable Logical Controllers (PLCs) to run. As such, the security of the internal control logic of the PLCs is the top concern of ICS. Reversing analysis and forensic work against PLC require extracting control logic from the control application running inside PLC, which is still an unresolved problem. To address the challenge, we propose a PLC decompile framework named CLEVER, which can analyze the control application and extract the control logic. First, we propose a simulation execution based code extraction method, which is utilized to filter the control logic related data. Then, to normalize the control application decompile process, an intermediate representation (IR) is designed, which can simplify the analysis process and enhance the extensibility of CLEVER. Finally, a heuristic data flow analysis algorithm is proposed to find variable dependency, and a sequential parsing method is utilized to reconstruct the source code from the control application. To evaluate our work, real world PLC hardware and programming software are used for the experiment. We use 22 real-world, 58 hand-written, and 150 auto-generated control applications to demonstrate the usability, correctness, and operational efficiency of CLEVER.},
	language = {English},
	journal = {IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Sang, Chao and Wu, Jun and Li, Jianhua and Guizani, Mohsen},
	year = {2024},
	note = {Type: Article},
	keywords = {Codes, Software, Reverse engineering, Process control, Feature extraction, program analysis, Data mining, industrial control system, Assembly, network forensics, PLC, Registers},
	pages = {8685--8700},
}

@article{sisco_loop_2023-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {Loop {Rerolling} for {Hardware} {Decompilation}},
	volume = {7},
	doi = {10.1145/3591237},
	abstract = {We introduce the new problem of hardware decompilation. Analogous to software decompilation, hardware decompilation is about analyzing a low-level artifact-in this case a netlist, i.e., a graph of wires and logical gates representing a digital circuit-in order to recover higher-level programming abstractions, and using those abstractions to generate code written in a hardware description language (HDL). The overall problem of hardware decompilation requires a number of pieces. In this paper we focus on one specific piece of the puzzle: a technique we call hardware loop rerolling. Hardware loop rerolling leverages clone detection and program synthesis techniques to identify repeated logic in netlists (such as would be synthesized from loops in the original HDL code) and reroll them into syntactic loops in the recovered HDL code. We evaluate hardware loop rerolling for hardware decompilation over a set of hardware design benchmarks written in the PyRTL HDL and industry standard SystemVerilog. Our implementation identifies and rerolls loops in 52 out of 53 of the netlists in our benchmark suite, and we show three examples of how hardware decompilation can provide concrete benefits: transpilation between HDLs, faster simulation times over netlists (with mean speedup of 6x), and artifact compaction (39\% smaller on average).},
	language = {English},
	number = {PLDI},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Sisco, Zachary D. and Balkind, Jonathan and Sherwood, Timothy and Hardekopf, Ben},
	month = jun,
	year = {2023},
	note = {Type: Article},
	keywords = {hardware decompilation, loop rerolling, program synthesis},
}

@article{park_static_2023-1,
	address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
	title = {Static {Analysis} of {JNI} {Programs} via {Binary} {Decompilation}},
	volume = {49},
	issn = {0098-5589},
	doi = {10.1109/TSE.2023.3241639},
	abstract = {JNI programs are widely used thanks to the combined benefits of C and Java programs. However, because understanding the interaction behaviors between two different programming languages is challenging, JNI program development is difficult to get right and vulnerable to security attacks. Thus, researchers have proposed static analysis of JNI program source code to detect bugs and security vulnerabilities in JNI programs. Unfortunately, such source code analysis is not applicable to compiled JNI programs that are not open-sourced or open-source JNI programs containing third-party binary libraries. While JN-SAF, the state-of-the-art analyzer for compiled JNI programs, can analyze binary code, it has several limitations due to its symbolic execution and summary-based bottom-up analysis. In this paper, we propose a novel approach to statically analyze compiled JNI programs without their source code using binary decompilation. Unlike JN-SAF that analyzes binaries directly, our approach decompiles binaries and analyzes JNI programs with the decompiled binaries using an existing JNI program analyzer for source code. To decompile binaries to compilable C source code with precise JNI-interoperation-related types, we improve an existing decompilation tool by leveraging the characteristics of JNI programs. Our evaluation shows that the approach is precise as almost the same as the state-of-the-art JNI program analyzer for source code, and more precise than JN-SAF.},
	language = {English},
	number = {5},
	journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
	publisher = {IEEE COMPUTER SOC},
	author = {Park, Jihee and Lee, Sungho and Hong, Jaemin and Ryu, Sukyoung},
	month = may,
	year = {2023},
	note = {Type: Article},
	keywords = {Codes, Security, Source coding, Java, Static analysis, Computer architecture, static analysis, Libraries, binary decompilation, Java native interface},
	pages = {3089--3105},
}

@article{dramko_dire_2023-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{DIRE} and its {Data}: {Neural} {Decompiled} {Variable} {Renamings} with {Respect} to {Software} {Class}},
	volume = {32},
	issn = {1049-331X},
	doi = {10.1145/3546946},
	abstract = {The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE's overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23\% more frequently than the original DIRE model.},
	language = {English},
	number = {2},
	journal = {ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Dramko, Luke and Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Ed and Allamanis, Miltiadis and Neubig, Graham and Vasilescu, Bogdan and Le Goues, Claire},
	month = apr,
	year = {2023},
	note = {Type: Article},
	keywords = {decompilation, Machine learning, data provenance},
}

@article{grech_elipmoc_2022-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {Elipmoc: {Advanced} {Decompilation} of {Ethereum} {Smart} {Contracts}},
	volume = {6},
	doi = {10.1145/3527321},
	abstract = {Smart contracts on the Ethereum blockchain greatly benefit from cutting-edge analysis techniques and pose significant challenges. A primary challenge is the extremely low-level representation of deployed contracts. We present Elipmoc, a decompiler for the next generation of smart contract analyses. Elipmoc is an evolution of Gigahorse, the top research decompiler, dramatically improving over it and over other state-of-the-art tools, by employing several high-precision techniques and making them scalable. Among these techniques are a new kind of context sensitivity (termed “transactional sensitivity”) that provides a more effective static abstraction of distinct dynamic executions; a path-sensitive (yet scalable, through path merging) algorithm for inference of function arguments and returns; and a fully context sensitive private function reconstruction process. As a result, smart contract security analyses and reverse-engineering tools built on top of Elipmoc achieve high scalability, precision and completeness. Elipmoc improves over all notable past decompilers, including its predecessor, Gigahorse, and the stateof-the-art industrial tool, Panoramix, integrated into the primary Ethereum blockchain explorer, Etherscan. Elipmoc produces decompiled contracts with fully resolved operands at a rate of 99.5\% (compared to 62.8\% for Gigahorse), and achieves much higher completeness in code decornpilation than Panoramix-e.g., up to 67\% more coverage of external call statements-while being over 5x faster. Elipmoc has been the enabler for recent (independent) discoveries of several exploitable vulnerabilities on popular protocols, over funds in the many millions of dollars.},
	language = {English},
	number = {OOPSLA},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Grech, Neville and Lagouvardos, Sifis and Tsatiris, Ilias and Smaragdakis, Yannis},
	month = apr,
	year = {2022},
	note = {Type: Article},
	keywords = {Security, Decompilation, Program Analysis, Datalog, Ethereum, Smart Contracts, Blockchain},
}

@article{harrand_java_2020,
	address = {STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA},
	title = {Java decompiler diversity and its application to meta-decompilation},
	volume = {168},
	issn = {0164-1212},
	doi = {10.1016/j.jss.2020.110645},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84\%, respectively 78\%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6\% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler. (C) 2020 Published by Elsevier Inc.},
	language = {English},
	journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
	publisher = {ELSEVIER SCIENCE INC},
	author = {Harrand, Nicolas and Soto-Valero, Cesar and Monperrus, Martin and Baudry, Benoit},
	month = oct,
	year = {2020},
	note = {Type: Article},
	keywords = {Decompilation, Reverse engineering, Java bytecode, Source code analysis},
}

@article{dallaglio_highliner_2025-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {Highliner: {Enhancing} {Binary} {Analysis} through {NLP}-{Based} {Instruction}-{Level} {Detection} of {C} plus plus {Inline} {Functions}},
	volume = {28},
	issn = {2471-2566},
	doi = {10.1145/3765521},
	abstract = {The complexities introduced by compiler optimization have long stood as a significant obstacle in binary analysis and reverse engineering. Function inlining, in particular, complicates function recognition by replacing function calls with the entire body of the callee, mixing code from multiple functions. State-of-the-art approaches can identify inlined functions at basic block granularity, but cannot determine which instructions belong to each function and precisely deduce inlined boundaries. Without this information, further analyses such as decompilation cannot be performed effectively. This article presents Highliner, a novel approach that improves state-of-the-art approaches by identifying inline instances at instruction-level granularity. Highliner operates downstream of block-level detectors: given basic blocks reported by state-of-the-art approaches as belonging to a specific inlined function, it labels each instruction as Inlined or Not inlined and recovers the inlined-function boundaries. We treat the problem as a sequence tagging task typical of NLP and implement a learning-based technique involving instruction embedding and recurrent neural networks. We compile a dataset of open-source projects with different optimizations and use the DWARF debug information standard to construct labeled sequences of inline instructions. We use this dataset to train, validate, and test a sequence labeling architecture in which instructions are encoded via the pre-trained assembly language transformer PalmTree and then processed by an RNN-based classifier to produce binary predictions. When evaluated as a binary classifier, Highliner achieves an F1-score of 0.94 overall. In addition, when specifically tested on recognizing function boundaries, Highliner achieves an Accuracy of 0.82 on initial boundaries and 0.83 on final boundaries.},
	language = {English},
	number = {4},
	journal = {ACM TRANSACTIONS ON PRIVACY AND SECURITY},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Dall'aglio, Lorenzo and Binosi, Lorenzo and Carminati, Michele and Zanero, Stefano and Polino, Mario},
	month = jan,
	year = {2025},
	note = {Type: Article},
	keywords = {reverse engineering, Binary analysis, function inlining, inline function recognition, natural language processing (NLP)},
}

@article{peng_bctd-ics_2025-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {{BCTD}-{ICS}: {A} {Blockchain}-{Aided} {Framework} for {Trusted} {Detection} of {Industrial} {Control} {System} {Components}},
	volume = {12},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2025.3583304},
	abstract = {The cybersecurity threats targeting industrial control systems (ICSs) are evolving with increasing sophistication. Addressing the detection blind spots in existing source code analysis techniques, this study reveals a dual security paradox arising from code sensitivity: privacy leakage risks caused by decompilation techniques and integrity verification deficiencies in reverse engineering. This article investigates three critical challenges: 1) what are the component flow process and detection elements of ICS component source code? 2) how can high-performance and reliable tracing and traceability be provided for ICS component source code exceptions and routine detection? and 3) how can privacy enhancement and trusted detection of ICS component source code with high sensitivity be achieved? This article proposes a blockchain-integrated trusted detection framework for ICS (BCTD-ICS), delivering groundbreaking solutions: 1) establishing a lifecycle circulation model that systematically maps component types, stakeholders, and detection parameters; 2) developing a tripartite collaborative architecture [blockchain- identification resolution zero-knowledge proofs (ZKPs)], featuring a traceability mechanism with trusted identification codes (resolution efficiency: 40 ms/105 queries) to eliminate decompilation-induced privacy risks; and 3) creating an industrial-oriented privacy enhancement system utilizing DBSCAN clustering for intelligent sampling (26\% compression rate on BCN3D Moveo) and optimizing ZK-SNARK protocols through Shamir's secret sharing, establishing a backdoor-resistant distributed parameter generation system (time delay increment {\textless} 100 ms). Experimentally verified, our solution enables ICS component code detection supply-chain-wise without sensitive data leakage in real-world industries. This work establishes a novel trusted detection paradigm for ICS, advancing detection efficiency and credibility under strict privacy preservation requirements, meeting Industry 4.0 security demands.},
	language = {English},
	number = {18},
	journal = {IEEE INTERNET OF THINGS JOURNAL},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Peng, Xiangzhen and Ma, Tianyu and Zheng, Chengliang and Shen, Zhidong and Cui, Xiaohui},
	month = sep,
	year = {2025},
	note = {Type: Article},
	keywords = {Codes, Privacy, Security, Source coding, Protocols, Supply chains, Internet of Things, blockchain, Artificial intelligence technology, Blockchains, identification resolution technology, industrial control systems (ICSs), Object recognition, source code detection, Telecommunication traffic, zero-knowledge proofs (ZKPs)},
	pages = {37552--37570},
}

@article{ghimire_survey_2025-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {A {Survey} on {Application} of {AI} on {Reverse} {Engineering} for {Software} {Analysis} and {Security}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3593456},
	abstract = {Reverse engineering process serves essential functions in software analysis and security auditing and malware detection but requires significant time and effort. Researchers and practitioners now investigate how Artificial Intelligence (AI) technology can automate and improve different reverse engineering procedures. This survey provides an extensive evaluation of recent AI-based reverse engineering techniques which focus on software decompilation and function identification as well as control flow recovery and vulnerability analysis. The paper presents a classification system for existing methods while comparing them through an analysis of their development from traditional rule-based systems to contemporary deep learning frameworks. The research examines fundamental datasets together with field tools and evaluation metrics. This paper establishes a fundamental understanding of AI integration in reverse engineering for software security while discussing future development directions.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Ghimire, Ashutosh and Lingala, Sahasra Rao and Zhang, Junjie and Alsulami, Faris and Amsaad, Fathi},
	year = {2025},
	note = {Type: Article},
	keywords = {Codes, Security, Source coding, Software, Reverse engineering, Malware, Static analysis, Logic, malware detection, Surveys, software security, anomalies, Artificial intelligence, threat analysis},
	pages = {152903--152913},
}

@article{nethala_deep_2025-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {A {Deep} {Learning}-{Based} {Ensemble} {Framework} for {Robust} {Android} {Malware} {Detection}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3551152},
	abstract = {The exponential growth of Android applications has resulted in a surge of malware threats, posing severe risks to user privacy and data security. To address these challenges, this study introduces a novel malware detection approach utilizing an ensemble of Convolutional Neural Networks (CNNs) for enhanced classification accuracy. The methodology incorporates a multi-phase process, starting with the extraction and preprocessing of APK (Android app) files. The preprocessing phase involves decompressing, decompiling, and transforming the APK files into bytecode and Dex files. The extracted byte data is converted into 1D vectors and reshaped into 2D grayscale images, enabling efficient feature learning through CNNs. The proposed ensemble of CNN-based models undergoes comprehensive training, validation, and evaluation, demonstrating superior performance compared to existing approaches. We used two popular Android datasets to evaluate the performance of our proposed model. Specifically, the model achieves an accuracy of 98.65\%, F1-score of 96.43\% on the Drebin dataset and attains 97.91\% accuracy, 96.73\% of F1-score on the AMD dataset. These results confirm the mode's ability to effectively identify Android malware with high precision and reliability, outperforming traditional techniques. This research not only underscores the potential of our proposed approach in malware detection but also sets a foundation for future advancements. Future efforts will focus on real-time malware detection, integration with mobile security frameworks, and evaluation across diverse datasets to ensure adaptability to emerging malware threats.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Nethala, Sainag and Chopra, Pronoy and Kamaluddin, Khaja and Alam, Shahid and Alharbi, Soltan and Alsaffar, Mohammad},
	year = {2025},
	note = {Type: Article},
	keywords = {Deep learning, Accuracy, Feature extraction, Malware, deep learning, Static analysis, machine learning, Machine learning, Support vector machines, Computational modeling, malware classification, Android malware detection, attention mechanism, convolutional neural networks, ensemble learning, Meta-CNN, Random forests, Real-time systems},
	pages = {46673--46696},
}

@article{priambodo_malqwen_2025-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {{MalQwen}: {Fine} {Tuned} {LLM} for {Static} {Android} {Malware} {Analysis} {Report}},
	volume = {13},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2025.3637047},
	abstract = {The Android operating system continues to face escalating security challenges, primarily due to its open-source nature and the rapid proliferation of applications from untrusted sources. Traditional static analysis tools lack the flexibility to capture evolving malware behaviors, limiting their interpretability and scalability. Large Language Models (LLMs) are now applied in cybersecurity for malware detection, phishing classification, and cyber threat intelligence. However, their use has not been extended to producing detailed and interpretable Android malware analysis reports. This study integrates LLMs into Android malware analysis by creating a dataset for instruction tuning and fine-tuning the Qwen-7B model using the LoRA method. The model MalQwen is developed by fine-tuning Qwen 2.5-7B with 429 malware samples containing decompiled code and expert labeled security reports. MalQwen outperforms models like Gemini and LLaMA, achieving a BERTscore of 0.84 for SMS malware and a Perplexity score of 3.30 for Scareware. These findings confirm MalQwen's superior performance in generating precise malware reports, validating LLMs as a powerful new method for Android malware analysis.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Priambodo, Tegar Ganang Satrio and Prabowo, Angela Oryza and Puspitarini, Annisa Dwi and Winarso, Raihan Adam Handoyo and Aisyah, Nur and Pratama, Mohammad Yoga and Purwitasari, Diana and Pratomo, Baskoro Adi},
	year = {2025},
	note = {Type: Article},
	keywords = {large language model, static analysis, Android malware analysis, LoRA fine-tuning, report generation},
	pages = {208483--208497},
}

@article{zhang_cf-gkat_2025-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{CF}-{GKAT}: {Efficient} {Validation} of {Control}-{Flow} {Transformations}},
	volume = {9},
	doi = {10.1145/3704857},
	abstract = {Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT's abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren's goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation.},
	language = {English},
	number = {POPL},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Zhang, Cheng and Kappe, Tobias and Narvaez, David E. and Naus, Nico},
	month = jan,
	year = {2025},
	note = {Type: Article},
	keywords = {control flow recovery, Kleene algebra, Program equivalence},
}

@article{udeshi_remaqe_2024-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{REMaQE}: {Reverse} {Engineering} {Math} {Equations} from {Executables}},
	volume = {8},
	issn = {2378-962X},
	doi = {10.1145/3699674},
	abstract = {Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This article proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer equations from object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool “tmon.” To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow.},
	language = {English},
	number = {4},
	journal = {ACM TRANSACTIONS ON CYBER-PHYSICAL SYSTEMS},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Udeshi, Meet and Krishnamurthy, Prashanth and Pearce, Hammond and Karri, Ramesh and Khorrami, Farshad},
	month = jan,
	year = {2024},
	note = {Type: Article},
	keywords = {symbolic execution, binary reverse engineering, embedded systems, mathematical equations},
}

@article{hartman_cross-architecture_2025-1,
	address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
	title = {Cross-{Architecture} {Binary} {Function} {Fingerprinting}},
	volume = {23},
	issn = {1540-7993},
	doi = {10.1109/MSEC.2024.3468153},
	abstract = {By combining the SLEIGH decompiler in Ghidra with an ML-based technique we can fingerprint reused functions, within stripped binaries, across processor architectures with high accuracy. This opens the door for reverse engineers and antivirus tools to more effectively identify vulnerable and malware code that is frequently shared across system architectures.},
	language = {English},
	number = {2},
	journal = {IEEE SECURITY \& PRIVACY},
	publisher = {IEEE COMPUTER SOC},
	author = {Hartman, Corey M. and Rimal, Bhaskar P. and de Leon, Daniel Conte and Budhathoki, Nirmal},
	month = apr,
	year = {2025},
	note = {Type: Article},
	keywords = {Codes, Source coding, Optimization, Accuracy, Malware, Libraries, Internet of Things, Object recognition, Fingerprint recognition, Systems architecture},
	pages = {71--80},
}

@article{li_varlifter_2024-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{VARLIFTER}: {Recovering} {Variables} and {Types} from {Bytecode} of {Solidity} {Smart} {Contracts}},
	volume = {8},
	doi = {10.1145/3689711},
	abstract = {Since funds or tokens in smart contracts are maintained through specific state variables, contract audit, an effective means for security assurance, particularly focuses on these variables and their related operations. However, the absence of publicly accessible source code for numerous contracts, with only bytecode exposed, hinders audit efforts. Recovering variables and their types from Solidity bytecode is thus a critical task in smart contract analysis and audit, yet this is a challenging task because the bytecode loses variable and type information, only with low-level data operated by stack manipulations and untyped memory/storage accesses. The state-of-the-art smart contract decompilers miss identifying many variables and incorrectly infer the types for many identified variables. To this end, we propose VARLIFTER, a lifter dedicated to the precise and efficient recovery of typed variables. VARLIFTER interprets every read or written field of a data region as at least one potential variable, and after discarding falsely identified variables, it progressively refines the variable types based on the variable behaviors in the form of operation sequences. We evaluate VARLIFTER on 34,832 real-world Solidity smart contracts. VARLIFTER attains a precision of 97.48\% and a recall of 91.84\% for typed variable recovery. Moreover, VARLIFTER finishes analyzing 77\% of smart contracts in around 10 seconds per contract. If VARLIFTER is used to replace the variable recovery modules of the two state-of-the-art Solidity bytecode decompilers, 52.4\%, and 74.6\% more typed variables will be correctly recovered, respectively. The applications of VARLIFTER to contract decompilation, contract audit, and contract bytecode fuzzing illustrate that the recovered variable information improves many contract analysis tasks.},
	language = {English},
	number = {OOPSLA},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Li, Yichuan and Song, Wei and Huang, Jeff},
	month = oct,
	year = {2024},
	note = {Type: Article},
	keywords = {smart contract, Blockchain, EVM, Solidity bytecode, variable recovery},
}

@article{pizzolotto_mitigating_2024-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {Mitigating {Debugger}-based {Attacks} to {Java} {Applications} with {Self}-debugging},
	volume = {33},
	issn = {1049-331X},
	doi = {10.1145/3631971},
	abstract = {Java bytecode is a quite high-level language and, as such, it is fairly easy to analyze and decompile with malicious intents, e.g., to tamper with code and skip license checks. Code obfuscation was a first attempt to mitigate malicious reverse-engineering based on static analysis. However, obfuscated code can still be dynamically analyzed with standard debuggers to perform step-wise execution and to inspect (or change) memory content at important execution points, e.g., to alter the verdict of license validity checks. Although some approaches have been proposed to mitigate debugger-based attacks, they are only applicable to binary compiled code and none address the challenge of protecting Java bytecode. In this article, we propose a novel approach to protect Java bytecode from malicious debugging. Our approach is based on automated program transformation to manipulate Java bytecode and split it into two binary processes that debug each other (i.e., a self-debugging solution). In fact, when the debugging interface is already engaged, an additional malicious debugger cannot attach. To be resilient against typical attacks, our approach adopts a series of technical solutions, e.g., an encoded channel is shared by the two processes to avoid leaking information, an authentication protocol is established to avoid Man-in-the-middle attacks, and the computation is spread between the two processes to prevent the attacker to replace or terminate either of them. We test our solution on 18 real-world Java applications, showing that our approach can effectively block the most common debugging tasks (either with the Java debugger or the GNU debugger) while preserving the functional correctness of the protected programs. While the final decision on when to activate this protection is still up to the developers, the observed performance overhead was acceptable for common desktop application domains.},
	language = {English},
	number = {4},
	journal = {ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Pizzolotto, Davide and Berlato, Stefano and Ceccato, Mariano},
	month = may,
	year = {2024},
	note = {Type: Article},
	keywords = {Anti-debugging, maliciuos reverse engineering, man at the end attacks, tampering attacks},
}

@article{chen_android_2024-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Android {Malware} {Family} {Clustering} {Based} on {Multiple} {Features}},
	volume = {73},
	issn = {0018-9529},
	doi = {10.1109/TR.2023.3332090},
	abstract = {Familiar analysis for malware plays an important role in comprehending the diversity of malicious behaviors and identifying the emerging security threats. Existing studies mainly focus on classifying malware into known families by supervised learning. However, these methods face two main challenges, 1) the lack of a large amount of labeled data and 2) the poor effectiveness in identifying unknown families of malware. To overcome these challenges, we propose a new method called multiple features (MulFC) based on unsupervised learning. In the method, we first leverage a decompiling tool to extract multiple features, including manifest features, application programming interface (API) features, and opcode features. Then, the opcode features are preprocessed to filter out the redundant ones to reduce the calculation cost. After that, we adopt the Jaccard index to calculate the similarities between malware and construct a malware network. Finally, InfoMap is applied to perform the clustering on the basis of the malware network. Overall, MulFC does not require the use of labeled data and can identify unknown families of malware. Experiments are conducted on two datasets for the performance evaluation of MulFC. The experimental results show that MulFC achieves 0.810 in terms of normalized mutual information, 0.576 in terms of adjusted rand index, 0.620 in terms of the Fowlkes-Mallows index, and 0.805 in terms of V-measure on average, and outperforms the state-of-the-art baseline method by 0.060, 0.054, 0.038, and 0.065, respectively.},
	language = {English},
	number = {2},
	journal = {IEEE TRANSACTIONS ON RELIABILITY},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Chen, Xin and Yu, Dongjin and Cai, Xinxin and Jiang, He and Yu, Haihua},
	month = jun,
	year = {2024},
	note = {Type: Article},
	keywords = {Android malware, InfoMap, malware family clustering, multiple features, unsupervised learning},
	pages = {1202--1215},
}

@article{zhang_bian_2023-1,
	address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
	title = {{BiAn}: {Smart} {Contract} {Source} {Code} {Obfuscation}},
	volume = {49},
	issn = {0098-5589},
	doi = {10.1109/TSE.2023.3298609},
	abstract = {With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174\% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8\% and 40.5\% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50\% of cases for ten widely-used static analysis detection tools.},
	language = {English},
	number = {9},
	journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
	publisher = {IEEE COMPUTER SOC},
	author = {Zhang, Pengcheng and Yu, Qifan and Xiao, Yan and Dong, Hai and Luo, Xiapu and Wang, Xiao and Zhang, Meng},
	month = sep,
	year = {2023},
	note = {Type: Article},
	keywords = {Codes, Security, Source coding, source code, obfuscation, Complexity theory, Intellectual property, smart contract, Smart contracts, Layout, Ethereum, Blockchain},
	pages = {4456--4476},
}

@article{da_costa_lightweight_2023,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {A {Lightweight} and {Multi}-{Stage} {Approach} for {Android} {Malware} {Detection} {Using} {Non}-{Invasive} {Machine} {Learning} {Techniques}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3296606},
	abstract = {Android has been a constant target of cybercriminals that try to attack one of the most used operating systems, commonly using malicious applications (denominated malware) that, once installed on a device, can harm users in several ways. Existing malware detection solutions are usually invasive as they obtain classification features by performing reverse engineering, decompilation, or disassembly of the analyzed application, which infringes licenses and terms of use of applications. In addition, these solutions often employ a single machine learning (ML) model to detect various types of malware, resulting in several false alarms. In this context, we propose an approach to detect Android malware consisting of a set of specific-type detectors in which each one performs a multi-stage analysis, based on rules and ML techniques, in different phases of the application cycle (before and after its installation). Our approach also differs from state-of-the-art solutions by being non-invasive, since it leverages a process to obtain application's features that does not infringe licenses and terms of use of applications. In addition, according to experiments performed on a real Android smartphone, our proposal presents the following additional advantages over state-of-the-art solutions: a more efficient process to classify applications that is three times faster and requires ten times less CPU usage in some cases (saving device energy); and a better detection performance, with higher balanced accuracy, nine times less false positive cases, and ten times less false negative cases.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {da Costa, Leonardo and Moia, Vitor},
	year = {2023},
	note = {Type: Article},
	keywords = {machine learning, Android, malware detection, multi-stage analysis, non-invasive feature extraction},
	pages = {73127--73144},
}

@article{rodriguez-bazan_android_2023-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Android {Ransomware} {Analysis} {Using} {Convolutional} {Neural} {Network} and {Fuzzy} {Hashing} {Features}},
	volume = {11},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2023.3328314},
	abstract = {Most of the time, cybercriminals look for new ways to bypass security controls by improving their attacks. In the 1980s, attackers developed malware to kidnap user data by requesting payments. Malware is called a ransomware. Recently, they have demanded payment in Bitcoin or any other cryptocurrency. Ransomware is one of the most dangerous threats on the Internet, and this type of malware could affect almost all devices. Malware cipher device data, making them inaccessible to users. In this study, a new method for Android ransomware classification was proposed. This method implements a Convolutional Neural Network (CNN) for malware classification based on images. This paper presents a novel method for transforming an Android Application Package (APK) into a grayscale image. The image creation relies on using Natural Language Processing (NLP) techniques for text cleaning and Fuzzy Hashing to represent the decompiled code from the APK in a set of hashes after preprocessing using NLP techniques. The image is composed of n fuzzy hashes that represent the APK. The method was tested using a dataset of 7,765 Android ransomware samples obtained from external researchers and public sources. The accuracy of the proposed method was higher than that of other methods in the literature.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Rodriguez-Bazan, Horacio and Sidorov, Grigori and Escamilla-Ambrosio, Ponciano Jorge},
	year = {2023},
	note = {Type: Article},
	keywords = {Deep learning, deep learning, Operating systems, Classification algorithms, Convolutional neural networks, Natural language processing, Android ransomware, Androids, convolutional neural network, fuzzy hashing, Fuzzy systems, malware classification, Matched filters, Metadata, ransomware, Ransomware, XML},
	pages = {121724--121738},
}

@article{ahmed_learning_2022-1,
	address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
	title = {Learning to {Find} {Usages} of {Library} {Functions} in {Optimized} {Binaries}},
	volume = {48},
	issn = {0098-5589},
	doi = {10.1109/TSE.2021.3106572},
	abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, “natural” source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to “decompile” binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
	language = {English},
	number = {10},
	journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
	publisher = {IEEE COMPUTER SOC},
	author = {Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},
	month = oct,
	year = {2022},
	note = {Type: Article},
	keywords = {Optimization, Reverse engineering, Tools, Training, Malware, deep learning, Databases, Libraries, software modeling},
	pages = {3862--3876},
}

@article{ullah_iot-based_2022,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{IoT}-based {Cloud} {Service} for {Secured} {Android} {Markets} using {PDG}-based {Deep} {Learning} {Classification}},
	volume = {22},
	issn = {1533-5399},
	doi = {10.1145/3418206},
	abstract = {Software piracy is an act of illegal stealing and distributing commercial software either for revenue or identify theft. Pirated applications on Android app stores are harming developers and their users by clone scammers. The scammers usually generate pirated versions of the same applications and publish them in different open-source app stores. There is no centralized system between these app stores to prevent scammers from publishing pirated applications. As most of the app stores are hosted on cloud storage, therefore a cloud-based interaction system can prevent scammers from publishing pirated applications. In this paper, we proposed IoT-based cloud architecture for clone detection using program dependency analysis. First, the newly submitted APK and possible original files are selected from app stores. The APK Extractor and JDEX decompiler extract APK and DEX files for Java source code analysis. The dependency graphs of Java files are generated to extract a set of weighted features. The Stacked-Long Short-Term Memory (S-LSTM) deep learning model is designed to predict possible clones. Experimental results have shown that the proposed approach can achieve an average accuracy of 95.48\% among clones from different application stores.},
	language = {English},
	number = {2},
	journal = {ACM TRANSACTIONS ON INTERNET TECHNOLOGY},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Ullah, Farhan and Naeem, Muhammad Rashid and Bajahzar, Abdullah S. and Al-Turjman, Fadi},
	month = may,
	year = {2022},
	note = {Type: Article},
	keywords = {deep learning, Internet of Things, Clone detection, cloud services, program dependency graph},
}

@article{gao_malware_2022-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Malware {Detection} by {Control}-{Flow} {Graph} {Level} {Representation} {Learning} {With} {Graph} {Isomorphism} {Network}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3215267},
	abstract = {With society's increasing reliance on computer systems and network technology, the threat of malicious software grows more and more serious. In the field of information security, malware detection has been a key problem that academia and industry are committed to solving. Machine learning is an effective method for processing large-scale data, such as the Gradient Boosting Decision Tree (GBDT) and deep neural network technology. Although these types of detection methods can deal with cyber threats, most feature extraction methods are based on the statistical information features of portable executable (PE) files and thus lack the decompiled code and execution flow structure of the PE samples. Therefore, we propose a Control-Flow Graph (CFG)- and Graph Isomorphism Network (GIN)-based malware classification system. The feature vectors of CFG basic blocks are generated using the large-scale pre-trained language model MiniLM, which is beneficial for the GIN to further learn and compress the CFG-based representation, and classified with multi-layer perceptron. In addition, we evaluated the effectiveness of the representation under different dimensions and classifiers. To evaluate our method, we set up a CFG-based malware detection graph dataset from a PE file of the Blue Hexagon Open Dataset for Malware Analysis (BODMAS), which we call the Malware Geometric Binary Dataset (MGD-BINARY) and collected the experimental results of CFG representation in different dimensions and classifier settings. The evaluation results show that our proposal has proved an Accuracy metric of 0.99160 and achieved 0.99148 Area Under the Curve (AUC) results.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Gao, Yun and Hasegawa, Hirokazu and Yamaguchi, Yukiko and Shimada, Hajime},
	year = {2022},
	note = {Type: Article},
	keywords = {Codes, Analytical models, Feature extraction, Malware, machine learning, Machine learning, Malware detection, Data mining, graph classification, Graphics, Natural language processing, static analysis},
	pages = {111830--111841},
}

@article{zhang_fault_2022-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Fault {Diagnosis} of {Power} {Transformer} {Based} on {SSA}-{MDS} {Pretreatment}},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3202982},
	abstract = {Aiming at the problems of coupling between transformer input characteristics and low accuracy of transformer fault diagnosis, SSA-MDS and other soft technologies are used to analyze the key characteristics of transformer faults, so as to improve the accuracy of transformer fault diagnosis. The SSA algorithm cascade MDS algorithm to process the DGA data is proposed. Subsequently, the TSSA-RF model is introduced to classify the DGA data. The DGA data is first mapped to a high-dimensional space. Next, the optimal feature subset is encoded using the SSA algorithm to reduce irrelevant and redundant features. In this study, the correlation between the optimal feature dimension and the transformer fault diagnosis accuracy is investigated. the expression of the optimal feature subset is obtained by decompiling the SSA operator. The pre-processed data are classified using the RF model, and the TSSA -RF model for classifying the DGA data is found with the highest accuracy through the comparison of different optimization algorithms. After the RF model is optimized using the TSSA algorithm, its accuracy increases by 7.89\%, and the accuracy of the TSSA -RF model is obtained as 92.11\%. The example results show that compared with the original data, the proposed data processing algorithm improves the diagnostic accuracy of transformer by 11.97 \% in the RF model. Compared with multiple preprocessing methods, SSA-MDS has the highest accuracy. Compared with the original data, the accuracy of TSSA-RF model increases by 11.64 \%.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Zhang, Mei and Chen, Wanli},
	year = {2022},
	note = {Type: Article},
	keywords = {Optimization, Data models, feature extraction, Classification algorithms, Classification tree analysis, fault diagnosis, Fault diagnosis, Oil insulation, Power transformer, Power transformer insulation, TSSA algorithm, RF∼model},
	pages = {92505--92515},
}

@article{phu_efficient_2021,
	address = {GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND},
	title = {An {Efficient} {Algorithm} to {Extract} {Control} {Flow}-{Based} {Features} for {IoT} {Malware} {Detection}},
	volume = {64},
	issn = {0010-4620},
	doi = {10.1093/comjnl/bxaa087},
	abstract = {Control flow-based feature extraction method has the ability to detect malicious code with higher accuracy than traditional text-based methods. Unfortunately, this method has been encountered with the NP-hard problem, which is infeasible for the large-sized and high-complexity programs. To tackle this, we propose a control flow-based feature extraction dynamic programming algorithm for fast extraction of control flow-based features with polynomial time O(N-2), where N is the number of basic blocks in decompiled executable codes. From the experimental results, it is demonstrated that the proposed algorithm is more efficient and effective in detecting malware than the existing ones. Applying our algorithm to an Internet of Things dataset gives better results on three measures: Accuracy = 99.05\%, False Positive Rate = 1.31\% and False Negative Rate = 0.66\%.},
	language = {English},
	number = {4},
	journal = {COMPUTER JOURNAL},
	publisher = {OXFORD UNIV PRESS},
	author = {Phu, Tran Nghi and Tho, Nguyen Dai and Hoang, Le Huy and Toan, Nguyen Ngoc and Binh, Nguyen Ngoc},
	month = apr,
	year = {2021},
	note = {Type: Article},
	keywords = {CFD, control flow-based features, dynamic programming, embedded malware, IoT malware detection},
	pages = {599--609},
}

@article{albert_taming_2020-1,
	address = {2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA},
	title = {Taming {Callbacks} for {Smart} {Contract} {Modularity}},
	volume = {4},
	doi = {10.1145/3428277},
	abstract = {Callbacks are an effective programming discipline for implementing event-driven programming, especially in environments like Ethereum which forbid shared global state and concurrency. Callbacks allow a callee to delegate the execution back to the caller. Though effective, they can lead to subtle mistakes principally in open environments where callbacks can be added in a new code. Indeed, several high profile bugs in smart contracts exploit callbacks. We present the first static technique ensuring modularity in the presence of callbacks and apply it to verify prominent smart contracts. Modularity ensures that external calls to other contracts cannot affect the behavior of the contract. Importantly, modularity is guaranteed without restricting programming. In general, checking modularity is undecidable -even for programs without loops. This paper describes an effective technique for soundly ensuring modularity harnessing SMT solvers. The main idea is to define a constructive version of modularity using commutativity and projection operations on program segments. We believe that this approach is also accessible to programmers, since counterexamples to modularity can be generated automatically by the SMT solvers, allowing programmers to understand and fix the error. We implemented our approach in order to demonstrate the precision of the modularity analysis and applied it to real smart contracts, including a subset of the 150 most active contracts in Ethereum. Our implementation decompiles bytecode programs into an intermediate representation and then implements the modularity checking using SMT queries. Overall, we argue that our experimental results indicate that the method can be applied to many realistic contracts, and that it is able to prove modularity where other methods fail.},
	language = {English},
	journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Albert, Elvira and Grossman, Shelly and Rinetzky, Noam and Rodriguez-Nunez, Clara and Rubio, Albert and Sagiv, Mooly},
	month = jan,
	year = {2020},
	note = {Type: Article},
	keywords = {program analysis, smart contracts, blockchain, invariants, logic and verification, program verification},
}

@article{grech_madmax_2020-1,
	address = {1601 Broadway, 10th Floor, NEW YORK, NY USA},
	title = {{MadMax}: {Analyzing} the {Out}-of-{Gas} {World} of {Smart} {Contracts}},
	volume = {63},
	issn = {0001-0782},
	doi = {10.1145/3416262},
	abstract = {Ethereum is a distributed blockchain platform, serving as an ecosystem for smart contracts: full-fledged intercommunicating programs that capture the transaction logic of an account. A gas limit caps the execution of an Ethereum smart contract: instructions, when executed, consume gas, and the execution proceeds as long as gas is available. Gas-focused vulnerabilities permit an attacker to force key contract functionality to run out of gas-effectively performing a permanent denial-of-service attack on the contract. Such vulnerabilities are among the hardest for programmers to protect against, as out-of-gas behavior may be uncommon in nonattack scenarios and reasoning about these vulnerabilities is nontrivial. In this paper, we identify gas-focused vulnerabilities and present MadMax: a static program analysis technique that automatically detects gas-focused vulnerabilities with very high confidence. MadMax combines a smart contract decompiler and semantic queries in Datalog. Our approach captures high-level program modeling concepts (such as “dynamic data structure storage” and “safely resumable loops”) and delivers high precision and scalability. MadMax analyzes the entirety of smart contracts in the Ethereum blockchain in just 10 hours and flags vulnerabilities in contracts with a monetary value in billions of dollars. Manual inspection of a sample of flagged contracts shows that 81\% of the sampled warnings do indeed lead to vulnerabilities.},
	language = {English},
	number = {10},
	journal = {COMMUNICATIONS OF THE ACM},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Grech, Neville and Kong, Michael and Jurisevic, Anton and Brent, Lexi and Scholz, Bernhard and Smaragdakis, Yannis},
	month = oct,
	year = {2020},
	note = {Type: Article},
	pages = {87--95},
}

@article{raychev_predicting_2019-1,
	address = {2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA},
	title = {Predicting {Program} {Properties} from `{Big} {Code}'},
	volume = {62},
	issn = {0001-0782},
	doi = {10.1145/3306204},
	abstract = {We present a new approach for predicting program properties from large codebases (aka “Big Code”). Our approach learns a probabilistic model from “Big Code” and uses this model to predict properties of new, unseen programs. The key idea of our work is to transform the program into a representation that allows us to formulate the problem of inferring program properties as structured prediction in machine learning. This enables us to leverage powerful probabilistic models such as Conditional Random Fields (CRFs) and perform joint prediction of program properties. As an example of our approach, we built a scalable prediction engine called JSNICE for solving two kinds of tasks in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNICE predicts correct names for 63\% of name identifiers and its type annotation predictions are correct in 81\% of cases. Since its public release at http://jsnice.org, JSNice has become a popular system with hundreds of thousands of uses. By formulating the problem of inferring program properties as structured prediction, our work opens up the possibility for a range of new “Big Code” applications such as de-obfuscators, decompilers, invariant generators, and others.},
	language = {English},
	number = {3},
	journal = {COMMUNICATIONS OF THE ACM},
	publisher = {ASSOC COMPUTING MACHINERY},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	month = mar,
	year = {2019},
	note = {Type: Article},
	pages = {99--107},
}

@article{chu_security_2019-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Security and {Privacy} {Analyses} of {Internet} of {Things} {Children}'s {Toys}},
	volume = {6},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2018.2866423},
	abstract = {This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many Internet of Things toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks.},
	language = {English},
	number = {1},
	journal = {IEEE INTERNET OF THINGS JOURNAL},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Chu, Gordon and Apthorpe, Noah and Feamster, Nick},
	month = feb,
	year = {2019},
	note = {Type: Article},
	keywords = {Data security, Internet of Things (IoT), privacy},
	pages = {978--985},
}

@misc{pochelu_jaxdecompiler_2024,
	title = {{JaxDecompiler}: {Redefining} {Gradient}-{Informed} {Software} {Design}},
	shorttitle = {{JaxDecompiler}},
	url = {http://arxiv.org/abs/2403.10571},
	doi = {10.48550/arXiv.2403.10571},
	abstract = {Among numerical libraries capable of computing gradient descent optimization, JAX stands out by offering more features, accelerated by an intermediate representation known as Jaxpr language. However, editing the Jaxpr code is not directly possible. This article introduces JaxDecompiler, a tool that transforms any JAX function into an editable Python code, especially useful for editing the JAX function generated by the gradient function. JaxDecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by JAX. We highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Pochelu, Pierrick},
	month = mar,
	year = {2024},
	note = {arXiv:2403.10571 [cs]},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/C5ST3ICD/Pochelu - 2024 - JaxDecompiler Redefining Gradient-Informed Software Design.pdf:application/pdf},
}

@misc{smith_there_2024,
	title = {There and {Back} {Again}: {A} {Netlist}'s {Tale} with {Much} {Egraphin}'},
	shorttitle = {There and {Back} {Again}},
	url = {http://arxiv.org/abs/2404.00786},
	doi = {10.48550/arXiv.2404.00786},
	abstract = {EDA toolchains are notoriously unpredictable, incomplete, and error-prone; the generally-accepted remedy has been to re-imagine EDA tasks as compilation problems. However, any compiler framework we apply must be prepared to handle the wide range of EDA tasks, including not only compilation tasks like technology mapping and optimization (the "there"\vphantom{\{}\} in our title), but also decompilation tasks like loop rerolling (the "back again"). In this paper, we advocate for equality saturation -- a term rewriting framework -- as the framework of choice when building hardware toolchains. Through a series of case studies, we show how the needs of EDA tasks line up conspicuously well with the features equality saturation provides.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Smith, Gus Henry and Sisco, Zachary D. and Techaumnuaiwit, Thanawat and Xia, Jingtao and Canumalla, Vishal and Cheung, Andrew and Tatlock, Zachary and Nandi, Chandrakana and Balkind, Jonathan},
	month = mar,
	year = {2024},
	note = {arXiv:2404.00786 [cs]},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/2YMV79Y6/Smith et al. - 2024 - There and Back Again A Netlist's Tale with Much Egraphin'.pdf:application/pdf},
}

@misc{chen_augmenting_2021,
	title = {Augmenting {Decompiler} {Output} with {Learned} {Variable} {Names} and {Types}},
	url = {http://arxiv.org/abs/2108.06363},
	doi = {10.48550/arXiv.2108.06363},
	abstract = {A common tool used by security professionals for reverse-engineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4\% of the time and the original types 75.8\% of the time.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J. and Goues, Claire Le and Neubig, Graham and Vasilescu, Bogdan},
	month = aug,
	year = {2021},
	note = {arXiv:2108.06363 [cs]},
	annote = {Comment: 17 pages to be published in USENIX Security '22},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/IESMXS8V/Chen et al. - 2021 - Augmenting Decompiler Output with Learned Variable Names and Types.pdf:application/pdf},
}

@misc{wong_refining_2023,
	title = {Refining {Decompiled} {C} {Code} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.06530},
	doi = {10.48550/arXiv.2310.06530},
	abstract = {A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly. This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable. We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler -- IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75\% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Wong, Wai Kin and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
	month = nov,
	year = {2023},
	note = {arXiv:2310.06530 [cs]},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/PKCTVHK5/Wong et al. - 2023 - Refining Decompiled C Code with Large Language Models.pdf:application/pdf},
}

@misc{kim_feature_2023,
	title = {Feature {Engineering} {Using} {File} {Layout} for {Malware} {Detection}},
	url = {http://arxiv.org/abs/2304.02260},
	doi = {10.48550/arXiv.2304.02260},
	abstract = {Malware detection on binary executables provides a high availability to even binaries which are not disassembled or decompiled. However, a binary-level approach could cause ambiguity problems. In this paper, we propose a new feature engineering technique that use minimal knowledge about the internal layout on a binary. The proposed feature avoids the ambiguity problems by integrating the information about the layout with structural entropy. The experimental results show that our feature improves accuracy and F1-score by 3.3\% and 0.07, respectively, on a CNN based malware detector with realistic benign and malicious samples.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Kim, Jeongwoo and Cho, Eun-Sun and Paik, Joon-Young},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02260 [cs]},
	annote = {Comment: 2pages, no figures, This manuscript was presented in the poster session of The Annual Computer Security Applications Conference (ACSAC) 2020},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/LX9Y5T74/Kim et al. - 2023 - Feature Engineering Using File Layout for Malware Detection.pdf:application/pdf},
}

@misc{kc_neural_2023,
	title = {Neural {Machine} {Translation} for {Code} {Generation}},
	url = {http://arxiv.org/abs/2305.13504},
	doi = {10.48550/arXiv.2305.13504},
	abstract = {Neural machine translation (NMT) methods developed for natural language processing have been shown to be highly successful in automating translation from one natural language to another. Recently, these NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. In the literature, a variety of different input scenarios have been explored, including generating code based on natural language description, lower-level representations such as binary or assembly (neural decompilation), partial representations of source code (code completion and repair), and source code in another language (code translation). In this paper we survey the NMT for code generation literature, cataloging the variety of methods that have been explored according to input and output representations, model architectures, optimization techniques used, data sets, and evaluation methods. We discuss the limitations of existing methods and future research directions},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {KC, Dharma and Morrison, Clayton T.},
	month = may,
	year = {2023},
	note = {arXiv:2305.13504 [cs]},
	annote = {Comment: 33 pages, 1 figure},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/9YQWNVNS/KC と Morrison - 2023 - Neural Machine Translation for Code Generation.pdf:application/pdf},
}

@misc{armengol-estape_slade_2024-2,
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	shorttitle = {{SLaDe}},
	url = {http://arxiv.org/abs/2305.12520},
	doi = {10.48550/arXiv.2305.12520},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from ExeBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, Michael F. P.},
	month = feb,
	year = {2024},
	note = {arXiv:2305.12520 [cs]},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/UXUVX5ZZ/Armengol-Estapé et al. - 2024 - SLaDe A Portable Small Language Model Decompiler for Optimized Assembly.pdf:application/pdf},
}

@misc{armengol-estape_learning_2022,
	title = {Learning {C} to x86 {Translation}: {An} {Experiment} in {Neural} {Compilation}},
	shorttitle = {Learning {C} to x86 {Translation}},
	url = {http://arxiv.org/abs/2108.07639},
	doi = {10.48550/arXiv.2108.07639},
	abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Armengol-Estapé, Jordi and O'Boyle, Michael F. P.},
	month = dec,
	year = {2022},
	note = {arXiv:2108.07639 [cs]},
	annote = {Comment: Published in AIPLANS 2021},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/ABBCCZY4/Armengol-Estapé と O'Boyle - 2022 - Learning C to x86 Translation An Experiment in Neural Compilation.pdf:application/pdf},
}

@misc{bielik_adversarial_2020,
	title = {Adversarial {Robustness} for {Code}},
	url = {http://arxiv.org/abs/2002.04694},
	doi = {10.48550/arXiv.2002.04694},
	abstract = {Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code such as finding and fixing bugs, code completion, decompilation, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) combining existing and novel techniques to improve robustness while preserving high accuracy.},
	urldate = {2026-01-15},
	publisher = {arXiv},
	author = {Bielik, Pavol and Vechev, Martin},
	month = aug,
	year = {2020},
	note = {arXiv:2002.04694 [cs]},
	annote = {Comment: Proceedings of the 37th International Conference on Machine Learning, Online, PMLR 119, 2020},
	file = {Preprint PDF:/Users/matsulab/Zotero/storage/P5WUCNVA/Bielik と Vechev - 2020 - Adversarial Robustness for Code.pdf:application/pdf},
}

@article{ahmed_learning_2021,
	title = {Learning to {Find} {Usages} of {Library} {Functions} in {Optimized} {Binaries}},
	url = {https://arxiv.org/pdf/2103.05221},
	doi = {https://doi.org/10.48550/arXiv.2103.05221},
	abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, "natural" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to "decompile" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
	author = {Ahmed, Toufique and Devanbu, Premkumar and Sawant, Ashok, Anand},
	month = sep,
	year = {2021},
}

@article{al-kaswan_extending_2023-1,
	title = {Extending {Source} {Code} {Pre}-{Trained} {Language} {Models} to {Summarise} {Decompiled} {Binaries}},
	url = {https://arxiv.org/pdf/2301.01701},
	doi = {https://doi.org/10.48550/arXiv.2301.01701},
	abstract = {Reverse engineering binaries is required to understand and analyse programs for which the source code is unavailable. Decompilers can transform the largely unreadable binaries into a more readable source code-like representation. However, reverse engineering is time-consuming, much of which is taken up by labelling the functions with semantic information. While the automated summarisation of decompiled code can help Reverse Engineers understand and analyse binaries, current work mainly focuses on summarising source code, and no suitable dataset exists for this task. In this work, we extend large pre-trained language models of source code to summarise decompiled binary functions. Furthermore, we investigate the impact of input and data properties on the performance of such models. Our approach consists of two main components; the data and the model. We first build CAPYBARA, a dataset of 214K decompiled function-documentation pairs across various compiler optimisations. We extend CAPYBARA further by generating synthetic datasets and deduplicating the data. Next, we fine-tune the CodeT5 base model with CAPYBARA to create BinT5. BinT5 achieves the state-of-the-art BLEU-4 score of 60.83, 58.82, and 44.21 for summarising source, decompiled, and synthetically stripped decompiled code, respectively. This indicates that these models can be extended to decompiled binaries successfully. Finally, we found that the performance of BinT5 is not heavily dependent on the dataset size and compiler optimisation level. We recommend future research to further investigate transferring knowledge when working with less expressive input formats such as stripped binaries.},
	author = {Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Ashok, Anand and Devanbu, Premkumar and Deursen, van, Arie},
	month = jan,
	year = {2023},
	annote = {SANER 2023 Technical Track Camera Ready},
}

@article{armengol-estape_learning_2022-1,
	title = {Learning {C} to x86 {Translation}: {An} {Experiment} in {Neural} {Compilation}},
	url = {https://arxiv.org/pdf/2108.07639},
	doi = {https://doi.org/10.48550/arXiv.2108.07639},
	abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
	author = {Armengol-Estapé, Jordi and O'Boyle, P., F., Michael},
	month = feb,
	year = {2022},
	annote = {Published in AIPLANS 2021},
}

@article{armengol-estape_slade_2024-3,
	title = {{SLaDe}: {A} {Portable} {Small} {Language} {Model} {Decompiler} for {Optimized} {Assembly}},
	url = {https://arxiv.org/pdf/2305.12520},
	doi = {https://doi.org/10.48550/arXiv.2305.12520},
	abstract = {Decompilation is a well-studied area with numerous high-quality tools available. These are frequently used for security tasks and to port legacy code. However, they regularly generate difficult-to-read programs and require a large amount of engineering effort to support new programming languages and ISAs. Recent interest in neural approaches has produced portable tools that generate readable code. However, to-date such techniques are usually restricted to synthetic programs without optimization, and no models have evaluated their portability. Furthermore, while the code generated may be more readable, it is usually incorrect. This paper presents SLaDe, a Small Language model Decompiler based on a sequence-to-sequence transformer trained over real-world code. We develop a novel tokenizer and exploit no-dropout training to produce high-quality code. We utilize type-inference to generate programs that are more readable and accurate than standard analytic and recent neural approaches. Unlike standard approaches, SLaDe can infer out-of-context types and unlike neural approaches, it generates correct code. We evaluate SLaDe on over 4,000 functions from ExeBench on two ISAs and at two optimizations levels. SLaDe is up to 6 times more accurate than Ghidra, a state-of-the-art, industrial-strength decompiler and up to 4 times more accurate than the large language model ChatGPT and generates significantly more readable code than both.},
	author = {Armengol-Estapé, Jordi and Woodruff, Jackson and Cummins, Chris and O'Boyle, P., F., Michael},
	month = feb,
	year = {2024},
}

@article{bielik_adversarial_2020-1,
	title = {Adversarial {Robustness} for {Code}},
	url = {https://arxiv.org/pdf/2002.04694},
	doi = {https://doi.org/10.48550/arXiv.2002.04694},
	abstract = {Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code such as finding and fixing bugs, code completion, decompilation, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work, we explore this issue by: (i) instantiating adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are vulnerable to adversarial attacks, and (iii) combining existing and novel techniques to improve robustness while preserving high accuracy.},
	author = {Bielik, Pavol and Vechev, Martin},
	month = aug,
	year = {2020},
	annote = {Proceedings of the 37th International Conference on Machine Learning, Online, PMLR 119, 2020},
}

@article{cao_revisiting_2023-1,
	title = {Revisiting {Deep} {Learning} for {Variable} {Type} {Recovery}},
	url = {https://arxiv.org/pdf/2304.03854},
	doi = {https://doi.org/10.48550/arXiv.2304.03854},
	abstract = {Compiled binary executables are often the only available artifact in reverse engineering, malware analysis, and software systems maintenance. Unfortunately, the lack of semantic information like variable types makes comprehending binaries difficult. In efforts to improve the comprehensibility of binaries, researchers have recently used machine learning techniques to predict semantic information contained in the original source code. Chen et al. implemented DIRTY, a Transformer-based Encoder-Decoder architecture capable of augmenting decompiled code with variable names and types by leveraging decompiler output tokens and variable size information. Chen et al. were able to demonstrate a substantial increase in name and type extraction accuracy on Hex-Rays decompiler outputs compared to existing static analysis and AI-based techniques. We extend the original DIRTY results by re-training the DIRTY model on a dataset produced by the open-source Ghidra decompiler. Although Chen et al. concluded that Ghidra was not a suitable decompiler candidate due to its difficulty in parsing and incorporating DWARF symbols during analysis, we demonstrate that straightforward parsing of variable data generated by Ghidra results in similar retyping performance. We hope this work inspires further interest and adoption of the Ghidra decompiler for use in research projects.},
	author = {Cao, Kevin and Leach, Kevin},
	month = apr,
	year = {2023},
	annote = {In The 31st International Conference on Program Comprehension(ICPC 2023 RENE)},
}

@article{cao_boosting_2023,
	title = {Boosting {Neural} {Networks} to {Decompile} {Optimized} {Binaries}},
	url = {https://arxiv.org/pdf/2301.00969},
	doi = {https://doi.org/10.48550/arXiv.2301.00969},
	abstract = {Decompilation aims to transform a low-level program language (LPL) (eg., binary file) into its functionally-equivalent high-level program language (HPL) (e.g., C/C++). It is a core technology in software security, especially in vulnerability discovery and malware analysis. In recent years, with the successful application of neural machine translation (NMT) models in natural language processing (NLP), researchers have tried to build neural decompilers by borrowing the idea of NMT. They formulate the decompilation process as a translation problem between LPL and HPL, aiming to reduce the human cost required to develop decompilation tools and improve their generalizability. However, state-of-the-art learning-based decompilers do not cope well with compiler-optimized binaries. Since real-world binaries are mostly compiler-optimized, decompilers that do not consider optimized binaries have limited practical significance. In this paper, we propose a novel learning-based approach named NeurDP, that targets compiler-optimized binaries. NeurDP uses a graph neural network (GNN) model to convert LPL to an intermediate representation (IR), which bridges the gap between source code and optimized binary. We also design an Optimized Translation Unit (OTU) to split functions into smaller code fragments for better translation performance. Evaluation results on datasets containing various types of statements show that NeurDP can decompile optimized binaries with 45.21\% higher accuracy than state-of-the-art neural decompilation frameworks.},
	author = {Cao, Ying and Liang, Ruigang and Chen, Kai and Hu, Peiwei},
	month = jan,
	year = {2023},
}

@article{chen_augmenting_2021-1,
	title = {Augmenting {Decompiler} {Output} with {Learned} {Variable} {Names} and {Types}},
	url = {https://arxiv.org/pdf/2108.06363},
	doi = {https://doi.org/10.48550/arXiv.2108.06363},
	abstract = {A common tool used by security professionals for reverse-engineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4\% of the time and the original types 75.8\% of the time.},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, J., Edward and Goues, Le, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = aug,
	year = {2021},
	annote = {17 pages to be published in USENIX Security '22},
}

@article{chen_suigpt_2025-1,
	title = {{SuiGPT} {MAD}: {Move} {AI} {Decompiler} to {Improve} {Transparency} and {Auditability} on {Non}-{Open}-{Source} {Blockchain} {Smart} {Contract}},
	url = {https://arxiv.org/pdf/2410.15275},
	doi = {https://doi.org/10.48550/arXiv.2410.15275},
	abstract = {The vision of Web3 is to improve user control over data and assets, but one challenge that complicates this vision is the prevalence of non-transparent, scam-prone applications and vulnerable smart contracts that put Web3 users at risk. While code audits are one solution to this problem, the lack of smart contracts source code on many blockchain platforms, such as Sui, hinders the ease of auditing. A promising approach to this issue is the use of a decompiler to reverse-engineer smart contract bytecode. However, existing decompilers for Sui produce code that is difficult to understand and cannot be directly recompiled. To address this, we developed the SuiGPT Move AI Decompiler (MAD), a Large Language Model (LLM)-powered web application that decompiles smart contract bytecodes on Sui into logically correct, human-readable, and re-compilable source code with prompt engineering. Our evaluation shows that MAD's output successfully passes original unit tests and achieves a 73.33\% recompilation success rate on real-world smart contracts. Additionally, newer models tend to deliver improved performance, suggesting that MAD's approach will become increasingly effective as LLMs continue to advance. In a user study involving 12 developers, we found that MAD significantly reduced the auditing workload compared to using traditional decompilers. Participants found MAD's outputs comparable to the original source code, improving accessibility for understanding and auditing non-open-source smart contracts. Through qualitative interviews with these developers and Web3 projects, we further discussed the strengths and concerns of MAD. MAD has practical implications for blockchain smart contract transparency, auditing, and education. It empowers users to easily and independently review and audit non-open-source smart contracts, fostering accountability and decentralization},
	author = {Chen, Eason and Tang, Xinyi and Xiao, Zimo and Li, Chuangji and Li, Shizhuo and Tingguan, Wu and Wang, Siyun and Chalkias, Kryptos, Kostas},
	month = jan,
	year = {2025},
	annote = {Paper accepted at ACM The Web Conference 2025},
}

@article{chu_security_2018,
	title = {Security and {Privacy} {Analyses} of {Internet} of {Things} {Children}'s {Toys}},
	url = {https://arxiv.org/pdf/1805.02751},
	doi = {https://doi.org/10.48550/arXiv.1805.02751},
	abstract = {This paper investigates the security and privacy of Internet-connected children's smart toys through case studies of three commercially-available products. We conduct network and application vulnerability analyses of each toy using static and dynamic analysis techniques, including application binary decompilation and network monitoring. We discover several publicly undisclosed vulnerabilities that violate the Children's Online Privacy Protection Rule (COPPA) as well as the toys' individual privacy policies. These vulnerabilities, especially security flaws in network communications with first-party servers, are indicative of a disconnect between many IoT toy developers and security and privacy best practices despite increased attention to Internet-connected toy hacking risks.},
	author = {Chu, Gordon and Apthorpe, Noah and Feamster, Nick},
	month = aug,
	year = {2018},
	annote = {8 pages, 8 figures; publication version},
}

@article{cotroneo_can_2025-1,
	title = {Can {Neural} {Decompilation} {Assist} {Vulnerability} {Prediction} on {Binary} {Code}?},
	url = {https://arxiv.org/pdf/2412.07538},
	doi = {https://doi.org/10.48550/arXiv.2412.07538},
	abstract = {Vulnerability prediction is valuable in identifying security issues efficiently, even though it requires the source code of the target software system, which is a restrictive hypothesis. This paper presents an experimental study to predict vulnerabilities in binary code without source code or complex representations of the binary, leveraging the pivotal idea of decompiling the binary file through neural decompilation and predicting vulnerabilities through deep learning on the decompiled source code. The results outperform the state-of-the-art in both neural decompilation and vulnerability prediction, showing that it is possible to identify vulnerable programs with this approach concerning bi-class (vulnerable/non-vulnerable) and multi-class (type of vulnerability) analysis.},
	author = {Cotroneo, D. and Grasso, C., F. and Natella, R. and Orbinato, V.},
	month = mar,
	year = {2025},
}

@article{dramko_fast_2025-1,
	title = {Fast, {Fine}-{Grained} {Equivalence} {Checking} for {Neural} {Decompilers}},
	url = {https://arxiv.org/pdf/2501.04811},
	doi = {https://doi.org/10.48550/arXiv.2501.04811},
	abstract = {Neural decompilers are machine learning models that reconstruct the source code from an executable program. Critical to the lifecycle of any machine learning model is an evaluation of its effectiveness. However, existing techniques for evaluating neural decompilation models have substantial weaknesses, especially when it comes to showing the correctness of the neural decompiler's predictions. To address this, we introduce codealign, a novel instruction-level code equivalence technique designed for neural decompilers. We provide a formal definition of a relation between equivalent instructions, which we term an equivalence alignment. We show how codealign generates equivalence alignments, then evaluate codealign by comparing it with symbolic execution. Finally, we show how the information codealign provides-which parts of the functions are equivalent and how well the variable names match-is substantially more detailed than existing state-of-the-art evaluation metrics, which report unitless numbers measuring similarity.},
	author = {Dramko, Luke and Goues, Le, Claire and Schwartz, J., Edward},
	month = jan,
	year = {2025},
}

@article{erinfolami_declassifier_2019-1,
	title = {{DeClassifier}: {Class}-{Inheritance} {Inference} {Engine} for {Optimized} {C}++ {Binaries}},
	url = {https://arxiv.org/pdf/1901.10073},
	doi = {https://doi.org/10.48550/arXiv.1901.10073},
	abstract = {Recovering class inheritance from C++ binaries has several security benefits including problems such as decompilation and program hardening. Thanks to the optimization guidelines prescribed by the C++ standard, commercial C++ binaries tend to be optimized. While state-of-the-art class inheritance inference solutions are effective in dealing with unoptimized code, their efficacy is impeded by optimization. Particularly, constructor inlining–or worse exclusion–due to optimization render class inheritance recovery challenging. Further, while modern solutions such as MARX can successfully group classes within an inheritance sub-tree, they fail to establish directionality of inheritance, which is crucial for security-related applications (e.g. decompilation). We implemented a prototype of DeClassifier using Binary Analysis Platform (BAP) and evaluated DeClassifier against 16 binaries compiled using gcc under multiple optimization settings. We show that (1) DeClassifier can recover 94.5\% and 71.4\% true positive directed edges in the class hierarchy tree under O0 and O2 optimizations respectively, (2) a combination of ctor+dtor analysis provides much better inference than ctor only analysis.},
	author = {Erinfolami, Ayomide, Rukayat and Prakash, Aravind},
	month = feb,
	year = {2019},
	annote = {13 pages of main paper including references, 1 page of appendix, 2 figures and 10 tables},
}

@article{erinfolami_devil_2020-1,
	title = {Devil is {Virtual}: {Reversing} {Virtual} {Inheritance} in {C}++ {Binaries}},
	url = {https://arxiv.org/pdf/2003.05039},
	doi = {https://doi.org/10.48550/arXiv.2003.05039},
	abstract = {Complexities that arise from implementation of object-oriented concepts in C++ such as virtual dispatch and dynamic type casting have attracted the attention of attackers and defenders alike. Binary-level defenses are dependent on full and precise recovery of class inheritance tree of a given program. While current solutions focus on recovering single and multiple inheritances from the binary, they are oblivious to virtual inheritance. Conventional wisdom among binary-level defenses is that virtual inheritance is uncommon and/or support for single and multiple inheritances provides implicit support for virtual inheritance. In this paper, we show neither to be true. Specifically, (1) we present an efficient technique to detect virtual inheritance in C++ binaries and show through a study that virtual inheritance can be found in non-negligible number (more than 10\% on Linux and 12.5\% on Windows) of real-world C++ programs including Mysql and libstdc++. (2) we show that failure to handle virtual inheritance introduces both false positives and false negatives in the hierarchy tree. These false positves and negatives either introduce attack surface when the hierarchy recovered is used to enforce CFI policies, or make the hierarchy difficult to understand when it is needed for program understanding (e.g., during decompilation). (3) We present a solution to recover virtual inheritance from COTS binaries. We recover a maximum of 95\% and 95.5\% (GCC -O0) and a minimum of 77.5\% and 73.8\% (Clang -O2) of virtual and intermediate bases respectively in the virtual inheritance tree.},
	author = {Erinfolami, Ayomide, Rukayat and Prakash, Aravind},
	month = jun,
	year = {2020},
	annote = {Accepted at CCS20. This is a technical report version},
}

@article{escalada_improving_2021,
	title = {Improving type information inferred by decompilers with supervised machine learning},
	url = {https://arxiv.org/pdf/2101.08116},
	doi = {https://doi.org/10.48550/arXiv.2101.08116},
	abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1\% F1-measure, whereas the best decompiler obtains a 30\% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers.},
	author = {Escalada, Javier and Scully, Ted and Ortin, Francisco},
	month = feb,
	year = {2021},
}

@article{harrand_strengths_2019-1,
	title = {The {Strengths} and {Behavioral} {Quirks} of {Java} {Bytecode} {Decompilers}},
	url = {https://arxiv.org/pdf/1908.06895},
	doi = {https://doi.org/10.48550/arXiv.1908.06895},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, the decompilation process, which aims at producing source code from bytecode, must establish some strategies to reconstruct the information that has been lost. Modern Java decompilers tend to use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. We study the effectiveness of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. This study relies on a benchmark set of 14 real-world open-source software projects to be decompiled (2041 classes in total). Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. Even the highest ranking decompiler in this study produces syntactically correct output for 84\% of classes of our dataset and semantically equivalent code output for 78\% of classes.},
	author = {Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit},
	month = aug,
	year = {2019},
	annote = {11 pages, 6 figures, 9 listings, 3 tables},
}

@article{harrand_java_2020-1,
	title = {Java {Decompiler} {Diversity} and its {Application} to {Meta}-decompilation},
	url = {https://arxiv.org/pdf/2005.11315},
	doi = {https://doi.org/10.48550/arXiv.2005.11315},
	abstract = {During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84\%, respectively 78\%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6\% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler.},
	author = {Harrand, Nicolas and Soto-Valero, César and Monperrus, Martin and Baudry, Benoit},
	month = may,
	year = {2020},
	annote = {arXiv admin note: substantial text overlap with arXiv:1908.06895},
}

@article{he_benchmarking_2025-1,
	title = {On {Benchmarking} {Code} {LLMs} for {Android} {Malware} {Analysis}},
	url = {https://arxiv.org/pdf/2504.00694},
	doi = {https://doi.org/10.48550/arXiv.2504.00694},
	abstract = {Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.},
	author = {He, Yiling and She, Hongyu and Qian, Xingzhi and Zheng, Xinran and Chen, Zhuo and Qin, Zhan and Cavallaro, Lorenzo},
	month = apr,
	year = {2025},
	annote = {This paper has been accepted to the 34th ACM SIGSOFT ISSTA Companion (LLMSC Workshop 2025)},
}

@article{kc_neural_2023-1,
	title = {Neural {Machine} {Translation} for {Code} {Generation}},
	url = {https://arxiv.org/pdf/2305.13504},
	doi = {https://doi.org/10.48550/arXiv.2305.13504},
	abstract = {Neural machine translation (NMT) methods developed for natural language processing have been shown to be highly successful in automating translation from one natural language to another. Recently, these NMT methods have been adapted to the generation of program code. In NMT for code generation, the task is to generate output source code that satisfies constraints expressed in the input. In the literature, a variety of different input scenarios have been explored, including generating code based on natural language description, lower-level representations such as binary or assembly (neural decompilation), partial representations of source code (code completion and repair), and source code in another language (code translation). In this paper we survey the NMT for code generation literature, cataloging the variety of methods that have been explored according to input and output representations, model architectures, optimization techniques used, data sets, and evaluation methods. We discuss the limitations of existing methods and future research directions},
	author = {KC, Dharma and Morrison, T., Clayton},
	month = may,
	year = {2023},
	annote = {33 pages, 1 figure},
}

@article{kim_feature_2023-1,
	title = {Feature {Engineering} {Using} {File} {Layout} for {Malware} {Detection}},
	url = {https://arxiv.org/pdf/2304.02260},
	doi = {https://doi.org/10.48550/arXiv.2304.02260},
	abstract = {Malware detection on binary executables provides a high availability to even binaries which are not disassembled or decompiled. However, a binary-level approach could cause ambiguity problems. In this paper, we propose a new feature engineering technique that use minimal knowledge about the internal layout on a binary. The proposed feature avoids the ambiguity problems by integrating the information about the layout with structural entropy. The experimental results show that our feature improves accuracy and F1-score by 3.3\% and 0.07, respectively, on a CNN based malware detector with realistic benign and malicious samples.},
	author = {Kim, Jeongwoo and Cho, Eun-Sun and Paik, Joon-Young},
	month = apr,
	year = {2023},
	annote = {2pages, no figures, This manuscript was presented in the poster session of The Annual Computer Security Applications Conference (ACSAC) 2020},
}

@article{lacomis_dire_2019-1,
	title = {{DIRE}: {A} {Neural} {Approach} to {Decompiled} {Identifier} {Naming}},
	url = {https://arxiv.org/pdf/1909.09029},
	doi = {https://doi.org/10.48550/arXiv.1909.09029},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, J., Edward and Allamanis, Miltiadis and Goues, Le, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = oct,
	year = {2019},
	annote = {2019 International Conference on Automated Software Engineering},
}

@article{lagouvardos_incredible_2025-1,
	title = {The {Incredible} {Shrinking} {Context}... in a {Decompiler} {Near} {You}},
	url = {https://arxiv.org/pdf/2409.11157},
	doi = {https://doi.org/10.48550/arXiv.2409.11157},
	abstract = {Decompilation of binary code has arisen as a highly-important application in the space of Ethereum VM (EVM) smart contracts. Major new decompilers appear nearly every year and attain popularity, for a multitude of reverse-engineering or tool-building purposes. Technically, the problem is fundamental: it consists of recovering high-level control flow from a highly-optimized continuation-passing-style (CPS) representation. Architecturally, decompilers can be built using either static analysis or symbolic execution techniques. We present Shrknr, a static-analysis-based decompiler succeeding the state-of-the-art Elipmoc decompiler. Shrknr manages to achieve drastic improvements relative to the state of the art, in all significant dimensions: scalability, completeness, precision. Chief among the techniques employed is a new variant of static analysis context: shrinking context sensitivity. Shrinking context sensitivity performs deep cuts in the static analysis context, eagerly "forgetting" control-flow history, in order to leave room for further precise reasoning. We compare Shrnkr to state-of-the-art decompilers, both static-analysis- and symbolic-execution-based. In a standard benchmark set, Shrnkr scales to over 99.5\% of contracts (compared to 95\%), covers (i.e., reaches and manages to decompile) 67\% more code, and reduces key imprecision metrics by over 65\%.},
	author = {Lagouvardos, Sifis and Bollanos, Yannis and Grech, Neville and Smaragdakis, Yannis},
	month = apr,
	year = {2025},
	annote = {Full version of ISSTA 2025 paper},
}

@article{li_stan_2020-1,
	title = {{STAN}: {Towards} {Describing} {Bytecodes} of {Smart} {Contract}},
	url = {https://arxiv.org/pdf/2007.09696},
	doi = {https://doi.org/10.48550/arXiv.2007.09696},
	abstract = {More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1\% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users.},
	author = {Li, Xiaoqi and Chen, Ting and Luo, Xiapu and Zhang, Tao and Yu, Le and Xu, Zhou},
	month = jul,
	year = {2020},
	annote = {In Proc. of the 20th IEEE International Conference on Software Quality, Reliability and Security (QRS), 2020},
}

@article{liao_augmenting_2025-2,
	title = {Augmenting {Smart} {Contract} {Decompiler} {Output} through {Fine}-grained {Dependency} {Analysis} and {LLM}-facilitated {Semantic} {Recovery}},
	url = {https://arxiv.org/pdf/2501.08670},
	doi = {https://doi.org/10.48550/arXiv.2501.08670},
	abstract = {Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, {\textbackslash}system\{\} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39\% for method boundaries, 90.39\% for variable types, and 80.65\% for contract attributes.},
	author = {Liao, Zeqin and Nan, Yuhong and Gao, Zixu and Liang, Henglong and Hao, Sicheng and Reng, Peifan and Zheng, Zibin},
	month = oct,
	year = {2025},
	annote = {This is the author version of the article accepted for publication in IEEE Transactions on Software Engineering},
}

@article{liu_proving_2021,
	title = {Proving {LTL} {Properties} of {Bitvector} {Programs} and {Decompiled} {Binaries} ({Extended})},
	url = {https://arxiv.org/pdf/2105.05159},
	doi = {https://doi.org/10.48550/arXiv.2105.05159},
	abstract = {There is increasing interest in applying verification tools to programs that have bitvector operations (eg., binaries). SMT solvers, which serve as a foundation for these tools, have thus increased support for bitvector reasoning through bit-blasting and linear arithmetic approximations. In this paper we show that similar linear arithmetic approximation of bitvector operations can be done at the source level through transformations. Specifically, we introduce new paths that over-approximate bitvector operations with linear conditions/constraints, increasing branching but allowing us to better exploit the well-developed integer reasoning and interpolation of verification tools. We show that, for reachability of bitvector programs, increased branching incurs negligible overhead yet, when combined with integer interpolation optimizations, enables more programs to be verified. We further show this exploitation of integer interpolation in the common case also enables competitive termination verification of bitvector programs and leads to the first effective technique for LTL verification of bitvector programs. Finally, we provide an in-depth case study of decompiled ("lifted") binary programs, which emulate X86 execution through frequent use of bitvector operations. We present a new tool DarkSea, the first tool capable of verifying reachability, termination, and LTL of lifted binaries.},
	author = {Liu, Cyrus, Yuandong and Pang, Chengbin and Dietsch, Daniel and Koskinen, Eric and Le, Ton-Chanh and Portokalidis, Georgios and Xu, Jun},
	month = aug,
	year = {2021},
	annote = {39 pages(including Appendix), 10 tables, 4 Postscript figures, accepted to APLAS 2021},
}

@article{nandi_synthesizing_2020-1,
	title = {Synthesizing {Structured} {CAD} {Models} with {Equality} {Saturation} and {Inverse} {Transformations}},
	url = {https://arxiv.org/pdf/1909.12252},
	doi = {https://doi.org/10.48550/arXiv.1909.12252},
	abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure. This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
	author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, R., James and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
	month = apr,
	year = {2020},
	annote = {14 pages},
}

@article{pochelu_jaxdecompiler_2024-1,
	title = {{JaxDecompiler}: {Redefining} {Gradient}-{Informed} {Software} {Design}},
	url = {https://arxiv.org/pdf/2403.10571},
	doi = {https://doi.org/10.48550/arXiv.2403.10571},
	abstract = {Among numerical libraries capable of computing gradient descent optimization, JAX stands out by offering more features, accelerated by an intermediate representation known as Jaxpr language. However, editing the Jaxpr code is not directly possible. This article introduces JaxDecompiler, a tool that transforms any JAX function into an editable Python code, especially useful for editing the JAX function generated by the gradient function. JaxDecompiler simplifies the processes of reverse engineering, understanding, customizing, and interoperability of software developed by JAX. We highlight its capabilities, emphasize its practical applications especially in deep learning and more generally gradient-informed software, and demonstrate that the decompiled code speed performance is similar to the original.},
	author = {Pochelu, Pierrick},
	month = mar,
	year = {2024},
}

@article{ringer_proof_2021-1,
	title = {Proof {Repair} across {Type} {Equivalences}},
	url = {https://arxiv.org/pdf/2010.00774},
	doi = {https://doi.org/10.48550/arXiv.2010.00774},
	abstract = {We describe a new approach to automatically repairing broken proofs in the Coq proof assistant in response to changes in types. Our approach combines a configurable proof term transformation with a decompiler from proof terms to tactic scripts. The proof term transformation implements transport across equivalences in a way that removes references to the old version of the changed type and does not rely on axioms beyond those Coq assumes. We have implemented this approach in PUMPKIN Pi, an extension to the PUMPKIN PATCH Coq plugin suite for proof repair. We demonstrate PUMPKIN Pi's flexibility on eight case studies, including supporting a benchmark from a user study, easing development with dependent types, porting functions and proofs between unary and binary numbers, and supporting an industrial proof engineer to interoperate between Coq and other verification tools more easily.},
	author = {Ringer, Talia and Porter, RanDair and Yazdani, Nathaniel and Leo, John and Grossman, Dan},
	month = may,
	year = {2021},
	annote = {Tool repository with code guide: https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/GUIDE.md},
}

@article{shang_binmetric_2025,
	title = {{BinMetric}: {A} {Comprehensive} {Binary} {Analysis} {Benchmark} for {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2505.07360},
	doi = {https://doi.org/10.48550/arXiv.2505.07360},
	abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
	author = {Shang, Xiuwei and Chen, Guoqiang and Cheng, Shaoyin and Wu, Benlong and Hu, Li and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
	month = may,
	year = {2025},
	annote = {23 pages, 5 figures, to be published in IJCAI 2025},
}

@article{she_wadec_2024-2,
	title = {{WaDec}: {Decompiling} {WebAssembly} {Using} {Large} {Language} {Model}},
	url = {https://arxiv.org/pdf/2406.11346},
	doi = {https://doi.org/10.48550/arXiv.2406.11346},
	abstract = {WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34\%, a dramatic 97\% reduction compared to the state-of-the-art's 116.94\%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11\%, a re-execution rate of 43.55\%, and an output consistency of 27.15\%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185\%, cyclomatic complexity by 8\%, and cosine similarity by 41\%, achieving an average code similarity above 50\%.},
	author = {She, Xinyu and Zhao, Yanjie and Wang, Haoyu},
	month = sep,
	year = {2024},
	annote = {This paper was accepted by ASE 2024},
}

@article{slawinski_applications_2019-1,
	title = {Applications of {Graph} {Integration} to {Function} {Comparison} and {Malware} {Classification}},
	url = {https://arxiv.org/pdf/1810.04789},
	doi = {https://doi.org/10.48550/arXiv.1810.04789},
	abstract = {We classify .NET files as either benign or malicious by examining directed graphs derived from the set of functions comprising the given file. Each graph is viewed probabilistically as a Markov chain where each node represents a code block of the corresponding function, and by computing the PageRank vector (Perron vector with transport), a probability measure can be defined over the nodes of the given graph. Each graph is vectorized by computing Lebesgue antiderivatives of hand-engineered functions defined on the vertex set of the given graph against the PageRank measure. Files are subsequently vectorized by aggregating the set of vectors corresponding to the set of graphs resulting from decompiling the given file. The result is a fast, intuitive, and easy-to-compute glass-box vectorization scheme, which can be leveraged for training a standalone classifier or to augment an existing feature space. We refer to this vectorization technique as PageRank Measure Integration Vectorization (PMIV). We demonstrate the efficacy of PMIV by training a vanilla random forest on 2.5 million samples of decompiled .NET, evenly split between benign and malicious, from our in-house corpus and compare this model to a baseline model which leverages a text-only feature space. The median time needed for decompilation and scoring was 24ms.},
	author = {Slawinski, A., Michael and Wortman, Andy},
	month = jan,
	year = {2019},
}

@article{smith_there_2024-1,
	title = {There and {Back} {Again}: {A} {Netlist}'s {Tale} with {Much} {Egraphin}'},
	url = {https://arxiv.org/pdf/2404.00786},
	doi = {https://doi.org/10.48550/arXiv.2404.00786},
	abstract = {EDA toolchains are notoriously unpredictable, incomplete, and error-prone; the generally-accepted remedy has been to re-imagine EDA tasks as compilation problems. However, any compiler framework we apply must be prepared to handle the wide range of EDA tasks, including not only compilation tasks like technology mapping and optimization (the "there"\vphantom{\{}\} in our title), but also decompilation tasks like loop rerolling (the "back again"). In this paper, we advocate for equality saturation – a term rewriting framework – as the framework of choice when building hardware toolchains. Through a series of case studies, we show how the needs of EDA tasks line up conspicuously well with the features equality saturation provides.},
	author = {Smith, Henry, Gus and Sisco, D., Zachary and Techaumnuaiwit, Thanawat and Xia, Jingtao and Canumalla, Vishal and Cheung, Andrew and Tatlock, Zachary and Nandi, Chandrakana and Balkind, Jonathan},
	month = mar,
	year = {2024},
}

@article{udeshi_remaqe_2024-2,
	title = {{REMaQE}: {Reverse} {Engineering} {Math} {Equations} from {Executables}},
	url = {https://arxiv.org/pdf/2305.06902},
	doi = {https://doi.org/10.48550/arXiv.2305.06902},
	abstract = {Cybersecurity attacks on embedded devices for industrial control systems and cyber-physical systems may cause catastrophic physical damage as well as economic loss. This could be achieved by infecting device binaries with malware that modifies the physical characteristics of the system operation. Mitigating such attacks benefits from reverse engineering tools that recover sufficient semantic knowledge in terms of mathematical equations of the implemented algorithm. Conventional reverse engineering tools can decompile binaries to low-level code, but offer little semantic insight. This paper proposes the REMaQE automated framework for reverse engineering of math equations from binary executables. Improving over state-of-the-art, REMaQE handles equation parameters accessed via registers, the stack, global memory, or pointers, and can reverse engineer object-oriented implementations such as C++ classes. Using REMaQE, we discovered a bug in the Linux kernel thermal monitoring tool "tmon". To evaluate REMaQE, we generate a dataset of 25,096 binaries with math equations implemented in C and Simulink. REMaQE successfully recovers a semantically matching equation for all 25,096 binaries. REMaQE executes in 0.48 seconds on average and in up to 2 seconds for complex equations. Real-time execution enables integration in an interactive math-oriented reverse engineering workflow.},
	author = {Udeshi, Meet and Krishnamurthy, Prashanth and Pearce, Hammond and Karri, Ramesh and Khorrami, Farshad},
	month = apr,
	year = {2024},
}

@article{verbeek_formally_2025-2,
	title = {Formally {Verified} {Binary}-level {Pointer} {Analysis}},
	url = {https://arxiv.org/pdf/2501.17766},
	doi = {https://doi.org/10.48550/arXiv.2501.17766},
	abstract = {Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows "meaningful" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.},
	author = {Verbeek, Freek and Shokri, Ali and Engel, Daniel and Ravindran, Binoy},
	month = jan,
	year = {2025},
}

@article{wang_salt4decompile_2025,
	title = {{SALT4Decompile}: {Inferring} {Source}-level {Abstract} {Logic} {Tree} for {LLM}-{Based} {Binary} {Decompilation}},
	url = {https://arxiv.org/pdf/2509.14646},
	doi = {https://doi.org/10.48550/arXiv.2509.14646},
	abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose {\textbackslash}saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of {\textbackslash}saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, {\textbackslash}saltm constructs a Source-level Abstract Logic Tree ({\textbackslash}salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed {\textbackslash}salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare {\textbackslash}saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that {\textbackslash}saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4\% TCP rate on Decompile-Eval with a 10.6\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
	author = {Wang, Yongpan and Xu, Xin and Zhu, Xiaojie and Gu, Xiaodong and Shen, Beijun},
	month = sep,
	year = {2025},
	annote = {13 pages, 7 figures},
}

@article{wong_refining_2023-1,
	title = {Refining {Decompiled} {C} {Code} with {Large} {Language} {Models}},
	url = {https://arxiv.org/pdf/2310.06530},
	doi = {https://doi.org/10.48550/arXiv.2310.06530},
	abstract = {A C decompiler converts an executable into source code. The recovered C source code, once re-compiled, is expected to produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications. Despite the prosperous development of C decompilers, it is widely acknowledged that decompiler outputs are mainly used for human consumption, and are not suitable for automatic recompilation. Often, a substantial amount of manual effort is required to fix the decompiler outputs before they can be recompiled and executed properly. This paper is motived by the recent success of large language models (LLMs) in comprehending dense corpus of natural language. To alleviate the tedious, costly and often error-prone manual effort in fixing decompiler outputs, we investigate the feasibility of using LLMs to augment decompiler outputs, thus delivering recompilable decompilation. Note that different from previous efforts that focus on augmenting decompiler outputs with higher readability (e.g., recovering type/variable names), we focus on augmenting decompiler outputs with recompilability, meaning to generate code that can be recompiled into an executable with the same functionality as the original executable. We conduct a pilot study to characterize the obstacles in recompiling the outputs of the de facto commercial C decompiler – IDA-Pro. We then propose a two-step, hybrid approach to augmenting decompiler outputs with LLMs. We evaluate our approach on a set of popular C test cases, and show that our approach can deliver a high recompilation success rate to over 75\% with moderate effort, whereas none of the IDA-Pro's original outputs can be recompiled. We conclude with a discussion on the limitations of our approach and promising future research directions.},
	author = {Wong, Kin, Wai and Wang, Huaijin and Li, Zongjie and Liu, Zhibo and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
	month = jan,
	year = {2023},
}

@article{zhang_novel_2024-1,
	title = {A {Novel} {Approach} to {Malicious} {Code} {Detection} {Using} {CNN}-{BiLSTM} and {Feature} {Fusion}},
	url = {https://arxiv.org/pdf/2410.09401},
	doi = {https://doi.org/10.48550/arXiv.2410.09401},
	abstract = {With the rapid advancement of Internet technology, the threat of malware to computer systems and network security has intensified. Malware affects individual privacy and security and poses risks to critical infrastructures of enterprises and nations. The increasing quantity and complexity of malware, along with its concealment and diversity, challenge traditional detection techniques. Static detection methods struggle against variants and packed malware, while dynamic methods face high costs and risks that limit their application. Consequently, there is an urgent need for novel and efficient malware detection techniques to improve accuracy and robustness. This study first employs the minhash algorithm to convert binary files of malware into grayscale images, followed by the extraction of global and local texture features using GIST and LBP algorithms. Additionally, the study utilizes IDA Pro to decompile and extract opcode sequences, applying N-gram and tf-idf algorithms for feature vectorization. The fusion of these features enables the model to comprehensively capture the behavioral characteristics of malware. In terms of model construction, a CNN-BiLSTM fusion model is designed to simultaneously process image features and opcode sequences, enhancing classification performance. Experimental validation on multiple public datasets demonstrates that the proposed method significantly outperforms traditional detection techniques in terms of accuracy, recall, and F1 score, particularly in detecting variants and obfuscated malware with greater stability. The research presented in this paper offers new insights into the development of malware detection technologies, validating the effectiveness of feature and model fusion, and holds promising application prospects.},
	author = {Zhang, Lixia and Liu, Tianxu and Shen, Kaihui and Chen, Cheng},
	month = oct,
	year = {2024},
}

@article{zhang_cf-gkat_2025-2,
	title = {{CF}-{GKAT}: {Efficient} {Validation} of {Control}-{Flow} {Transformations}},
	url = {https://arxiv.org/pdf/2411.13220},
	doi = {https://doi.org/10.48550/arXiv.2411.13220},
	abstract = {Guarded Kleene Algebra with Tests (GKAT) provides a sound and complete framework to reason about trace equivalence between simple imperative programs. However, there are still several notable limitations. First, GKAT is completely agnostic with respect to the meaning of primitives, to keep equivalence decidable. Second, GKAT excludes non-local control flow such as goto, break, and return. To overcome these limitations, we introduce Control-Flow GKAT (CF-GKAT), a system that allows reasoning about programs that include non-local control flow as well as hardcoded values. CF-GKAT is able to soundly and completely verify trace equivalence of a larger class of programs, while preserving the nearly-linear efficiency of GKAT. This makes CF-GKAT suitable for the verification of control-flow manipulating procedures, such as decompilation and goto-elimination. To demonstrate CF-GKAT's abilities, we validated the output of several highly non-trivial program transformations, such as Erosa and Hendren's goto-elimination procedure and the output of Ghidra decompiler. CF-GKAT opens up the application of Kleene Algebra to a wider set of challenges, and provides an important verification tool that can be applied to the field of decompilation and control-flow transformation.},
	author = {Zhang, Cheng and Kappé, Tobias and Narváez, E., David and Naus, Nico},
	month = jan,
	year = {2025},
}

@article{zou_d-lift_2025,
	title = {D-{LiFT}: {Improving} {LLM}-based {Decompiler} {Backend} via {Code} {Quality}-driven {Fine}-tuning},
	url = {https://arxiv.org/pdf/2506.10125},
	doi = {https://doi.org/10.48550/arXiv.2506.10125},
	abstract = {As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3\% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2\% of all the functions produced by the native decompiler.},
	author = {Zou, Muqi and Cai, Hongyu and Wu, Hongwei and Basque, Leonahenahe, Zion and Khan, Arslan and Celik, Berkay and {Dave} and {Tian} and Bianchi, Antonio and {Ruoyu} and {Wang} and Xu, Dongyan},
	month = aug,
	year = {2025},
}

@article{tehranijamsaz_graphbinmatch_2023,
	title = {{GraphBinMatch}: {Graph}-based {Similarity} {Learning} for {Cross}-{Language} {Binary} and {Source} {Code} {Matching}},
	url = {https://arxiv.org/abs/2304.04658v1},
	abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate our approach performance on single-language binary-to-source code matching. Experimental results show that GraphBinMatch outperforms state-of-the-art significantly, with improvements as high as 15\% over the F1 score.},
	journal = {arXiv preprint arXiv:2304.04658},
	author = {TehraniJamsaz, Ali and Chen, Hanze and Jannesari, Ali},
	month = apr,
	year = {2023},
	note = {\_eprint: 2304.04658},
}

@article{shang_binmetric_2025-1,
	title = {{BinMetric}: {A} {Comprehensive} {Binary} {Analysis} {Benchmark} for {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2505.07360v1},
	abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
	journal = {arXiv preprint arXiv:2505.07360},
	author = {Shang, Xiuwei and Chen, Guoqiang and Cheng, Shaoyin and Wu, Benlong and Hu, Li and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
	month = may,
	year = {2025},
	note = {\_eprint: 2505.07360},
	annote = {23 pages, 5 figures, to be published in IJCAI 2025},
}

@article{wang_salt4decompile_2025-1,
	title = {{SALT4Decompile}: {Inferring} {Source}-level {Abstract} {Logic} {Tree} for {LLM}-{Based} {Binary} {Decompilation}},
	url = {https://arxiv.org/abs/2509.14646v1},
	abstract = {Decompilation is widely used in reverse engineering to recover high-level language code from binary executables. While recent approaches leveraging Large Language Models (LLMs) have shown promising progress, they typically treat assembly code as a linear sequence of instructions, overlooking arbitrary jump patterns and isolated data segments inherent to binary files. This limitation significantly hinders their ability to correctly infer source code semantics from assembly code. To address this limitation, we propose {\textbackslash}saltm, a novel binary decompilation method that abstracts stable logical features shared between binary and source code. The core idea of {\textbackslash}saltm is to abstract selected binary-level operations, such as specific jumps, into a high-level logic framework that better guides LLMs in semantic recovery. Given a binary function, {\textbackslash}saltm constructs a Source-level Abstract Logic Tree ({\textbackslash}salt) from assembly code to approximate the logic structure of high-level language. It then fine-tunes an LLM using the reconstructed {\textbackslash}salt to generate decompiled code. Finally, the output is refined through error correction and symbol recovery to improve readability and correctness. We compare {\textbackslash}saltm to three categories of baselines (general-purpose LLMs, commercial decompilers, and decompilation methods) using three well-known datasets (Decompile-Eval, MBPP, Exebench). Our experimental results demonstrate that {\textbackslash}saltm is highly effective in recovering the logic of the source code, significantly outperforming state-of-the-art methods (e.g., 70.4{\textbackslash}\% TCP rate on Decompile-Eval with a 10.6{\textbackslash}\% improvement). The results further validate its robustness against four commonly used obfuscation techniques. Additionally, analyses of real-world software and a user study confirm that our decompiled output offers superior assistance to human analysts in comprehending binary functions.},
	journal = {arXiv preprint arXiv:2509.14646},
	author = {Wang, Yongpan and Xu, Xin and Zhu, Xiaojie and Gu, Xiaodong and Shen, Beijun},
	month = sep,
	year = {2025},
	note = {\_eprint: 2509.14646},
	annote = {13 pages, 7 figures},
}

@article{jiang_binaryai_2024,
	title = {{BinaryAI}: {Binary} {Software} {Composition} {Analysis} via {Intelligent} {Binary} {Source} {Code} {Matching}},
	url = {https://arxiv.org/abs/2401.11161v3},
	abstract = {While third-party libraries are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis, proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@1 and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product.},
	journal = {arXiv preprint arXiv:2401.11161},
	author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
	month = jan,
	year = {2024},
	note = {\_eprint: 2401.11161},
	annote = {In Proceedings of the 46th International Conference on Software Engineering (ICSE'24)},
}

@article{zou_d-lift_2025-1,
	title = {D-{LiFT}: {Improving} {LLM}-based {Decompiler} {Backend} via {Code} {Quality}-driven {Fine}-tuning},
	url = {https://arxiv.org/abs/2506.10125v2},
	abstract = {As one of the key tools in many security tasks, decompilers reconstruct human-readable source code from binaries. Yet, despite recent advances, their outputs often suffer from syntactic and semantic errors and remain difficult to read. Recently, with the advent of large language models (LLMs), researchers began to explore the potential of LLMs to refine decompiler output. Nevertheless, our study of these approaches reveals their problems, such as introducing new errors and relying on unreliable accuracy validation. In this paper, we present D-LIFT, an enhanced decompiler-LLM pipeline with a fine-tuned LLM using code quality-aware reinforcement learning. Unlike prior work that overlooks preserving accuracy, D-LIFT adheres to a key principle for enhancing the quality of decompiled code: preserving accuracy while improving readability. Central to D-LIFT, we propose D-Score, an integrated code quality assessment system to score the decompiled source code from multiple aspects, and use it to guide reinforcement learning fine-tuning and to select the best output during inference. In line with our principle, D-Score assigns low scores to any inaccurate output and only awards higher scores for readability to code that passes the accuracy check. Our implementation, based on Ghidra and a range of LLMs, demonstrates significant improvements for the accurate decompiled code from the coreutils and util-linux projects. Compared to baseline LLMs without D-Score-driven fine-tuning, our trained LLMs produce 55.3\% more improved decompiled functions, as measured by D-Score. Overall, D-LIFT improves the quality of 68.2\% of all the functions produced by the native decompiler.},
	journal = {arXiv preprint arXiv:2506.10125},
	author = {Zou, Muqi and Cai, Hongyu and Wu, Hongwei and Basque, Zion Leonahenahe and Khan, Arslan and Celik, Berkay and {Dave} and {Tian} and Bianchi, Antonio and {Ruoyu} and {Wang} and Xu, Dongyan},
	month = jun,
	year = {2025},
	note = {\_eprint: 2506.10125},
}

@article{escalada_improving_2021-1,
	title = {Improving type information inferred by decompilers with supervised machine learning},
	url = {https://arxiv.org/abs/2101.08116v2},
	abstract = {In software reverse engineering, decompilation is the process of recovering source code from binary files. Decompilers are used when it is necessary to understand or analyze software for which the source code is not available. Although existing decompilers commonly obtain source code with the same behavior as the binaries, that source code is usually hard to interpret and certainly differs from the original code written by the programmer. Massive codebases could be used to build supervised machine learning models aimed at improving existing decompilers. In this article, we build different classification models capable of inferring the high-level type returned by functions, with significantly higher accuracy than existing decompilers. We automatically instrument C source code to allow the association of binary patterns with their corresponding high-level constructs. A dataset is created with a collection of real open-source applications plus a huge number of synthetic programs. Our system is able to predict function return types with a 79.1\% F1-measure, whereas the best decompiler obtains a 30\% F1-measure. Moreover, we document the binary patterns used by our classifier to allow their addition in the implementation of existing decompilers.},
	journal = {arXiv preprint arXiv:2101.08116},
	author = {Escalada, Javier and Scully, Ted and Ortin, Francisco},
	month = jan,
	year = {2021},
	note = {\_eprint: 2101.08116},
}

@article{armengol-estape_learning_2021,
	title = {Learning {C} to x86 {Translation}: {An} {Experiment} in {Neural} {Compilation}},
	url = {https://arxiv.org/abs/2108.07639v2},
	abstract = {Deep learning has had a significant impact on many fields. Recently, code-to-code neural models have been used in code translation, code refinement and decompilation. However, the question of whether these models can automate compilation has yet to be investigated. In this work, we explore neural compilation, building and evaluating Transformer models that learn how to produce x86 assembler from C code. Although preliminary results are relatively weak, we make our data, models and code publicly available to encourage further research in this area.},
	journal = {arXiv preprint arXiv:2108.07639},
	author = {Armengol-Estapé, Jordi and O'Boyle, Michael F. P.},
	month = aug,
	year = {2021},
	note = {\_eprint: 2108.07639},
	annote = {Journal reference: Armengol-Estapé, J. and O'Boyle, M. Learning C to x86 translation: An experiment in neural compilation. In Advances in Programming Languages and Neurosymbolic Systems Workshop, 2021. URL \{https://openreview.net/forum?id=444ug\_EYXet\}},
	annote = {Published in AIPLANS 2021},
}

@article{liu_proving_2021-1,
	title = {Proving {LTL} {Properties} of {Bitvector} {Programs} and {Decompiled} {Binaries} ({Extended})},
	url = {https://arxiv.org/abs/2105.05159v2},
	abstract = {There is increasing interest in applying verification tools to programs that have bitvector operations (eg., binaries). SMT solvers, which serve as a foundation for these tools, have thus increased support for bitvector reasoning through bit-blasting and linear arithmetic approximations. In this paper we show that similar linear arithmetic approximation of bitvector operations can be done at the source level through transformations. Specifically, we introduce new paths that over-approximate bitvector operations with linear conditions/constraints, increasing branching but allowing us to better exploit the well-developed integer reasoning and interpolation of verification tools. We show that, for reachability of bitvector programs, increased branching incurs negligible overhead yet, when combined with integer interpolation optimizations, enables more programs to be verified. We further show this exploitation of integer interpolation in the common case also enables competitive termination verification of bitvector programs and leads to the first effective technique for LTL verification of bitvector programs. Finally, we provide an in-depth case study of decompiled ("lifted") binary programs, which emulate X86 execution through frequent use of bitvector operations. We present a new tool DarkSea, the first tool capable of verifying reachability, termination, and LTL of lifted binaries.},
	journal = {arXiv preprint arXiv:2105.05159},
	author = {Liu, Yuandong Cyrus and Pang, Chengbin and Dietsch, Daniel and Koskinen, Eric and Le, Ton-Chanh and Portokalidis, Georgios and Xu, Jun},
	month = may,
	year = {2021},
	note = {\_eprint: 2105.05159},
	annote = {39 pages(including Appendix), 10 tables, 4 Postscript figures, accepted to APLAS 2021},
}

@article{ahmed_learning_2021-1,
	title = {Learning to {Find} {Usages} of {Library} {Functions} in {Optimized} {Binaries}},
	url = {https://arxiv.org/abs/2103.05221v2},
	doi = {10.1109/TSE.2021.3106572},
	abstract = {Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, "natural" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to "decompile" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization.},
	journal = {arXiv preprint arXiv:2103.05221},
	author = {Ahmed, Toufique and Devanbu, Premkumar and Sawant, Anand Ashok},
	month = mar,
	year = {2021},
	note = {\_eprint: 2103.05221},
	annote = {Journal reference: Transactions on Software Engineering (2021)},
}

@inproceedings{jiang_binaryai_2024-1,
	title = {{BinaryAI}: {Binary} {Software} {Composition} {Analysis} via {Intelligent} {Binary} {Source} {Code} {Matching}},
	issn = {1558-1225},
	doi = {10.1145/3597503.3639100},
	abstract = {While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@l and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product Black Duck. E-https://www.binaryai.net},
	booktitle = {2024 {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
	month = apr,
	year = {2024},
	keywords = {Codes, Redundancy, Semantics, Software, Software Composition Analysis, Source coding, Static Binary Analysis, Syntactics, Transformers},
	pages = {2771--2783},
}

@inproceedings{tehranijamsaz_graphbinmatch_2024,
	title = {{GraphBinMatch}: {Graph}-{Based} {Similarity} {Learning} for {Cross}-{Language} {Binary} and {Source} {Code} {Matching}},
	doi = {10.1109/IPDPSW63119.2024.00103},
	abstract = {Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there exist methods that try to match source code with binary code to accelerate the reverse engineering process, most of them are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages. In this paper, we address the problem of cross-language binary source code matching. We propose GraphBinMatch, an approach based on a graph neural network that learns the similarity between binary and source codes. We evaluate GraphBinMatch on several tasks, such as cross-language binary-to-source code matching and cross-language source-to-source matching. We also evaluate the performance of our approach on single-language binary-to-source code matching. Experimental results show that GraphBinMatch significantly outperforms state-of-the-art, with improvements as high as 15\% over the F1 score.},
	booktitle = {2024 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {TehraniJamsaz, Ali and Chen, Hanze and Jannesari, Ali},
	month = may,
	year = {2024},
	keywords = {Binary codes, binary-source matching, code similarity, Computer languages, cross-language, Distributed processing, Graph neural networks, Programming, Reverse engineering, Source coding},
	pages = {506--515},
}

@article{park_object-aware_2025,
	title = {Object-{Aware} {Impedance} {Control} for {Human}–{Robot} {Collaborative} {Task} {With} {Online} {Object} {Parameter} {Estimation}},
	volume = {22},
	issn = {1558-3783},
	doi = {10.1109/TASE.2024.3477471},
	abstract = {Physical human-robot interactions (pHRIs) can improve robot autonomy and reduce physical demands on humans. In this paper, we consider a collaborative task with a considerably long object and no prior knowledge of the object’s parameters. An integrated control framework with an online object parameter estimator and a Cartesian object-aware impedance controller is proposed to realize complicated scenarios. During the transportation task, the object parameters are estimated online while a robot and human keep lifting an object. The perturbation motion is incorporated into the null space of the desired trajectory to enhance the estimator precision. An object-aware impedance controller is designed by incorporating the real-time estimation results to effectively transmit the intended human motion to the robot through the object. Experimental demonstrations of collaborative tasks, including object transportation and assembly, are implemented to show the effectiveness of our proposed method. The proposed controller was also compared to a conventional impedance controller through subjective testing and found to be more sensitive, requiring less human effort. Note to Practitioners—This research was motivated by the need to facilitate collaboration between humans and robots in handling heavy or considerable long objects, which can be challenging for a single operator. This paper proposes a physical Human-Robot Interaction (pHRI) approach that enables physical interaction between the human and the robot through the object without additional sensors such as camera or human-machine interfaces. To achieve collaborative task, the separation between the intended human motion and the object dynamics is essential. Most of real-world situations involve uncertain or unknown objects, making it challenging to assume prior knowledge of the target object’s properties. Therefore, this research introduces a real-time approach for estimating the dynamic parameters of unknown objects during collaboration, without requiring additional operational time. Consequently, the design of object-aware impedance controller can be achieved by real-time incorporation of object dynamics. Collaborative transportation and assembly task is demonstrated with whole-body controlled mobile manipulator and 1.5m long object. In future research, we will focus on enhancing the estimation precision through the use of physical informed neural network methods.},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Park, Jinseong and Shin, Yong-Sik and Kim, Sanghyun},
	year = {2025},
	keywords = {Cartesian impedance control, Collaboration, Dynamics, Estimation, Impedance, mobile manipulation, object-aware control, online object parameter estimation, Parameter estimation, Physical human-robot interaction, Robot kinematics, Robot sensing systems, Robots, Sensors, Transportation},
	pages = {8081--8094},
}

@article{jose_robust_2020,
	title = {Robust {Classification} of {Intramuscular} {EMG} {Signals} to {Aid} the {Diagnosis} of {Neuromuscular} {Disorders}},
	volume = {1},
	issn = {2644-1276},
	doi = {10.1109/OJEMB.2020.3017130},
	abstract = {Goal: This article presents the design and validation of an accurate automatic diagnostic system to classify intramuscular EMG (iEMG) signals into healthy, myopathy, or neuropathy categories to aid the diagnosis of neuromuscular diseases. Methods: First, an iEMG signal is decimated to produce a set of “disjoint” downsampled signals, which are decomposed by the lifting wavelet transform (LWT). The Higuchi's fractal dimensions (FDs) of LWT coefficients in the subbands are computed. The FDs of LWT subband coefficients are fused with one-dimensional local binary pattern derived from each downsampled signal. Next, a multilayer perceptron neural network (MLPNN) determines the class labels of downsampled signals. Finally, the sequence of class labels is fed to the Boyer-Moore majority vote (BMMV) algorithm, which assigns a class to every iEMG signal. Results: The MLPNN-BMMV classifier was experimented with 250 iEMG signals belonging to three categories. The performance of the classifier was validated in comparison with state-of-the-art approaches. The MLPNN-BMMV has resulted in impressive performance measures (\%) using a 10-fold cross-validation—accuracy = 99.87{\textbackslash}pm 0.25, sensitivity (normal) = 99.97{\textbackslash}pm 0.13, sensitivity (myopathy) = 99.68{\textbackslash}pm 0.95, sensitivity (neuropathy) = 99.76{\textbackslash}pm 0.66, specificity (normal) = 99.72{\textbackslash}pm 0.61, specificity (myopathy) = 99.98{\textbackslash}pm 0.10, and specificity (neuropathy) = 99.96{\textbackslash}pm 0.14—surpassing the existing approaches. Conclusions: A future research direction is to validate the classifier performance with diverse iEMG datasets, which would lead to the design of an affordable real-time expert system for neuromuscular disorder diagnosis.},
	journal = {IEEE Open Journal of Engineering in Medicine and Biology},
	author = {Jose, Shobha and George, S. Thomas and Subathra, M. S. P. and Handiru, Vikram Shenoy and Jeevanandam, Poornaselvan Kittu and Amato, Umberto and Suviseshamuthu, Easter Selvan},
	year = {2020},
	keywords = {Classification algorithms, Electromyography, Feature extraction, Fractal dimension, intramuscular electromyography, lifting wavelet transform, local binary pattern, majority vote, multilayer perceptron neural network, Neuromuscular, neuromuscular disorders, Sensitivity, Wavelet transforms},
	pages = {235--242},
}

@article{lee_overflow-detectable_2024,
	title = {Overflow-{Detectable} {Floating}-{Point} {Fully} {Homomorphic} {Encryption}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3351738},
	abstract = {A floating-point fully homomorphic encryption (FPFHE) is proposed, which is based on torus fully homomorphic encryption equipped with programmable bootstrapping. Specifically, FPFHE for 32-bit and 64-bit floating-point messages are implemented, the latter showing the state-of-the-art precision among FHEs. Also, a ciphertext is constructed for checking if an overflow has occurred or not while evaluating arithmetic circuits with the proposed FPFHE, which is useful when the message space or arithmetic circuit is too complex to estimate a bound of outputs such as some deep learning applications. Also, homomorphic algorithms, which are crucial components of overflow detectable (OD)-FPFHE, are constructed. First, a state-of-the-art bootstrapping method of TFHE is extended to bootstrap larger messages by using NTT-friendly integer modulus. Second, a subgaussian analysis method is proposed without assuming independent heuristic on AP/GINX-bootstrapping even if the deterministic gadget decomposition is used. Third, the blind rotation algorithm of TFHE is modified such that any secret key having finite non-zero values can be used while keeping the number of NTT operations the same as when the binary key is used. Fourth, various homomorphic algorithms are proposed such as evaluating min and max, lifting a constant message to the monomial exponent, counting the number of consecutive zeros from the most significant in the fraction, and performing carryover after homomorphic operation of floating-point numbers. Finally, 32-bit and 64-bit OD-FPFHEs are implemented and simulation results are provided to confirm that they work well even for extreme cases. Also, it is verified that homomorphic overflow detection is well-operated.},
	journal = {IEEE Access},
	author = {Lee, Seunghwan and Shin, Dong-Joon},
	year = {2024},
	keywords = {Deep learning, Encoding, Error analysis, Floating-point arithmetic, Fully homomorphic encryption, Gaussian processes, Homomorphic encryption, homomorphic floating-point arithmetic, homomorphic overflow detection, Random variables, subgaussian error analysis, Transforms},
	pages = {6160--6180},
}

@article{su_disco_2025-1,
	address = {New York, NY, USA},
	title = {{DiSCo}: {Towards} {Decompiling} {EVM} {Bytecode} to {Source} {Code} using {Large} {Language} {Models}},
	volume = {2},
	url = {https://doi.org/10.1145/3729373},
	doi = {10.1145/3729373},
	abstract = {Understanding the Ethereum smart contract bytecode is essential for ensuring cryptoeconomics security. However, existing decompilers primarily convert bytecode into pseudocode, which is not easily comprehensible for general users, potentially leading to misunderstanding of contract behavior and increased vulnerability to scams or exploits. In this paper, we propose DiSCo, the first LLMs-based EVM decompilation pipeline, which aims to enable LLMs to understand the opaque bytecode and lift it into smart contract code. DiSCo introduces three core technologies. First, a logic-invariant intermediate representation is proposed to reproject the low-level bytecode into high-level abstracted units. The second technique involves semantic enhancement based on a novel type-aware graph model to infer stripped variables during compilation, enhancing the lifting effect. The third technology is a flexible method incorporating code specifications to construct LLM-comprehensible prompts for source code generation. Extensive experiments illustrate that our generated code guarantees a high compilability rate at 75\%, with differential fuzzing pass rate averaging at 50\%. Manual validation results further indicate that the generated solidity contracts significantly outperforms baseline methods in tasks such as code comprehension and attack reproduction.},
	number = {FSE},
	journal = {Proc. ACM Softw. Eng.},
	publisher = {Association for Computing Machinery},
	author = {Su, Xing and Liang, Hanzhong and Wu, Hao and Niu, Ben and Xu, Fengyuan and Zhong, Sheng},
	month = jun,
	year = {2025},
	keywords = {Decompilation, EVM bytecode, Large Language Models, Smart Contract, Source Code Generation},
}

@inproceedings{jiang_binaryai_2024-2,
	address = {New York, NY, USA},
	series = {{ICSE} '24},
	title = {{BinaryAI}: {Binary} {Software} {Composition} {Analysis} via {Intelligent} {Binary} {Source} {Code} {Matching}},
	isbn = {979-8-4007-0217-4},
	url = {https://doi.org/10.1145/3597503.3639100},
	doi = {10.1145/3597503.3639100},
	abstract = {While third-party libraries (TPLs) are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. Software composition analysis (SCA), proposed to identify reused TPLs for reducing such risks, has become an essential procedure within modern DevSecOps. As one of the mainstream SCA techniques, binary-to-source SCA identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. The existing binary-to-source SCA techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale TPL dataset, leading to inevitable false positives and compromised recall. To mitigate these limitations, we introduce BinaryAI, a novel binary-to-source SCA technique with two-phase binary source code matching to capture both syntactic and semantic code features. First, BinaryAI trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. Then by applying the link-time locality to facilitate function matching, BinaryAI detects the reused TPLs based on the ratio of matched source functions. Our experimental results demonstrate the superior performance of BinaryAI in terms of binary source code matching and the downstream SCA task. Specifically, our embedding model outperforms the state-of-the-art model CodeCMR, i.e., achieving 22.54\% recall@1 and 0.34 MRR compared with 10.75\% and 0.17 respectively. Additionally, BinaryAI outperforms all existing binary-to-source SCA tools in TPL detection, increasing the precision from 73.36\% to 85.84\% and recall from 59.81\% to 64.98\% compared with the well-recognized commercial SCA product Black Duck.https://www.binaryai.net},
	booktitle = {Proceedings of the {IEEE}/{ACM} 46th {International} {Conference} on {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
	year = {2024},
	keywords = {software composition analysis, static binary analysis},
}

@inproceedings{zhao_deepinfer_2023-1,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE} 2023},
	title = {{DeepInfer}: {Deep} {Type} {Inference} from {Smart} {Contract} {Bytecode}},
	isbn = {979-8-4007-0327-0},
	url = {https://doi.org/10.1145/3611643.3616343},
	doi = {10.1145/3611643.3616343},
	abstract = {Smart contracts play an increasingly important role in Ethereum platform. It provides various functions implementing numerous services, whose bytecode runs on Ethereum Virtual Machine. To use services by invoking corresponding functions, the callers need to know the function signatures. Moreover, such signatures provide crucial information for many downstream applications, e.g., identifying smart contracts, fuzzing, detecting vulnerabilities, etc. However, it is challenging to infer function signatures from the bytecode due to a lack of type information. Existing work solving this problem depended heavily on limited databases or hard-coded heuristic patterns. However, these approaches are hard to be adapted to semantic differences in distinct languages and various compiler versions when developing smart contracts. In this paper, we propose a novel framework DeepInfer that first leverages deep learning techniques to automatically infer function signatures and returns. The novelties of DeepInfer are: 1) DeepInfer lifts the bytecode into the Intermediate Representation (IR) to preserve code semantics; 2) DeepInfer extracts the type-related knowledge (e.g., critical data flows, constant values, and control flow graphs) from the IR to recover function signatures and returns. We conduct experiments on Solidity and Vyper smart contracts and the results show that DeepInfer performs faster and more accurate than existing tools, while being immune to changes in different languages and various compiler versions.},
	booktitle = {Proceedings of the 31st {ACM} {Joint} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Zhao, Kunsong and Li, Zihao and Li, Jianfeng and Ye, He and Luo, Xiapu and Chen, Ting},
	year = {2023},
	keywords = {Deep Learning, Smart Contract, Type Inference},
	pages = {745--757},
}

@inproceedings{chen_clearagent_2025-1,
	address = {New York, NY, USA},
	series = {{LMPL} '25},
	title = {{ClearAgent}: {Agentic} {Binary} {Analysis} for {Effective} {Vulnerability} {Detection}},
	isbn = {979-8-4007-2148-9},
	url = {https://doi.org/10.1145/3759425.3763397},
	doi = {10.1145/3759425.3763397},
	abstract = {Statically detecting vulnerabilities at the binary level is crucial for the security of Commercial-Off-The-Shelf (COTS) software when source code is not available. However, traditional methods suffer from the inherent limitations of binary translation and static analysis, which hinder their scalability for complex real-world binaries. Recent efforts that leverage Large Language Models (LLMs) for vulnerability detection are still limited by possible hallucination, inaccurate code property retrieval, and insufficient guidance. In this paper, we propose a new agentic binary analysis framework ClearAgent, which features a novel binary interface that provides both LLM-friendly and analyzer-friendly tools to facilitate effective understanding of binary code semantics with rich context. ClearAgent works by automatically interacting with the interface and iteratively exploring for buggy binary code. For candidate bug reports, ClearAgent further tries to verify the existence of the vulnerability by constructing concrete inputs that can trigger the buggy locations.},
	booktitle = {Proceedings of the 1st {ACM} {SIGPLAN} {International} {Workshop} on {Language} {Models} and {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Xiang and Zhou, Anshunkang and Ye, Chengfeng and Zhang, Charles},
	year = {2025},
	keywords = {Agent, Binary Analysis, Vulnerability Detection},
	pages = {130--137},
}

@inproceedings{zhou_regraph_2025-1,
	address = {New York, NY, USA},
	series = {{ISSTA} {Companion} '25},
	title = {{ReGraph}: {A} {Tool} for {Binary} {Similarity} {Identification}},
	isbn = {979-8-4007-1474-0},
	url = {https://doi.org/10.1145/3713081.3731728},
	doi = {10.1145/3713081.3731728},
	abstract = {Binary Code Similarity Detection (BCSD) is not only essential for security tasks such as vulnerability identification but also for code copying detection, yet it remains challenging due to binary stripping and diverse compilation environments. Existing methods tend to adopt increasingly complex neural networks for better accuracy performance. The computation time increases with the complexity. Even with powerful GPUs, the treatment of large-scale software becomes time-consuming. To address these issues, we present a framework called ReGraph to efficiently compare binary code functions across architectures and optimization levels. Our evaluation with public datasets highlights that ReGraph exhibits a significant speed advantage, performing 700 times faster than Natural Language Processing (NLP)-based methods while maintaining comparable accuracy results with respect to the state-of-the-art models.},
	booktitle = {Proceedings of the 34th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Li and Dacier, Marc and Konstantinou, Charalambos},
	year = {2025},
	keywords = {binary code re-optimization, binary code similarity detection, code lifting, code property graph, graph neural network},
	pages = {6--10},
}

@inproceedings{verbeek_formally_2022-1,
	address = {New York, NY, USA},
	series = {{PLDI} 2022},
	title = {Formally verified lifting of {C}-compiled x86-64 binaries},
	isbn = {978-1-4503-9265-5},
	url = {https://doi.org/10.1145/3519939.3523702},
	doi = {10.1145/3519939.3523702},
	abstract = {Lifting binaries to a higher-level representation is an essential step for decompilation, binary verification, patching and security analysis. In this paper, we present the first approach to provably overapproximative x86-64 binary lifting. A stripped binary is verified for certain sanity properties such as return address integrity and calling convention adherence. Establishing these properties allows the binary to be lifted to a representation that contains an overapproximation of all possible execution paths of the binary. The lifted representation contains disassembled instructions, reconstructed control flow, invariants and proof obligations that are sufficient to prove the sanity properties as well as correctness of the lifted representation. We apply this approach to Linux Foundation and Intel’s Xen Hypervisor covering about 400K instructions. This demonstrates our approach is the first approach to provably overapproximative binary lifting scalable to commercial off-the-shelf systems. The lifted representation is exportable to the Isabelle/HOL theorem prover, allowing formal verification of its correctness. If our technique succeeds and the proofs obligations are proven true, then – under the generated assumptions – the lifted representation is correct.},
	booktitle = {Proceedings of the 43rd {ACM} {SIGPLAN} {International} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {Association for Computing Machinery},
	author = {Verbeek, Freek and Bockenek, Joshua and Fu, Zhoulai and Ravindran, Binoy},
	year = {2022},
	keywords = {Binary Analysis, Disassembly, Formal Verification},
	pages = {934--949},
}

@article{alrabaee_survey_2022-1,
	address = {New York, NY, USA},
	title = {A {Survey} of {Binary} {Code} {Fingerprinting} {Approaches}: {Taxonomy}, {Methodologies}, and {Features}},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3486860},
	doi = {10.1145/3486860},
	abstract = {Binary code fingerprinting is crucial in many security applications. Examples include malware detection, software infringement, vulnerability analysis, and digital forensics. It is also useful for security researchers and reverse engineers since it enables high fidelity reasoning about the binary code such as revealing the functionality, authorship, libraries used, and vulnerabilities. Numerous studies have investigated binary code with the goal of extracting fingerprints that can illuminate the semantics of a target application. However, extracting fingerprints is a challenging task since a substantial amount of significant information will be lost during compilation, notably, variable and function naming, the original data and control flow structures, comments, semantic information, and the code layout. This article provides the first systematic review of existing binary code fingerprinting approaches and the contexts in which they are used. In addition, it discusses the applications that rely on binary code fingerprints, the information that can be captured during the fingerprinting process, and the approaches used and their implementations. It also addresses limitations and open questions related to the fingerprinting process and proposes future directions.},
	number = {1},
	journal = {ACM Comput. Surv.},
	publisher = {Association for Computing Machinery},
	author = {Alrabaee, Saed and Debbabi, Mourad and Wang, Lingyu},
	month = jan,
	year = {2022},
	keywords = {Binary code analysis, reverse engineering, software security},
}

@article{lee_overflow-detectable_2024-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Overflow-{Detectable} {Floating}-{Point} {Fully} {Homomorphic} {Encryption}},
	volume = {12},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2024.3351738},
	abstract = {A floating-point fully homomorphic encryption (FPFHE) is proposed, which is based on torus fully homomorphic encryption equipped with programmable bootstrapping. Specifically, FPFHE for 32-bit and 64-bit floating-point messages are implemented, the latter showing the state-of-the-art precision among FHEs. Also, a ciphertext is constructed for checking if an overflow has occurred or not while evaluating arithmetic circuits with the proposed FPFHE, which is useful when the message space or arithmetic circuit is too complex to estimate a bound of outputs such as some deep learning applications. Also, homomorphic algorithms, which are crucial components of overflow detectable (OD)-FPFHE, are constructed. First, a state-of-the-art bootstrapping method of TFHE is extended to bootstrap larger messages by using NTT-friendly integer modulus. Second, a subgaussian analysis method is proposed without assuming independent heuristic on AP/GINX-bootstrapping even if the deterministic gadget decomposition is used. Third, the blind rotation algorithm of TFHE is modified such that any secret key having finite non-zero values can be used while keeping the number of NTT operations the same as when the binary key is used. Fourth, various homomorphic algorithms are proposed such as evaluating min and max, lifting a constant message to the monomial exponent, counting the number of consecutive zeros from the most significant in the fraction, and performing carryover after homomorphic operation of floating-point numbers. Finally, 32-bit and 64-bit OD-FPFHEs are implemented and simulation results are provided to confirm that they work well even for extreme cases. Also, it is verified that homomorphic overflow detection is well-operated.},
	language = {English},
	journal = {IEEE ACCESS},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Lee, Seunghwan and Shin, Dong-Joon},
	year = {2024},
	note = {Type: Article},
	keywords = {Fully homomorphic encryption, homomorphic floating-point arithmetic, homomorphic overflow detection, subgaussian error analysis},
	pages = {6160--6180},
}

@article{jose_robust_2020-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Robust {Classification} of {Intramuscular} {EMG} {Signals} to {Aid} the {Diagnosis} of {Neuromuscular} {Disorders}},
	volume = {1},
	doi = {10.1109/OJEMB.2020.3017130},
	abstract = {Goal: This article presents the design and validation of an accurate automatic diagnostic system to classify intramuscular EMG (iEMG) signals into healthy, myopathy, or neuropathy categories to aid the diagnosis of neuromuscular diseases. Methods: First, an iEMG signal is decimated to produce a set of “disjoint” downsampled signals, which are decomposed by the lifting wavelet transform (LWT). The Higuchi's fractal dimensions (FDs) of LWT coefficients in the subbands are computed. The FDs of LWT subband coefficients are fused with one-dimensional local binary pattern derived from each downsampled signal. Next, a multilayer perceptron neural network (MLPNN) determines the class labels of downsampled signals. Finally, the sequence of class labels is fed to the Boyer-Moore majority vote (BMMV) algorithm, which assigns a class to every iEMG signal. Results: The MLPNN-BMMV classifier was experimented with 250 iEMG signals belonging to three categories. The performance of the classifier was validated in comparison with state-of-the-art approaches. The MLPNN-BMMV has resulted in impressive performance measures (\%) using a 10-fold cross-validation-accuracy = 99.87 +/- 0.25, sensitivity (normal) = 99.97 +/- 0.13, sensitivity (myopathy) = 99.68 +/- 0.95, sensitivity (neuropathy) = 99.76 +/- 0.66, specificity (normal) = 99.72 +/- 0.61, specificity (myopathy) = 99.98 +/- 0.10, and specificity (neuropathy) = 99.96 +/- 0.14-surpassing the existing approaches. Conclusions: A future research direction is to validate the classifier performance with diverse iEMG datasets, which would lead to the design of an affordable real-time expert system for neuromuscular disorder diagnosis.},
	language = {English},
	journal = {IEEE OPEN JOURNAL OF ENGINEERING IN MEDICINE AND BIOLOGY},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Jose, Shobha and George, S. Thomas and Subathra, M. S. P. and Handiru, Vikram Shenoy and Jeevanandam, Poornaselvan Kittu and Amato, Umberto and Suviseshamuthu, Easter Selvan},
	year = {2020},
	note = {Type: Article},
	keywords = {Fractal dimension, intramuscular electromyography, lifting wavelet transform, local binary pattern, majority vote, multilayer perceptron neural network, neuromuscular disorders},
	pages = {235--242},
}

@article{park_object-aware_2025-1,
	address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
	title = {Object-{Aware} {Impedance} {Control} for {Human}-{Robot} {Collaborative} {Task} {With} {Online} {Object} {Parameter} {Estimation}},
	volume = {22},
	issn = {1545-5955},
	doi = {10.1109/TASE.2024.3477471},
	abstract = {Physical human-robot interactions (pHRIs) can improve robot autonomy and reduce physical demands on humans. In this paper, we consider a collaborative task with a considerably long object and no prior knowledge of the object's parameters. An integrated control framework with an online object parameter estimator and a Cartesian object-aware impedance controller is proposed to realize complicated scenarios. During the transportation task, the object parameters are estimated online while a robot and human keep lifting an object. The perturbation motion is incorporated into the null space of the desired trajectory to enhance the estimator precision. An object-aware impedance controller is designed by incorporating the real-time estimation results to effectively transmit the intended human motion to the robot through the object. Experimental demonstrations of collaborative tasks, including object transportation and assembly, are implemented to show the effectiveness of our proposed method. The proposed controller was also compared to a conventional impedance controller through subjective testing and found to be more sensitive, requiring less human effort. Note to Practitioners-This research was motivated by the need to facilitate collaboration between humans and robots in handling heavy or considerable long objects, which can be challenging for a single operator. This paper proposes a physical Human-Robot Interaction (pHRI) approach that enables physical interaction between the human and the robot through the object without additional sensors such as camera or human-machine interfaces. To achieve collaborative task, the separation between the intended human motion and the object dynamics is essential. Most of real-world situations involve uncertain or unknown objects, making it challenging to assume prior knowledge of the target object's properties. Therefore, this research introduces a real-time approach for estimating the dynamic parameters of unknown objects during collaboration, without requiring additional operational time. Consequently, the design of object-aware impedance controller can be achieved by real-time incorporation of object dynamics. Collaborative transportation and assembly task is demonstrated with whole-body controlled mobile manipulator and 1.5m long object. In future research, we will focus on enhancing the estimation precision through the use of physical informed neural network methods.},
	language = {English},
	journal = {IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING},
	publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
	author = {Park, Jinseong and Shin, Yong-Sik and Kim, Sanghyun},
	year = {2025},
	note = {Type: Article},
	keywords = {Cartesian impedance control, mobile manipulation, object-aware control, online object parameter estimation, Physical human-robot interaction},
	pages = {8081--8094},
}
